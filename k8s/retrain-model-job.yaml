apiVersion: batch/v1
kind: Job
metadata:
  name: retrain-business-model
  namespace: neural-hive
spec:
  backoffLimit: 1
  template:
    spec:
      containers:
      - name: trainer
        image: python:3.11-slim
        command: ["python3", "-c"]
        args:
        - |
          import sys, subprocess, os
          from datetime import datetime

          # Install dependencies
          subprocess.check_call([sys.executable, "-m", "pip", "install", "pymongo", "scikit-learn", "pandas", "numpy", "mlflow", "-q"])

          import pymongo
          import numpy as np
          import pandas as pd
          from sklearn.ensemble import RandomForestClassifier
          from sklearn.model_selection import train_test_split
          from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report
          import mlflow
          import mlflow.sklearn

          # Configuration
          MONGO_URI = "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
          MLFLOW_URI = "http://mlflow.mlflow.svc.cluster.local:5000"
          SPECIALIST_TYPE = "business"

          print(f"=== Retraining {SPECIALIST_TYPE} model with balanced data ===")

          # Connect to MongoDB
          client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
          db = client.neural_hive
          opinions_col = db.specialist_opinions
          feedback_col = db.specialist_feedback

          # Get feedbacks for this specialist
          feedbacks = list(feedback_col.find({"specialist_type": SPECIALIST_TYPE}))
          print(f"Found {len(feedbacks)} feedbacks for {SPECIALIST_TYPE}")

          # Get opinions with feedback
          opinion_ids = [f["opinion_id"] for f in feedbacks]
          opinions = list(opinions_col.find({"opinion_id": {"$in": opinion_ids}}))
          opinions_dict = {o["opinion_id"]: o for o in opinions}
          print(f"Found {len(opinions_dict)} matching opinions")

          # Build training dataset from feedbacks
          training_data = []
          for fb in feedbacks:
              opinion_id = fb["opinion_id"]
              opinion = opinions_dict.get(opinion_id)
              if not opinion:
                  continue

              opinion_data = opinion.get("opinion", {})
              conf = opinion_data.get("confidence_score", 0.5)
              risk = opinion_data.get("risk_score", 0.5)
              rec = opinion_data.get("recommendation", "review_required")

              # Features
              features = {
                  "confidence_score": conf,
                  "risk_score": risk,
                  "opinion_rec_approve": 1 if rec == "approve" else 0,
                  "opinion_rec_reject": 1 if rec == "reject" else 0,
                  "opinion_rec_review": 1 if rec == "review_required" else 0,
                  "opinion_rec_conditional": 1 if rec == "conditional" else 0,
                  "human_rating": fb.get("human_rating", 0.5),
              }

              # Label from human feedback (1=approve, 0=other)
              human_rec = fb.get("human_recommendation", "review_required")
              label = 1 if human_rec == "approve" else (2 if human_rec == "reject" else 0)

              training_data.append({**features, "label": label})

          print(f"Training dataset size: {len(training_data)}")

          # Check class distribution
          df = pd.DataFrame(training_data)
          label_dist = df["label"].value_counts().sort_index()
          print(f"Label distribution: {dict(label_dist)}")

          # Map labels: 0=review, 1=approve, 2=reject
          label_names = {0: "review", 1: "approve", 2: "reject"}

          # For binary classification: approve vs not-approve
          df_y_binary = (df["label"] == 1).astype(int)
          print(f"Binary distribution: approve={df_y_binary.sum()}, not_approve={len(df_y_binary) - df_y_binary.sum()}")

          if len(df) < 100:
              print("ERROR: Not enough training data!")
              sys.exit(1)

          # Features
          X = df[["confidence_score", "risk_score", "opinion_rec_approve", "opinion_rec_reject", "opinion_rec_review", "opinion_rec_conditional", "human_rating"]]
          y = df_y_binary

          # Split data
          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

          print(f"Train size: {len(X_train)}, Test size: {len(X_test)}")
          print(f"Positive ratio in train: {y_train.mean():.2f}")

          # Configure MLflow
          mlflow.set_tracking_uri(MLFLOW_URI)
          mlflow.set_experiment(f"specialist-{SPECIALIST_TYPE}-retraining")

          with mlflow.start_run(run_name=f"{SPECIALIST_TYPE}_retrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
              print(f"MLflow Run ID: {mlflow.active_run().info.run_id}")

              # Log parameters
              mlflow.log_param("specialist_type", SPECIALIST_TYPE)
              mlflow.log_param("training_samples", len(X_train))
              mlflow.log_param("test_samples", len(X_test))
              mlflow.log_param("feedback_count", len(feedbacks))
              mlflow.log_param("model_type", "random_forest")
              mlflow.log_param("positive_ratio", float(y_train.mean()))

              # Train model
              print("Training Random Forest...")
              model = RandomForestClassifier(
                  n_estimators=100,
                  max_depth=10,
                  min_samples_split=5,
                  random_state=42,
                  class_weight="balanced"
              )
              model.fit(X_train, y_train)

              # Predictions
              y_pred = model.predict(X_test)
              y_proba = model.predict_proba(X_test)[:, 1]

              # Metrics
              accuracy = accuracy_score(y_test, y_pred)
              precision = precision_score(y_test, y_pred, zero_division=0)
              recall = recall_score(y_test, y_pred, zero_division=0)
              f1 = f1_score(y_test, y_pred, zero_division=0)

              print(f"\\n=== Metrics ===")
              print(f"Accuracy:  {accuracy:.3f}")
              print(f"Precision: {precision:.3f}")
              print(f"Recall:    {recall:.3f}")
              print(f"F1 Score:  {f1:.3f}")

              # Classification report
              print(f"\\nClassification Report:")
              print(classification_report(y_test, y_pred, zero_division=0))

              # Log metrics
              mlflow.log_metric("accuracy", accuracy)
              mlflow.log_metric("precision", precision)
              mlflow.log_metric("recall", recall)
              mlflow.log_metric("f1", f1)

              # Feature importances
              importances = dict(zip(X.columns, model.feature_importances_.tolist()))
              mlflow.log_dict(importances, "feature_importances.json")
              print(f"\\nFeature Importances: {importances}")

              # Log model (without registration to avoid 404)
              mlflow.sklearn.log_model(model, "model")

              print(f"\\n=== Training Complete ===")
              print(f"Run ID: {mlflow.active_run().info.run_id}")
              print(f"Model artifacts saved in MLflow")

          print("\\n=== SUCCESS ===")
        env:
        - name: MONGODB_URI
          value: "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow.mlflow.svc.cluster.local:5000"
      restartPolicy: Never
