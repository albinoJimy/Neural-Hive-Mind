apiVersion: batch/v1
kind: Job
metadata:
  name: feature-engineering-retrain-v2
  namespace: neural-hive
spec:
  backoffLimit: 1
  template:
    spec:
      containers:
      - name: trainer
        image: python:3.11-slim
        command: ["python3", "-c"]
        args:
        - |
          import sys, subprocess
          from datetime import datetime
          from collections import defaultdict

          subprocess.check_call([sys.executable, "-m", "pip", "install", "pymongo", "scikit-learn", "pandas", "numpy", "mlflow", "-q"])

          import pymongo
          import numpy as np
          import pandas as pd
          from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
          from sklearn.model_selection import train_test_split, cross_val_score
          from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score
          from sklearn.preprocessing import StandardScaler
          import mlflow
          import mlflow.sklearn

          MONGO_URI = "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
          MLFLOW_URI = "http://mlflow.mlflow.svc.cluster.local:5000"
          SPECIALIST_TYPE = "business"

          print(f"=== Feature Engineering V2 - Threshold Optimization ===")

          client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
          db = client.neural_hive
          opinions_col = db.specialist_opinions
          feedback_col = db.specialist_feedback
          plans_col = db.cognitive_ledger

          # Get feedbacks
          feedbacks = list(feedback_col.find({"specialist_type": SPECIALIST_TYPE}))
          print(f"Feedbacks: {len(feedbacks)}")

          # Get opinions
          opinion_ids = [f["opinion_id"] for f in feedbacks]
          opinions = list(opinions_col.find({"opinion_id": {"$in": opinion_ids}}))
          opinions_dict = {o["opinion_id"]: o for o in opinions}

          # Get plans
          plan_ids = [o.get("plan_id") for o in opinions if o.get("plan_id")]
          plans = list(plans_col.find({"plan_id": {"$in": plan_ids}}))
          plans_dict = {p["plan_id"]: p for p in plans}

          # Extract reasoning factors
          reasoning_factors_data = {}
          for op in opinions:
              oid = op["opinion_id"]
              factors = op.get("opinion", {}).get("reasoning_factors", [])
              for f in factors:
                  reasoning_factors_data[(oid, f["factor_name"])] = {
                      "weight": f.get("weight", 0),
                      "score": f.get("score", 0)
                  }

          # Build dataset
          training_data = []
          for fb in feedbacks:
              opinion_id = fb["opinion_id"]
              opinion = opinions_dict.get(opinion_id)
              if not opinion:
                  continue

              opinion_data = opinion.get("opinion", {})
              plan_id = opinion.get("plan_id")
              plan = plans_dict.get(plan_id, {})

              conf = opinion_data.get("confidence_score", 0.5)
              risk = opinion_data.get("risk_score", 0.5)
              rec = opinion_data.get("recommendation", "review_required")

              factor_scores = {}
              for factor_name in ["ml_confidence", "ml_risk", "semantic_quality_analysis",
                                   "complexity_evaluation", "kpi_alignment", "cost_effectiveness"]:
                  key = (opinion_id, factor_name)
                  if key in reasoning_factors_data:
                      factor_scores[f"{factor_name}_score"] = reasoning_factors_data[key]["score"]
                      factor_scores[f"{factor_name}_weight"] = reasoning_factors_data[key]["weight"]
                  else:
                      factor_scores[f"{factor_name}_score"] = 0.5
                      factor_scores[f"{factor_name}_weight"] = 0.0

              complexity_score = plan.get("complexity_score", 0.5)
              plan_risk_score = plan.get("risk_score", 0.5)
              estimated_duration = plan.get("estimated_total_duration_ms", 0)
              original_domain = plan.get("original_domain", "UNKNOWN")
              original_priority = plan.get("original_priority", "normal")

              tasks = plan.get("tasks", [])
              num_tasks = len(tasks)

              # Features
              features = {
                  "confidence_score": conf,
                  "risk_score": risk,
                  "model_rec_approve": 1 if rec == "approve" else 0,
                  "model_rec_reject": 1 if rec == "reject" else 0,
                  "model_rec_review": 1 if rec == "review_required" else 0,
                  "plan_complexity": complexity_score,
                  "plan_risk": plan_risk_score,
                  "num_tasks": num_tasks,
                  "estimated_duration_log": np.log1p(estimated_duration),
                  "is_complex": 1 if complexity_score > 0.6 else 0,
                  "is_high_risk": 1 if plan_risk_score > 0.6 else 0,
                  "is_high_priority": 1 if original_priority in ["high", "critical"] else 0,
                  "ml_conf_score": factor_scores.get("ml_confidence_score", 0.5),
                  "ml_risk_score": factor_scores.get("ml_risk_score", 0.5),
                  "semantic_quality": factor_scores.get("semantic_quality_analysis_score", 0.5),
                  "complexity_eval": factor_scores.get("complexity_evaluation_score", 0.5),
                  "kpi_align": factor_scores.get("kpi_alignment_score", 0.5),
              }

              human_rec = fb.get("human_recommendation", "review_required")
              label = 1 if human_rec == "approve" else 0
              training_data.append({**features, "label": label})

          df = pd.DataFrame(training_data)
          print(f"Dataset size: {len(df)}")

          feature_cols = [c for c in df.columns if c != "label"]
          X = df[feature_cols]
          y = df["label"]

          print(f"Positive ratio: {y.mean():.3f}")

          # Split
          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
          print(f"Train: {len(X_train)}, Test: {len(X_test)}")

          # Scale
          scaler = StandardScaler()
          X_train_scaled = scaler.fit_transform(X_train)
          X_test_scaled = scaler.transform(X_test)

          # MLflow
          mlflow.set_tracking_uri(MLFLOW_URI)
          mlflow.set_experiment(f"specialist-{SPECIALIST_TYPE}-retraining")

          with mlflow.start_run(run_name=f"{SPECIALIST_TYPE}_v4_opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
              print(f"Run ID: {mlflow.active_run().info.run_id}")

              mlflow.log_param("specialist_type", SPECIALIST_TYPE)
              mlflow.log_param("samples", len(X_train))
              mlflow.log_param("features", len(feature_cols))
              mlflow.log_param("model", "gradient_boosting")
              mlflow.log_param("feature_engineering", True)

              # Train Gradient Boosting
              print("Training Gradient Boosting...")
              model = GradientBoostingClassifier(
                  n_estimators=150,
                  max_depth=5,
                  learning_rate=0.05,
                  min_samples_split=10,
                  min_samples_leaf=5,
                  random_state=42
              )
              model.fit(X_train_scaled, y_train)

              # Get probabilities
              y_proba = model.predict_proba(X_test_scaled)[:, 1]

              # Find optimal threshold
              precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
              f1_scores = 2 * precision * recall / (precision + recall + 1e-10)
              best_idx = np.argmax(f1_scores)
              best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5

              print(f"Optimal threshold: {best_threshold:.3f}")

              # Predict with optimal threshold
              y_pred = (y_proba >= best_threshold).astype(int)

              # Metrics with optimal threshold
              accuracy = accuracy_score(y_test, y_pred)
              precision_val = precision_score(y_test, y_pred, zero_division=0)
              recall_val = recall_score(y_test, y_pred, zero_division=0)
              f1_val = f1_score(y_test, y_pred, zero_division=0)
              auc = roc_auc_score(y_test, y_proba)

              print(f"\\n=== Test Metrics (optimal threshold={best_threshold:.3f}) ===")
              print(f"Accuracy:  {accuracy:.3f}")
              print(f"Precision: {precision_val:.3f}")
              print(f"Recall:    {recall_val:.3f}")
              print(f"F1 Score:  {f1_val:.3f}")
              print(f"AUC-ROC:   {auc:.3f}")

              print(f"\\nClassification Report:")
              print(classification_report(y_test, y_pred, zero_division=0))

              # Log metrics
              mlflow.log_metric("accuracy", accuracy)
              mlflow.log_metric("precision", precision_val)
              mlflow.log_metric("recall", recall_val)
              mlflow.log_metric("f1", f1_val)
              mlflow.log_metric("auc_roc", auc)
              mlflow.log_metric("optimal_threshold", best_threshold)

              # Feature importances
              importances = dict(zip(feature_cols, model.feature_importances_.tolist()))
              sorted_imp = sorted(importances.items(), key=lambda x: x[1], reverse=True)
              print(f"\\n=== Top 10 Feature Importances ===")
              for feat, imp in sorted_imp[:10]:
                  print(f"  {feat}: {imp:.4f}")
              mlflow.log_dict(dict(sorted_imp), "feature_importances.json")

              # Save model
              mlflow.sklearn.log_model(model, "model")
              mlflow.log_dict({"optimal_threshold": str(best_threshold)}, "model_config.json")
              print(f"\\nModel saved to MLflow with threshold {best_threshold:.3f}")

          print("\\n=== SUCCESS ===")
        env:
        - name: MONGODB_URI
          value: "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow.mlflow.svc.cluster.local:5000"
      restartPolicy: Never
