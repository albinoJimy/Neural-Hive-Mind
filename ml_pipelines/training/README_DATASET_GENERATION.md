# Dataset Generation with AI

Este m√≥dulo gera datasets de treino realistas para os 5 especialistas do Neural Hive Mind usando LLMs (Large Language Models).

## üìã Vis√£o Geral

O gerador de datasets usa LLMs para criar:
1. **Cognitive Plans** variados (diferentes dom√≠nios, complexidades, riscos)
2. **Specialist Opinions** com labels corretos (approve/reject/review_required/conditional)
3. **Features num√©ricas** extra√≠das via `FeatureExtractor` (26+ dimens√µes)
4. **Datasets em Parquet** prontos para treinamento

## üèóÔ∏è Arquitetura

```
LLM (OpenAI/Anthropic/Ollama)
    ‚Üì
Prompt Templates (por especialista)
    ‚Üì
Cognitive Plan JSON + Specialist Opinion JSON
    ‚Üì
FeatureExtractor (ontology + graph + embeddings)
    ‚Üì
Features (26+ dimens√µes) + Label
    ‚Üì
Parquet Dataset
```

## üöÄ Quick Start

### 1. Configurar Environment

```bash
cd ml_pipelines/training
cp .env.example .env
# Editar .env com suas configura√ß√µes
```

### 2. Instalar Depend√™ncias

```bash
pip install -r requirements-dataset-generation.txt
```

### 3. Gerar Datasets

#### Op√ß√£o A: Gerar todos os especialistas

```bash
./generate_all_datasets.sh
```

#### Op√ß√£o B: Gerar um especialista espec√≠fico

```bash
python generate_training_datasets.py \
    --specialist-type technical \
    --num-samples 1000 \
    --output-path /data/training/specialist_technical_base.parquet
```

#### Op√ß√£o C: Usar MLflow

```bash
mlflow run . -e generate_datasets \
    -P specialist_type=technical \
    -P num_samples=1000
```

## üéØ Especialistas Suportados

| Especialista | Foco | Reasoning Factors |
|--------------|------|-------------------|
| **technical** | Feasibility, complexity, technical risk | technical_feasibility, implementation_complexity, technical_risk, code_quality |
| **business** | ROI, strategic alignment, stakeholder value | business_value, strategic_alignment, stakeholder_impact, market_competitiveness |
| **behavior** | UX, accessibility, cognitive load | user_experience, accessibility, cognitive_load, adoption_potential |
| **evolution** | Maintainability, scalability, technical debt | maintainability, scalability, technical_debt, adaptability |
| **architecture** | Design patterns, SOLID, modularity | design_quality, modularity, coupling, pattern_adherence |

## üìä Estrutura do Dataset Gerado

### Features (26+ dimens√µes)

**Metadata Features (7):**
- num_tasks, priority_score, total_duration_ms, avg_duration_ms
- has_risk_score, risk_score, complexity_score

**Ontology Features (6):**
- domain_id, domain_risk_weight, avg_task_complexity_factor
- num_patterns_detected, num_anti_patterns_detected, avg_pattern_quality, total_anti_pattern_penalty

**Graph Features (8+):**
- num_nodes, num_edges, avg_degree, max_degree
- num_connected_components, avg_clustering_coefficient
- num_bottlenecks, has_bottlenecks, graph_complexity_score

**Embedding Features (5+):**
- embedding_mean, embedding_std, embedding_min, embedding_max
- cosine_similarity_mean

### Labels

- **0**: reject
- **1**: approve
- **2**: review_required
- **3**: conditional

**Distribui√ß√£o recomendada:**
- approve: 40%
- reject: 20%
- review_required: 25%
- conditional: 15%

## üîß Configura√ß√£o Avan√ßada

### LLM Providers

#### OpenAI

```bash
export LLM_PROVIDER=openai
export LLM_API_KEY=sk-...
export LLM_MODEL=gpt-4
```

#### Anthropic

```bash
export LLM_PROVIDER=anthropic
export LLM_API_KEY=sk-ant-...
export LLM_MODEL=claude-3-opus-20240229
```

#### Ollama (Local)

```bash
export LLM_PROVIDER=local
export LLM_MODEL=llama2
export LLM_BASE_URL=http://ollama:11434/api
```

### Customizar Prompts

Editar templates em `ml_pipelines/training/prompts/`:
- `cognitive_plan_template.txt`
- `specialist_{type}_template.txt`

## üìà Valida√ß√£o de Qualidade

O gerador valida automaticamente:

‚úÖ JSON v√°lido contra schemas Avro
‚úÖ Features t√™m 26+ dimens√µes
‚úÖ Distribui√ß√£o de labels balanceada
‚úÖ Cognitive plans formam DAGs v√°lidos
‚úÖ Risk scores correlacionados com recommendations

## üêõ Troubleshooting

### Erro: "LLM API key not found"

```bash
# Verificar .env
cat .env | grep LLM_API_KEY

# Ou exportar diretamente
export LLM_API_KEY=your-key-here
```

### Erro: "Invalid JSON generated"

- Aumentar temperature para mais diversidade
- Ou diminuir temperature para mais consist√™ncia
- Verificar se modelo suporta JSON output

### Erro: "Feature extraction failed"

```bash
# Verificar se ontologias est√£o dispon√≠veis
ls -la /app/ontologies/

# Verificar se sentence-transformers est√° instalado
pip list | grep sentence-transformers
```

## üìö Refer√™ncias

- Schema CognitivePlan: `schemas/cognitive-plan/cognitive-plan.avsc`
- Schema SpecialistOpinion: `schemas/specialist-opinion/specialist-opinion.avsc`
- FeatureExtractor: `libraries/python/neural_hive_specialists/feature_extraction/`
- Training Pipeline: `ml_pipelines/training/train_specialist_model.py`

## üîÑ Pr√≥ximos Passos

Ap√≥s gerar datasets:

1. **Revisar datasets gerados**:
   ```bash
   python -c "import pandas as pd; df = pd.read_parquet('/data/training/specialist_technical_base.parquet'); print(df.head()); print(df.describe())"
   ```

2. **Treinar modelos**:
   ```bash
   ./train_all_specialists.sh
   ```

3. **Validar no MLflow**:
   ```bash
   # Acessar MLflow UI
   kubectl port-forward -n mlflow svc/mlflow 5000:5000
   # Abrir http://localhost:5000
   ```

4. **Validar carregamento dos modelos**:
   ```bash
   ./validate_models_loaded.sh
   ```

5. **Reiniciar pods se necess√°rio**:
   ```bash
   kubectl rollout restart deployment -n semantic-translation -l app.kubernetes.io/component=specialist
   ```

6. **Verificar specialists ficam Ready**:
   ```bash
   kubectl get pods -n semantic-translation -l app=specialist-technical
   ```

## üéì Training Models

### Prerequisites

Antes de treinar os modelos, certifique-se que:

‚úÖ **Dataset generation completo** - Datasets base existem em `/data/training/specialist_*_base.parquet`
‚úÖ **MLflow rodando** - Acess√≠vel em `http://mlflow.mlflow:5000`
‚úÖ **MongoDB acess√≠vel** (opcional) - Para incluir feedback humano no treinamento

### Quick Start - Treinar Todos os Especialistas

O jeito mais r√°pido de treinar todos os 5 modelos de especialistas:

```bash
cd ml_pipelines/training
./train_all_specialists.sh
```

Este script:
- ‚úÖ Verifica conectividade MLflow e MongoDB
- ‚úÖ Confirma que datasets existem
- ‚úÖ Treina modelos sequencialmente (technical, business, behavior, evolution, architecture)
- ‚úÖ Auto-promove para Production se m√©tricas atingirem thresholds
- ‚úÖ Reporta progresso e erros em tempo real

### Treinar Especialista Individual

Para treinar apenas um especialista espec√≠fico:

```bash
python train_specialist_model.py \
  --specialist-type technical \
  --model-type random_forest \
  --hyperparameter-tuning false \
  --promote-if-better true
```

**Par√¢metros dispon√≠veis:**
- `--specialist-type`: `technical`, `business`, `behavior`, `evolution`, `architecture`
- `--model-type`: `random_forest`, `gradient_boosting`, `neural_network`
- `--hyperparameter-tuning`: `true` (GridSearchCV) ou `false` (defaults)
- `--promote-if-better`: `true` (auto-promote) ou `false` (manual)
- `--window-days`: Janela de feedbacks a incluir (default: 30 dias)
- `--min-feedback-quality`: Rating m√≠nimo de feedback (default: 0.5)

### Usar MLflow

Treinar todos os especialistas via MLflow:

```bash
mlflow run . -e train_all
```

Treinar especialista individual via MLflow:

```bash
mlflow run . -e main \
  -P specialist_type=technical \
  -P model_type=random_forest \
  -P hyperparameter_tuning=false
```

### Conven√ß√£o de Nomes dos Modelos

**IMPORTANTE:** Os modelos s√£o registrados com nomes espec√≠ficos que correspondem √†s configura√ß√µes dos especialistas:

- **Nome do modelo**: `{specialist_type}-evaluator`
  - Exemplos: `technical-evaluator`, `business-evaluator`, `behavior-evaluator`
  - Refer√™ncia: `services/specialist-{type}/src/config.py`

- **Nome do experimento**: `{specialist_type}-specialist`
  - Exemplos: `technical-specialist`, `business-specialist`
  - Refer√™ncia: `services/specialist-{type}/src/config.py` (linha 21)

Esta conven√ß√£o garante que os especialistas consigam carregar os modelos via `mlflow_client.load_model_with_fallback()`.

### Thresholds de Promo√ß√£o

Os modelos s√£o **automaticamente promovidos para Production** se atenderem os seguintes crit√©rios:

**1. Thresholds M√≠nimos Absolutos (TODOS devem ser atendidos):**

| M√©trica | Threshold M√≠nimo |
|---------|-----------------|
| **Precision** | ‚â• 0.75 |
| **Recall** | ‚â• 0.70 |
| **F1 Score** | ‚â• 0.72 |

**2. Crit√©rio de Melhoria (se baseline existe):**

Se j√° existe um modelo baseline em Production, o novo modelo deve ser **5% melhor em precision** para ser promovido.

**L√≥gica de Decis√£o:**

1. **Verificar thresholds absolutos**: precision ‚â• 0.75 E recall ‚â• 0.70 E f1 ‚â• 0.72
2. **Se n√£o h√° baseline**: promover automaticamente (thresholds atendidos)
3. **Se h√° baseline**: verificar se precision melhorou ‚â• 5% (0.05 absoluto)

**Exemplo de decis√£o de promo√ß√£o:**

```
Novo modelo:     precision=0.82, recall=0.78, f1=0.80
Baseline:        precision=0.75, recall=0.72, f1=0.73
Improvement:     +7% precision
Decis√£o:         ‚úÖ PROMOVER (atende 3 thresholds + 5% improvement)
```

```
Novo modelo:     precision=0.76, recall=0.71, f1=0.73
Baseline:        precision=0.75, recall=0.72, f1=0.73
Improvement:     +1% precision
Decis√£o:         ‚ùå N√ÉO PROMOVER (atende thresholds mas improvement < 5%)
```

```
Novo modelo:     precision=0.76, recall=0.68, f1=0.72
Baseline:        precision=0.75, recall=0.72, f1=0.73
Decis√£o:         ‚ùå N√ÉO PROMOVER (recall abaixo de 0.70)
```

Se `--promote-if-better false`, os modelos ficam em **Staging** e promo√ß√£o √© manual via MLflow UI.

### Valida√ß√£o de Carregamento

Ap√≥s treinar os modelos, valide que os especialistas conseguem carreg√°-los:

```bash
./validate_models_loaded.sh
```

Este script:
1. ‚úÖ Verifica modelos registrados no MLflow (`{type}-evaluator`)
2. ‚úÖ Confirma vers√µes em Production stage
3. ‚úÖ Verifica pods de especialistas est√£o Running
4. ‚úÖ Query endpoint `/status` de cada pod
5. ‚úÖ Valida `model_loaded: true` na resposta

**Output esperado:**

```
‚úÖ 5/5 especialistas carregaram modelos com sucesso

Specialist: technical
   Model Loaded: true
   MLflow Connected: true

Specialist: business
   Model Loaded: true
   MLflow Connected: true
...
```

### Troubleshooting

**Problema:** `model_loaded: false` no health check

**Solu√ß√µes:**

1. **Verificar conectividade MLflow:**
   ```bash
   kubectl exec -n semantic-translation deployment/specialist-technical -- \
     curl -s http://mlflow.mlflow:5000/health
   ```

2. **Verificar nome do modelo:**
   ```bash
   # Deve ser {specialist}-evaluator
   curl -s http://mlflow.mlflow:5000/api/2.0/mlflow/registered-models/list | \
     jq '.registered_models[] | select(.name | contains("evaluator"))'
   ```

3. **Verificar logs do especialista:**
   ```bash
   kubectl logs -n semantic-translation -l app=specialist-technical --tail=100 | \
     grep -i "mlflow\|model"
   ```

4. **Reiniciar pods para for√ßar reload:**
   ```bash
   kubectl rollout restart deployment -n semantic-translation specialist-technical
   kubectl rollout status deployment -n semantic-translation specialist-technical
   ```

**Problema:** Treinamento falha com "Dataset not found"

**Solu√ß√£o:** Gerar datasets primeiro:
```bash
./generate_all_datasets.sh
ls -lh /data/training/specialist_*_base.parquet
```

## üìä Resultados de Gera√ß√£o - Specialist TECHNICAL (2025-12-14)

### Estat√≠sticas

| M√©trica | Valor |
|---------|-------|
| Total solicitado | 300 |
| Amostras geradas | 80 |
| Taxa de sucesso | 26.7% |
| Falhas | 0 |

### Qualidade de Descri√ß√µes

| M√©trica | Valor |
|---------|-------|
| Score m√©dio | 0.909 |
| Score m√≠nimo | 0.730 |
| Score m√°ximo | 1.000 |
| Rejeitados por baixa qualidade | 0 |

### Distribui√ß√£o de Labels

| Label | Quantidade | Percentual |
|-------|------------|------------|
| approve | 40 | 50.0% |
| reject | 0 | 0.0% |
| review_required | 25 | 31.2% |
| conditional | 15 | 18.8% |

### ‚ö†Ô∏è Problemas Identificados

1. **Taxa de sucesso baixa (26.7%)**: Das 300 amostras solicitadas, apenas 80 foram geradas com sucesso. Poss√≠veis causas:
   - Timeouts na API LLM
   - JSONs malformados rejeitados
   - Valida√ß√µes de schema falhando

2. **Aus√™ncia de amostras "reject" (0%)**: O modelo n√£o aprender√° a identificar casos que devem ser rejeitados. Distribui√ß√£o recomendada √© 20% reject.

3. **Desbalanceamento de classes**: Distribui√ß√£o atual muito diferente da recomendada (approve: 40%, reject: 20%, review_required: 25%, conditional: 15%).

### Estat√≠sticas das Features

```
       num_tasks  priority_score  avg_diversity      label
count   80.00000       80.000000      80.000000  80.000000
mean     5.66250        0.381322       3.974241   1.687500
std      3.15002        0.226442       0.285761   0.772858
min      2.00000        0.152120       3.001797   1.000000
max     13.00000        1.000000       4.366861   3.000000
```

### A√ß√µes Recomendadas

1. **Gerar mais amostras com foco em "reject"**:
   ```bash
   python generate_training_datasets.py \
       --specialist-type technical \
       --num-samples 200 \
       --target-label reject \
       --output-path data/specialist_technical_reject.parquet
   ```

2. **Combinar datasets**:
   ```bash
   python -c "
   import pandas as pd
   df1 = pd.read_parquet('data/specialist_technical_new.parquet')
   df2 = pd.read_parquet('data/specialist_technical_reject.parquet')
   combined = pd.concat([df1, df2], ignore_index=True)
   combined.to_parquet('data/specialist_technical_base.parquet')
   print(f'Combined: {len(combined)} samples')
   print(combined['label'].value_counts())
   "
   ```

3. **Verificar logs de falhas**:
   ```bash
   cat ml_pipelines/training/logs/technical_generation.log | grep -i "error\|failed"
   ```

**Problema:** MLflow n√£o acess√≠vel

**Solu√ß√£o:** Verificar deployment:
```bash
kubectl get pods -n mlflow
kubectl port-forward -n mlflow svc/mlflow 5000:5000
curl http://localhost:5000/health
```
