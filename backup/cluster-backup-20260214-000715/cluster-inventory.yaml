apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-02-02T16:38:00+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8000"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-11T10:24:37Z"
    generateName: approval-service-586bb5bd7-
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 586bb5bd7
    name: approval-service-586bb5bd7-fzp7j
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: approval-service-586bb5bd7
      uid: 3f081183-2bae-4fc5-b06f-554de948e962
    resourceVersion: "29939305"
    uid: df7b7f92-96bd-4713-a86f-adf8d99ba2e9
  spec:
    containers:
    - envFrom:
      - configMapRef:
          name: approval-service-config
      - secretRef:
          name: approval-service-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/approval-service:10d7a42
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: approval-service
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8000
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 200m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/schemas
        name: schemas
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-r8knz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: cognitive-plan-schema
      name: schemas
    - name: kube-api-access-r8knz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:46Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:24:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:45Z"
      message: 'containers with unready status: [approval-service]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:45Z"
      message: 'containers with unready status: [approval-service]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:24:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d345676788a1ec93c0fa66ae9b13429b7713f3de8c34486d8e09be1c7003adb7
      image: ghcr.io/albinojimy/neural-hive-mind/approval-service:10d7a42
      imageID: ghcr.io/albinojimy/neural-hive-mind/approval-service@sha256:8b5ab040d426ce983b9a85281d3176428b97563a4dd548026155ff8500904151
      lastState:
        terminated:
          containerID: containerd://d345676788a1ec93c0fa66ae9b13429b7713f3de8c34486d8e09be1c7003adb7
          exitCode: 137
          finishedAt: "2026-02-13T23:05:44Z"
          reason: Error
          startedAt: "2026-02-13T23:05:13Z"
      name: approval-service
      ready: false
      restartCount: 663
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=approval-service pod=approval-service-586bb5bd7-fzp7j_approval(df7b7f92-96bd-4713-a86f-adf8d99ba2e9)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.6
    podIPs:
    - ip: 10.244.4.6
    qosClass: Burstable
    startTime: "2026-02-11T10:24:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, ephemeral-storage,
        memory request for container cert-manager-controller; cpu, ephemeral-storage,
        memory limit for container cert-manager-controller'
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-22T16:38:22Z"
    generateName: cert-manager-69c44b659d-
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 69c44b659d
    name: cert-manager-69c44b659d-mbksx
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-69c44b659d
      uid: 41c3e826-e69a-46c5-b771-a3cfe33de8f4
    resourceVersion: "25557393"
    uid: 648dc955-43ab-4da8-b233-4dcde242546c
  spec:
    containers:
    - args:
      - --v=2
      - --cluster-resource-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=kube-system
      - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.2
      - --dns01-recursive-nameservers-only
      - --dns01-recursive-nameservers=162.159.44.25:53,108.162.194.2:53
      - --max-concurrent-challenges=60
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-controller:v1.19.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          path: /livez
          port: http-healthz
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: cert-manager-controller
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      - containerPort: 9403
        name: http-healthz
        protocol: TCP
      resources:
        limits:
          cpu: 200m
          ephemeral-storage: 1Gi
          memory: 256Mi
        requests:
          cpu: 50m
          ephemeral-storage: 200Mi
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w66mh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: vmi3002938
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager
    serviceAccountName: cert-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-w66mh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:39:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-01T13:46:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-01T13:46:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://68bfce90b43f9c61db7545bd0ff76e8b3b0c63867ce152c892074f405c478d58
      image: quay.io/jetstack/cert-manager-controller:v1.19.2
      imageID: quay.io/jetstack/cert-manager-controller@sha256:d4ffb81a6d89a0e690ee80d07fcdcb6cccb99118010e3eb0808442e506e18ab0
      lastState:
        terminated:
          containerID: containerd://f667efaca28b3e48af00b937b4fad6642b45fdfd82924083a49f710e0a764f96
          exitCode: 1
          finishedAt: "2026-02-01T13:46:42Z"
          reason: Error
          startedAt: "2026-01-22T16:54:38Z"
      name: cert-manager-controller
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2026-02-01T13:46:49Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.68
    podIPs:
    - ip: 10.244.3.68
    qosClass: Burstable
    startTime: "2026-01-22T16:38:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, ephemeral-storage,
        memory request for container cert-manager-cainjector; cpu, ephemeral-storage,
        memory limit for container cert-manager-cainjector'
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-22T16:43:56Z"
    generateName: cert-manager-cainjector-6b8d568764-
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 6b8d568764
    name: cert-manager-cainjector-6b8d568764-jv9rg
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-cainjector-6b8d568764
      uid: 1b6b6b3d-9bb5-493f-b1fb-2e5f1951f7d3
    resourceVersion: "22400302"
    uid: b7e37c3d-85e1-4316-aaad-c2f39e12ebc6
  spec:
    containers:
    - args:
      - --v=2
      - --leader-election-namespace=kube-system
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-cainjector:v1.19.2
      imagePullPolicy: IfNotPresent
      name: cert-manager-cainjector
      ports:
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      resources:
        limits:
          cpu: 200m
          ephemeral-storage: 1Gi
          memory: 256Mi
        requests:
          cpu: 50m
          ephemeral-storage: 200Mi
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mvm9v
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: vmi2911680
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-cainjector
    serviceAccountName: cert-manager-cainjector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-mvm9v
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:57Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://41cb0caa5bd9296c8cca3674e3579460877b2db0b0815be9728728537a1ba967
      image: quay.io/jetstack/cert-manager-cainjector:v1.19.2
      imageID: quay.io/jetstack/cert-manager-cainjector@sha256:1a24f9848e7ae83a787187f4e75fdf5e9adf401f44de2f156c09c6fae1bf046e
      lastState: {}
      name: cert-manager-cainjector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:57:55Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.20
    podIPs:
    - ip: 10.244.1.20
    qosClass: Burstable
    startTime: "2026-01-22T16:57:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, ephemeral-storage,
        memory request for container cert-manager-webhook; cpu, ephemeral-storage,
        memory limit for container cert-manager-webhook'
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-22T16:38:22Z"
    generateName: cert-manager-webhook-8546d8cdff-
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 8546d8cdff
    name: cert-manager-webhook-8546d8cdff-l77gp
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cert-manager-webhook-8546d8cdff
      uid: 7ee70508-a7f6-47e3-a024-890fdd45499c
    resourceVersion: "26774995"
    uid: f416b323-9b05-491a-bddd-ad3cd65285b6
  spec:
    containers:
    - args:
      - --v=2
      - --secure-port=10250
      - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
      - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
      - --dynamic-serving-dns-names=cert-manager-webhook
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
      - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-webhook:v1.19.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: healthcheck
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: cert-manager-webhook
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      - containerPort: 6080
        name: healthcheck
        protocol: TCP
      - containerPort: 9402
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: healthcheck
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          ephemeral-storage: 1Gi
          memory: 256Mi
        requests:
          cpu: 50m
          ephemeral-storage: 200Mi
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xchg2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    nodeName: vmi3002938
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: cert-manager-webhook
    serviceAccountName: cert-manager-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-xchg2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:39:00Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8ebf7499fe12c5d74c4354328042581e055203fc75b4132fff0483b3ac04523e
      image: quay.io/jetstack/cert-manager-webhook:v1.19.2
      imageID: quay.io/jetstack/cert-manager-webhook@sha256:2ba4918d1581ed29bd5d68a017244560b353bcaf9932c814a8713887589bda6c
      lastState: {}
      name: cert-manager-webhook
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:38:59Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.67
    podIPs:
    - ip: 10.244.3.67
    qosClass: Burstable
    startTime: "2026-01-22T16:38:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-14T21:33:23Z"
    generateName: chi-neural-hive-clickhouse-cluster-0-0-
    labels:
      apps.kubernetes.io/pod-index: "0"
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chi: neural-hive-clickhouse
      clickhouse.altinity.com/cluster: cluster
      clickhouse.altinity.com/namespace: clickhouse-operator
      clickhouse.altinity.com/ready: "yes"
      clickhouse.altinity.com/replica: "0"
      clickhouse.altinity.com/shard: "0"
      controller-revision-hash: chi-neural-hive-clickhouse-cluster-0-0-7864d4d5c5
      statefulset.kubernetes.io/pod-name: chi-neural-hive-clickhouse-cluster-0-0-0
    name: chi-neural-hive-clickhouse-cluster-0-0-0
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: chi-neural-hive-clickhouse-cluster-0-0
      uid: 1b951c50-d5f5-4f1b-93e8-973d6731f99d
    resourceVersion: "26774958"
    uid: cd4a3cca-fe93-4435-9137-9c54103efd4a
  spec:
    containers:
    - env:
      - name: CLICKHOUSE_SKIP_USER_SETUP
        value: "1"
      image: clickhouse/clickhouse-server:23.8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /ping
          port: http
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      name: clickhouse
      ports:
      - containerPort: 9000
        name: tcp
        protocol: TCP
      - containerPort: 8123
        name: http
        protocol: TCP
      - containerPort: 9009
        name: interserver
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ping
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 3
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/clickhouse-server/config.d/
        name: chi-neural-hive-clickhouse-common-configd
      - mountPath: /etc/clickhouse-server/users.d/
        name: chi-neural-hive-clickhouse-common-usersd
      - mountPath: /etc/clickhouse-server/conf.d/
        name: chi-neural-hive-clickhouse-deploy-confd-cluster-0-0
      - mountPath: /var/lib/clickhouse
        name: data-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t7vz7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostAliases:
    - hostnames:
      - chi-neural-hive-clickhouse-cluster-0-0
      ip: 127.0.0.1
    hostname: chi-neural-hive-clickhouse-cluster-0-0-0
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: chi-neural-hive-clickhouse-cluster-0-0
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data-volume
      persistentVolumeClaim:
        claimName: data-volume-chi-neural-hive-clickhouse-cluster-0-0-0
    - configMap:
        defaultMode: 420
        name: chi-neural-hive-clickhouse-common-configd
      name: chi-neural-hive-clickhouse-common-configd
    - configMap:
        defaultMode: 420
        name: chi-neural-hive-clickhouse-common-usersd
      name: chi-neural-hive-clickhouse-common-usersd
    - configMap:
        defaultMode: 420
        name: chi-neural-hive-clickhouse-deploy-confd-cluster-0-0
      name: chi-neural-hive-clickhouse-deploy-confd-cluster-0-0
    - name: kube-api-access-t7vz7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T21:34:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T21:33:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T21:33:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://363a7c00be129e701cde56a1eb1bad425136100e372bbd408b0e3ac7cc600301
      image: docker.io/clickhouse/clickhouse-server:23.8
      imageID: docker.io/clickhouse/clickhouse-server@sha256:512bb8a214831bfb66e0fd51368c10c13ed319a50fa0d3864f88c01c0e8295fe
      lastState: {}
      name: clickhouse
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-14T21:34:08Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.230
    podIPs:
    - ip: 10.244.3.230
    qosClass: Burstable
    startTime: "2026-01-14T21:33:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/confd-files: 5edfe9bdcd34fb98956d853ad29d712abd4894854a8135746b003d2b9c133577
      checksum/configd-files: c17d6658e0c99f87a8d73e095c45603c62ded9d66a0041aa588d83f1d66caeee
      checksum/files: e715c2eb8d54de48fe1a0b1ea2b2a5ee693cc53b6bc089374b49ac1e4e7a9095
      checksum/keeper-confd-files: 21795302d930e9eb1b9cb5b1020199582189134766632e46c162b3ada2649a92
      checksum/keeper-configd-files: d93fa1304ddab91942cfa7de911ada5fb02306b234bf5328938efbd78563f7bd
      checksum/keeper-templatesd-files: 6c0b64e4c96322864592ff1c6eb84ed9ddc4f96880c55878f3d15036e6d6b424
      checksum/keeper-usersd-files: 9b0fb56434072a9301b625c5b5857152d637e519ace77e54e0059f20df789e52
      checksum/templatesd-files: f32867861e9a45819ac6bf9ef21943190fcff1e00ff39f5045e98b98b37fdfb3
      checksum/usersd-files: a6af808e6d3a67c93fa43ab9443a22d35cd71fc1429a41dee8314a656b2577c6
      clickhouse-operator-metrics/port: "9999"
      clickhouse-operator-metrics/scrape: "true"
      kubectl.kubernetes.io/restartedAt: "2026-01-14T22:19:21+01:00"
      prometheus.io/port: "8888"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-14T21:19:56Z"
    generateName: clickhouse-operator-altinity-clickhouse-operator-6599ffbd4c-
    labels:
      app.kubernetes.io/instance: clickhouse-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/version: 0.25.5
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chop: 0.25.5
      clickhouse.altinity.com/chop-commit: 9ab22d8
      clickhouse.altinity.com/chop-date: 2025-10-24T08.40.12
      helm.sh/chart: altinity-clickhouse-operator-0.25.5
      pod-template-hash: 6599ffbd4c
    name: clickhouse-operator-altinity-clickhouse-operator-6599ffbd4c87vq
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: clickhouse-operator-altinity-clickhouse-operator-6599ffbd4c
      uid: 979376b2-b516-4dea-9e97-e83a7e946580
    resourceVersion: "25557676"
    uid: 260baf99-21a7-438e-8201-b7aa9625b667
  spec:
    affinity: {}
    containers:
    - env:
      - name: OPERATOR_POD_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OPERATOR_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OPERATOR_POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OPERATOR_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: OPERATOR_POD_SERVICE_ACCOUNT
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.serviceAccountName
      - name: OPERATOR_CONTAINER_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            containerName: altinity-clickhouse-operator
            divisor: "0"
            resource: requests.cpu
      - name: OPERATOR_CONTAINER_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            containerName: altinity-clickhouse-operator
            divisor: "0"
            resource: limits.cpu
      - name: OPERATOR_CONTAINER_MEM_REQUEST
        valueFrom:
          resourceFieldRef:
            containerName: altinity-clickhouse-operator
            divisor: "0"
            resource: requests.memory
      - name: OPERATOR_CONTAINER_MEM_LIMIT
        valueFrom:
          resourceFieldRef:
            containerName: altinity-clickhouse-operator
            divisor: "0"
            resource: limits.memory
      image: altinity/clickhouse-operator:0.25.5
      imagePullPolicy: IfNotPresent
      name: altinity-clickhouse-operator
      ports:
      - containerPort: 9999
        name: op-metrics
        protocol: TCP
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/clickhouse-operator
        name: etc-clickhouse-operator-folder
      - mountPath: /etc/clickhouse-operator/chi/conf.d
        name: etc-clickhouse-operator-confd-folder
      - mountPath: /etc/clickhouse-operator/chi/config.d
        name: etc-clickhouse-operator-configd-folder
      - mountPath: /etc/clickhouse-operator/chi/templates.d
        name: etc-clickhouse-operator-templatesd-folder
      - mountPath: /etc/clickhouse-operator/chi/users.d
        name: etc-clickhouse-operator-usersd-folder
      - mountPath: /etc/clickhouse-operator/chk/conf.d
        name: etc-keeper-operator-confd-folder
      - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
        name: etc-keeper-operator-configd-folder
      - mountPath: /etc/clickhouse-operator/chk/templates.d
        name: etc-keeper-operator-templatesd-folder
      - mountPath: /etc/clickhouse-operator/chk/users.d
        name: etc-keeper-operator-usersd-folder
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7hnfs
        readOnly: true
    - env:
      - name: OPERATOR_POD_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OPERATOR_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OPERATOR_POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: OPERATOR_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: OPERATOR_POD_SERVICE_ACCOUNT
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.serviceAccountName
      - name: OPERATOR_CONTAINER_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            containerName: altinity-clickhouse-operator
            divisor: "0"
            resource: requests.cpu
      - name: OPERATOR_CONTAINER_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            containerName: altinity-clickhouse-operator
            divisor: "0"
            resource: limits.cpu
      - name: OPERATOR_CONTAINER_MEM_REQUEST
        valueFrom:
          resourceFieldRef:
            containerName: altinity-clickhouse-operator
            divisor: "0"
            resource: requests.memory
      - name: OPERATOR_CONTAINER_MEM_LIMIT
        valueFrom:
          resourceFieldRef:
            containerName: altinity-clickhouse-operator
            divisor: "0"
            resource: limits.memory
      image: altinity/metrics-exporter:0.25.5
      imagePullPolicy: IfNotPresent
      name: metrics-exporter
      ports:
      - containerPort: 8888
        name: ch-metrics
        protocol: TCP
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/clickhouse-operator
        name: etc-clickhouse-operator-folder
      - mountPath: /etc/clickhouse-operator/chi/conf.d
        name: etc-clickhouse-operator-confd-folder
      - mountPath: /etc/clickhouse-operator/chi/config.d
        name: etc-clickhouse-operator-configd-folder
      - mountPath: /etc/clickhouse-operator/chi/templates.d
        name: etc-clickhouse-operator-templatesd-folder
      - mountPath: /etc/clickhouse-operator/chi/users.d
        name: etc-clickhouse-operator-usersd-folder
      - mountPath: /etc/clickhouse-operator/chk/conf.d
        name: etc-keeper-operator-confd-folder
      - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
        name: etc-keeper-operator-configd-folder
      - mountPath: /etc/clickhouse-operator/chk/templates.d
        name: etc-keeper-operator-templatesd-folder
      - mountPath: /etc/clickhouse-operator/chk/users.d
        name: etc-keeper-operator-usersd-folder
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7hnfs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: clickhouse-operator-altinity-clickhouse-operator
    serviceAccountName: clickhouse-operator-altinity-clickhouse-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-files
      name: etc-clickhouse-operator-folder
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-confd-files
      name: etc-clickhouse-operator-confd-folder
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-configd-files
      name: etc-clickhouse-operator-configd-folder
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-templatesd-files
      name: etc-clickhouse-operator-templatesd-folder
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-usersd-files
      name: etc-clickhouse-operator-usersd-folder
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-keeper-confd-files
      name: etc-keeper-operator-confd-folder
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-keeper-configd-files
      name: etc-keeper-operator-configd-folder
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-keeper-templatesd-files
      name: etc-keeper-operator-templatesd-folder
    - configMap:
        defaultMode: 420
        name: clickhouse-operator-altinity-clickhouse-operator-keeper-usersd-files
      name: etc-keeper-operator-usersd-folder
    - name: kube-api-access-7hnfs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T21:20:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T21:19:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T21:20:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T21:20:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T21:19:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8fd041eb06280c1a857d1aeaab5e9360d76bce503f5a5bd1de186337c5175937
      image: docker.io/altinity/clickhouse-operator:0.25.5
      imageID: docker.io/altinity/clickhouse-operator@sha256:8fadc501000fd2cbfd13fe67906c42c933af4e0844d51836222f9024db98a873
      lastState: {}
      name: altinity-clickhouse-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-14T21:20:04Z"
    - containerID: containerd://44cf012fa6fafff53b748b7ceda104cd6360a4726c1b205f8f4fd4fb10c0a437
      image: docker.io/altinity/metrics-exporter:0.25.5
      imageID: docker.io/altinity/metrics-exporter@sha256:5a8fb8f3242767920a1812f243bf4cc9fa9592cf77867c105db42e2bc1820a9f
      lastState: {}
      name: metrics-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-14T21:20:10Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.229
    podIPs:
    - ip: 10.244.3.229
    qosClass: BestEffort
    startTime: "2026-01-14T21:19:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-08T10:33:01Z"
    generateName: clickhouse-
    labels:
      app: clickhouse
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: clickhouse-67d874c5fb
      statefulset.kubernetes.io/pod-name: clickhouse-0
    name: clickhouse-0
    namespace: clickhouse
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: clickhouse
      uid: b9dd2de5-ee0f-4ea2-a233-641f359dd97e
    resourceVersion: "27840714"
    uid: ab72c660-5fe7-4f5d-b1a8-1ebcadbf088f
  spec:
    containers:
    - image: clickhouse/clickhouse-server:23.8
      imagePullPolicy: IfNotPresent
      name: clickhouse
      ports:
      - containerPort: 8123
        name: http
        protocol: TCP
      - containerPort: 9000
        name: native
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/clickhouse
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4w8ks
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: clickhouse-0
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: clickhouse
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: clickhouse-data
    - name: kube-api-access-4w8ks
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T10:38:36Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T10:38:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T10:38:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T10:38:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T10:38:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://66512bef26f2dcfeb84004504a266a91d57b581bd53e6bdbcbf1c4fbcf038e04
      image: docker.io/clickhouse/clickhouse-server:23.8
      imageID: docker.io/clickhouse/clickhouse-server@sha256:512bb8a214831bfb66e0fd51368c10c13ed319a50fa0d3864f88c01c0e8295fe
      lastState: {}
      name: clickhouse
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-08T10:38:36Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.62
    podIPs:
    - ip: 10.244.1.62
    qosClass: Burstable
    startTime: "2026-02-08T10:38:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"kaniko-context-loader","namespace":"default"},"spec":{"containers":[{"command":["sleep","infinity"],"image":"busybox","name":"busybox","volumeMounts":[{"mountPath":"/context","name":"context"}]}],"volumes":[{"name":"context","persistentVolumeClaim":{"claimName":"kaniko-build-context"}}]}}
    creationTimestamp: "2026-01-26T20:22:20Z"
    name: kaniko-context-loader
    namespace: default
    resourceVersion: "23702768"
    uid: a3937513-3413-4cf5-a76b-2cebfd7f8f4f
  spec:
    containers:
    - command:
      - sleep
      - infinity
      image: busybox
      imagePullPolicy: Always
      name: busybox
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /context
        name: context
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nm2lc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: context
      persistentVolumeClaim:
        claimName: kaniko-build-context
    - name: kube-api-access-nm2lc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:22:42Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:22:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:22:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:22:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-26T20:22:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f49c89122fbfee3ede62be5fe952a0a03409af76ebda7246c95d3f7b8888b87a
      image: docker.io/library/busybox:latest
      imageID: docker.io/library/busybox@sha256:e226d6308690dbe282443c8c7e57365c96b5228f0fe7f40731b5d84d37a06839
      lastState: {}
      name: busybox
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-26T20:22:41Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.16
    podIPs:
    - ip: 10.244.1.16
    qosClass: BestEffort
    startTime: "2026-01-26T20:22:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-08T17:59:26Z"
    labels:
      run: redis-cli
    name: redis-cli
    namespace: default
    resourceVersion: "27960509"
    uid: 36e10057-6617-4dfa-b1a7-b8681f855216
  spec:
    containers:
    - command:
      - sleep
      - "3600"
      image: redis:7
      imagePullPolicy: IfNotPresent
      name: redis-cli
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x2vbt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-x2vbt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T18:59:35Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T17:59:26Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T18:59:34Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T18:59:34Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T17:59:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3f28b607396d46b6c92b7fe49cf7eb487e24e550758d435db64ba19ac24371d2
      image: docker.io/library/redis:7
      imageID: docker.io/library/redis@sha256:976ab0f0939b78c06d802bcd3bea44c5142ea9a756a57b861e08819817f768c8
      lastState: {}
      name: redis-cli
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3f28b607396d46b6c92b7fe49cf7eb487e24e550758d435db64ba19ac24371d2
          exitCode: 0
          finishedAt: "2026-02-08T18:59:33Z"
          reason: Completed
          startedAt: "2026-02-08T17:59:33Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Succeeded
    podIP: 10.244.2.194
    podIPs:
    - ip: 10.244.2.194
    qosClass: BestEffort
    startTime: "2026-02-08T17:59:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-15T09:25:58+01:00"
    creationTimestamp: "2026-02-09T23:43:50Z"
    generateName: ingress-nginx-controller-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      controller-revision-hash: 857cf989bb
      helm.sh/chart: ingress-nginx-4.14.1
      pod-template-generation: "4"
    name: ingress-nginx-controller-6qdzv
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ingress-nginx-controller
      uid: 85482b5e-ff51-46a9-a591-3f06673347eb
    resourceVersion: "29938820"
    uid: c2958a8c-3cc9-422d-84d6-b2ae6b4c30e2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      - --enable-metrics=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        hostPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        name: https
        protocol: TCP
      - containerPort: 10254
        hostPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8443
        hostPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hx95s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi3075398
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-hx95s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T22:59:01Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6c3ab3442366666004615e7710b00ebde7c73ef6ee0097af5e54ed2740ff30f4
      image: sha256:8043403e50094a07ba382a116497ecfb317f3196e9c8063c89be209f4f654810
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      lastState:
        terminated:
          containerID: containerd://92fd74a79e01cf8b399542e9c9531bcdf863a2c1b5490f0ac046a7f5ebd6c1b7
          exitCode: 0
          finishedAt: "2026-02-13T22:58:59Z"
          reason: Completed
          startedAt: "2026-02-13T22:58:46Z"
      name: controller
      ready: true
      restartCount: 1092
      started: true
      state:
        running:
          startedAt: "2026-02-13T23:04:00Z"
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 144.91.115.90
    podIPs:
    - ip: 144.91.115.90
    qosClass: Burstable
    startTime: "2026-02-09T23:43:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-15T09:25:58+01:00"
    creationTimestamp: "2026-01-15T22:20:14Z"
    generateName: ingress-nginx-controller-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      controller-revision-hash: 857cf989bb
      helm.sh/chart: ingress-nginx-4.14.1
      pod-template-generation: "4"
    name: ingress-nginx-controller-fgrwr
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ingress-nginx-controller
      uid: 85482b5e-ff51-46a9-a591-3f06673347eb
    resourceVersion: "26438765"
    uid: e696c971-a1e0-416e-a6f1-05c537aa95c1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      - --enable-metrics=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        hostPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        name: https
        protocol: TCP
      - containerPort: 10254
        hostPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8443
        hostPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s6lrt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2911680
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-s6lrt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:20:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:20:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:20:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ec527885ed7b80ec6692cf6a55e09854815d5f71f587e78404254ed0ae86d2b3
      image: sha256:8043403e50094a07ba382a116497ecfb317f3196e9c8063c89be209f4f654810
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      lastState:
        terminated:
          containerID: containerd://3872642ad6f79855e7822e82e079bf7c538b4856ca52e44dd96cdfc8444f8e6a
          exitCode: 0
          finishedAt: "2026-01-22T16:58:02Z"
          reason: Completed
          startedAt: "2026-01-15T22:20:15Z"
      name: controller
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:03Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 158.220.101.216
    podIPs:
    - ip: 158.220.101.216
    qosClass: Burstable
    startTime: "2026-01-15T22:20:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-15T09:25:58+01:00"
    creationTimestamp: "2026-01-15T22:19:18Z"
    generateName: ingress-nginx-controller-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      controller-revision-hash: 857cf989bb
      helm.sh/chart: ingress-nginx-4.14.1
      pod-template-generation: "4"
    name: ingress-nginx-controller-g2q2z
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ingress-nginx-controller
      uid: 85482b5e-ff51-46a9-a591-3f06673347eb
    resourceVersion: "26435517"
    uid: b25eef14-789f-44c7-b0ca-9e4b5d1d5954
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      - --enable-metrics=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        hostPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        name: https
        protocol: TCP
      - containerPort: 10254
        hostPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8443
        hostPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ckn6r
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2911681
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-ckn6r
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:19:19Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:19:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:19:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1e3776458d5387962f0c136fc04d4f90d8909962fd95b05816ff38ab8df19985
      image: sha256:8043403e50094a07ba382a116497ecfb317f3196e9c8063c89be209f4f654810
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      lastState: {}
      name: controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-15T22:19:19Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 84.247.138.35
    podIPs:
    - ip: 84.247.138.35
    qosClass: Burstable
    startTime: "2026-01-15T22:19:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-15T09:25:58+01:00"
    creationTimestamp: "2026-01-15T22:18:26Z"
    generateName: ingress-nginx-controller-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      controller-revision-hash: 857cf989bb
      helm.sh/chart: ingress-nginx-4.14.1
      pod-template-generation: "4"
    name: ingress-nginx-controller-nkr97
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ingress-nginx-controller
      uid: 85482b5e-ff51-46a9-a591-3f06673347eb
    resourceVersion: "29827507"
    uid: 1c27616b-af9f-4ca0-8262-c260d9d35850
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      - --enable-metrics=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        hostPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        name: https
        protocol: TCP
      - containerPort: 10254
        hostPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8443
        hostPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6x4rm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2092350.contaboserver.net
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-6x4rm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:18:56Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:18:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T16:15:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T16:15:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:18:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://978aaa8a480b2ab7355f3d22cc584c20a863d4bd42f360b72df38a651edeaac4
      image: sha256:8043403e50094a07ba382a116497ecfb317f3196e9c8063c89be209f4f654810
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      lastState: {}
      name: controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-15T22:18:56Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 37.60.241.150
    podIPs:
    - ip: 37.60.241.150
    qosClass: Burstable
    startTime: "2026-01-15T22:18:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-15T09:25:58+01:00"
    creationTimestamp: "2026-01-15T22:19:41Z"
    generateName: ingress-nginx-controller-
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      controller-revision-hash: 857cf989bb
      helm.sh/chart: ingress-nginx-4.14.1
      pod-template-generation: "4"
    name: ingress-nginx-controller-q7lzh
    namespace: ingress-nginx
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ingress-nginx-controller
      uid: 85482b5e-ff51-46a9-a591-3f06673347eb
    resourceVersion: "26775049"
    uid: 680380dd-5541-42d2-bb01-ade62e54402c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    automountServiceAccountToken: true
    containers:
    - args:
      - /nginx-ingress-controller
      - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
      - --election-id=ingress-nginx-leader
      - --controller-class=k8s.io/ingress-nginx
      - --ingress-class=nginx
      - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
      - --validating-webhook=:8443
      - --validating-webhook-certificate=/usr/local/certificates/cert
      - --validating-webhook-key=/usr/local/certificates/key
      - --enable-metrics=true
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_PRELOAD
        value: /usr/local/lib/libmimalloc.so
      image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /wait-shutdown
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: controller
      ports:
      - containerPort: 80
        hostPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        name: https
        protocol: TCP
      - containerPort: 10254
        hostPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 8443
        hostPort: 8443
        name: webhook
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsGroup: 82
        runAsNonRoot: true
        runAsUser: 101
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/certificates/
        name: webhook-cert
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-grtqt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi3002938
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ingress-nginx
    serviceAccountName: ingress-nginx
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: webhook-cert
      secret:
        defaultMode: 420
        secretName: ingress-nginx-admission
    - name: kube-api-access-grtqt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:19:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:19:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T22:19:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://da719b05e8aa813fb48da5286f559e37d46031d318187813420e9ee78846f3f2
      image: sha256:8043403e50094a07ba382a116497ecfb317f3196e9c8063c89be209f4f654810
      imageID: registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
      lastState: {}
      name: controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-15T22:19:42Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 89.117.60.74
    podIPs:
    - ip: 89.117.60.74
    qosClass: Burstable
    startTime: "2026-01-15T22:19:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-01-30T23:42:13+01:00"
    creationTimestamp: "2026-02-06T15:57:10Z"
    generateName: apicurio-registry-69fbd98587-
    labels:
      app: apicurio-registry
      component: kafka
      pod-template-hash: 69fbd98587
    name: apicurio-registry-69fbd98587-n7zlj
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: apicurio-registry-69fbd98587
      uid: fb4370e6-77db-4065-a8cf-5a91b26ef094
    resourceVersion: "27243887"
    uid: 9539f58e-d54c-4900-9cb9-a648e5a8104d
  spec:
    containers:
    - env:
      - name: REGISTRY_KAFKASQL_BOOTSTRAP_SERVERS
        value: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
      - name: ENABLE_CCOMPAT_LEGACY_ID_MODE
        value: "true"
      - name: QUARKUS_HTTP_SSL_CERTIFICATE_FILES
        value: /etc/tls/tls.crt
      - name: QUARKUS_HTTP_SSL_CERTIFICATE_KEY_FILES
        value: /etc/tls/tls.key
      - name: QUARKUS_HTTP_SSL_PORT
        value: "8443"
      - name: QUARKUS_HTTP_INSECURE_REQUESTS
        value: enabled
      - name: QUARKUS_OTEL_ENABLED
        value: "true"
      - name: QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT
        value: http://otel-collector.observability.svc.cluster.local:4317
      - name: QUARKUS_MICROMETER_EXPORT_PROMETHEUS_ENABLED
        value: "true"
      - name: QUARKUS_MICROMETER_BINDER_JVM_ENABLED
        value: "true"
      - name: QUARKUS_MICROMETER_BINDER_HTTP_SERVER_ENABLED
        value: "true"
      - name: QUARKUS_HTTP_PORT
        value: "8080"
      - name: REGISTRY_CCOMPAT_ENABLED
        value: "true"
      - name: REGISTRY_CCOMPAT-storage-enabled
        value: "true"
      image: apicurio/apicurio-registry-kafkasql:2.5.11.Final
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/live
          port: 8443
          scheme: HTTPS
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: apicurio-registry
      ports:
      - containerPort: 8443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/ready
          port: 8443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/tls
        name: tls-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m6wmt
        readOnly: true
    - command:
      - /bin/sh
      - -c
      - |
        # Instalar dependencias minimas
        pip install --quiet --no-cache-dir requests urllib3

        # Criar servidor HTTP de health check
        cat > /tmp/health_server.py << 'PYTHON_EOF'
        import http.server
        import socketserver
        import json
        import requests
        import threading
        import time
        import urllib3

        # Disable SSL verification for localhost (self-signed cert)
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        # Estado global do health check
        health_state = {
            "schemas_ok": False,
            "registry_ready": False,
            "missing_schemas": [],
            "last_check": None
        }

        CRITICAL_SCHEMAS = ["plans.ready-value", "execution.tickets-value"]
        REGISTRY_URL = "https://localhost:8443"

        def check_schemas():
            """Verifica schemas criticos periodicamente"""
            global health_state
            while True:
                try:
                    # Verificar se registry esta pronto
                    try:
                        resp = requests.get(f"{REGISTRY_URL}/health/ready", timeout=5, verify=False)
                        health_state["registry_ready"] = resp.status_code == 200
                    except:
                        health_state["registry_ready"] = False
                        health_state["schemas_ok"] = False
                        time.sleep(30)
                        continue

                    # Verificar schemas criticos
                    missing = []
                    for subject in CRITICAL_SCHEMAS:
                        try:
                            resp = requests.get(
                                f"{REGISTRY_URL}/apis/ccompat/v6/subjects/{subject}/versions/latest",
                                timeout=5,
                                verify=False
                            )
                            if resp.status_code != 200:
                                missing.append(subject)
                        except:
                            missing.append(subject)

                    health_state["missing_schemas"] = missing
                    health_state["schemas_ok"] = len(missing) == 0
                    health_state["last_check"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

                    status = "OK" if health_state["schemas_ok"] else "DEGRADED"
                    print(f"[{health_state['last_check']}] Schema check: {status}, missing: {missing}")

                except Exception as e:
                    print(f"Error checking schemas: {e}")

                time.sleep(60)

        class HealthHandler(http.server.BaseHTTPRequestHandler):
            def log_message(self, format, *args):
                pass  # Silenciar logs de requisicao

            def do_GET(self):
                if self.path == "/health/schemas":
                    if health_state["schemas_ok"]:
                        self.send_response(200)
                        status = "healthy"
                    else:
                        self.send_response(503)
                        status = "unhealthy"

                    self.send_header("Content-Type", "application/json")
                    self.end_headers()
                    response = {
                        "status": status,
                        "registry_ready": health_state["registry_ready"],
                        "schemas_ok": health_state["schemas_ok"],
                        "missing_schemas": health_state["missing_schemas"],
                        "critical_schemas": CRITICAL_SCHEMAS,
                        "last_check": health_state["last_check"]
                    }
                    self.wfile.write(json.dumps(response).encode())

                elif self.path == "/health/live":
                    self.send_response(200)
                    self.send_header("Content-Type", "text/plain")
                    self.end_headers()
                    self.wfile.write(b"OK")

                else:
                    self.send_response(404)
                    self.end_headers()

        if __name__ == "__main__":
            # Iniciar thread de verificacao de schemas
            checker_thread = threading.Thread(target=check_schemas, daemon=True)
            checker_thread.start()

            # Iniciar servidor HTTP
            PORT = 8090
            with socketserver.TCPServer(("", PORT), HealthHandler) as httpd:
                print(f"Schema health server running on port {PORT}")
                httpd.serve_forever()
        PYTHON_EOF

        python /tmp/health_server.py
      env:
      - name: QUARKUS_HTTP_INSECURE_REQUESTS
        value: enabled
      - name: QUARKUS_HTTP_PORT
        value: "8080"
      - name: REGISTRY_CCOMPAT_ENABLED
        value: "true"
      - name: REGISTRY_CCOMPAT-storage-enabled
        value: "true"
      image: python:3.11-alpine
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/live
          port: 8090
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: schema-health-checker
      ports:
      - containerPort: 8090
        name: health
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 20m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-m6wmt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-certs
      secret:
        defaultMode: 420
        secretName: schema-registry-tls-secret
    - name: kube-api-access-m6wmt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:57:41Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:57:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:58:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:58:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:57:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d04b56d058f217a3b6676a07587a1ececf9fa5100cdfedf500454776f340ea93
      image: docker.io/apicurio/apicurio-registry-kafkasql:2.5.11.Final
      imageID: docker.io/apicurio/apicurio-registry-kafkasql@sha256:9af5ca1d203ccf68625e05d4f6dd73a5802753c850b2e40363ee5ebf935b365c
      lastState: {}
      name: apicurio-registry
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-06T15:57:34Z"
    - containerID: containerd://89f24dab49c5d572e639d17c46e9f5e15ec7b745e3bf37c8ac27c2cecc6b56a0
      image: docker.io/library/python:3.11-alpine
      imageID: docker.io/library/python@sha256:6ce68f8bfbb40866c43b271be97d7fccc4f700c0af2a07d2ef3c7c7da93e1f8a
      lastState: {}
      name: schema-health-checker
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-06T15:57:40Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.9
    podIPs:
    - ip: 10.244.1.9
    qosClass: Burstable
    startTime: "2026-02-06T15:57:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      strimzi.io/clients-ca-cert-generation: "0"
      strimzi.io/cluster-ca-cert-generation: "0"
      strimzi.io/cluster-ca-key-generation: "0"
      strimzi.io/configuration-hash: 511083a2
      strimzi.io/kafka-version: 4.1.0
      strimzi.io/revision: 7a875412
      strimzi.io/server-cert-hash: d1c03b9dfd9fcf945f9e71e6c3bb377c55956a43
    creationTimestamp: "2026-01-22T17:11:23Z"
    labels:
      app.kubernetes.io/instance: neural-hive-kafka
      app.kubernetes.io/managed-by: strimzi-cluster-operator
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: strimzi-neural-hive-kafka
      strimzi.io/broker-role: "true"
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/component-type: kafka
      strimzi.io/controller: strimzipodset
      strimzi.io/controller-name: neural-hive-kafka-broker
      strimzi.io/controller-role: "false"
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-kafka
      strimzi.io/pod-name: neural-hive-kafka-broker-0
      strimzi.io/pool-name: broker
    name: neural-hive-kafka-broker-0
    namespace: kafka
    ownerReferences:
    - apiVersion: core.strimzi.io/v1beta2
      blockOwnerDeletion: false
      controller: true
      kind: StrimziPodSet
      name: neural-hive-kafka-broker
      uid: e7835a3e-a959-4058-aca7-629cf6aaa629
    resourceVersion: "26775039"
    uid: 4c7baf0c-f409-425e-95b5-276b739c2cd9
  spec:
    affinity: {}
    containers:
    - args:
      - /opt/kafka/kafka_run.sh
      env:
      - name: KAFKA_JMX_EXPORTER_ENABLED
        value: "false"
      - name: STRIMZI_KAFKA_GC_LOG_ENABLED
        value: "false"
      - name: STRIMZI_DYNAMIC_HEAP_PERCENTAGE
        value: "50"
      - name: STRIMZI_DYNAMIC_HEAP_MAX
        value: "5368709120"
      image: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /opt/kafka/kafka_liveness.sh
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kafka
      ports:
      - containerPort: 8443
        name: tcp-kafkaagent
        protocol: TCP
      - containerPort: 9091
        name: tcp-replication
        protocol: TCP
      - containerPort: 9092
        name: tcp-clients
        protocol: TCP
      - containerPort: 9093
        name: tcp-clientstls
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /opt/kafka/kafka_readiness.sh
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kafka/data
        name: data
      - mountPath: /tmp
        name: strimzi-tmp
      - mountPath: /opt/kafka/cluster-ca-certs
        name: cluster-ca
      - mountPath: /opt/kafka/broker-certs
        name: broker-certs
      - mountPath: /opt/kafka/client-ca-certs
        name: client-ca-cert
      - mountPath: /opt/kafka/custom-config/
        name: kafka-metrics-and-logging
      - mountPath: /var/opt/kafka
        name: ready-files
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q7rtv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: neural-hive-kafka-broker-0
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: neural-hive-kafka-kafka
    serviceAccountName: neural-hive-kafka-kafka
    subdomain: neural-hive-kafka-kafka-brokers
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir:
        medium: Memory
        sizeLimit: 5Mi
      name: strimzi-tmp
    - name: cluster-ca
      secret:
        defaultMode: 292
        secretName: neural-hive-kafka-cluster-ca-cert
    - name: broker-certs
      secret:
        defaultMode: 292
        secretName: neural-hive-kafka-broker-0
    - name: client-ca-cert
      secret:
        defaultMode: 292
        secretName: neural-hive-kafka-clients-ca-cert
    - configMap:
        defaultMode: 420
        name: neural-hive-kafka-broker-0
      name: kafka-metrics-and-logging
    - emptyDir:
        medium: Memory
        sizeLimit: 1Ki
      name: ready-files
    - name: kube-api-access-q7rtv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T17:11:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T17:11:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T17:11:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://177bb03e0761fc91f7aa2958ba0d739dbedf8b7cfa56408fcfba689b0c712518
      image: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      imageID: quay.io/strimzi/kafka@sha256:34450afc1a3399a9026fc3c6ec90e9f6adfec9faf6520545b4d84e90cda34964
      lastState: {}
      name: kafka
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T17:11:24Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.76
    podIPs:
    - ip: 10.244.3.76
    qosClass: Burstable
    startTime: "2026-01-22T17:11:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      strimzi.io/clients-ca-cert-generation: "0"
      strimzi.io/cluster-ca-cert-generation: "0"
      strimzi.io/cluster-ca-key-generation: "0"
      strimzi.io/configuration-hash: ac7dc67c
      strimzi.io/kafka-version: 4.1.0
      strimzi.io/revision: 94ae8f4a
      strimzi.io/server-cert-hash: c0def81ca8844854aec81d9a595395d4362edf06
    creationTimestamp: "2026-01-22T16:58:28Z"
    labels:
      app.kubernetes.io/instance: neural-hive-kafka
      app.kubernetes.io/managed-by: strimzi-cluster-operator
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: strimzi-neural-hive-kafka
      strimzi.io/broker-role: "false"
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/component-type: kafka
      strimzi.io/controller: strimzipodset
      strimzi.io/controller-name: neural-hive-kafka-controller
      strimzi.io/controller-role: "true"
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-kafka
      strimzi.io/pod-name: neural-hive-kafka-controller-1
      strimzi.io/pool-name: controller
    name: neural-hive-kafka-controller-1
    namespace: kafka
    ownerReferences:
    - apiVersion: core.strimzi.io/v1beta2
      blockOwnerDeletion: false
      controller: true
      kind: StrimziPodSet
      name: neural-hive-kafka-controller
      uid: 3f517f7a-d6d3-4e53-9c54-065c38445bac
    resourceVersion: "26438799"
    uid: 904d193c-9e1f-40eb-9989-9ad8cc9d0a65
  spec:
    affinity: {}
    containers:
    - args:
      - /opt/kafka/kafka_run.sh
      env:
      - name: KAFKA_JMX_EXPORTER_ENABLED
        value: "false"
      - name: STRIMZI_KAFKA_GC_LOG_ENABLED
        value: "false"
      - name: STRIMZI_DYNAMIC_HEAP_PERCENTAGE
        value: "50"
      - name: STRIMZI_DYNAMIC_HEAP_MAX
        value: "5368709120"
      image: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /opt/kafka/kafka_liveness.sh
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kafka
      ports:
      - containerPort: 8443
        name: tcp-kafkaagent
        protocol: TCP
      - containerPort: 9090
        name: tcp-ctrlplane
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /opt/kafka/kafka_readiness.sh
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kafka/data
        name: data
      - mountPath: /tmp
        name: strimzi-tmp
      - mountPath: /opt/kafka/cluster-ca-certs
        name: cluster-ca
      - mountPath: /opt/kafka/broker-certs
        name: broker-certs
      - mountPath: /opt/kafka/client-ca-certs
        name: client-ca-cert
      - mountPath: /opt/kafka/custom-config/
        name: kafka-metrics-and-logging
      - mountPath: /var/opt/kafka
        name: ready-files
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hwbhc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: neural-hive-kafka-controller-1
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: neural-hive-kafka-kafka
    serviceAccountName: neural-hive-kafka-kafka
    subdomain: neural-hive-kafka-kafka-brokers
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: data
    - emptyDir:
        medium: Memory
        sizeLimit: 5Mi
      name: strimzi-tmp
    - name: cluster-ca
      secret:
        defaultMode: 292
        secretName: neural-hive-kafka-cluster-ca-cert
    - name: broker-certs
      secret:
        defaultMode: 292
        secretName: neural-hive-kafka-controller-1
    - name: client-ca-cert
      secret:
        defaultMode: 292
        secretName: neural-hive-kafka-clients-ca-cert
    - configMap:
        defaultMode: 420
        name: neural-hive-kafka-controller-1
      name: kafka-metrics-and-logging
    - emptyDir:
        medium: Memory
        sizeLimit: 1Ki
      name: ready-files
    - name: kube-api-access-hwbhc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:29Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://52df20d11881996c860e9d4ad3bfb13e6400b79d42b9e4a493795f2940e5758f
      image: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      imageID: quay.io/strimzi/kafka@sha256:34450afc1a3399a9026fc3c6ec90e9f6adfec9faf6520545b4d84e90cda34964
      lastState: {}
      name: kafka
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:29Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.33
    podIPs:
    - ip: 10.244.1.33
    qosClass: Burstable
    startTime: "2026-01-22T16:58:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      strimzi.io/cluster-ca-cert-generation: "0"
      strimzi.io/cluster-ca-key-generation: "0"
      strimzi.io/server-cert-hash: 37244d10
    creationTimestamp: "2026-01-13T13:32:01Z"
    generateName: neural-hive-kafka-entity-operator-67df88987b-
    labels:
      app.kubernetes.io/instance: neural-hive-kafka
      app.kubernetes.io/managed-by: strimzi-cluster-operator
      app.kubernetes.io/name: entity-operator
      app.kubernetes.io/part-of: strimzi-neural-hive-kafka
      pod-template-hash: 67df88987b
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/component-type: entity-operator
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-entity-operator
    name: neural-hive-kafka-entity-operator-67df88987b-nkl8w
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: neural-hive-kafka-entity-operator-67df88987b
      uid: c8f9eef1-d6ff-4406-adf6-5c8388aa487d
    resourceVersion: "26775197"
    uid: 058bbf8e-1e05-427c-a556-3edfc623211d
  spec:
    containers:
    - args:
      - /opt/strimzi/bin/topic_operator_run.sh
      env:
      - name: STRIMZI_RESOURCE_LABELS
        value: strimzi.io/cluster=neural-hive-kafka
      - name: STRIMZI_KAFKA_BOOTSTRAP_SERVERS
        value: neural-hive-kafka-kafka-bootstrap:9091
      - name: STRIMZI_NAMESPACE
        value: kafka
      - name: STRIMZI_SECURITY_PROTOCOL
        value: SSL
      - name: STRIMZI_TLS_ENABLED
        value: "true"
      - name: STRIMZI_GC_LOG_ENABLED
        value: "false"
      image: quay.io/strimzi/operator:0.48.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthy
          port: healthcheck
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: topic-operator
      ports:
      - containerPort: 8080
        name: healthcheck
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: healthcheck
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 200m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      startupProbe:
        failureThreshold: 12
        httpGet:
          path: /healthy
          port: healthcheck
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: strimzi-to-tmp
      - mountPath: /opt/topic-operator/custom-config/
        name: entity-topic-operator-metrics-and-logging
      - mountPath: /etc/eto-certs/
        name: eto-certs
      - mountPath: /etc/tls-sidecar/cluster-ca-certs/
        name: cluster-ca-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-h54nk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: neural-hive-kafka-entity-operator
    serviceAccountName: neural-hive-kafka-entity-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: neural-hive-kafka-entity-topic-operator-config
      name: entity-topic-operator-metrics-and-logging
    - emptyDir:
        medium: Memory
        sizeLimit: 5Mi
      name: strimzi-to-tmp
    - name: eto-certs
      secret:
        defaultMode: 292
        secretName: neural-hive-kafka-entity-topic-operator-certs
    - name: cluster-ca-certs
      secret:
        defaultMode: 292
        secretName: neural-hive-kafka-cluster-ca-cert
    - name: kube-api-access-h54nk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:32:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:32:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:32:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2f7d229cd096b0d1517b479c0f4e5cd1329915f68a4f50031f8ca8e5bae41d2a
      image: quay.io/strimzi/operator:0.48.0
      imageID: quay.io/strimzi/operator@sha256:73dc9555c4a73094b497ffb14d816de5ff40144ffc470efd75ccf128afa22778
      lastState:
        terminated:
          containerID: containerd://77003cbdf44cfd2dc1e7d72aac3a829bb41aecfb0c34a50280fe94b994619d4d
          exitCode: 0
          finishedAt: "2026-02-01T13:46:51Z"
          reason: Completed
          startedAt: "2026-01-13T13:32:05Z"
      name: topic-operator
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-02-01T13:46:52Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.201
    podIPs:
    - ip: 10.244.3.201
    qosClass: Burstable
    startTime: "2026-01-13T13:32:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-13T13:31:49Z"
    generateName: strimzi-cluster-operator-fd565f467-
    labels:
      name: strimzi-cluster-operator
      pod-template-hash: fd565f467
      strimzi.io/kind: cluster-operator
    name: strimzi-cluster-operator-fd565f467-gm9t4
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: strimzi-cluster-operator-fd565f467
      uid: 52b787ef-f045-43ae-a16c-d3224d156421
    resourceVersion: "26775256"
    uid: 6fed55da-a890-4da1-a076-a9d127c2d693
  spec:
    containers:
    - args:
      - /opt/strimzi/bin/cluster_operator_run.sh
      env:
      - name: STRIMZI_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS
        value: "120000"
      - name: STRIMZI_OPERATION_TIMEOUT_MS
        value: "300000"
      - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE
        value: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE
        value: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      - name: STRIMZI_KAFKA_IMAGES
        value: |
          4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
          4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      - name: STRIMZI_KAFKA_CONNECT_IMAGES
        value: |
          4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
          4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES
        value: |
          4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
          4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
      - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE
        value: quay.io/strimzi/operator:0.48.0
      - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE
        value: quay.io/strimzi/operator:0.48.0
      - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE
        value: quay.io/strimzi/operator:0.48.0
      - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE
        value: quay.io/strimzi/kafka-bridge:0.33.1
      - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE
        value: quay.io/strimzi/kaniko-executor:0.48.0
      - name: STRIMZI_DEFAULT_MAVEN_BUILDER
        value: quay.io/strimzi/maven-builder:0.48.0
      - name: STRIMZI_OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: STRIMZI_FEATURE_GATES
      - name: STRIMZI_LEADER_ELECTION_ENABLED
        value: "true"
      - name: STRIMZI_LEADER_ELECTION_LEASE_NAME
        value: strimzi-cluster-operator
      - name: STRIMZI_LEADER_ELECTION_LEASE_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: STRIMZI_LEADER_ELECTION_IDENTITY
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: quay.io/strimzi/operator:0.48.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthy
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      name: strimzi-cluster-operator
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 384Mi
        requests:
          cpu: 200m
          memory: 384Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: strimzi-tmp
      - mountPath: /opt/strimzi/custom-config/
        name: co-config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kbm69
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: strimzi-cluster-operator
    serviceAccountName: strimzi-cluster-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 1Mi
      name: strimzi-tmp
    - configMap:
        defaultMode: 420
        name: strimzi-cluster-operator
      name: co-config-volume
    - name: kube-api-access-kbm69
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:32:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:31:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:31:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://683131d7153e35242de329f5d50991073f297e0c7415247727d2c07a11362b1d
      image: quay.io/strimzi/operator:0.48.0
      imageID: quay.io/strimzi/operator@sha256:73dc9555c4a73094b497ffb14d816de5ff40144ffc470efd75ccf128afa22778
      lastState:
        terminated:
          containerID: containerd://5a4fb9fbb50322bd10665e8a0c9cb7143b7730f5f9b193c6939e45037cc1e41d
          exitCode: 1
          finishedAt: "2026-02-01T13:46:43Z"
          reason: Error
          startedAt: "2026-01-22T16:54:38Z"
      name: strimzi-cluster-operator
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2026-02-01T13:46:49Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.200
    podIPs:
    - ip: 10.244.3.200
    qosClass: Burstable
    startTime: "2026-01-13T13:31:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 7206792e292638a83360af2cccaf0cf0710804f5815079b6f460602bfcdb47f6
      checksum/secret: 3859a7c073bbb858b1d0db6e1eb821fefc09594aadca3657309caed214199e81
      neural-hive.io/data-classification: confidential
      neural-hive.io/monitoring: enabled
      prometheus.io/path: /metrics
      prometheus.io/port: "9000"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-11T10:25:06Z"
    generateName: keycloak-5cdb4cbfd-
    labels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
      neural-hive.io/component: auth
      neural-hive.io/layer: security
      pod-template-hash: 5cdb4cbfd
    name: keycloak-5cdb4cbfd-6d965
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: keycloak-5cdb4cbfd
      uid: 840e3d53-fc2b-47d7-870e-564fa09c90dd
    resourceVersion: "29939630"
    uid: 7a27ec10-4885-4e6e-ab9f-b69bfe784367
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: keycloak
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - args:
      - start
      - --import-realm
      env:
      - name: KEYCLOAK_ADMIN
        value: admin
      - name: KEYCLOAK_ADMIN_PASSWORD
        value: Admin123\!
      - name: KC_DB_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: keycloak-postgresql
      - name: KC_HEALTH_ENABLED
        value: "true"
      - name: KC_METRICS_ENABLED
        value: "true"
      - name: KC_LOG_LEVEL
        value: INFO
      - name: JAVA_OPTS_APPEND
        value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
      image: quay.io/keycloak/keycloak:22.0.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/live
          port: http
          scheme: HTTP
        initialDelaySeconds: 300
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: keycloak
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8443
        name: https
        protocol: TCP
      - containerPort: 9000
        name: management
        protocol: TCP
      - containerPort: 9000
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/keycloak/conf/keycloak.conf
        name: keycloak-config
        readOnly: true
        subPath: keycloak.conf
      - mountPath: /opt/keycloak/data/import/realm.json
        name: realm-import
        readOnly: true
        subPath: realm.json
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4t2pg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - sh
      - -c
      - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...'; sleep
        2; done; echo 'Database is ready!'
      env:
      - name: KEYCLOAK_ADMIN
        value: admin
      - name: KEYCLOAK_ADMIN_PASSWORD
        value: Admin123\!
      image: busybox:1.35
      imagePullPolicy: IfNotPresent
      name: wait-for-db
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4t2pg
        readOnly: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: keycloak
    serviceAccountName: keycloak
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: keycloak-config
      name: keycloak-config
    - configMap:
        defaultMode: 420
        name: keycloak-realm
      name: realm-import
    - name: kube-api-access-4t2pg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:42Z"
      message: 'containers with unready status: [keycloak]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:42Z"
      message: 'containers with unready status: [keycloak]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ece0ec9b438f16a7b147d708590bb6f01042c32d63cebd5d2e8e488b0ac41f69
      image: quay.io/keycloak/keycloak:22.0.5
      imageID: quay.io/keycloak/keycloak@sha256:bfa8852e52c279f0857fe8da239c0ad6bbd2cc07793a28a6770f7e24c1e25444
      lastState:
        terminated:
          containerID: containerd://ece0ec9b438f16a7b147d708590bb6f01042c32d63cebd5d2e8e488b0ac41f69
          exitCode: 143
          finishedAt: "2026-02-13T23:06:41Z"
          reason: Error
          startedAt: "2026-02-13T23:04:41Z"
      name: keycloak
      ready: false
      restartCount: 662
      started: false
      state:
        waiting:
          message: back-off 1m20s restarting failed container=keycloak pod=keycloak-5cdb4cbfd-6d965_keycloak(7a27ec10-4885-4e6e-ab9f-b69bfe784367)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    initContainerStatuses:
    - containerID: containerd://85accc27af213320fba0a7c5deecec7087726c379851676cd9a7882b8fb4e2b2
      image: docker.io/library/busybox:1.35
      imageID: docker.io/library/busybox@sha256:98ad9d1a2be345201bb0709b0d38655eb1b370145c7d94ca1fe9c421f76e245a
      lastState: {}
      name: wait-for-db
      ready: true
      restartCount: 12
      started: false
      state:
        terminated:
          containerID: containerd://85accc27af213320fba0a7c5deecec7087726c379851676cd9a7882b8fb4e2b2
          exitCode: 0
          finishedAt: "2026-02-13T23:06:43Z"
          reason: Completed
          startedAt: "2026-02-13T23:06:43Z"
    phase: Running
    podIP: 10.244.4.10
    podIPs:
    - ip: 10.244.4.10
    qosClass: Burstable
    startTime: "2026-02-11T10:25:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 7206792e292638a83360af2cccaf0cf0710804f5815079b6f460602bfcdb47f6
      checksum/secret: 3859a7c073bbb858b1d0db6e1eb821fefc09594aadca3657309caed214199e81
      neural-hive.io/data-classification: confidential
      neural-hive.io/monitoring: enabled
      prometheus.io/path: /metrics
      prometheus.io/port: "9000"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-06T15:41:31Z"
    generateName: keycloak-5cdb4cbfd-
    labels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
      neural-hive.io/component: auth
      neural-hive.io/layer: security
      pod-template-hash: 5cdb4cbfd
    name: keycloak-5cdb4cbfd-mpcn6
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: keycloak-5cdb4cbfd
      uid: 840e3d53-fc2b-47d7-870e-564fa09c90dd
    resourceVersion: "27241958"
    uid: 2ac7c4fc-417a-4285-97c2-09f42a36e7d3
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: keycloak
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - args:
      - start
      - --import-realm
      env:
      - name: KEYCLOAK_ADMIN
        value: admin
      - name: KEYCLOAK_ADMIN_PASSWORD
        value: Admin123\!
      - name: KC_DB_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: keycloak-postgresql
      - name: KC_HEALTH_ENABLED
        value: "true"
      - name: KC_METRICS_ENABLED
        value: "true"
      - name: KC_LOG_LEVEL
        value: INFO
      - name: JAVA_OPTS_APPEND
        value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
      image: quay.io/keycloak/keycloak:22.0.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/live
          port: http
          scheme: HTTP
        initialDelaySeconds: 300
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: keycloak
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8443
        name: https
        protocol: TCP
      - containerPort: 9000
        name: management
        protocol: TCP
      - containerPort: 9000
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/keycloak/conf/keycloak.conf
        name: keycloak-config
        readOnly: true
        subPath: keycloak.conf
      - mountPath: /opt/keycloak/data/import/realm.json
        name: realm-import
        readOnly: true
        subPath: realm.json
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-spj4j
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - sh
      - -c
      - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...'; sleep
        2; done; echo 'Database is ready!'
      env:
      - name: KEYCLOAK_ADMIN
        value: admin
      - name: KEYCLOAK_ADMIN_PASSWORD
        value: Admin123\!
      image: busybox:1.35
      imagePullPolicy: IfNotPresent
      name: wait-for-db
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-spj4j
        readOnly: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: keycloak
    serviceAccountName: keycloak
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: keycloak-config
      name: keycloak-config
    - configMap:
        defaultMode: 420
        name: keycloak-realm
      name: realm-import
    - name: kube-api-access-spj4j
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:50:13Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:50:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:50:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:50:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-06T15:50:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://091bb573fd38c4cb6376f61df8e7efb83f99c87c16cb97dd253bc884e771791c
      image: 37.60.241.150:30500/keycloak/keycloak:22.0.5
      imageID: 37.60.241.150:30500/keycloak/keycloak@sha256:9556701767952f4941c877720267bc1fcb7aeb95328721aac0c71281a636d1e0
      lastState: {}
      name: keycloak
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-06T15:50:13Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    initContainerStatuses:
    - containerID: containerd://3102e2febde69769bb3e01b0b7b8127ce890d802f8208f9084c528672f9bb893
      image: docker.io/library/busybox:1.35
      imageID: docker.io/library/busybox@sha256:98ad9d1a2be345201bb0709b0d38655eb1b370145c7d94ca1fe9c421f76e245a
      lastState: {}
      name: wait-for-db
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://3102e2febde69769bb3e01b0b7b8127ce890d802f8208f9084c528672f9bb893
          exitCode: 0
          finishedAt: "2026-02-06T15:50:13Z"
          reason: Completed
          startedAt: "2026-02-06T15:50:12Z"
    phase: Running
    podIP: 10.244.2.110
    podIPs:
    - ip: 10.244.2.110
    qosClass: Burstable
    startTime: "2026-02-06T15:50:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-01-31T16:57:04+01:00"
    creationTimestamp: "2026-01-31T16:08:56Z"
    generateName: keycloak-postgresql-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 15.4.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: keycloak-postgresql-55b558dd
      helm.sh/chart: postgresql-12.12.10
      statefulset.kubernetes.io/pod-name: keycloak-postgresql-0
    name: keycloak-postgresql-0
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: keycloak-postgresql
      uid: e95186f5-8f43-43f3-87b3-dce0712850fb
    resourceVersion: "26775085"
    uid: 481dd968-d6dd-442c-967b-1f8b5016c618
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: keycloak
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: keycloak
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: keycloak-postgresql
      - name: POSTGRES_POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: postgres-password
            name: keycloak-postgresql
      - name: POSTGRES_DATABASE
        value: keycloak
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: docker.io/bitnami/postgresql:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "keycloak" -d "dbname=keycloak" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "keycloak" -d "dbname=keycloak" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        runAsGroup: 0
        runAsNonRoot: true
        runAsUser: 1001
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-552vm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: keycloak-postgresql-0
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: default
    serviceAccountName: default
    subdomain: keycloak-postgresql-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-keycloak-postgresql-0
    - emptyDir:
        medium: Memory
      name: dshm
    - name: kube-api-access-552vm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-31T16:09:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-31T16:08:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-31T16:08:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7e637cabbf50ca1415a6359827518b072aebc4615c5e3f3cca83671d1629a42f
      image: docker.io/bitnami/postgresql:latest
      imageID: docker.io/bitnami/postgresql@sha256:e9d4bdd350e8446c630d84dfd56da4ab1c9779802d47d043948861eff452b895
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-31T16:09:09Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.216
    podIPs:
    - ip: 10.244.3.216
    qosClass: Burstable
    startTime: "2026-01-31T16:08:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-14T23:52:28Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: b954df96
      k8s-app: flannel
      pod-template-generation: "1"
      tier: node
    name: kube-flannel-ds-4cwm7
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: dc2f3b90-3449-443a-b297-6a85f1f02101
    resourceVersion: "23972110"
    uid: a9a9dc26-13f0-487d-8421-5b584f27f9fb
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      - name: CONT_WHEN_CACHE_NOT_READY
        value: "false"
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-97q6r
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-97q6r
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-97q6r
        readOnly: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-97q6r
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:37Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://44b25e8fb33f744fde9638dc4c78063051df00050f8e89cc5d0e3f328b8c3f4e
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: kube-flannel
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T23:52:42Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    initContainerStatuses:
    - containerID: containerd://54fd613d9dd81776b1fab158bbcac4852f26b415d972d574ce8be3997b5a92c4
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imageID: ghcr.io/flannel-io/flannel-cni-plugin@sha256:25bd091c1867d0237432a4bcb5da720f39198b7d80edcae3bdf08262d242985c
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://54fd613d9dd81776b1fab158bbcac4852f26b415d972d574ce8be3997b5a92c4
          exitCode: 0
          finishedAt: "2025-11-14T23:52:36Z"
          reason: Completed
          startedAt: "2025-11-14T23:52:36Z"
    - containerID: containerd://e24ebc3f13b6a4569e04d3b25a496b5c0685a2b3bcea2e78003e6d78b05cd901
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://e24ebc3f13b6a4569e04d3b25a496b5c0685a2b3bcea2e78003e6d78b05cd901
          exitCode: 0
          finishedAt: "2025-11-14T23:52:41Z"
          reason: Completed
          startedAt: "2025-11-14T23:52:41Z"
    phase: Running
    podIP: 84.247.138.35
    podIPs:
    - ip: 84.247.138.35
    qosClass: Burstable
    startTime: "2025-11-14T23:52:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-09T23:43:34Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: b954df96
      k8s-app: flannel
      pod-template-generation: "1"
      tier: node
    name: kube-flannel-ds-52mcj
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: dc2f3b90-3449-443a-b297-6a85f1f02101
    resourceVersion: "29939230"
    uid: 528fd42c-3f85-4fc3-a647-3c7b7a08c9dd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      - name: CONT_WHEN_CACHE_NOT_READY
        value: "false"
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dbvkp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dbvkp
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dbvkp
        readOnly: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-dbvkp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:30Z"
      message: 'containers with unready status: [kube-flannel]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:30Z"
      message: 'containers with unready status: [kube-flannel]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a6dc4e386f9cb7c4847c5fb45036077caa2007fe0e3b300d808fca1826efdd64
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState:
        terminated:
          containerID: containerd://a6dc4e386f9cb7c4847c5fb45036077caa2007fe0e3b300d808fca1826efdd64
          exitCode: 0
          finishedAt: "2026-02-13T23:05:29Z"
          reason: Completed
          startedAt: "2026-02-13T23:00:06Z"
      name: kube-flannel
      ready: false
      restartCount: 1033
      started: false
      state:
        waiting:
          message: back-off 2m40s restarting failed container=kube-flannel pod=kube-flannel-ds-52mcj_kube-flannel(528fd42c-3f85-4fc3-a647-3c7b7a08c9dd)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    initContainerStatuses:
    - containerID: containerd://86f87b9b52860c5c4424f17e08e8162015ab98100cef393fa33e239181a49025
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imageID: ghcr.io/flannel-io/flannel-cni-plugin@sha256:25bd091c1867d0237432a4bcb5da720f39198b7d80edcae3bdf08262d242985c
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 1
      started: false
      state:
        terminated:
          containerID: containerd://86f87b9b52860c5c4424f17e08e8162015ab98100cef393fa33e239181a49025
          exitCode: 0
          finishedAt: "2026-02-13T23:05:30Z"
          reason: Completed
          startedAt: "2026-02-13T23:05:30Z"
    - containerID: containerd://f0946511a12639c98a92c13b270d94733beb4578892b68058d7e3a3b94252005
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://f0946511a12639c98a92c13b270d94733beb4578892b68058d7e3a3b94252005
          exitCode: 0
          finishedAt: "2026-02-13T23:05:31Z"
          reason: Completed
          startedAt: "2026-02-13T23:05:31Z"
    phase: Running
    podIP: 144.91.115.90
    podIPs:
    - ip: 144.91.115.90
    qosClass: Burstable
    startTime: "2026-02-09T23:43:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-14T23:52:28Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: b954df96
      k8s-app: flannel
      pod-template-generation: "1"
      tier: node
    name: kube-flannel-ds-jgmhd
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: dc2f3b90-3449-443a-b297-6a85f1f02101
    resourceVersion: "22400152"
    uid: 84bf0870-1b5d-45e5-bdbe-2dfe0118251a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      - name: CONT_WHEN_CACHE_NOT_READY
        value: "false"
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p9tn5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p9tn5
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p9tn5
        readOnly: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-p9tn5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:52:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b74abea659c1e04946f89bf4e7e9fd99f6ee903edbbbf35211a4257386f2a260
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: kube-flannel
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T23:52:35Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    initContainerStatuses:
    - containerID: containerd://9c49d30abc2688704bb6f5f1d5489379ec48bf86b137d8b3fc8f21e69679782e
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imageID: ghcr.io/flannel-io/flannel-cni-plugin@sha256:25bd091c1867d0237432a4bcb5da720f39198b7d80edcae3bdf08262d242985c
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://9c49d30abc2688704bb6f5f1d5489379ec48bf86b137d8b3fc8f21e69679782e
          exitCode: 0
          finishedAt: "2025-11-14T23:52:31Z"
          reason: Completed
          startedAt: "2025-11-14T23:52:31Z"
    - containerID: containerd://73b6b470f2069aa6900775a595d18b387c59913c404d44c4dc3ff417307beadc
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://73b6b470f2069aa6900775a595d18b387c59913c404d44c4dc3ff417307beadc
          exitCode: 0
          finishedAt: "2025-11-14T23:52:35Z"
          reason: Completed
          startedAt: "2025-11-14T23:52:35Z"
    phase: Running
    podIP: 158.220.101.216
    podIPs:
    - ip: 158.220.101.216
    qosClass: Burstable
    startTime: "2025-11-14T23:52:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-03T20:54:30Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: b954df96
      k8s-app: flannel
      pod-template-generation: "1"
      tier: node
    name: kube-flannel-ds-mk4fz
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: dc2f3b90-3449-443a-b297-6a85f1f02101
    resourceVersion: "25557603"
    uid: 21e21d71-c533-4e3a-a25c-50eff8929fcd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      - name: CONT_WHEN_CACHE_NOT_READY
        value: "false"
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6tr92
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6tr92
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6tr92
        readOnly: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-6tr92
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7c6e7a82192064423b5947b4ac22af050bee6a666074a8d6631f547cf8cf40a2
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: kube-flannel
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:54:43Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    initContainerStatuses:
    - containerID: containerd://bbf24f1eb21ee70dd361b58c6dc1ede98feff8d93c138279b1f9ab0d828eb05a
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imageID: ghcr.io/flannel-io/flannel-cni-plugin@sha256:25bd091c1867d0237432a4bcb5da720f39198b7d80edcae3bdf08262d242985c
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://bbf24f1eb21ee70dd361b58c6dc1ede98feff8d93c138279b1f9ab0d828eb05a
          exitCode: 0
          finishedAt: "2026-01-03T20:54:38Z"
          reason: Completed
          startedAt: "2026-01-03T20:54:38Z"
    - containerID: containerd://1c17d062a5c03d483550d47486d0618db8f824910bcc4e1384bb0d466d041487
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://1c17d062a5c03d483550d47486d0618db8f824910bcc4e1384bb0d466d041487
          exitCode: 0
          finishedAt: "2026-01-03T20:54:42Z"
          reason: Completed
          startedAt: "2026-01-03T20:54:42Z"
    phase: Running
    podIP: 89.117.60.74
    podIPs:
    - ip: 89.117.60.74
    qosClass: Burstable
    startTime: "2026-01-03T20:54:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-29T10:55:17Z"
    generateName: kube-flannel-ds-
    labels:
      app: flannel
      controller-revision-hash: b954df96
      k8s-app: flannel
      pod-template-generation: "1"
      tier: node
    name: kube-flannel-ds-nb5bh
    namespace: kube-flannel
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-flannel-ds
      uid: dc2f3b90-3449-443a-b297-6a85f1f02101
    resourceVersion: "486"
    uid: 3f47e2c9-0dc5-41e4-9785-f5cdbea1a42d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    containers:
    - args:
      - --ip-masq
      - --kube-subnet-mgr
      command:
      - /opt/bin/flanneld
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      - name: CONT_WHEN_CACHE_NOT_READY
        value: "false"
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: kube-flannel
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/flannel
        name: run
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jxk7p
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      command:
      - cp
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imagePullPolicy: IfNotPresent
      name: install-cni-plugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/cni/bin
        name: cni-plugin
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jxk7p
        readOnly: true
    - args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      command:
      - cp
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cni/net.d
        name: cni
      - mountPath: /etc/kube-flannel/
        name: flannel-cfg
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jxk7p
        readOnly: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: flannel
    serviceAccountName: flannel
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/flannel
        type: ""
      name: run
    - hostPath:
        path: /opt/cni/bin
        type: ""
      name: cni-plugin
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni
    - configMap:
        defaultMode: 420
        name: kube-flannel-cfg
      name: flannel-cfg
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: kube-api-access-jxk7p
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:17Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://204eb85fd7e413d6583e02c62ba148d39cfa0deb349cc825ccb38434b7dd0379
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: kube-flannel
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-10-29T10:55:26Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    initContainerStatuses:
    - containerID: containerd://32a6177c3979006c8fbb935ca4e7a5edb3cbd1b8764314f59b0464a0632fafca
      image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
      imageID: ghcr.io/flannel-io/flannel-cni-plugin@sha256:25bd091c1867d0237432a4bcb5da720f39198b7d80edcae3bdf08262d242985c
      lastState: {}
      name: install-cni-plugin
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://32a6177c3979006c8fbb935ca4e7a5edb3cbd1b8764314f59b0464a0632fafca
          exitCode: 0
          finishedAt: "2025-10-29T10:55:21Z"
          reason: Completed
          startedAt: "2025-10-29T10:55:21Z"
    - containerID: containerd://0644d55ec53d4403e65e48c14415b76ed2920ce57dcc41b4fc07188425b0b679
      image: ghcr.io/flannel-io/flannel:v0.27.4
      imageID: ghcr.io/flannel-io/flannel@sha256:2ff3c5cb44d0e27b09f27816372084c98fa12486518ca95cb4a970f4a1a464c4
      lastState: {}
      name: install-cni
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://0644d55ec53d4403e65e48c14415b76ed2920ce57dcc41b4fc07188425b0b679
          exitCode: 0
          finishedAt: "2025-10-29T10:55:26Z"
          reason: Completed
          startedAt: "2025-10-29T10:55:26Z"
    phase: Running
    podIP: 37.60.241.150
    podIPs:
    - ip: 37.60.241.150
    qosClass: Burstable
    startTime: "2025-10-29T10:55:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-29T10:54:50Z"
    generateName: coredns-76f75df574-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 76f75df574
    name: coredns-76f75df574-h9rcg
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-76f75df574
      uid: 41b0f393-d087-4af1-9083-db4575f72821
    resourceVersion: "29485636"
    uid: 3867e39a-cd0f-4765-8c64-c255ee0117c5
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5g9bv
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-5g9bv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T19:10:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T19:10:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5907cc1bf298804d131419ea1949e0cceb3a4b11c14367273798fe5caea41992
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-10-29T10:55:39Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.3
    podIPs:
    - ip: 10.244.0.3
    qosClass: Burstable
    startTime: "2025-10-29T10:55:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-29T10:54:50Z"
    generateName: coredns-76f75df574-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 76f75df574
    name: coredns-76f75df574-svlrw
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-76f75df574
      uid: 41b0f393-d087-4af1-9083-db4575f72821
    resourceVersion: "29800163"
    uid: fc301077-6545-406b-9f9f-5f05e35f43d1
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - kube-dns
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v44s8
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: kube-api-access-v44s8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:35:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:35:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:55:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c8f6b0fed8f4b1041a3ce9ed5e8f6bb152c480548bcf5b07775ad3e1d54550a7
      image: registry.k8s.io/coredns/coredns:v1.11.1
      imageID: registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-10-29T10:55:38Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.2
    podIPs:
    - ip: 10.244.0.2
    qosClass: Burstable
    startTime: "2025-10-29T10:55:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/etcd.advertise-client-urls: https://37.60.241.150:2379
      kubernetes.io/config.hash: 45e44f77746ce4db32b124d0b312245b
      kubernetes.io/config.mirror: 45e44f77746ce4db32b124d0b312245b
      kubernetes.io/config.seen: "2025-10-29T11:54:28.550806976+01:00"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-10-29T10:54:35Z"
    labels:
      component: etcd
      tier: control-plane
    name: etcd-vmi2092350.contaboserver.net
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: vmi2092350.contaboserver.net
      uid: 49234673-440b-4393-9aff-61ab34c42eb5
    resourceVersion: "326"
    uid: 96a86d91-7bcf-42b1-863c-f5e299bf5606
  spec:
    containers:
    - command:
      - etcd
      - --advertise-client-urls=https://37.60.241.150:2379
      - --cert-file=/etc/kubernetes/pki/etcd/server.crt
      - --client-cert-auth=true
      - --data-dir=/var/lib/etcd
      - --experimental-initial-corrupt-check=true
      - --experimental-watch-progress-notify-interval=5s
      - --initial-advertise-peer-urls=https://37.60.241.150:2380
      - --initial-cluster=vmi2092350.contaboserver.net=https://37.60.241.150:2380
      - --key-file=/etc/kubernetes/pki/etcd/server.key
      - --listen-client-urls=https://127.0.0.1:2379,https://37.60.241.150:2379
      - --listen-metrics-urls=http://127.0.0.1:2381
      - --listen-peer-urls=https://37.60.241.150:2380
      - --name=vmi2092350.contaboserver.net
      - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      - --peer-client-cert-auth=true
      - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      - --snapshot-count=10000
      - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      image: registry.k8s.io/etcd:3.5.16-0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /health?exclude=NOSPACE&serializable=true
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /health?serializable=false
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd-data
      - mountPath: /etc/kubernetes/pki/etcd
        name: etcd-certs
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki/etcd
        type: DirectoryOrCreate
      name: etcd-certs
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd-data
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://058833eb297ebb1780759ea3ba646fb93724ff2ca325d2cdf51b96fc80604090
      image: registry.k8s.io/etcd:3.5.16-0
      imageID: registry.k8s.io/etcd@sha256:c6a9d11cc5c04b114ccdef39a9265eeef818e3d02f5359be035ae784097fdec5
      lastState: {}
      name: etcd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-10-29T10:54:30Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 37.60.241.150
    podIPs:
    - ip: 37.60.241.150
    qosClass: Burstable
    startTime: "2025-10-29T10:54:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-13T13:51:35Z"
    generateName: external-dns-55c5bfbf97-
    labels:
      app.kubernetes.io/instance: external-dns
      app.kubernetes.io/name: external-dns
      pod-template-hash: 55c5bfbf97
    name: external-dns-55c5bfbf97-8bkxv
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: external-dns-55c5bfbf97
      uid: 24ecf122-7247-49f1-90f3-dd4efc122daa
    resourceVersion: "29903085"
    uid: 3c9bf1dc-7d37-4043-a0ed-adec5cde83a2
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --log-level=info
      - --log-format=text
      - --interval=1m
      - --source=ingress
      - --source=service
      - --policy=sync
      - --registry=txt
      - --txt-owner-id=neural-hive-k8s
      - --domain-filter=elysiumii.site
      - --provider=cloudflare
      env:
      - name: CF_API_TOKEN
        valueFrom:
          secretKeyRef:
            key: api-token
            name: cloudflare-api-token
      image: registry.k8s.io/external-dns/external-dns:v0.19.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 2
        httpGet:
          path: /healthz
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: external-dns
      ports:
      - containerPort: 7979
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        httpGet:
          path: /healthz
          port: http
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 65532
        runAsNonRoot: true
        runAsUser: 65532
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zjlxj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: external-dns
    serviceAccountName: external-dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-zjlxj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:51:41Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:51:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:01:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:01:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-13T13:51:35Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ced8cb57be97559e481d17a06f6e8871c75c23eb5682eb8f3d3bf9144ebe2a40
      image: registry.k8s.io/external-dns/external-dns:v0.19.0
      imageID: registry.k8s.io/external-dns/external-dns@sha256:f76114338104264f655b23138444481b20bb9d6125742c7240fac25936fe164e
      lastState:
        terminated:
          containerID: containerd://223ae3fce0c54359af8b21d7104335e176ef0da42e148d966538db9207f1bf59
          exitCode: 1
          finishedAt: "2026-02-13T21:01:19Z"
          reason: Error
          startedAt: "2026-02-13T20:37:04Z"
      name: external-dns
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2026-02-13T21:01:20Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.202
    podIPs:
    - ip: 10.244.3.202
    qosClass: Burstable
    startTime: "2026-01-13T13:51:35Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 37.60.241.150:6443
      kubernetes.io/config.hash: cc9e8aa9b33f07a7cba0c715bf33243f
      kubernetes.io/config.mirror: cc9e8aa9b33f07a7cba0c715bf33243f
      kubernetes.io/config.seen: "2025-10-29T11:54:38.865960942+01:00"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-10-29T10:54:39Z"
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-vmi2092350.contaboserver.net
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: vmi2092350.contaboserver.net
      uid: 49234673-440b-4393-9aff-61ab34c42eb5
    resourceVersion: "28329754"
    uid: a9796a30-a21d-477b-93dd-f80e9c59fd73
  spec:
    containers:
    - command:
      - kube-apiserver
      - --advertise-address=37.60.241.150
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --enable-admission-plugins=NodeRestriction
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --requestheader-allowed-names=front-proxy-client
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --secure-port=6443
      - --service-account-issuer=https://kubernetes.default.svc.cluster.local
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/12
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      image: registry.k8s.io/kube-apiserver:v1.29.15
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 37.60.241.150
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 37.60.241.150
          path: /readyz
          port: 6443
          scheme: HTTPS
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 250m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 37.60.241.150
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T21:30:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T21:30:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4f005a26ad558949aa36f7d5d518cacfa45d03553fd49420708f2f21bea3b71c
      image: registry.k8s.io/kube-apiserver:v1.29.15
      imageID: registry.k8s.io/kube-apiserver@sha256:fd82c74e0773a10396055904753126411b3a9584cc453e1c7152c81018a933b6
      lastState:
        terminated:
          containerID: containerd://b457796c83d3940072d70b4790a4bce451fbc398192cec75107a0855fa4d9f2a
          exitCode: 137
          finishedAt: "2025-11-12T14:32:48Z"
          reason: Error
          startedAt: "2025-10-29T10:54:30Z"
      name: kube-apiserver
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-11-12T14:32:48Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 37.60.241.150
    podIPs:
    - ip: 37.60.241.150
    qosClass: Burstable
    startTime: "2025-10-29T10:54:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 62aa855d7739dcf1096e45ad22b13a03
      kubernetes.io/config.mirror: 62aa855d7739dcf1096e45ad22b13a03
      kubernetes.io/config.seen: "2025-10-29T11:54:28.550816735+01:00"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-10-29T10:54:35Z"
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-vmi2092350.contaboserver.net
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: vmi2092350.contaboserver.net
      uid: 49234673-440b-4393-9aff-61ab34c42eb5
    resourceVersion: "13813247"
    uid: ca327ae8-ef1a-4869-9b33-b9a15237dc09
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --bind-address=127.0.0.1
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --cluster-cidr=10.244.0.0/16
      - --cluster-name=kubernetes
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --controllers=*,bootstrapsigner,tokencleaner
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --leader-elect=true
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=10.96.0.0/12
      - --use-service-account-credentials=true
      image: registry.k8s.io/kube-controller-manager:v1.29.15
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T10:02:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T10:02:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://85de790db318fff2d05bbcb9165607d214e7ace9dd55ead653ee14ba57430199
      image: registry.k8s.io/kube-controller-manager:v1.29.15
      imageID: registry.k8s.io/kube-controller-manager@sha256:4f05be2c0667d9f4975bcc43d5e136b2436946f84c8f7dc2d2da14392e761a71
      lastState:
        terminated:
          containerID: containerd://58b3af0b1605b0f7b355362b15ffa4258bc806906f62557551350a0283f59f23
          exitCode: 1
          finishedAt: "2025-12-27T10:01:38Z"
          reason: Error
          startedAt: "2025-12-27T09:57:11Z"
      name: kube-controller-manager
      ready: true
      restartCount: 12
      started: true
      state:
        running:
          startedAt: "2025-12-27T10:02:30Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 37.60.241.150
    podIPs:
    - ip: 37.60.241.150
    qosClass: Burstable
    startTime: "2025-10-29T10:54:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-10-29T10:54:50Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5787cd6d6c
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-52dwk
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 8d9f43df-3467-418c-b6d0-13ba7748664e
    resourceVersion: "398"
    uid: 113c9a6d-fcc5-41c3-a3e5-7afb44c851cc
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.15
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-76hvx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2092350.contaboserver.net
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-76hvx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://24165e657fe12d65bfdc2ee06a54b27dd830db257ef4e654ebf3e09e8b268f01
      image: registry.k8s.io/kube-proxy:v1.29.15
      imageID: registry.k8s.io/kube-proxy@sha256:243026cfce3209b89d9f883789108276ffec87d98190ac2a77776edd4e0e6015
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-10-29T10:54:51Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 37.60.241.150
    podIPs:
    - ip: 37.60.241.150
    qosClass: BestEffort
    startTime: "2025-10-29T10:54:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-14T23:49:03Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5787cd6d6c
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-96t6f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 8d9f43df-3467-418c-b6d0-13ba7748664e
    resourceVersion: "22400230"
    uid: 0f48fbc1-4f0f-4fda-a614-6d41951ee3c1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.15
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v2b8b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2911680
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-v2b8b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:49:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:49:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:49:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:49:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:49:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://37a99477e69aa68335fbc10bc33c594e882e070375d4deea9a8cf15ad97a8809
      image: registry.k8s.io/kube-proxy:v1.29.15
      imageID: registry.k8s.io/kube-proxy@sha256:243026cfce3209b89d9f883789108276ffec87d98190ac2a77776edd4e0e6015
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T23:49:54Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 158.220.101.216
    podIPs:
    - ip: 158.220.101.216
    qosClass: BestEffort
    startTime: "2025-11-14T23:49:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-03T20:54:29Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5787cd6d6c
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-dwmkz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 8d9f43df-3467-418c-b6d0-13ba7748664e
    resourceVersion: "25557760"
    uid: d90c9850-d4ed-4c03-bf5e-d82aa303e659
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.15
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vtq4r
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi3002938
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-vtq4r
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ad682c92010fee05af079b03caeb37241dc87dc2259c34ccd96b7d7df740ce2f
      image: registry.k8s.io/kube-proxy:v1.29.15
      imageID: registry.k8s.io/kube-proxy@sha256:243026cfce3209b89d9f883789108276ffec87d98190ac2a77776edd4e0e6015
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:54:34Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 89.117.60.74
    podIPs:
    - ip: 89.117.60.74
    qosClass: BestEffort
    startTime: "2026-01-03T20:54:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-11-14T23:49:34Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5787cd6d6c
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-mb7mz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 8d9f43df-3467-418c-b6d0-13ba7748664e
    resourceVersion: "23972065"
    uid: f93f1e13-a20b-4b11-997f-552c0844c661
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.15
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5tnmw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2911681
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-5tnmw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:50:52Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:49:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:51:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:51:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-14T23:49:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c7827747bd8711b9c724cede11390cdbc85f7846ce9c83e1c1300b25e0cc0337
      image: registry.k8s.io/kube-proxy:v1.29.15
      imageID: registry.k8s.io/kube-proxy@sha256:243026cfce3209b89d9f883789108276ffec87d98190ac2a77776edd4e0e6015
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-14T23:51:29Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 84.247.138.35
    podIPs:
    - ip: 84.247.138.35
    qosClass: BestEffort
    startTime: "2025-11-14T23:49:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-09T23:43:34Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 5787cd6d6c
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-xtw2h
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: 8d9f43df-3467-418c-b6d0-13ba7748664e
    resourceVersion: "29939310"
    uid: eb28179b-88b3-4dde-9c4f-fce2563e8e5e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: registry.k8s.io/kube-proxy:v1.29.15
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zpsjn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi3075398
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-zpsjn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:47Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:46Z"
      message: 'containers with unready status: [kube-proxy]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:46Z"
      message: 'containers with unready status: [kube-proxy]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4294513b3d9a47ee5e6e3769c1ce07519ea99d54623054063bd1367743548458
      image: registry.k8s.io/kube-proxy:v1.29.15
      imageID: registry.k8s.io/kube-proxy@sha256:243026cfce3209b89d9f883789108276ffec87d98190ac2a77776edd4e0e6015
      lastState:
        terminated:
          containerID: containerd://4294513b3d9a47ee5e6e3769c1ce07519ea99d54623054063bd1367743548458
          exitCode: 2
          finishedAt: "2026-02-13T23:05:46Z"
          reason: Error
          startedAt: "2026-02-13T23:04:38Z"
      name: kube-proxy
      ready: false
      restartCount: 961
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=kube-proxy pod=kube-proxy-xtw2h_kube-system(eb28179b-88b3-4dde-9c4f-fce2563e8e5e)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 144.91.115.90
    podIPs:
    - ip: 144.91.115.90
    qosClass: BestEffort
    startTime: "2026-02-09T23:43:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 2162d450dde1ec87f3232e768b8813dd
      kubernetes.io/config.mirror: 2162d450dde1ec87f3232e768b8813dd
      kubernetes.io/config.seen: "2025-10-29T11:54:38.865965122+01:00"
      kubernetes.io/config.source: file
    creationTimestamp: "2025-10-29T10:54:39Z"
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-vmi2092350.contaboserver.net
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: vmi2092350.contaboserver.net
      uid: 49234673-440b-4393-9aff-61ab34c42eb5
    resourceVersion: "13812079"
    uid: b4fb4439-0e27-4111-8aff-a2421bdb866d
  spec:
    containers:
    - command:
      - kube-scheduler
      - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      - --bind-address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --leader-elect=true
      image: registry.k8s.io/kube-scheduler:v1.29.15
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T09:57:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-27T09:57:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-10-29T10:54:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://93c09d9ea75282719a46aa7699924dd58afe78d2e7b0454d410764d3a7d6e72e
      image: registry.k8s.io/kube-scheduler:v1.29.15
      imageID: registry.k8s.io/kube-scheduler@sha256:276108a4541a51894a010633230f7b6d10e92e730274f24bd21e827e64243d66
      lastState: {}
      name: kube-scheduler
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2025-12-27T09:57:17Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 37.60.241.150
    podIPs:
    - ip: 37.60.241.150
    qosClass: Burstable
    startTime: "2025-10-29T10:54:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-15T13:29:45Z"
    generateName: metrics-server-59d465df9f-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 59d465df9f
    name: metrics-server-59d465df9f-6bsqq
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-59d465df9f
      uid: edd0abce-57fe-4f7a-bd83-a5c298b077c3
    resourceVersion: "26774954"
    uid: 57e082b3-18dc-43e0-bc2f-6e1dff6665f9
  spec:
    containers:
    - args:
      - --cert-dir=/tmp
      - --secure-port=10250
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --kubelet-insecure-tls
      image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-85tsh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp-dir
    - name: kube-api-access-85tsh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:30:37Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b491a90dc76ac03e22055f99eeb275afac6814df5504d4b31fee34a8e89838ba
      image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
      imageID: registry.k8s.io/metrics-server/metrics-server@sha256:89258156d0e9af60403eafd44da9676fd66f600c7934d468ccc17e42b199aee2
      lastState:
        terminated:
          containerID: containerd://427b1335576e9cdbd7ec111a730faaca88484fdab533721eb6d9c475a12c645e
          exitCode: 0
          finishedAt: "2026-02-01T13:46:49Z"
          reason: Completed
          startedAt: "2026-01-22T16:54:38Z"
      name: metrics-server
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2026-02-01T13:46:50Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.7
    podIPs:
    - ip: 10.244.3.7
    qosClass: Burstable
    startTime: "2026-01-15T13:29:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-15T13:29:43Z"
    generateName: local-path-provisioner-844bd8758f-
    labels:
      app: local-path-provisioner
      pod-template-hash: 844bd8758f
    name: local-path-provisioner-844bd8758f-2rdfh
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: local-path-provisioner-844bd8758f
      uid: 6e7d7a0a-5858-4b45-afb5-91c7dc96869d
    resourceVersion: "25557843"
    uid: e925d54a-f26f-4ef9-a6c6-d6fdad834604
  spec:
    containers:
    - command:
      - local-path-provisioner
      - --debug
      - start
      - --config
      - /etc/config/config.json
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rancher/local-path-provisioner:v0.0.24
      imagePullPolicy: IfNotPresent
      name: local-path-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config/
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4rczc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: local-path-provisioner-service-account
    serviceAccountName: local-path-provisioner-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: local-path-config
      name: config-volume
    - name: kube-api-access-4rczc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:30:03Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:30:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:30:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://697cf3273597faa3f5b1de72fb5cbcbcb2a4f4dba5cccbfdcfbc2c7232b097db
      image: docker.io/rancher/local-path-provisioner:v0.0.24
      imageID: docker.io/rancher/local-path-provisioner@sha256:5bb33992a4ec3034c28b5e0b3c4c2ac35d3613b25b79455eb4b1a95adc82cdc0
      lastState: {}
      name: local-path-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-15T13:30:03Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.250
    podIPs:
    - ip: 10.244.3.250
    qosClass: BestEffort
    startTime: "2026-01-15T13:29:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
    creationTimestamp: "2026-01-15T13:29:44Z"
    generateName: csi-attacher-57f8656cc6-
    labels:
      app: csi-attacher
      pod-template-hash: 57f8656cc6
    name: csi-attacher-57f8656cc6-8jw8c
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-attacher-57f8656cc6
      uid: 5f3ee051-73a6-47e7-992d-52401f719ffa
    resourceVersion: "25557795"
    uid: a46e2d98-6e9a-424a-be47-9bd01e8cd33c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - csi-attacher
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --timeout=1m50s
      - --leader-election
      - --leader-election-namespace=$(POD_NAMESPACE)
      - --kube-api-qps=50
      - --kube-api-burst=100
      - --http-endpoint=:8000
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: longhornio/csi-attacher:v4.10.0-20251030
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      ports:
      - containerPort: 8000
        name: csi-attacher
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7b7rq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - name: kube-api-access-7b7rq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:30:15Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:56:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:56:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8d38fa09f64fe9f5a18c4718aa1a08c3eba17b35aba6fadf689567a73c060d42
      image: docker.io/longhornio/csi-attacher:v4.10.0-20251030
      imageID: docker.io/longhornio/csi-attacher@sha256:af29125d83075b95894e95862dde03908b1970858f1d6399917305b92d2713a6
      lastState:
        terminated:
          containerID: containerd://c6739c17c9a4e10ff00bf5c5875996d14da287d7377c37048109144d51552f65
          exitCode: 1
          finishedAt: "2026-01-22T16:56:22Z"
          reason: Error
          startedAt: "2026-01-15T13:30:14Z"
      name: csi-attacher
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:56:24Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.252
    podIPs:
    - ip: 10.244.3.252
    qosClass: BestEffort
    startTime: "2026-01-15T13:29:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
    creationTimestamp: "2026-01-15T13:29:43Z"
    generateName: csi-provisioner-6f58d758d9-
    labels:
      app: csi-provisioner
      pod-template-hash: 6f58d758d9
    name: csi-provisioner-6f58d758d9-lb65c
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-provisioner-6f58d758d9
      uid: 21da8bbf-0c13-4888-93b3-6415faff5f90
    resourceVersion: "25557784"
    uid: 1cf90233-1cb4-4985-9149-2d01b74eabb7
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - csi-provisioner
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --timeout=1m50s
      - --leader-election
      - --leader-election-namespace=$(POD_NAMESPACE)
      - --default-fstype=ext4
      - --enable-capacity
      - --capacity-ownerref-level=2
      - --kube-api-qps=50
      - --kube-api-burst=100
      - --http-endpoint=:8000
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: longhornio/csi-provisioner:v5.3.0-20251030
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      ports:
      - containerPort: 8000
        name: csi-provisioner
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-268g7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - name: kube-api-access-268g7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:30:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:56:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:56:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bcb560f18df9c692c3881b024e0594bf0f14e0253a193bcca86ba1bbec4eeb8e
      image: docker.io/longhornio/csi-provisioner:v5.3.0-20251030
      imageID: docker.io/longhornio/csi-provisioner@sha256:fe31c68584e80a2af9ae14e0682b3cea076218a0756ae7add0a421c21486a4d0
      lastState:
        terminated:
          containerID: containerd://a0e9dad2234f0de581bf3f02da04ba3b2d792f07dfebcee3a90b99f612089712
          exitCode: 1
          finishedAt: "2026-01-22T16:56:22Z"
          reason: Error
          startedAt: "2026-01-15T13:30:10Z"
      name: csi-provisioner
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:56:24Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.251
    podIPs:
    - ip: 10.244.3.251
    qosClass: BestEffort
    startTime: "2026-01-15T13:29:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
    creationTimestamp: "2026-01-15T13:29:43Z"
    generateName: csi-resizer-755f48c7c9-
    labels:
      app: csi-resizer
      pod-template-hash: 755f48c7c9
    name: csi-resizer-755f48c7c9-fkndr
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-resizer-755f48c7c9
      uid: fd835bd0-3b46-4c5c-a0af-b2e4a76bd2db
    resourceVersion: "25557658"
    uid: 193a78e6-5a98-4948-bc2f-ee97e74b434c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - csi-resizer
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --timeout=1m50s
      - --leader-election
      - --leader-election-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=$(POD_NAMESPACE)
      - --kube-api-qps=50
      - --kube-api-burst=100
      - --http-endpoint=:8000
      - --handle-volume-inuse-error=false
      - --feature-gates=RecoverVolumeExpansionFailure=false
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: longhornio/csi-resizer:v1.14.0-20251030
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      ports:
      - containerPort: 8000
        name: csi-resizer
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fxr9x
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - name: kube-api-access-fxr9x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:59Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:56:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:56:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://52fd7693d0d3d7eb03bae83d9b45d4e318ccc281244283fc314e8d23fd4fbdd3
      image: docker.io/longhornio/csi-resizer:v1.14.0-20251030
      imageID: docker.io/longhornio/csi-resizer@sha256:748b61cb91ba4cc4bc35ba38bdac73c130602262458f388b0116eb96a4690e94
      lastState:
        terminated:
          containerID: containerd://454c22990614bd110c0200fe8cb0c9b8723e0fd2ebdd391e3dcb5584b71d86c7
          exitCode: 1
          finishedAt: "2026-01-22T16:56:22Z"
          reason: Error
          startedAt: "2026-01-15T13:29:58Z"
      name: csi-resizer
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:56:24Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.248
    podIPs:
    - ip: 10.244.3.248
    qosClass: BestEffort
    startTime: "2026-01-15T13:29:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
    creationTimestamp: "2026-01-15T13:29:44Z"
    generateName: csi-snapshotter-7959bd58bb-
    labels:
      app: csi-snapshotter
      pod-template-hash: 7959bd58bb
    name: csi-snapshotter-7959bd58bb-vrk2b
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-snapshotter-7959bd58bb
      uid: 56c67273-a2c5-4b15-b59f-70cf172dd5f1
    resourceVersion: "25557680"
    uid: 0b7a83c8-54c1-4e40-962d-731337aed85a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - csi-snapshotter
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --timeout=1m50s
      - --leader-election
      - --leader-election-namespace=$(POD_NAMESPACE)
      - --kube-api-qps=50
      - --kube-api-burst=100
      - --http-endpoint=:8000
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: longhornio/csi-snapshotter:v8.4.0-20251030
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      ports:
      - containerPort: 8000
        name: csi-snapshotter
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zjr8c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - name: kube-api-access-zjr8c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:30:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:56:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:56:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6b5d5535d2e81f8645b7d2cf7ed42f261ea26fa1ee371aebd7311c7b02fa3555
      image: docker.io/longhornio/csi-snapshotter:v8.4.0-20251030
      imageID: docker.io/longhornio/csi-snapshotter@sha256:2cc2f9e653c2b397e8eefffbfb4ccf8eabc5f7d84e1f875d42608484a66c307f
      lastState:
        terminated:
          containerID: containerd://075d7607fffd0a467d53cf08452384086eb049564478559938df2e7e8599d18a
          exitCode: 1
          finishedAt: "2026-01-22T16:56:22Z"
          reason: Error
          startedAt: "2026-01-15T13:30:32Z"
      name: csi-snapshotter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:56:24Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.254
    podIPs:
    - ip: 10.244.3.254
    qosClass: BestEffort
    startTime: "2026-01-15T13:29:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-09T23:43:50Z"
    generateName: engine-image-ei-3154f3aa-
    labels:
      controller-revision-hash: fd54c6bbf
      longhorn.io/component: engine-image
      longhorn.io/engine-image: ei-3154f3aa
      pod-template-generation: "1"
    name: engine-image-ei-3154f3aa-dwxfc
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: engine-image-ei-3154f3aa
      uid: 6d65259d-8d6c-4a6e-bc7b-0be4f036838b
    resourceVersion: "29938572"
    uid: 3a4bd1bc-5180-45a4-99fb-4d6144a2e04c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    containers:
    - args:
      - -c
      - diff /usr/local/bin/longhorn /data/longhorn > /dev/null 2>&1; if [ $? -ne
        0 ]; then cp -p /usr/local/bin/longhorn /data/ && echo installed; fi && trap
        'rm /data/longhorn* && echo cleaned up' EXIT && sleep infinity
      command:
      - /bin/bash
      image: longhornio/longhorn-engine:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: engine-image-ei-3154f3aa
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - ls /data/longhorn && /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data/
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7zgfx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/longhornio-longhorn-engine-v1.10.1
        type: ""
      name: data
    - name: kube-api-access-7zgfx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:03:23Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:03:06Z"
      message: 'containers with unready status: [engine-image-ei-3154f3aa]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:03:06Z"
      message: 'containers with unready status: [engine-image-ei-3154f3aa]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8c65420fadce0461178bc996bb08e2bcd0425d463328d7ee9117814b232757e1
      image: docker.io/longhornio/longhorn-engine:v1.10.1
      imageID: docker.io/longhornio/longhorn-engine@sha256:da4b45d615bff7cfe0702c920c1d389f799368d6b4643c409d8e58b1658af8e7
      lastState:
        terminated:
          containerID: containerd://8c65420fadce0461178bc996bb08e2bcd0425d463328d7ee9117814b232757e1
          exitCode: 137
          finishedAt: "2026-02-13T23:03:21Z"
          reason: Error
          startedAt: "2026-02-13T23:02:44Z"
      name: engine-image-ei-3154f3aa
      ready: false
      restartCount: 986
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=engine-image-ei-3154f3aa
            pod=engine-image-ei-3154f3aa-dwxfc_longhorn-system(3a4bd1bc-5180-45a4-99fb-4d6144a2e04c)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.253
    podIPs:
    - ip: 10.244.4.253
    qosClass: BestEffort
    startTime: "2026-02-09T23:43:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-09T21:22:47Z"
    generateName: engine-image-ei-3154f3aa-
    labels:
      controller-revision-hash: fd54c6bbf
      longhorn.io/component: engine-image
      longhorn.io/engine-image: ei-3154f3aa
      pod-template-generation: "1"
    name: engine-image-ei-3154f3aa-jk2gg
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: engine-image-ei-3154f3aa
      uid: 6d65259d-8d6c-4a6e-bc7b-0be4f036838b
    resourceVersion: "26435415"
    uid: 043735e1-9685-4e0e-8852-70d705440090
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    containers:
    - args:
      - -c
      - diff /usr/local/bin/longhorn /data/longhorn > /dev/null 2>&1; if [ $? -ne
        0 ]; then cp -p /usr/local/bin/longhorn /data/ && echo installed; fi && trap
        'rm /data/longhorn* && echo cleaned up' EXIT && sleep infinity
      command:
      - /bin/bash
      image: longhornio/longhorn-engine:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: engine-image-ei-3154f3aa
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - ls /data/longhorn && /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data/
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-czgkh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/longhornio-longhorn-engine-v1.10.1
        type: ""
      name: data
    - name: kube-api-access-czgkh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:23:12Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b6f1d7b78d6d60c19b823afd3193505239a199016d80735f1b4a53ac0a8f851b
      image: docker.io/longhornio/longhorn-engine:v1.10.1
      imageID: docker.io/longhornio/longhorn-engine@sha256:da4b45d615bff7cfe0702c920c1d389f799368d6b4643c409d8e58b1658af8e7
      lastState:
        terminated:
          containerID: containerd://2a45bf1e75920a18a5c2922ddc8f839dd8828059886d5d66bbd1b3620a0d7302
          exitCode: 137
          finishedAt: "2026-01-15T13:33:39Z"
          reason: Error
          startedAt: "2025-12-10T22:29:48Z"
      name: engine-image-ei-3154f3aa
      ready: true
      restartCount: 6
      started: true
      state:
        running:
          startedAt: "2026-01-15T13:33:39Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.28
    podIPs:
    - ip: 10.244.2.28
    qosClass: BestEffort
    startTime: "2025-12-09T21:22:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-03T20:54:42Z"
    generateName: engine-image-ei-3154f3aa-
    labels:
      controller-revision-hash: fd54c6bbf
      longhorn.io/component: engine-image
      longhorn.io/engine-image: ei-3154f3aa
      pod-template-generation: "1"
    name: engine-image-ei-3154f3aa-qc89r
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: engine-image-ei-3154f3aa
      uid: 6d65259d-8d6c-4a6e-bc7b-0be4f036838b
    resourceVersion: "26774947"
    uid: 76489556-492b-4e02-adb3-121cb2262ecd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    containers:
    - args:
      - -c
      - diff /usr/local/bin/longhorn /data/longhorn > /dev/null 2>&1; if [ $? -ne
        0 ]; then cp -p /usr/local/bin/longhorn /data/ && echo installed; fi && trap
        'rm /data/longhorn* && echo cleaned up' EXIT && sleep infinity
      command:
      - /bin/bash
      image: longhornio/longhorn-engine:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: engine-image-ei-3154f3aa
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - ls /data/longhorn && /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data/
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-v8k2g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/longhornio-longhorn-engine-v1.10.1
        type: ""
      name: data
    - name: kube-api-access-v8k2g
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:55:18Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d0b5d21a541c7120eb12f6a90ea6bd7632d856b78321067c180ad192ed6ea65b
      image: docker.io/longhornio/longhorn-engine:v1.10.1
      imageID: docker.io/longhornio/longhorn-engine@sha256:da4b45d615bff7cfe0702c920c1d389f799368d6b4643c409d8e58b1658af8e7
      lastState: {}
      name: engine-image-ei-3154f3aa
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:55:17Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.4
    podIPs:
    - ip: 10.244.3.4
    qosClass: BestEffort
    startTime: "2026-01-03T20:54:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-09T21:22:47Z"
    generateName: engine-image-ei-3154f3aa-
    labels:
      controller-revision-hash: fd54c6bbf
      longhorn.io/component: engine-image
      longhorn.io/engine-image: ei-3154f3aa
      pod-template-generation: "1"
    name: engine-image-ei-3154f3aa-s84zj
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: engine-image-ei-3154f3aa
      uid: 6d65259d-8d6c-4a6e-bc7b-0be4f036838b
    resourceVersion: "26438639"
    uid: e80f651a-4cc7-4a58-9cb8-70c3be486fdf
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    containers:
    - args:
      - -c
      - diff /usr/local/bin/longhorn /data/longhorn > /dev/null 2>&1; if [ $? -ne
        0 ]; then cp -p /usr/local/bin/longhorn /data/ && echo installed; fi && trap
        'rm /data/longhorn* && echo cleaned up' EXIT && sleep infinity
      command:
      - /bin/bash
      image: longhornio/longhorn-engine:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: engine-image-ei-3154f3aa
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - ls /data/longhorn && /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data/
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qltkg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/longhornio-longhorn-engine-v1.10.1
        type: ""
      name: data
    - name: kube-api-access-qltkg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:58Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8afa3c743e6213579a50a21b33919fd4b371bfcc6cecad5179b9e9df00b5cff8
      image: docker.io/longhornio/longhorn-engine:v1.10.1
      imageID: docker.io/longhornio/longhorn-engine@sha256:da4b45d615bff7cfe0702c920c1d389f799368d6b4643c409d8e58b1658af8e7
      lastState:
        terminated:
          containerID: containerd://4cb7f238b0309c09064345fedcb4478525a7a080497c63337e33ff780818f7f0
          exitCode: 137
          finishedAt: "2026-01-22T16:58:21Z"
          reason: Error
          startedAt: "2025-12-09T21:22:57Z"
      name: engine-image-ei-3154f3aa
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:22Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.152
    podIPs:
    - ip: 10.244.1.152
    qosClass: BestEffort
    startTime: "2025-12-09T21:22:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-07T23:49:23Z"
    generateName: engine-image-ei-3154f3aa-
    labels:
      controller-revision-hash: fd54c6bbf
      longhorn.io/component: engine-image
      longhorn.io/engine-image: ei-3154f3aa
      pod-template-generation: "1"
    name: engine-image-ei-3154f3aa-t4xz5
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: engine-image-ei-3154f3aa
      uid: 6d65259d-8d6c-4a6e-bc7b-0be4f036838b
    resourceVersion: "27681113"
    uid: b1ab306f-6ec5-456d-84f9-054f34bb401c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    containers:
    - args:
      - -c
      - diff /usr/local/bin/longhorn /data/longhorn > /dev/null 2>&1; if [ $? -ne
        0 ]; then cp -p /usr/local/bin/longhorn /data/ && echo installed; fi && trap
        'rm /data/longhorn* && echo cleaned up' EXIT && sleep infinity
      command:
      - /bin/bash
      image: longhornio/longhorn-engine:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: engine-image-ei-3154f3aa
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - ls /data/longhorn && /data/longhorn version --client-only
        failureThreshold: 3
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data/
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cf5fs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/longhornio-longhorn-engine-v1.10.1
        type: ""
      name: data
    - name: kube-api-access-cf5fs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:51:46Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:49:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:51:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:51:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:49:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://47809febe2044a825f331ad89967303e4f14f2cbc47627096fe91d1caf743908
      image: docker.io/longhornio/longhorn-engine:v1.10.1
      imageID: docker.io/longhornio/longhorn-engine@sha256:da4b45d615bff7cfe0702c920c1d389f799368d6b4643c409d8e58b1658af8e7
      lastState: {}
      name: engine-image-ei-3154f3aa
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-07T23:51:45Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.56
    podIPs:
    - ip: 10.244.0.56
    qosClass: BestEffort
    startTime: "2026-02-07T23:49:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2026-01-22T16:58:06Z"
    labels:
      longhorn.io/component: instance-manager
      longhorn.io/data-engine: v1
      longhorn.io/instance-manager-image: imi-9fecca02
      longhorn.io/instance-manager-type: aio
      longhorn.io/managed-by: longhorn-manager
      longhorn.io/node: vmi2911680
    name: instance-manager-335effa362cbff8911dc97dffe2b84e2
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: longhorn.io/v1beta2
      blockOwnerDeletion: true
      controller: true
      kind: InstanceManager
      name: instance-manager-335effa362cbff8911dc97dffe2b84e2
      uid: 23953c72-d00f-4e3a-9b86-8e464a4d93ec
    resourceVersion: "22400762"
    uid: cecfa232-f7cb-4556-98ff-13832ea0628e
  spec:
    containers:
    - args:
      - instance-manager
      - --debug
      - daemon
      - --listen
      - :8500
      env:
      - name: TLS_DIR
        value: /tls-files/
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: DATA_ENGINE
        value: v1
      image: longhornio/longhorn-instance-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - test $(nc -zv localhost 8500 > /dev/null 2>&1 && nc -zv localhost 8501
            > /dev/null 2>&1 && nc -zv localhost 8502 > /dev/null 2>&1 && nc -zv localhost
            8503 > /dev/null 2>&1; echo $?) -eq 0
        failureThreshold: 6
        initialDelaySeconds: 3
        periodSeconds: 11
        successThreshold: 1
        timeoutSeconds: 10
      name: instance-manager
      resources:
        requests:
          cpu: 720m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host
      - mountPath: /engine-binaries/
        mountPropagation: HostToContainer
        name: engine-binaries
      - mountPath: /host/var/lib/longhorn/unix-domain-socket/
        name: unix-domain-socket
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /log
        mountPropagation: HostToContainer
        name: log
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wxl9z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/
        type: ""
      name: engine-binaries
    - hostPath:
        path: /var/lib/longhorn/unix-domain-socket/
        type: ""
      name: unix-domain-socket
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - hostPath:
        path: /var/lib/longhorn/logs
        type: DirectoryOrCreate
      name: log
    - name: kube-api-access-wxl9z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6257fec98c27c693e7cdba83973e5cf7a98872995ee64ca7f8d4b376e141912f
      image: docker.io/longhornio/longhorn-instance-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-instance-manager@sha256:84e0a5c1d67599a445f5b4fa853152ff53f6b1bd42a7cf7c01f4152cf60782af
      lastState: {}
      name: instance-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:09Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.30
    podIPs:
    - ip: 10.244.1.30
    qosClass: Burstable
    startTime: "2026-01-22T16:58:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2026-02-07T23:52:14Z"
    labels:
      longhorn.io/component: instance-manager
      longhorn.io/data-engine: v1
      longhorn.io/instance-manager-image: imi-9fecca02
      longhorn.io/instance-manager-type: aio
      longhorn.io/managed-by: longhorn-manager
      longhorn.io/node: vmi2092350.contaboserver.net
    name: instance-manager-55e8452bfea0c8734af27d1c6587fde1
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: longhorn.io/v1beta2
      blockOwnerDeletion: true
      controller: true
      kind: InstanceManager
      name: instance-manager-55e8452bfea0c8734af27d1c6587fde1
      uid: 93e8d586-9185-4e38-a33e-e271b2f399ea
    resourceVersion: "27681957"
    uid: b829527d-da5d-4131-a23b-b5e2f018bf8d
  spec:
    containers:
    - args:
      - instance-manager
      - --debug
      - daemon
      - --listen
      - :8500
      env:
      - name: TLS_DIR
        value: /tls-files/
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: DATA_ENGINE
        value: v1
      image: longhornio/longhorn-instance-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - test $(nc -zv localhost 8500 > /dev/null 2>&1 && nc -zv localhost 8501
            > /dev/null 2>&1 && nc -zv localhost 8502 > /dev/null 2>&1 && nc -zv localhost
            8503 > /dev/null 2>&1; echo $?) -eq 0
        failureThreshold: 6
        initialDelaySeconds: 3
        periodSeconds: 11
        successThreshold: 1
        timeoutSeconds: 10
      name: instance-manager
      resources:
        requests:
          cpu: 960m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host
      - mountPath: /engine-binaries/
        mountPropagation: HostToContainer
        name: engine-binaries
      - mountPath: /host/var/lib/longhorn/unix-domain-socket/
        name: unix-domain-socket
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /log
        mountPropagation: HostToContainer
        name: log
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7dpkr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/
        type: ""
      name: engine-binaries
    - hostPath:
        path: /var/lib/longhorn/unix-domain-socket/
        type: ""
      name: unix-domain-socket
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - hostPath:
        path: /var/lib/longhorn/logs
        type: DirectoryOrCreate
      name: log
    - name: kube-api-access-7dpkr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:54:51Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:52:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:54:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:54:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:52:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://accc8f4abec7ffa44e39bcf5c47d6a24816b7adb291064189cd190efdc804588
      image: docker.io/longhornio/longhorn-instance-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-instance-manager@sha256:84e0a5c1d67599a445f5b4fa853152ff53f6b1bd42a7cf7c01f4152cf60782af
      lastState: {}
      name: instance-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-07T23:54:51Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.60
    podIPs:
    - ip: 10.244.0.60
    qosClass: Burstable
    startTime: "2026-02-07T23:52:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2026-01-03T20:56:00Z"
    labels:
      longhorn.io/component: instance-manager
      longhorn.io/data-engine: v1
      longhorn.io/instance-manager-image: imi-9fecca02
      longhorn.io/instance-manager-type: aio
      longhorn.io/managed-by: longhorn-manager
      longhorn.io/node: vmi3002938
    name: instance-manager-bcc6eed753e19e6cdc45f431bfca9095
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: longhorn.io/v1beta2
      blockOwnerDeletion: true
      controller: true
      kind: InstanceManager
      name: instance-manager-bcc6eed753e19e6cdc45f431bfca9095
      uid: f4747780-e5f4-43a5-9748-c3d7e31f5a3e
    resourceVersion: "25557806"
    uid: 4aedfe94-dd42-4e80-bc35-55a527c2cdf8
  spec:
    containers:
    - args:
      - instance-manager
      - --debug
      - daemon
      - --listen
      - :8500
      env:
      - name: TLS_DIR
        value: /tls-files/
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: DATA_ENGINE
        value: v1
      image: longhornio/longhorn-instance-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - test $(nc -zv localhost 8500 > /dev/null 2>&1 && nc -zv localhost 8501
            > /dev/null 2>&1 && nc -zv localhost 8502 > /dev/null 2>&1 && nc -zv localhost
            8503 > /dev/null 2>&1; echo $?) -eq 0
        failureThreshold: 6
        initialDelaySeconds: 3
        periodSeconds: 11
        successThreshold: 1
        timeoutSeconds: 10
      name: instance-manager
      resources:
        requests:
          cpu: 720m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host
      - mountPath: /engine-binaries/
        mountPropagation: HostToContainer
        name: engine-binaries
      - mountPath: /host/var/lib/longhorn/unix-domain-socket/
        name: unix-domain-socket
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /log
        mountPropagation: HostToContainer
        name: log
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-thdqc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/
        type: ""
      name: engine-binaries
    - hostPath:
        path: /var/lib/longhorn/unix-domain-socket/
        type: ""
      name: unix-domain-socket
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - hostPath:
        path: /var/lib/longhorn/logs
        type: DirectoryOrCreate
      name: log
    - name: kube-api-access-thdqc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:56:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:56:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:56:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:56:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:56:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f106ec08356dbb1491b214975d9edd71020ad35eefe33243f777bef8ff9652f4
      image: docker.io/longhornio/longhorn-instance-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-instance-manager@sha256:84e0a5c1d67599a445f5b4fa853152ff53f6b1bd42a7cf7c01f4152cf60782af
      lastState: {}
      name: instance-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:56:35Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.9
    podIPs:
    - ip: 10.244.3.9
    qosClass: Burstable
    startTime: "2026-01-03T20:56:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2026-01-27T16:30:32Z"
    labels:
      longhorn.io/component: instance-manager
      longhorn.io/data-engine: v1
      longhorn.io/instance-manager-image: imi-9fecca02
      longhorn.io/instance-manager-type: aio
      longhorn.io/managed-by: longhorn-manager
      longhorn.io/node: vmi2911681
    name: instance-manager-cbf6e6950ea173ae29641668c160e4f3
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: longhorn.io/v1beta2
      blockOwnerDeletion: true
      controller: true
      kind: InstanceManager
      name: instance-manager-cbf6e6950ea173ae29641668c160e4f3
      uid: 1fc0822b-d13d-489b-bab8-83dbbde48e97
    resourceVersion: "23972795"
    uid: f6d776fd-2366-4596-8097-7904aa6d1ae9
  spec:
    containers:
    - args:
      - instance-manager
      - --debug
      - daemon
      - --listen
      - :8500
      env:
      - name: TLS_DIR
        value: /tls-files/
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: DATA_ENGINE
        value: v1
      image: longhornio/longhorn-instance-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - test $(nc -zv localhost 8500 > /dev/null 2>&1 && nc -zv localhost 8501
            > /dev/null 2>&1 && nc -zv localhost 8502 > /dev/null 2>&1 && nc -zv localhost
            8503 > /dev/null 2>&1; echo $?) -eq 0
        failureThreshold: 6
        initialDelaySeconds: 3
        periodSeconds: 11
        successThreshold: 1
        timeoutSeconds: 10
      name: instance-manager
      resources:
        requests:
          cpu: 720m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host
      - mountPath: /engine-binaries/
        mountPropagation: HostToContainer
        name: engine-binaries
      - mountPath: /host/var/lib/longhorn/unix-domain-socket/
        name: unix-domain-socket
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /log
        mountPropagation: HostToContainer
        name: log
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cbbnk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/
        type: ""
      name: engine-binaries
    - hostPath:
        path: /var/lib/longhorn/unix-domain-socket/
        type: ""
      name: unix-domain-socket
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - hostPath:
        path: /var/lib/longhorn/logs
        type: DirectoryOrCreate
      name: log
    - name: kube-api-access-cbbnk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:34Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://939d953213a4a48ee2d5aae2ec371ad11efd8e37d2ef74743d14e6be64993b9b
      image: docker.io/longhornio/longhorn-instance-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-instance-manager@sha256:84e0a5c1d67599a445f5b4fa853152ff53f6b1bd42a7cf7c01f4152cf60782af
      lastState: {}
      name: instance-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-27T16:30:33Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.235
    podIPs:
    - ip: 10.244.2.235
    qosClass: Burstable
    startTime: "2026-01-27T16:30:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2026-02-13T22:59:11Z"
    labels:
      longhorn.io/component: instance-manager
      longhorn.io/data-engine: v1
      longhorn.io/instance-manager-image: imi-9fecca02
      longhorn.io/instance-manager-type: aio
      longhorn.io/managed-by: longhorn-manager
      longhorn.io/node: vmi3075398
    name: instance-manager-f2206d1c236ec31c66885bf05231058d
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: longhorn.io/v1beta2
      blockOwnerDeletion: true
      controller: true
      kind: InstanceManager
      name: instance-manager-f2206d1c236ec31c66885bf05231058d
      uid: 8c40cc09-f1cc-4707-aeb6-e312b11ff292
    resourceVersion: "29938892"
    uid: e36e2742-2bad-4000-99d1-b55dd8134ad7
  spec:
    containers:
    - args:
      - instance-manager
      - --debug
      - daemon
      - --listen
      - :8500
      env:
      - name: TLS_DIR
        value: /tls-files/
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: DATA_ENGINE
        value: v1
      image: longhornio/longhorn-instance-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - test $(nc -zv localhost 8500 > /dev/null 2>&1 && nc -zv localhost 8501
            > /dev/null 2>&1 && nc -zv localhost 8502 > /dev/null 2>&1 && nc -zv localhost
            8503 > /dev/null 2>&1; echo $?) -eq 0
        failureThreshold: 6
        initialDelaySeconds: 3
        periodSeconds: 11
        successThreshold: 1
        timeoutSeconds: 10
      name: instance-manager
      resources:
        requests:
          cpu: 720m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host
      - mountPath: /engine-binaries/
        mountPropagation: HostToContainer
        name: engine-binaries
      - mountPath: /host/var/lib/longhorn/unix-domain-socket/
        name: unix-domain-socket
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /log
        mountPropagation: HostToContainer
        name: log
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gqlfs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Never
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /
        type: ""
      name: host
    - hostPath:
        path: /var/lib/longhorn/engine-binaries/
        type: ""
      name: engine-binaries
    - hostPath:
        path: /var/lib/longhorn/unix-domain-socket/
        type: ""
      name: unix-domain-socket
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - hostPath:
        path: /var/lib/longhorn/logs
        type: DirectoryOrCreate
      name: log
    - name: kube-api-access-gqlfs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:21Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T22:59:11Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:21Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:21Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T22:59:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9195f7e30cd162c2d16e1397466250d688c0a58ce1095887e0fc9db7480598c5
      image: docker.io/longhornio/longhorn-instance-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-instance-manager@sha256:84e0a5c1d67599a445f5b4fa853152ff53f6b1bd42a7cf7c01f4152cf60782af
      lastState: {}
      name: instance-manager
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://9195f7e30cd162c2d16e1397466250d688c0a58ce1095887e0fc9db7480598c5
          exitCode: 0
          finishedAt: "2026-02-13T23:04:21Z"
          reason: Completed
          startedAt: "2026-02-13T22:59:12Z"
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Succeeded
    podIP: 10.244.4.238
    podIPs:
    - ip: 10.244.4.238
    qosClass: Burstable
    startTime: "2026-02-13T22:59:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-09T21:23:24Z"
    generateName: longhorn-csi-plugin-
    labels:
      app: longhorn-csi-plugin
      controller-revision-hash: 697988885f
      pod-template-generation: "1"
    name: longhorn-csi-plugin-2cfh7
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-csi-plugin
      uid: b37e2ff1-0cce-471e-85ac-03298aaf4f13
    resourceVersion: "26435458"
    uid: 13801771-77cb-4b5c-906f-ff9d98874385
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=/var/lib/kubelet/plugins/driver.longhorn.io/csi.sock
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      image: longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/driver.longhorn.io /registration/driver.longhorn.io-reg.sock
              /csi//*
      name: node-driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w4z4b
        readOnly: true
    - args:
      - --v=4
      - --csi-address=/csi/csi.sock
      image: longhornio/livenessprobe:v2.17.0-20251030
      imagePullPolicy: IfNotPresent
      name: longhorn-liveness-probe
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w4z4b
        readOnly: true
    - args:
      - longhorn-manager
      - -d
      - csi
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --drivername=driver.longhorn.io
      - --manager-url=http://longhorn-backend:9500/v1
      env:
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -f /csi//*
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: longhorn-csi-plugin
      ports:
      - containerPort: 9808
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      startupProbe:
        failureThreshold: 36
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/lib/kubelet/plugins/kubernetes.io/csi
        mountPropagation: Bidirectional
        name: kubernetes-csi-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /host/proc
        name: host-proc
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w4z4b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/kubernetes.io/csi
        type: DirectoryOrCreate
      name: kubernetes-csi-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: DirectoryOrCreate
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /proc
        type: ""
      name: host-proc
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-w4z4b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:23:41Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:23:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:23:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2b9da31b6abb7f53e4b9a573844ef22b3fb9213d91f89421b6ad8274c313f90b
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState:
        terminated:
          containerID: containerd://4c3c16d55c05a468477f2f0847cfea3d8bf66720f7b9cf4490946dcdae4acb13
          exitCode: 128
          finishedAt: "2026-01-15T13:28:43Z"
          message: 'failed to create containerd task: failed to create shim task:
            context deadline exceeded'
          reason: StartError
          startedAt: "1970-01-01T00:00:00Z"
      name: longhorn-csi-plugin
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2026-01-15T13:33:12Z"
    - containerID: containerd://5f9aaba3cc6c4115271b5745948261d4323ceef959658696c9ff0ec911eca74c
      image: docker.io/longhornio/livenessprobe:v2.17.0-20251030
      imageID: docker.io/longhornio/livenessprobe@sha256:7aed397ffdcb125374fa4dd44d95251598e9e97940bc5b84187b3cd3ffd655fd
      lastState: {}
      name: longhorn-liveness-probe
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-09T21:23:39Z"
    - containerID: containerd://53cc14c93fa039b06e60648d2a89dedec0ef84b050553c88f1802bcaa524f7f5
      image: docker.io/longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imageID: docker.io/longhornio/csi-node-driver-registrar@sha256:cc125aa681e8ff41ef97bf72b8c16cafffa84f8493ecbcf606e5463f90e9cbd3
      lastState: {}
      name: node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-09T21:23:33Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.30
    podIPs:
    - ip: 10.244.2.30
    qosClass: BestEffort
    startTime: "2025-12-09T21:23:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-09T21:23:24Z"
    generateName: longhorn-csi-plugin-
    labels:
      app: longhorn-csi-plugin
      controller-revision-hash: 697988885f
      pod-template-generation: "1"
    name: longhorn-csi-plugin-2kcsl
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-csi-plugin
      uid: b37e2ff1-0cce-471e-85ac-03298aaf4f13
    resourceVersion: "26438722"
    uid: 6619bdf4-3d64-449e-8da3-ec2a0a9ab033
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=/var/lib/kubelet/plugins/driver.longhorn.io/csi.sock
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      image: longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/driver.longhorn.io /registration/driver.longhorn.io-reg.sock
              /csi//*
      name: node-driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jl9nf
        readOnly: true
    - args:
      - --v=4
      - --csi-address=/csi/csi.sock
      image: longhornio/livenessprobe:v2.17.0-20251030
      imagePullPolicy: IfNotPresent
      name: longhorn-liveness-probe
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jl9nf
        readOnly: true
    - args:
      - longhorn-manager
      - -d
      - csi
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --drivername=driver.longhorn.io
      - --manager-url=http://longhorn-backend:9500/v1
      env:
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -f /csi//*
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: longhorn-csi-plugin
      ports:
      - containerPort: 9808
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      startupProbe:
        failureThreshold: 36
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/lib/kubelet/plugins/kubernetes.io/csi
        mountPropagation: Bidirectional
        name: kubernetes-csi-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /host/proc
        name: host-proc
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jl9nf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/kubernetes.io/csi
        type: DirectoryOrCreate
      name: kubernetes-csi-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: DirectoryOrCreate
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /proc
        type: ""
      name: host-proc
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-jl9nf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:24:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:23:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:23:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6c1613c223eaedc7175896c5af375bcb66f9cdaa604a66d758d6a23325bac11b
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState:
        terminated:
          containerID: containerd://1891b8c70574d5eb768ebbb4a5546e65baa8d552758d7f3a215d220a71d82aa2
          exitCode: 2
          finishedAt: "2026-01-22T16:57:52Z"
          reason: Error
          startedAt: "2025-12-09T21:24:09Z"
      name: longhorn-csi-plugin
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:57:55Z"
    - containerID: containerd://bbf96df04b39089b61df4208b61fc7146e001fb410dafea2dd9e1948daf85101
      image: docker.io/longhornio/livenessprobe:v2.17.0-20251030
      imageID: docker.io/longhornio/livenessprobe@sha256:7aed397ffdcb125374fa4dd44d95251598e9e97940bc5b84187b3cd3ffd655fd
      lastState: {}
      name: longhorn-liveness-probe
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-09T21:24:09Z"
    - containerID: containerd://6b1a91216687853c628f7ee2995812af4cb71386b3abb6a893a54d1ad17657cd
      image: docker.io/longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imageID: docker.io/longhornio/csi-node-driver-registrar@sha256:cc125aa681e8ff41ef97bf72b8c16cafffa84f8493ecbcf606e5463f90e9cbd3
      lastState: {}
      name: node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-09T21:24:02Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.158
    podIPs:
    - ip: 10.244.1.158
    qosClass: BestEffort
    startTime: "2025-12-09T21:23:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-03T20:54:42Z"
    generateName: longhorn-csi-plugin-
    labels:
      app: longhorn-csi-plugin
      controller-revision-hash: 697988885f
      pod-template-generation: "1"
    name: longhorn-csi-plugin-c2chf
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-csi-plugin
      uid: b37e2ff1-0cce-471e-85ac-03298aaf4f13
    resourceVersion: "26775108"
    uid: e4179167-69f2-465d-95bb-7134e41201e2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=/var/lib/kubelet/plugins/driver.longhorn.io/csi.sock
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      image: longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/driver.longhorn.io /registration/driver.longhorn.io-reg.sock
              /csi//*
      name: node-driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9qscm
        readOnly: true
    - args:
      - --v=4
      - --csi-address=/csi/csi.sock
      image: longhornio/livenessprobe:v2.17.0-20251030
      imagePullPolicy: IfNotPresent
      name: longhorn-liveness-probe
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9qscm
        readOnly: true
    - args:
      - longhorn-manager
      - -d
      - csi
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --drivername=driver.longhorn.io
      - --manager-url=http://longhorn-backend:9500/v1
      env:
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -f /csi//*
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: longhorn-csi-plugin
      ports:
      - containerPort: 9808
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      startupProbe:
        failureThreshold: 36
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/lib/kubelet/plugins/kubernetes.io/csi
        mountPropagation: Bidirectional
        name: kubernetes-csi-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /host/proc
        name: host-proc
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9qscm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/kubernetes.io/csi
        type: DirectoryOrCreate
      name: kubernetes-csi-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: DirectoryOrCreate
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /proc
        type: ""
      name: host-proc
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-9qscm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:55:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://90e22c7ccf884f1c1728357bc3508a79d46b4f0fcd095dade0873b805a555fd7
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState: {}
      name: longhorn-csi-plugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:55:31Z"
    - containerID: containerd://34c0e51bdd7f83ca8c59396c0d37034b242e3c2c3f04f7ea89fc218af10eb7e0
      image: docker.io/longhornio/livenessprobe:v2.17.0-20251030
      imageID: docker.io/longhornio/livenessprobe@sha256:7aed397ffdcb125374fa4dd44d95251598e9e97940bc5b84187b3cd3ffd655fd
      lastState: {}
      name: longhorn-liveness-probe
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:55:31Z"
    - containerID: containerd://05d791c3f05cc540b837672f88a84ed5e09755712671ae2898d452afddb57ded
      image: docker.io/longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imageID: docker.io/longhornio/csi-node-driver-registrar@sha256:cc125aa681e8ff41ef97bf72b8c16cafffa84f8493ecbcf606e5463f90e9cbd3
      lastState: {}
      name: node-driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:55:20Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.6
    podIPs:
    - ip: 10.244.3.6
    qosClass: BestEffort
    startTime: "2026-01-03T20:54:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-07T23:49:23Z"
    generateName: longhorn-csi-plugin-
    labels:
      app: longhorn-csi-plugin
      controller-revision-hash: 697988885f
      pod-template-generation: "1"
    name: longhorn-csi-plugin-gpnr7
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-csi-plugin
      uid: b37e2ff1-0cce-471e-85ac-03298aaf4f13
    resourceVersion: "27681427"
    uid: 702c666e-c322-4260-87c5-c4513704da14
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=/var/lib/kubelet/plugins/driver.longhorn.io/csi.sock
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      image: longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/driver.longhorn.io /registration/driver.longhorn.io-reg.sock
              /csi//*
      name: node-driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kk7rc
        readOnly: true
    - args:
      - --v=4
      - --csi-address=/csi/csi.sock
      image: longhornio/livenessprobe:v2.17.0-20251030
      imagePullPolicy: IfNotPresent
      name: longhorn-liveness-probe
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kk7rc
        readOnly: true
    - args:
      - longhorn-manager
      - -d
      - csi
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --drivername=driver.longhorn.io
      - --manager-url=http://longhorn-backend:9500/v1
      env:
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -f /csi//*
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: longhorn-csi-plugin
      ports:
      - containerPort: 9808
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      startupProbe:
        failureThreshold: 36
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/lib/kubelet/plugins/kubernetes.io/csi
        mountPropagation: Bidirectional
        name: kubernetes-csi-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /host/proc
        name: host-proc
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kk7rc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/kubernetes.io/csi
        type: DirectoryOrCreate
      name: kubernetes-csi-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: DirectoryOrCreate
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /proc
        type: ""
      name: host-proc
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-kk7rc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:52:51Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:49:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:52:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:52:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:49:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7dcd0f17043b570c4c12e943197e571f8ff3355ab052cbb705c951997ef7971a
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState: {}
      name: longhorn-csi-plugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-07T23:52:50Z"
    - containerID: containerd://57538535732015d386fef2bc4ef7d510708c8f11da55578908bd3c60d458a868
      image: docker.io/longhornio/livenessprobe:v2.17.0-20251030
      imageID: docker.io/longhornio/livenessprobe@sha256:7aed397ffdcb125374fa4dd44d95251598e9e97940bc5b84187b3cd3ffd655fd
      lastState: {}
      name: longhorn-liveness-probe
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-07T23:52:49Z"
    - containerID: containerd://8da47e173f30a14aeee6cc203993f68da256a508ad8bc4143226dc1863f06393
      image: docker.io/longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imageID: docker.io/longhornio/csi-node-driver-registrar@sha256:cc125aa681e8ff41ef97bf72b8c16cafffa84f8493ecbcf606e5463f90e9cbd3
      lastState:
        terminated:
          containerID: containerd://11761c20130ebe937cff6ee139f6046fbed2b1b6e3210944acd69dc26a573343
          exitCode: 1
          finishedAt: "2026-02-07T23:51:34Z"
          reason: Error
          startedAt: "2026-02-07T23:51:03Z"
      name: node-driver-registrar
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-02-07T23:52:56Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.57
    podIPs:
    - ip: 10.244.0.57
    qosClass: BestEffort
    startTime: "2026-02-07T23:49:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-09T23:43:50Z"
    generateName: longhorn-csi-plugin-
    labels:
      app: longhorn-csi-plugin
      controller-revision-hash: 697988885f
      pod-template-generation: "1"
    name: longhorn-csi-plugin-zrzb4
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-csi-plugin
      uid: b37e2ff1-0cce-471e-85ac-03298aaf4f13
    resourceVersion: "29938524"
    uid: 1a988a9f-cbab-4ed4-8b49-61181c93909e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    containers:
    - args:
      - --v=2
      - --csi-address=$(ADDRESS)
      - --kubelet-registration-path=/var/lib/kubelet/plugins/driver.longhorn.io/csi.sock
      env:
      - name: ADDRESS
        value: /csi/csi.sock
      image: longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -rf /registration/driver.longhorn.io /registration/driver.longhorn.io-reg.sock
              /csi//*
      name: node-driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cvzbg
        readOnly: true
    - args:
      - --v=4
      - --csi-address=/csi/csi.sock
      image: longhornio/livenessprobe:v2.17.0-20251030
      imagePullPolicy: IfNotPresent
      name: longhorn-liveness-probe
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cvzbg
        readOnly: true
    - args:
      - longhorn-manager
      - -d
      - csi
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --drivername=driver.longhorn.io
      - --manager-url=http://longhorn-backend:9500/v1
      env:
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -f /csi//*
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      name: longhorn-csi-plugin
      ports:
      - containerPort: 9808
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      startupProbe:
        failureThreshold: 36
        httpGet:
          path: /healthz
          port: 9808
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 4
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi/
        name: socket-dir
      - mountPath: /var/lib/kubelet/plugins/kubernetes.io/csi
        mountPropagation: Bidirectional
        name: kubernetes-csi-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /host/proc
        name: host-proc
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cvzbg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/kubernetes.io/csi
        type: DirectoryOrCreate
      name: kubernetes-csi-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry
        type: DirectoryOrCreate
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/plugins/driver.longhorn.io
        type: DirectoryOrCreate
      name: socket-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: DirectoryOrCreate
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /proc
        type: ""
      name: host-proc
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-api-access-cvzbg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:03:12Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T19:26:25Z"
      message: 'containers with unready status: [node-driver-registrar longhorn-liveness-probe
        longhorn-csi-plugin]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T19:26:25Z"
      message: 'containers with unready status: [node-driver-registrar longhorn-liveness-probe
        longhorn-csi-plugin]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://48ec4e11a69c80a5e8e4c5611e114ca05e60ae1d2de23c40d4de2946924e0468
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState:
        terminated:
          containerID: containerd://48ec4e11a69c80a5e8e4c5611e114ca05e60ae1d2de23c40d4de2946924e0468
          exitCode: 2
          finishedAt: "2026-02-13T23:03:10Z"
          reason: Error
          startedAt: "2026-02-13T23:03:09Z"
      name: longhorn-csi-plugin
      ready: false
      restartCount: 1097
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=longhorn-csi-plugin pod=longhorn-csi-plugin-zrzb4_longhorn-system(1a988a9f-cbab-4ed4-8b49-61181c93909e)
          reason: CrashLoopBackOff
    - containerID: containerd://5e2af033a5e70b6b27c68aa801da29a89f5ea19273510b9fc2eb912f43265a27
      image: docker.io/longhornio/livenessprobe:v2.17.0-20251030
      imageID: docker.io/longhornio/livenessprobe@sha256:7aed397ffdcb125374fa4dd44d95251598e9e97940bc5b84187b3cd3ffd655fd
      lastState:
        terminated:
          containerID: containerd://5e2af033a5e70b6b27c68aa801da29a89f5ea19273510b9fc2eb912f43265a27
          exitCode: 2
          finishedAt: "2026-02-13T23:03:10Z"
          reason: Error
          startedAt: "2026-02-13T23:03:09Z"
      name: longhorn-liveness-probe
      ready: false
      restartCount: 1097
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=longhorn-liveness-probe
            pod=longhorn-csi-plugin-zrzb4_longhorn-system(1a988a9f-cbab-4ed4-8b49-61181c93909e)
          reason: CrashLoopBackOff
    - containerID: containerd://6ea03722c5b206abdab8300d79db3c4ccc33c3759ba55c8180cc37f483d8807e
      image: docker.io/longhornio/csi-node-driver-registrar:v2.15.0-20251030
      imageID: docker.io/longhornio/csi-node-driver-registrar@sha256:cc125aa681e8ff41ef97bf72b8c16cafffa84f8493ecbcf606e5463f90e9cbd3
      lastState:
        terminated:
          containerID: containerd://6ea03722c5b206abdab8300d79db3c4ccc33c3759ba55c8180cc37f483d8807e
          exitCode: 2
          finishedAt: "2026-02-13T23:03:10Z"
          reason: Error
          startedAt: "2026-02-13T23:03:09Z"
      name: node-driver-registrar
      ready: false
      restartCount: 1098
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=node-driver-registrar
            pod=longhorn-csi-plugin-zrzb4_longhorn-system(1a988a9f-cbab-4ed4-8b49-61181c93909e)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.252
    podIPs:
    - ip: 10.244.4.252
    qosClass: BestEffort
    startTime: "2026-02-09T23:43:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
    creationTimestamp: "2026-01-22T16:38:23Z"
    generateName: longhorn-driver-deployer-6bcdb9864b-
    labels:
      app: longhorn-driver-deployer
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
      pod-template-hash: 6bcdb9864b
    name: longhorn-driver-deployer-6bcdb9864b-qgbbc
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: longhorn-driver-deployer-6bcdb9864b
      uid: 9f10720e-61c5-491d-a5c6-5ce42e6f4131
    resourceVersion: "25557820"
    uid: e61ca6fb-4ab1-4f3b-83d7-aeb18f005fe9
  spec:
    containers:
    - command:
      - longhorn-manager
      - -d
      - deploy-driver
      - --manager-image
      - longhornio/longhorn-manager:v1.10.1
      - --manager-url
      - http://longhorn-backend:9500/v1
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: SERVICE_ACCOUNT
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.serviceAccountName
      - name: CSI_ATTACHER_IMAGE
        value: longhornio/csi-attacher:v4.10.0-20251030
      - name: CSI_PROVISIONER_IMAGE
        value: longhornio/csi-provisioner:v5.3.0-20251030
      - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE
        value: longhornio/csi-node-driver-registrar:v2.15.0-20251030
      - name: CSI_RESIZER_IMAGE
        value: longhornio/csi-resizer:v1.14.0-20251030
      - name: CSI_SNAPSHOTTER_IMAGE
        value: longhornio/csi-snapshotter:v8.4.0-20251030
      - name: CSI_LIVENESS_PROBE_IMAGE
        value: longhornio/livenessprobe:v2.17.0-20251030
      - name: CSI_ATTACHER_REPLICA_COUNT
        value: "1"
      - name: CSI_PROVISIONER_REPLICA_COUNT
        value: "1"
      - name: CSI_RESIZER_REPLICA_COUNT
        value: "1"
      - name: CSI_SNAPSHOTTER_REPLICA_COUNT
        value: "1"
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: longhorn-driver-deployer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-btplc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - sh
      - -c
      - while [ $(curl -m 1 -s -o /dev/null -w "%{http_code}" http://longhorn-backend:9500/v1)
        != "200" ]; do echo waiting; sleep 2; done
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: wait-longhorn-manager
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-btplc
        readOnly: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-btplc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:27Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://017c3c70e7116338037f594158423522fc59d7179e569a0c39d248f8be2da042
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState: {}
      name: longhorn-driver-deployer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:38:28Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    initContainerStatuses:
    - containerID: containerd://067bcd7691e50d365a69b3453304bb92bc1c93a6ffa2056fb955e33241c7e55f
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState: {}
      name: wait-longhorn-manager
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://067bcd7691e50d365a69b3453304bb92bc1c93a6ffa2056fb955e33241c7e55f
          exitCode: 0
          finishedAt: "2026-01-22T16:38:27Z"
          reason: Completed
          startedAt: "2026-01-22T16:38:27Z"
    phase: Running
    podIP: 10.244.3.70
    podIPs:
    - ip: 10.244.3.70
    qosClass: BestEffort
    startTime: "2026-01-22T16:38:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-07T23:49:23Z"
    generateName: longhorn-manager-
    labels:
      app: longhorn-manager
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      controller-revision-hash: c568b7bc
      helm.sh/chart: longhorn-1.10.1
      longhorn.io/admission-webhook: longhorn-admission-webhook
      longhorn.io/recovery-backend: longhorn-recovery-backend
      pod-template-generation: "1"
    name: longhorn-manager-8gz97
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-manager
      uid: 3978df76-cbfa-4bf8-808f-f13074ac837d
    resourceVersion: "29855940"
    uid: b80dee9e-f3e3-4ad3-bb39-6fdfd6458096
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    containers:
    - command:
      - longhorn-manager
      - -d
      - daemon
      - --engine-image
      - longhornio/longhorn-engine:v1.10.1
      - --instance-manager-image
      - longhornio/longhorn-instance-manager:v1.10.1
      - --share-manager-image
      - longhornio/longhorn-share-manager:v1.10.1
      - --backing-image-manager-image
      - longhornio/backing-image-manager:v1.10.1
      - --support-bundle-manager-image
      - longhornio/support-bundle-kit:v0.0.71
      - --manager-image
      - longhornio/longhorn-manager:v1.10.1
      - --service-account
      - longhorn-service-account
      - --upgrade-version-check
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: longhorn-manager
      ports:
      - containerPort: 9500
        name: manager
        protocol: TCP
      - containerPort: 9502
        name: admission-wh
        protocol: TCP
      - containerPort: 9503
        name: recov-backend
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /v1/healthz
          port: 9502
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/boot/
        name: boot
        readOnly: true
      - mountPath: /host/dev/
        name: dev
      - mountPath: /host/proc/
        name: proc
        readOnly: true
      - mountPath: /host/etc/
        name: etc
        readOnly: true
      - mountPath: /var/lib/longhorn/
        mountPropagation: Bidirectional
        name: longhorn
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d7kms
        readOnly: true
    - command:
      - sh
      - -c
      - echo share-manager image pulled && sleep infinity
      image: longhornio/longhorn-share-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: pre-pull-share-manager-image
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d7kms
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot/
        type: ""
      name: boot
    - hostPath:
        path: /dev/
        type: ""
      name: dev
    - hostPath:
        path: /proc/
        type: ""
      name: proc
    - hostPath:
        path: /etc/
        type: ""
      name: etc
    - hostPath:
        path: /var/lib/longhorn/
        type: ""
      name: longhorn
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - name: kube-api-access-d7kms
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:53:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:49:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T18:04:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T18:04:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:49:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://abbbe474b772176637a721cf20cf0c99ad06b7f5bcb0dca1ce763953d2ed880c
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState: {}
      name: longhorn-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-07T23:52:08Z"
    - containerID: containerd://5d94de9ebe348af672c780b9081229c72ee4743b9a4c37342ebbd26ff9a8f71e
      image: docker.io/longhornio/longhorn-share-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-share-manager@sha256:1edc95ae8f9e9699f9b082bf0eac82b338b2f120462424201957cb6287b2e3e9
      lastState: {}
      name: pre-pull-share-manager-image
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-07T23:53:05Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.55
    podIPs:
    - ip: 10.244.0.55
    qosClass: BestEffort
    startTime: "2026-02-07T23:49:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-09T21:22:06Z"
    generateName: longhorn-manager-
    labels:
      app: longhorn-manager
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      controller-revision-hash: c568b7bc
      helm.sh/chart: longhorn-1.10.1
      longhorn.io/admission-webhook: longhorn-admission-webhook
      longhorn.io/recovery-backend: longhorn-recovery-backend
      pod-template-generation: "1"
    name: longhorn-manager-8skhp
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-manager
      uid: 3978df76-cbfa-4bf8-808f-f13074ac837d
    resourceVersion: "26438649"
    uid: 76bf672c-fdef-45ec-a082-b664b3d4253c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    containers:
    - command:
      - longhorn-manager
      - -d
      - daemon
      - --engine-image
      - longhornio/longhorn-engine:v1.10.1
      - --instance-manager-image
      - longhornio/longhorn-instance-manager:v1.10.1
      - --share-manager-image
      - longhornio/longhorn-share-manager:v1.10.1
      - --backing-image-manager-image
      - longhornio/backing-image-manager:v1.10.1
      - --support-bundle-manager-image
      - longhornio/support-bundle-kit:v0.0.71
      - --manager-image
      - longhornio/longhorn-manager:v1.10.1
      - --service-account
      - longhorn-service-account
      - --upgrade-version-check
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: longhorn-manager
      ports:
      - containerPort: 9500
        name: manager
        protocol: TCP
      - containerPort: 9502
        name: admission-wh
        protocol: TCP
      - containerPort: 9503
        name: recov-backend
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /v1/healthz
          port: 9502
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/boot/
        name: boot
        readOnly: true
      - mountPath: /host/dev/
        name: dev
      - mountPath: /host/proc/
        name: proc
        readOnly: true
      - mountPath: /host/etc/
        name: etc
        readOnly: true
      - mountPath: /var/lib/longhorn/
        mountPropagation: Bidirectional
        name: longhorn
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rz7w8
        readOnly: true
    - command:
      - sh
      - -c
      - echo share-manager image pulled && sleep infinity
      image: longhornio/longhorn-share-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: pre-pull-share-manager-image
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rz7w8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot/
        type: ""
      name: boot
    - hostPath:
        path: /dev/
        type: ""
      name: dev
    - hostPath:
        path: /proc/
        type: ""
      name: proc
    - hostPath:
        path: /etc/
        type: ""
      name: etc
    - hostPath:
        path: /var/lib/longhorn/
        type: ""
      name: longhorn
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - name: kube-api-access-rz7w8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8ee6630b5b14215c05a28af32b18e192dcb801e32261bd66f48b3bd5b0ec5c7d
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState: {}
      name: longhorn-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-09T21:22:18Z"
    - containerID: containerd://705638f9937e3a9fbf51d21a9762db168faa458d8511bada5e93bfb9416a0b28
      image: docker.io/longhornio/longhorn-share-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-share-manager@sha256:1edc95ae8f9e9699f9b082bf0eac82b338b2f120462424201957cb6287b2e3e9
      lastState: {}
      name: pre-pull-share-manager-image
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-09T21:22:28Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.149
    podIPs:
    - ip: 10.244.1.149
    qosClass: BestEffort
    startTime: "2025-12-09T21:22:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-09T23:43:50Z"
    generateName: longhorn-manager-
    labels:
      app: longhorn-manager
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      controller-revision-hash: c568b7bc
      helm.sh/chart: longhorn-1.10.1
      longhorn.io/admission-webhook: longhorn-admission-webhook
      longhorn.io/recovery-backend: longhorn-recovery-backend
      pod-template-generation: "1"
    name: longhorn-manager-k4rxg
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-manager
      uid: 3978df76-cbfa-4bf8-808f-f13074ac837d
    resourceVersion: "29939638"
    uid: f7faaf6c-0c80-459f-b677-344615cf7fd1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    containers:
    - command:
      - longhorn-manager
      - -d
      - daemon
      - --engine-image
      - longhornio/longhorn-engine:v1.10.1
      - --instance-manager-image
      - longhornio/longhorn-instance-manager:v1.10.1
      - --share-manager-image
      - longhornio/longhorn-share-manager:v1.10.1
      - --backing-image-manager-image
      - longhornio/backing-image-manager:v1.10.1
      - --support-bundle-manager-image
      - longhornio/support-bundle-kit:v0.0.71
      - --manager-image
      - longhornio/longhorn-manager:v1.10.1
      - --service-account
      - longhorn-service-account
      - --upgrade-version-check
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: longhorn-manager
      ports:
      - containerPort: 9500
        name: manager
        protocol: TCP
      - containerPort: 9502
        name: admission-wh
        protocol: TCP
      - containerPort: 9503
        name: recov-backend
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /v1/healthz
          port: 9502
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/boot/
        name: boot
        readOnly: true
      - mountPath: /host/dev/
        name: dev
      - mountPath: /host/proc/
        name: proc
        readOnly: true
      - mountPath: /host/etc/
        name: etc
        readOnly: true
      - mountPath: /var/lib/longhorn/
        mountPropagation: Bidirectional
        name: longhorn
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ggmgn
        readOnly: true
    - command:
      - sh
      - -c
      - echo share-manager image pulled && sleep infinity
      image: longhornio/longhorn-share-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: pre-pull-share-manager-image
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ggmgn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot/
        type: ""
      name: boot
    - hostPath:
        path: /dev/
        type: ""
      name: dev
    - hostPath:
        path: /proc/
        type: ""
      name: proc
    - hostPath:
        path: /etc/
        type: ""
      name: etc
    - hostPath:
        path: /var/lib/longhorn/
        type: ""
      name: longhorn
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - name: kube-api-access-ggmgn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T22:43:38Z"
      message: 'containers with unready status: [longhorn-manager pre-pull-share-manager-image]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T22:43:38Z"
      message: 'containers with unready status: [longhorn-manager pre-pull-share-manager-image]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e9e4d4b28e0157a5109d496240d422bf3e93fd3154a4ca462de0ab2693ebde83
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState:
        terminated:
          containerID: containerd://e9e4d4b28e0157a5109d496240d422bf3e93fd3154a4ca462de0ab2693ebde83
          exitCode: 1
          finishedAt: "2026-02-13T23:04:17Z"
          reason: Error
          startedAt: "2026-02-13T23:04:16Z"
      name: longhorn-manager
      ready: false
      restartCount: 1105
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=longhorn-manager pod=longhorn-manager-k4rxg_longhorn-system(f7faaf6c-0c80-459f-b677-344615cf7fd1)
          reason: CrashLoopBackOff
    - containerID: containerd://10c9008b2848c7728fe07fddc62eb902b6db63ef0b1ae1882a18996cbceae60e
      image: docker.io/longhornio/longhorn-share-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-share-manager@sha256:1edc95ae8f9e9699f9b082bf0eac82b338b2f120462424201957cb6287b2e3e9
      lastState:
        terminated:
          containerID: containerd://10c9008b2848c7728fe07fddc62eb902b6db63ef0b1ae1882a18996cbceae60e
          exitCode: 137
          finishedAt: "2026-02-13T23:06:47Z"
          reason: Error
          startedAt: "2026-02-13T23:06:17Z"
      name: pre-pull-share-manager-image
      ready: false
      restartCount: 984
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=pre-pull-share-manager-image
            pod=longhorn-manager-k4rxg_longhorn-system(f7faaf6c-0c80-459f-b677-344615cf7fd1)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.11
    podIPs:
    - ip: 10.244.4.11
    qosClass: BestEffort
    startTime: "2026-02-09T23:43:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2025-12-09T21:22:06Z"
    generateName: longhorn-manager-
    labels:
      app: longhorn-manager
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      controller-revision-hash: c568b7bc
      helm.sh/chart: longhorn-1.10.1
      longhorn.io/admission-webhook: longhorn-admission-webhook
      longhorn.io/recovery-backend: longhorn-recovery-backend
      pod-template-generation: "1"
    name: longhorn-manager-qrkw4
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-manager
      uid: 3978df76-cbfa-4bf8-808f-f13074ac837d
    resourceVersion: "26435471"
    uid: 83d4c5e5-3f2e-4910-a906-e371f28ac0ee
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    containers:
    - command:
      - longhorn-manager
      - -d
      - daemon
      - --engine-image
      - longhornio/longhorn-engine:v1.10.1
      - --instance-manager-image
      - longhornio/longhorn-instance-manager:v1.10.1
      - --share-manager-image
      - longhornio/longhorn-share-manager:v1.10.1
      - --backing-image-manager-image
      - longhornio/backing-image-manager:v1.10.1
      - --support-bundle-manager-image
      - longhornio/support-bundle-kit:v0.0.71
      - --manager-image
      - longhornio/longhorn-manager:v1.10.1
      - --service-account
      - longhorn-service-account
      - --upgrade-version-check
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: longhorn-manager
      ports:
      - containerPort: 9500
        name: manager
        protocol: TCP
      - containerPort: 9502
        name: admission-wh
        protocol: TCP
      - containerPort: 9503
        name: recov-backend
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /v1/healthz
          port: 9502
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/boot/
        name: boot
        readOnly: true
      - mountPath: /host/dev/
        name: dev
      - mountPath: /host/proc/
        name: proc
        readOnly: true
      - mountPath: /host/etc/
        name: etc
        readOnly: true
      - mountPath: /var/lib/longhorn/
        mountPropagation: Bidirectional
        name: longhorn
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f2nss
        readOnly: true
    - command:
      - sh
      - -c
      - echo share-manager image pulled && sleep infinity
      image: longhornio/longhorn-share-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: pre-pull-share-manager-image
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f2nss
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot/
        type: ""
      name: boot
    - hostPath:
        path: /dev/
        type: ""
      name: dev
    - hostPath:
        path: /proc/
        type: ""
      name: proc
    - hostPath:
        path: /etc/
        type: ""
      name: etc
    - hostPath:
        path: /var/lib/longhorn/
        type: ""
      name: longhorn
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - name: kube-api-access-f2nss
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-09T21:22:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://88103865f93bc119249520de417b221cb6cc2c1b651c0c967946bc32de825947
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState:
        terminated:
          containerID: containerd://5a97fdb7839d6c4468f49809bc90b169136333419c91d782402613d1bf692295
          exitCode: 1
          finishedAt: "2025-12-09T21:22:46Z"
          reason: Error
          startedAt: "2025-12-09T21:22:31Z"
      name: longhorn-manager
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-12-09T21:22:49Z"
    - containerID: containerd://1d7c5d3878bfe6f8790f13de3740b7033350d320eda59b276f966ba079eaf89b
      image: docker.io/longhornio/longhorn-share-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-share-manager@sha256:1edc95ae8f9e9699f9b082bf0eac82b338b2f120462424201957cb6287b2e3e9
      lastState: {}
      name: pre-pull-share-manager-image
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-09T21:22:43Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.25
    podIPs:
    - ip: 10.244.2.25
    qosClass: BestEffort
    startTime: "2025-12-09T21:22:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-03T20:54:42Z"
    generateName: longhorn-manager-
    labels:
      app: longhorn-manager
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      controller-revision-hash: c568b7bc
      helm.sh/chart: longhorn-1.10.1
      longhorn.io/admission-webhook: longhorn-admission-webhook
      longhorn.io/recovery-backend: longhorn-recovery-backend
      pod-template-generation: "1"
    name: longhorn-manager-wnw27
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: longhorn-manager
      uid: 3978df76-cbfa-4bf8-808f-f13074ac837d
    resourceVersion: "26775096"
    uid: 19ed071d-2717-44f4-b539-ffc077951147
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    containers:
    - command:
      - longhorn-manager
      - -d
      - daemon
      - --engine-image
      - longhornio/longhorn-engine:v1.10.1
      - --instance-manager-image
      - longhornio/longhorn-instance-manager:v1.10.1
      - --share-manager-image
      - longhornio/longhorn-share-manager:v1.10.1
      - --backing-image-manager-image
      - longhornio/backing-image-manager:v1.10.1
      - --support-bundle-manager-image
      - longhornio/support-bundle-kit:v0.0.71
      - --manager-image
      - longhornio/longhorn-manager:v1.10.1
      - --service-account
      - longhorn-service-account
      - --upgrade-version-check
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: longhornio/longhorn-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: longhorn-manager
      ports:
      - containerPort: 9500
        name: manager
        protocol: TCP
      - containerPort: 9502
        name: admission-wh
        protocol: TCP
      - containerPort: 9503
        name: recov-backend
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /v1/healthz
          port: 9502
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/boot/
        name: boot
        readOnly: true
      - mountPath: /host/dev/
        name: dev
      - mountPath: /host/proc/
        name: proc
        readOnly: true
      - mountPath: /host/etc/
        name: etc
        readOnly: true
      - mountPath: /var/lib/longhorn/
        mountPropagation: Bidirectional
        name: longhorn
      - mountPath: /tls-files/
        name: longhorn-grpc-tls
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mv5tl
        readOnly: true
    - command:
      - sh
      - -c
      - echo share-manager image pulled && sleep infinity
      image: longhornio/longhorn-share-manager:v1.10.1
      imagePullPolicy: IfNotPresent
      name: pre-pull-share-manager-image
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mv5tl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-service-account
    serviceAccountName: longhorn-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /boot/
        type: ""
      name: boot
    - hostPath:
        path: /dev/
        type: ""
      name: dev
    - hostPath:
        path: /proc/
        type: ""
      name: proc
    - hostPath:
        path: /etc/
        type: ""
      name: etc
    - hostPath:
        path: /var/lib/longhorn/
        type: ""
      name: longhorn
    - name: longhorn-grpc-tls
      secret:
        defaultMode: 420
        optional: true
        secretName: longhorn-grpc-tls
    - name: kube-api-access-mv5tl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:55:36Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://da618190b8fed65cfb2bc7ea25f561206679157ddb33fc2cf692bec226f33a63
      image: docker.io/longhornio/longhorn-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-manager@sha256:afda26c16e7ab106f94dbc11da1bc91f410487d2e66609ebd126f0d908f7243a
      lastState: {}
      name: longhorn-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:55:28Z"
    - containerID: containerd://3ac8302e3358c8da02a13c49f6e2ecdc05203dc784071da81d9f7154e5643e4e
      image: docker.io/longhornio/longhorn-share-manager:v1.10.1
      imageID: docker.io/longhornio/longhorn-share-manager@sha256:1edc95ae8f9e9699f9b082bf0eac82b338b2f120462424201957cb6287b2e3e9
      lastState: {}
      name: pre-pull-share-manager-image
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:55:35Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.8
    podIPs:
    - ip: 10.244.3.8
    qosClass: BestEffort
    startTime: "2026-01-03T20:54:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
    creationTimestamp: "2026-01-15T13:29:43Z"
    generateName: longhorn-ui-56bc6dd9cb-
    labels:
      app: longhorn-ui
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
      pod-template-hash: 56bc6dd9cb
    name: longhorn-ui-56bc6dd9cb-rdrlg
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: longhorn-ui-56bc6dd9cb
      uid: d9a245ba-6c67-4f5b-850f-e2d94c5061d3
    resourceVersion: "25557837"
    uid: 50f265da-2d9c-45e4-8761-4d9cc5c03e50
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - longhorn-ui
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: LONGHORN_MANAGER_IP
        value: http://longhorn-backend:9500
      - name: LONGHORN_UI_PORT
        value: "8000"
      image: longhornio/longhorn-ui:v1.10.1
      imagePullPolicy: IfNotPresent
      name: longhorn-ui
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/cache/nginx/
        name: nginx-cache
      - mountPath: /var/config/nginx/
        name: nginx-config
      - mountPath: /var/run/
        name: var-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9br2g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 1000000000
    priorityClassName: longhorn-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: longhorn-ui-service-account
    serviceAccountName: longhorn-ui-service-account
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: nginx-cache
    - emptyDir: {}
      name: nginx-config
    - emptyDir: {}
      name: var-run
    - name: kube-api-access-9br2g
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0da0a44845d53af82c4ad9dd50dbb095106a026bd43d6d0b5c142df58b470f8f
      image: docker.io/longhornio/longhorn-ui:v1.10.1
      imageID: docker.io/longhornio/longhorn-ui@sha256:62fd171f4fbed01ebb51653674c68ea1c531aa562dab23cb029033dffd6bccc6
      lastState: {}
      name: longhorn-ui
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-15T13:29:54Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.249
    podIPs:
    - ip: 10.244.3.249
    qosClass: BestEffort
    startTime: "2026-01-15T13:29:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-10T22:39:04+01:00"
    creationTimestamp: "2026-02-11T10:25:13Z"
    generateName: mlflow-6f55659b89-
    labels:
      app: mlflow
      pod-template-hash: 6f55659b89
    name: mlflow-6f55659b89-f6xvk
    namespace: mlflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: mlflow-6f55659b89
      uid: 61107e71-57dc-4a5c-b23f-c17bc61a5dc5
    resourceVersion: "29873438"
    uid: 82327db8-9cd5-42fe-abcf-f20169cf00d0
  spec:
    containers:
    - command:
      - mlflow
      - server
      - --host
      - 0.0.0.0
      - --port
      - "5000"
      - --backend-store-uri
      - sqlite:////mlflow/mlflow.db
      - --default-artifact-root
      - mlflow-artifacts:/
      - --artifacts-destination
      - /mlflow/artifacts
      - --serve-artifacts
      image: ghcr.io/mlflow/mlflow:v2.9.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 5000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: mlflow
      ports:
      - containerPort: 5000
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 5000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mlflow
        name: mlflow-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-snk69
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: mlflow-data
      persistentVolumeClaim:
        claimName: mlflow-artifacts-pvc
    - name: kube-api-access-snk69
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:15Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T19:11:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T19:11:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1dd6c7cbb247f12bb2c35e5808aea2922e105da26d95a5e1e484bb707518422a
      image: ghcr.io/mlflow/mlflow:v2.9.2
      imageID: ghcr.io/mlflow/mlflow@sha256:27f71413b741506d3836f29e7d5965e60e5aacdc2e68f2ef1644517d69e0adad
      lastState: {}
      name: mlflow
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-11T10:25:14Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.34
    podIPs:
    - ip: 10.244.2.34
    qosClass: Burstable
    startTime: "2026-02-11T10:25:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-22T16:58:03Z"
    generateName: mlflow-postgresql-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: mlflow-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 18.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: mlflow-postgresql-55cd7bb5c6
      helm.sh/chart: postgresql-18.2.0
      statefulset.kubernetes.io/pod-name: mlflow-postgresql-0
    name: mlflow-postgresql-0
    namespace: mlflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: mlflow-postgresql
      uid: efbed39c-c01b-476a-a211-0f213f7e7fc6
    resourceVersion: "26438713"
    uid: 33101bef-99e3-4c8f-a8ab-db6a20c83830
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: mlflow-postgresql
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: OPENSSL_FIPS
        value: "yes"
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: mlflow
      - name: POSTGRES_PASSWORD_FILE
        value: /opt/bitnami/postgresql/secrets/password
      - name: POSTGRES_POSTGRES_PASSWORD_FILE
        value: /opt/bitnami/postgresql/secrets/postgres-password
      - name: POSTGRES_DATABASE
        value: mlflow
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: registry-1.docker.io/bitnami/postgresql:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "mlflow" -d "dbname=mlflow" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "mlflow" -d "dbname=mlflow" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/postgresql/conf
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /opt/bitnami/postgresql/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /opt/bitnami/postgresql/secrets/
        name: postgresql-password
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: mlflow-postgresql-0
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: mlflow-postgresql
    serviceAccountName: mlflow-postgresql
    subdomain: mlflow-postgresql-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-mlflow-postgresql-0
    - emptyDir: {}
      name: empty-dir
    - name: postgresql-password
      secret:
        defaultMode: 420
        secretName: mlflow-postgres-secret
    - emptyDir:
        medium: Memory
      name: dshm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e56c1be7dccb9d995546c3f39feaa4a365bc70817d4b494d97d5b232fbe378f3
      image: registry-1.docker.io/bitnami/postgresql:latest
      imageID: registry-1.docker.io/bitnami/postgresql@sha256:9dc1c74aff55144030fb0307666dfa97aefadf5914749696a348fb8f36eb4d7d
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:04Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.24
    podIPs:
    - ip: 10.244.1.24
    qosClass: Burstable
    startTime: "2026-01-22T16:58:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:28:42+01:00"
    creationTimestamp: "2026-01-27T16:23:34Z"
    generateName: mongodb-677c7746c4-
    labels:
      app.kubernetes.io/component: mongodb
      app.kubernetes.io/instance: mongodb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: mongodb-18.1.10
      pod-template-hash: 677c7746c4
    name: mongodb-677c7746c4-tkh9k
    namespace: mongodb-cluster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: mongodb-677c7746c4
      uid: 1c7ed1e0-db07-4fac-a3f5-2e5631e3b7fd
    resourceVersion: "26435404"
    uid: 5a87010f-94db-4d49-8268-17719fae9ab7
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: mongodb
                app.kubernetes.io/instance: mongodb
                app.kubernetes.io/name: mongodb
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: MONGODB_ROOT_USER
        value: root
      - name: MONGODB_ROOT_PASSWORD_FILE
        value: /opt/bitnami/mongodb/secrets/mongodb-root-password
      - name: OPENSSL_FIPS
        value: "yes"
      - name: ALLOW_EMPTY_PASSWORD
        value: "no"
      - name: MONGODB_SYSTEM_LOG_VERBOSITY
        value: "0"
      - name: MONGODB_DISABLE_SYSTEM_LOG
        value: "no"
      - name: MONGODB_DISABLE_JAVASCRIPT
        value: "no"
      - name: MONGODB_ENABLE_JOURNAL
        value: "yes"
      - name: MONGODB_PORT_NUMBER
        value: "27017"
      - name: MONGODB_ENABLE_IPV6
        value: "no"
      - name: MONGODB_ENABLE_DIRECTORY_PER_DB
        value: "no"
      image: registry-1.docker.io/bitnami/mongodb:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bitnami/scripts/ping-mongodb.sh
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 10
      name: mongodb
      ports:
      - containerPort: 27017
        name: mongodb
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bitnami/scripts/readiness-probe.sh
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/mongodb/conf
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /opt/bitnami/mongodb/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /opt/bitnami/mongodb/logs
        name: empty-dir
        subPath: app-logs-dir
      - mountPath: /.mongodb
        name: empty-dir
        subPath: mongosh-home
      - mountPath: /bitnami/mongodb
        name: datadir
      - mountPath: /bitnami/scripts
        name: common-scripts
      - mountPath: /opt/bitnami/mongodb/secrets
        name: mongodb-secrets
    - args:
      - "export MONGODB_ROOT_PASSWORD=\"$(< $MONGODB_ROOT_PASSWORD_FILE)\"\n/bin/mongodb_exporter
        \ --collector.diagnosticdata --collector.replicasetstatus --compatible-mode
        --mongodb.direct-connect --mongodb.global-conn-pool --web.listen-address \":9216\"
        --mongodb.uri \"mongodb://$MONGODB_ROOT_USER:$(echo $MONGODB_ROOT_PASSWORD
        | sed -r \"s/@/%40/g;s/:/%3A/g\")@$(hostname -s):27017/admin?\" \n"
      command:
      - /bin/bash
      - -ec
      env:
      - name: MONGODB_ROOT_USER
        value: root
      - name: MONGODB_ROOT_PASSWORD_FILE
        value: /opt/bitnami/mongodb/secrets/mongodb-root-password
      - name: OPENSSL_FIPS
        value: "yes"
      - name: GODEBUG
        value: fips140=on
      image: registry-1.docker.io/bitnami/mongodb-exporter:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: metrics
        timeoutSeconds: 10
      name: metrics
      ports:
      - containerPort: 9216
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: metrics
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 10
      resources:
        limits:
          cpu: 150m
          ephemeral-storage: 2Gi
          memory: 192Mi
        requests:
          cpu: 100m
          ephemeral-storage: 50Mi
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/mongodb/secrets
        name: mongodb-secrets
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - -ec
      - |
        ln -sf /dev/stdout "/opt/bitnami/mongodb/logs/mongodb.log"
      command:
      - /bin/bash
      env:
      - name: OPENSSL_FIPS
        value: "yes"
      image: registry-1.docker.io/bitnami/mongodb:latest
      imagePullPolicy: IfNotPresent
      name: log-dir
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/bitnami/mongodb/logs
        name: empty-dir
        subPath: app-logs-dir
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: mongodb
    serviceAccountName: mongodb
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: empty-dir
    - configMap:
        defaultMode: 360
        name: mongodb-common-scripts
      name: common-scripts
    - name: mongodb-secrets
      secret:
        defaultMode: 420
        secretName: mongodb
    - name: datadir
      persistentVolumeClaim:
        claimName: mongodb
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:02:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:02:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://18ea6a15af437c677974de39332c6040134862e6c1c723aac57abda1f5716b1c
      image: registry-1.docker.io/bitnami/mongodb-exporter:latest
      imageID: registry-1.docker.io/bitnami/mongodb-exporter@sha256:7f203aa48d5580fa309ca00cd2d0be8d6c935a5abcb7c117b0bde44de30ed6db
      lastState: {}
      name: metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-27T16:30:07Z"
    - containerID: containerd://970c9ab81f306e9753b7642ef0639842d93b1469e89bd71619ab8e2bae725eab
      image: registry-1.docker.io/bitnami/mongodb:latest
      imageID: registry-1.docker.io/bitnami/mongodb@sha256:219bea2e10ac98619a0a2edbce0c5119635ffd2518852cd75c77da234e610327
      lastState: {}
      name: mongodb
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-27T16:30:07Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    initContainerStatuses:
    - containerID: containerd://55b2855568f0631561cca3fe3d910352b95a7efb9944a70325e6414e55d63b3e
      image: registry-1.docker.io/bitnami/mongodb:latest
      imageID: registry-1.docker.io/bitnami/mongodb@sha256:219bea2e10ac98619a0a2edbce0c5119635ffd2518852cd75c77da234e610327
      lastState: {}
      name: log-dir
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://55b2855568f0631561cca3fe3d910352b95a7efb9944a70325e6414e55d63b3e
          exitCode: 0
          finishedAt: "2026-01-27T16:30:06Z"
          reason: Completed
          startedAt: "2026-01-27T16:30:06Z"
    phase: Running
    podIP: 10.244.2.227
    podIPs:
    - ip: 10.244.2.227
    qosClass: Burstable
    startTime: "2026-01-27T16:30:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-27T16:23:33Z"
    generateName: mongodb-kubernetes-operator-7669bd584-
    labels:
      name: mongodb-kubernetes-operator
      pod-template-hash: 7669bd584
    name: mongodb-kubernetes-operator-7669bd584-hjbwm
    namespace: mongodb-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: mongodb-kubernetes-operator-7669bd584
      uid: 8de9d88f-376c-4dc0-94b2-df7eafef526c
    resourceVersion: "23970743"
    uid: e47f793b-e58d-4220-b8ff-36e02f7d1c0c
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: name
              operator: In
              values:
              - mongodb-kubernetes-operator
          topologyKey: kubernetes.io/hostname
    containers:
    - command:
      - /usr/local/bin/entrypoint
      env:
      - name: WATCH_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: OPERATOR_NAME
        value: mongodb-kubernetes-operator
      - name: AGENT_IMAGE
        value: quay.io/mongodb/mongodb-agent-ubi:108.0.6.8796-1
      - name: VERSION_UPGRADE_HOOK_IMAGE
        value: quay.io/mongodb/mongodb-kubernetes-operator-version-upgrade-post-start-hook:1.0.10
      - name: READINESS_PROBE_IMAGE
        value: quay.io/mongodb/mongodb-kubernetes-readinessprobe:1.0.23
      - name: MONGODB_IMAGE
        value: mongodb-community-server
      - name: MONGODB_REPO_URL
        value: docker.io/mongodb
      - name: MDB_IMAGE_TYPE
        value: ubi8
      image: quay.io/mongodb/mongodb-kubernetes-operator:0.13.0
      imagePullPolicy: Always
      name: mongodb-kubernetes-operator
      resources:
        limits:
          cpu: 1100m
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-k97kv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 2000
    serviceAccount: mongodb-kubernetes-operator
    serviceAccountName: mongodb-kubernetes-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-k97kv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:25:20Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:23:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:25:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:25:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:23:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b5d1c38388f9fe5d8b5e08762308d68b68493604c77c8a4395d6032611785dac
      image: quay.io/mongodb/mongodb-kubernetes-operator:0.13.0
      imageID: quay.io/mongodb/mongodb-kubernetes-operator@sha256:2dcc6393e6f78b92c223598b2e0e79401064ef3fda6c774a923acdf2cbc170ff
      lastState: {}
      name: mongodb-kubernetes-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-27T16:25:20Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.77
    podIPs:
    - ip: 10.244.1.77
    qosClass: Burstable
    startTime: "2026-01-27T16:23:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/neo4j-config: 7b1c7f7d5de3e577bb84d000d258e3eaa792a0c59c63308d3d50356ee5291c15
      checksum/neo4j-env: 3d96f134e74d10ffd64a0c222eed03129b8758ba573b3e98ad53a211449e6385
    creationTimestamp: "2026-01-14T09:54:44Z"
    generateName: neo4j-
    labels:
      app: neo4j
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: neo4j-7d878b5566
      helm.neo4j.com/clustering: "false"
      helm.neo4j.com/instance: neo4j
      helm.neo4j.com/neo4j.loadbalancer: include
      helm.neo4j.com/neo4j.name: neo4j
      helm.neo4j.com/pod_category: neo4j-instance
      statefulset.kubernetes.io/pod-name: neo4j-0
    name: neo4j-0
    namespace: neo4j-cluster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: neo4j
      uid: 1ce91a52-7535-4c04-bcdc-b22ddae11346
    resourceVersion: "19731281"
    uid: d4ae1ad2-1000-45a9-af4c-56665c2d67e7
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: neo4j
              helm.neo4j.com/pod_category: neo4j-instance
          topologyKey: kubernetes.io/hostname
    containers:
    - env:
      - name: HELM_NEO4J_VERSION
        value: 2025.10.1
      - name: HELM_CHART_VERSION
        value: 2025.10.1
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SERVICE_NEO4J_ADMIN
        value: neo4j-admin.neo4j-cluster.svc.cluster.local
      - name: SERVICE_NEO4J_INTERNALS
        value: neo4j-internals.neo4j-cluster.svc.cluster.local
      - name: SERVICE_NEO4J
        value: neo4j.neo4j-cluster.svc.cluster.local
      envFrom:
      - configMapRef:
          name: neo4j-env
      image: neo4j:2025.10.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 40
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 7687
        timeoutSeconds: 10
      name: neo4j
      ports:
      - containerPort: 7474
        name: http
        protocol: TCP
      - containerPort: 7687
        name: bolt
        protocol: TCP
      readinessProbe:
        failureThreshold: 20
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 7687
        timeoutSeconds: 10
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: "1"
          memory: 2Gi
      securityContext:
        capabilities:
          drop:
          - ALL
        runAsGroup: 7474
        runAsNonRoot: true
        runAsUser: 7474
      startupProbe:
        failureThreshold: 1000
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 7687
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config/neo4j.conf
        name: neo4j-conf
      - mountPath: /config/server-logs.xml
        name: neo4j-server-logs
      - mountPath: /config/user-logs.xml
        name: neo4j-user-logs
      - mountPath: /config/neo4j-auth
        name: neo4j-auth
      - mountPath: /backups
        name: data
        subPathExpr: backups
      - mountPath: /data
        name: data
        subPathExpr: data
      - mountPath: /import
        name: data
        subPathExpr: import
      - mountPath: /licenses
        name: data
        subPathExpr: licenses
      - mountPath: /logs
        name: data
        subPathExpr: logs/$(POD_NAME)
      - mountPath: /metrics
        name: data
        subPathExpr: metrics/$(POD_NAME)
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-66kj2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: neo4j-0
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 7474
      fsGroupChangePolicy: Always
      runAsGroup: 7474
      runAsNonRoot: true
      runAsUser: 7474
    serviceAccount: default
    serviceAccountName: default
    subdomain: neo4j
    terminationGracePeriodSeconds: 3600
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-neo4j-0
    - name: neo4j-conf
      projected:
        defaultMode: 288
        sources:
        - configMap:
            name: neo4j-default-config
        - configMap:
            name: neo4j-user-config
        - configMap:
            name: neo4j-k8s-config
    - configMap:
        defaultMode: 420
        name: neo4j-server-logs-config
      name: neo4j-server-logs
    - configMap:
        defaultMode: 420
        name: neo4j-user-logs-config
      name: neo4j-user-logs
    - name: neo4j-auth
      secret:
        defaultMode: 420
        secretName: neo4j-auth
    - name: kube-api-access-66kj2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:54:48Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:54:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:57:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:57:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:54:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d39335e8b5553ce2ee60390daed1def616455929f69fe254c0a50ec314204977
      image: docker.io/library/neo4j:2025.10.1
      imageID: docker.io/library/neo4j@sha256:155c8aad10d5c838bc3bbc476c0418779086547822acb214ec5e3d49ba336907
      lastState: {}
      name: neo4j
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-14T09:54:47Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.50
    podIPs:
    - ip: 10.244.0.50
    qosClass: Guaranteed
    startTime: "2026-01-14T09:54:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-02-11T10:25:04Z"
    generateName: postgres-sla-6d998b77dd-
    labels:
      app: postgres-sla
      pod-template-hash: 6d998b77dd
    name: postgres-sla-6d998b77dd-vvfgg
    namespace: neural-hive-data
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: postgres-sla-6d998b77dd
      uid: 3394b565-8cbb-4eab-99b2-961dc08e971e
    resourceVersion: "28924622"
    uid: 2914cc1d-532c-48c1-847c-27662d45c472
  spec:
    containers:
    - envFrom:
      - secretRef:
          name: postgres-sla-credentials
      image: postgres:15-alpine
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - pg_isready
          - -U
          - sla_user
          - -d
          - sla_management
        failureThreshold: 3
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: postgres
      ports:
      - containerPort: 5432
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - pg_isready
          - -U
          - sla_user
          - -d
          - sla_management
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/postgresql/data
        name: postgres-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-22spr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: postgres-data
      persistentVolumeClaim:
        claimName: postgres-sla-data
    - name: kube-api-access-22spr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T10:25:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://211814d2d85dd549f85f8be414b6fefe73e499e9b39182b4d422ee43614bae1c
      image: docker.io/library/postgres:15-alpine
      imageID: docker.io/library/postgres@sha256:b3968e348b48f1198cc6de6611d055dbad91cd561b7990c406c3fc28d7095b21
      lastState: {}
      name: postgres
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-11T10:25:05Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.224
    podIPs:
    - ip: 10.244.1.224
    qosClass: Burstable
    startTime: "2026-02-11T10:25:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 2fcd3a724c65b72bb9230ccc74c77ede86a18c8082703be8cd7739eaabaef9ed
      kubectl.kubernetes.io/restartedAt: "2026-02-11T12:11:44+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-11T11:11:46Z"
    generateName: analyst-agents-5fb8dfd548-
    labels:
      app.kubernetes.io/instance: analyst-agents
      app.kubernetes.io/name: analyst-agents
      pod-template-hash: 5fb8dfd548
    name: analyst-agents-5fb8dfd548-5444s
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: analyst-agents-5fb8dfd548
      uid: f82ab932-a967-4562-8016-f0582644864a
    resourceVersion: "28941014"
    uid: 0c7ddb2e-e679-47fc-922b-7b9bb64d1128
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - analyst-agents
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      envFrom:
      - configMapRef:
          name: analyst-agents
      image: ghcr.io/albinojimy/neural-hive-mind/analyst-agents:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 5
      name: analyst-agents
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lxznq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: analyst-agents
    serviceAccountName: analyst-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-lxznq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:14:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:11:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:15:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:15:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:11:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://39c4a6a214625644debeb4d0fcbb757f5bcd65c1ac30ac264d1dcbd25fc3449f
      image: ghcr.io/albinojimy/neural-hive-mind/analyst-agents:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/analyst-agents@sha256:aa9e2e3c747c581b629b018a21cbd12737a5f7e74f382143f41b16c87091d222
      lastState: {}
      name: analyst-agents
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-11T11:14:42Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.45
    podIPs:
    - ip: 10.244.2.45
    qosClass: Burstable
    startTime: "2026-02-11T11:11:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-02-02T13:35:28+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-09T23:47:54Z"
    generateName: approval-service-77d49c4bd8-
    labels:
      app.kubernetes.io/name: approval-service
      pod-template-hash: 77d49c4bd8
    name: approval-service-77d49c4bd8-jhbqq
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: approval-service-77d49c4bd8
      uid: 52d76c1c-e286-4dad-8c36-9371541604eb
    resourceVersion: "28360882"
    uid: eeaeee86-ca0a-43a0-8752-193b57f11f1b
  spec:
    containers:
    - env:
      - name: KAFKA_BOOTSTRAP_SERVERS
        value: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
      - name: KAFKA_SECURITY_PROTOCOL
        value: PLAINTEXT
      - name: MONGODB_URI
        value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017
      - name: ENVIRONMENT
        value: development
      - name: LOG_LEVEL
        value: INFO
      - name: APPROVAL_SERVICE_REQUIRE_AUTH
        value: "false"
      image: ghcr.io/albinojimy/neural-hive-mind/approval-service:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 40
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: approval-service
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sv76f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-sv76f
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:47:56Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:47:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:48:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:48:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:47:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://800b2ae7e5865c0e8ac4465a134dd2222f89d8c6d41371f5cc1f437bd1ab70d9
      image: ghcr.io/albinojimy/neural-hive-mind/approval-service:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/approval-service@sha256:10402e3c5219b1036fb43a724b0b9525a0e10a4d7d9f5d29b793c49ddf45b13b
      lastState: {}
      name: approval-service
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-09T23:47:55Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.87
    podIPs:
    - ip: 10.244.3.87
    qosClass: Burstable
    startTime: "2026-02-09T23:47:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 04a5ce5560e16ceba234402ba0d53ac1085437c75b520af6e7f9fad989923a22
      kubectl.kubernetes.io/restartedAt: "2026-02-11T11:57:48+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:06:34Z"
    generateName: code-forge-5b5cb4f857-
    labels:
      app.kubernetes.io/instance: code-forge
      app.kubernetes.io/name: code-forge
      pod-template-hash: 5b5cb4f857
    name: code-forge-5b5cb4f857-849jl
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: code-forge-5b5cb4f857
      uid: e565f2ae-ea73-4ffc-ab79-b73817c70bfd
    resourceVersion: "29938800"
    uid: c9cdea87-bd49-4fe9-8ebc-618ce2c749d0
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - code-forge
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - envFrom:
      - configMapRef:
          name: code-forge-config
      - secretRef:
          name: code-forge-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/code-forge:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 5
      name: code-forge
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 2200m
          memory: 3584Mi
        requests:
          cpu: 800m
          memory: 1536Mi
      startupProbe:
        failureThreshold: 25
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sm8bc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: code-forge
    serviceAccountName: code-forge
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: code-forge
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - name: kube-api-access-sm8bc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:07Z"
      message: 'containers with unready status: [code-forge]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:07Z"
      message: 'containers with unready status: [code-forge]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://86b015d6ff7430eac2789a9d9959fc0e979d7d59c8752dc051ad57fca5e13b57
      image: ghcr.io/albinojimy/neural-hive-mind/code-forge:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/code-forge@sha256:217c9d27f297a831be3b39e484e91f0102f8a154b2ce06bc92fada8966331484
      lastState:
        terminated:
          containerID: containerd://86b015d6ff7430eac2789a9d9959fc0e979d7d59c8752dc051ad57fca5e13b57
          exitCode: 137
          finishedAt: "2026-02-13T23:04:06Z"
          reason: Error
          startedAt: "2026-02-13T23:03:36Z"
      name: code-forge
      ready: false
      restartCount: 23
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=code-forge pod=code-forge-5b5cb4f857-849jl_neural-hive(c9cdea87-bd49-4fe9-8ebc-618ce2c749d0)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.2
    podIPs:
    - ip: 10.244.4.2
    qosClass: Burstable
    startTime: "2026-02-13T21:06:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: ac2b81a4a6b9013bfc630ab07ea38dedf7dab374bf25c85afb35c19760cc8b22
      checksum/secret: c53e1323967446c56b88efd346e3f29681d2227762fc8bfcd3c4e2f8efc1074f
      kubectl.kubernetes.io/restartedAt: "2026-02-08T12:22:06+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-12T07:38:44Z"
    generateName: consensus-engine-6fbd8d768f-
    labels:
      app.kubernetes.io/component: consensus-aggregator
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/name: consensus-engine
      neural-hive.io/domain: consensus
      neural-hive.io/layer: cognitiva
      pod-template-hash: 6fbd8d768f
    name: consensus-engine-6fbd8d768f-sqzq2
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: consensus-engine-6fbd8d768f
      uid: dd2956e9-1a8b-4a7d-a4fc-c522e882b9f7
    resourceVersion: "29319750"
    uid: af9bd3ee-bd3b-4fde-9dc3-42d21f7e0c68
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - consensus-engine
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: SCHEMA_REGISTRY_URL
        value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
      - name: PYTHONUNBUFFERED
        value: "1"
      envFrom:
      - configMapRef:
          name: consensus-engine-config
      - secretRef:
          name: consensus-engine-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/consensus-engine:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: consensus-engine
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources:
        limits:
          cpu: 1500m
          memory: 1280Mi
        requests:
          cpu: 400m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 15
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /app/logs
        name: logs
      - mountPath: /app/src/models/consolidated_decision.py
        name: decision-hotfix
        subPath: consolidated_decision.py
      - mountPath: /app/src/services/consensus_orchestrator.py
        name: consensus-orchestrator-hotfix
        subPath: consensus_orchestrator.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-spbdr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: consensus-engine
    serviceAccountName: consensus-engine
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: consensus-engine
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: logs
    - configMap:
        defaultMode: 420
        name: consensus-engine-decision-hotfix
      name: decision-hotfix
    - configMap:
        defaultMode: 420
        name: consensus-orchestrator-hotfix
      name: consensus-orchestrator-hotfix
    - name: kube-api-access-spbdr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T07:38:47Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T07:38:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T08:40:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T08:40:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T07:38:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://efb6378bc2b054dd503b006bec841d4266f4b0080024f4ec35a691dd201f55f7
      image: ghcr.io/albinojimy/neural-hive-mind/consensus-engine:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/consensus-engine@sha256:bfb16a9c94965cec779a98e5549ea6eea39531774607f94c78f9a6e6293483de
      lastState: {}
      name: consensus-engine
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-12T07:38:47Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.90
    podIPs:
    - ip: 10.244.2.90
    qosClass: Burstable
    startTime: "2026-02-12T07:38:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: e723785aafd84b52629c9ca2364d1dc4f4a8026ebfdf1e80f2a4949c98d47679
      kubectl.kubernetes.io/restartedAt: "2026-02-06T09:07:47+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:06:31Z"
    generateName: execution-ticket-service-54988fd44f-
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: execution-ticket-service
      app.kubernetes.io/name: execution-ticket-service
      neural-hive.io/layer: orchestration
      pod-template-hash: 54988fd44f
    name: execution-ticket-service-54988fd44f-qtlvh
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: execution-ticket-service-54988fd44f
      uid: 1a005206-af25-43c1-8541-13288f2c7088
    resourceVersion: "29905069"
    uid: 96546d11-34bc-4b86-9845-4c2bb1a7166d
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - execution-ticket-service
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: POSTGRES_PASSWORD
        valueFrom:
          secretKeyRef:
            key: POSTGRES_PASSWORD
            name: execution-ticket-service-secrets
      - name: MONGODB_URI
        valueFrom:
          secretKeyRef:
            key: MONGODB_URI
            name: execution-ticket-service-secrets
      - name: JWT_SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: JWT_SECRET_KEY
            name: execution-ticket-service-secrets
      - name: MAX_CONNECTION_RETRIES
        value: "5"
      - name: INITIAL_RETRY_DELAY_SECONDS
        value: "1"
      envFrom:
      - configMapRef:
          name: execution-ticket-service-config
      image: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: execution-ticket-service
      ports:
      - containerPort: 50052
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4clv8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: execution-ticket-service
    serviceAccountName: execution-ticket-service
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: execution-ticket-service
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - name: kube-api-access-4clv8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:40Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://008805c8f8f317f56a9772545fb2d0caae5b14a3befe7204c08e60bd051fb6c5
      image: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service@sha256:ab51a993b2e1161a557fc68d278dcf216f0bb4a6db38f0c9bb5df6b27557cce2
      lastState: {}
      name: execution-ticket-service
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-13T21:06:40Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.74
    podIPs:
    - ip: 10.244.1.74
    qosClass: Burstable
    startTime: "2026-02-13T21:06:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-02-08T15:44:50+01:00"
    creationTimestamp: "2026-02-08T14:45:20Z"
    generateName: feedback-collection-service-db8c9f98b-
    labels:
      app: feedback-collection
      pod-template-hash: db8c9f98b
    name: feedback-collection-service-db8c9f98b-n54xx
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: feedback-collection-service-db8c9f98b
      uid: 6ad48897-62f2-453a-8651-7ef5db3f22b6
    resourceVersion: "27898409"
    uid: 5cb8649a-465f-4d8b-a3b5-313a7e9d8d33
  spec:
    containers:
    - args:
      - /scripts/feedback_service.py
      command:
      - python3
      image: python:3.11-slim
      imagePullPolicy: IfNotPresent
      name: feedback-service
      ports:
      - containerPort: 8080
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /scripts
        name: scripts
      - mountPath: /app/ui
        name: ui
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xx6h6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-pull
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: feedback-script
      name: scripts
    - configMap:
        defaultMode: 420
        name: feedback-ui
      name: ui
    - name: kube-api-access-xx6h6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T14:45:21Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T14:45:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T14:45:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T14:45:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T14:45:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bc23a47a376e5f4cffcea6024183f59de11ae8a810e6bb3d6ff394165ba9555d
      image: docker.io/library/python:3.11-slim
      imageID: docker.io/library/python@sha256:0b23cfb7425d065008b778022a17b1551c82f8b4866ee5a7a200084b7e2eafbf
      lastState: {}
      name: feedback-service
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-08T14:45:21Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.212
    podIPs:
    - ip: 10.244.3.212
    qosClass: BestEffort
    startTime: "2026-02-08T14:45:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
      checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
      kubectl.kubernetes.io/restartedAt: "2026-02-13T22:12:06+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:12:08Z"
    generateName: gateway-intencoes-644bd4fd7d-
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: 644bd4fd7d
    name: gateway-intencoes-644bd4fd7d-75j5f
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gateway-intencoes-644bd4fd7d
      uid: 191f4d55-db88-4753-b512-f26319bc57f9
    resourceVersion: "29907241"
    uid: e4897f74-ccbe-4d0f-9105-983e9652d946
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - gateway-intencoes
            topologyKey: kubernetes.io/hostname
          weight: 100
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - gateway-intencoes
          topologyKey: topology.kubernetes.io/zone
    containers:
    - env:
      - name: ENVIRONMENT
        valueFrom:
          configMapKeyRef:
            key: environment
            name: gateway-intencoes-config
      - name: LOG_LEVEL
        valueFrom:
          configMapKeyRef:
            key: log_level
            name: gateway-intencoes-config
      - name: KAFKA_BOOTSTRAP_SERVERS
        valueFrom:
          configMapKeyRef:
            key: kafka_bootstrap_servers
            name: gateway-intencoes-config
      - name: SCHEMA_REGISTRY_URL
        valueFrom:
          configMapKeyRef:
            key: schema_registry_url
            name: gateway-intencoes-config
      - name: KAFKA_SECURITY_PROTOCOL
        valueFrom:
          configMapKeyRef:
            key: kafka_security_protocol
            name: gateway-intencoes-config
      - name: KAFKA_BATCH_SIZE
        valueFrom:
          configMapKeyRef:
            key: kafka_batch_size
            name: gateway-intencoes-config
      - name: KAFKA_LINGER_MS
        valueFrom:
          configMapKeyRef:
            key: kafka_linger_ms
            name: gateway-intencoes-config
      - name: KAFKA_COMPRESSION_TYPE
        valueFrom:
          configMapKeyRef:
            key: kafka_compression_type
            name: gateway-intencoes-config
      - name: KAFKA_SSL_CA_LOCATION
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_ca_location
            name: gateway-intencoes-config
      - name: KAFKA_SSL_CERTIFICATE_LOCATION
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_certificate_location
            name: gateway-intencoes-config
      - name: KAFKA_SSL_KEY_LOCATION
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_key_location
            name: gateway-intencoes-config
      - name: KAFKA_SSL_ENABLED
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_enabled
            name: gateway-intencoes-config
      - name: KAFKA_SSL_CA_VERIFY_MODE
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_ca_verify_mode
            name: gateway-intencoes-config
      - name: KAFKA_SSL_PROTOCOL
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_protocol
            name: gateway-intencoes-config
      - name: KAFKA_SASL_ENABLED
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_enabled
            name: gateway-intencoes-config
      - name: KAFKA_SASL_MECHANISM
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_mechanism
            name: gateway-intencoes-config
      - name: KAFKA_SASL_OAUTH2_ENABLED
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_oauth2_enabled
            name: gateway-intencoes-config
      - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_oauth2_token_endpoint
            name: gateway-intencoes-config
      - name: KAFKA_SASL_OAUTH2_SCOPE
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_oauth2_scope
            name: gateway-intencoes-config
      - name: REDIS_CLUSTER_NODES
        valueFrom:
          configMapKeyRef:
            key: redis_cluster_nodes
            name: gateway-intencoes-config
      - name: REDIS_DEFAULT_TTL
        valueFrom:
          configMapKeyRef:
            key: redis_default_ttl
            name: gateway-intencoes-config
      - name: REDIS_MAX_CONNECTIONS
        valueFrom:
          configMapKeyRef:
            key: redis_max_connections
            name: gateway-intencoes-config
      - name: REDIS_POOL_SIZE
        valueFrom:
          configMapKeyRef:
            key: redis_pool_size
            name: gateway-intencoes-config
      - name: REDIS_TIMEOUT
        valueFrom:
          configMapKeyRef:
            key: redis_timeout
            name: gateway-intencoes-config
      - name: REDIS_RETRY_ON_TIMEOUT
        valueFrom:
          configMapKeyRef:
            key: redis_retry_on_timeout
            name: gateway-intencoes-config
      - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
        valueFrom:
          configMapKeyRef:
            key: redis_connection_pool_max_connections
            name: gateway-intencoes-config
      - name: REDIS_SSL_ENABLED
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_enabled
            name: gateway-intencoes-config
      - name: REDIS_SSL_CERT_REQS
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_cert_reqs
            name: gateway-intencoes-config
      - name: REDIS_SSL_CA_CERTS
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_ca_certs
            name: gateway-intencoes-config
      - name: REDIS_SSL_CERTFILE
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_certfile
            name: gateway-intencoes-config
      - name: REDIS_SSL_KEYFILE
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_keyfile
            name: gateway-intencoes-config
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: gateway-intencoes-secret
      - name: KEYCLOAK_URL
        valueFrom:
          configMapKeyRef:
            key: keycloak_url
            name: gateway-intencoes-config
      - name: KEYCLOAK_REALM
        valueFrom:
          configMapKeyRef:
            key: keycloak_realm
            name: gateway-intencoes-config
      - name: KEYCLOAK_CLIENT_ID
        valueFrom:
          configMapKeyRef:
            key: keycloak_client_id
            name: gateway-intencoes-config
      - name: JWKS_URI
        valueFrom:
          configMapKeyRef:
            key: jwks_uri
            name: gateway-intencoes-config
      - name: TOKEN_VALIDATION_ENABLED
        valueFrom:
          configMapKeyRef:
            key: token_validation_enabled
            name: gateway-intencoes-config
      - name: KEYCLOAK_CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            key: keycloak-client-secret
            name: gateway-intencoes-secret
      - name: SCHEMA_REGISTRY_TLS_ENABLED
        valueFrom:
          configMapKeyRef:
            key: schema_registry_tls_enabled
            name: gateway-intencoes-config
      - name: SCHEMA_REGISTRY_TLS_VERIFY
        valueFrom:
          configMapKeyRef:
            key: schema_registry_tls_verify
            name: gateway-intencoes-config
      - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
        valueFrom:
          configMapKeyRef:
            key: schema_registry_ssl_ca_location
            name: gateway-intencoes-config
      - name: JWT_SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: jwt-secret-key
            name: gateway-intencoes-secret
      - name: ASR_MODEL_NAME
        value: base
      - name: ASR_DEVICE
        value: cpu
      - name: ASR_LAZY_LOADING
        value: "true"
      - name: ASR_MODEL_CACHE_DIR
        value: /app/models/whisper
      - name: NLU_LANGUAGE_MODEL
        value: pt_core_news_sm
      - name: NLU_MODEL_CACHE_DIR
        value: /app/models/spacy
      - name: NLU_CONFIDENCE_THRESHOLD
        value: "0.6"
      - name: NLU_CONFIDENCE_THRESHOLD_STRICT
        value: "0.75"
      - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
        value: "false"
      - name: NLU_RULES_CONFIG_PATH
        value: /app/config/nlu_rules.yaml
      - name: NLU_ROUTING_THRESHOLD_HIGH
        valueFrom:
          configMapKeyRef:
            key: nlu_routing_threshold_high
            name: gateway-intencoes-config
      - name: NLU_ROUTING_THRESHOLD_LOW
        valueFrom:
          configMapKeyRef:
            key: nlu_routing_threshold_low
            name: gateway-intencoes-config
      - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
        valueFrom:
          configMapKeyRef:
            key: nlu_routing_use_adaptive_for_decisions
            name: gateway-intencoes-config
      - name: JAEGER_ENDPOINT
        value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
      - name: ENABLE_TRACING
        value: "true"
      - name: OTEL_ENABLED
        valueFrom:
          configMapKeyRef:
            key: otel_enabled
            name: gateway-intencoes-config
      - name: OTEL_ENDPOINT
        valueFrom:
          configMapKeyRef:
            key: otel_endpoint
            name: gateway-intencoes-config
      - name: RATE_LIMIT_ENABLED
        valueFrom:
          configMapKeyRef:
            key: rate_limit_enabled
            name: gateway-intencoes-config
      - name: RATE_LIMIT_REQUESTS_PER_MINUTE
        valueFrom:
          configMapKeyRef:
            key: rate_limit_requests_per_minute
            name: gateway-intencoes-config
      - name: RATE_LIMIT_BURST_SIZE
        valueFrom:
          configMapKeyRef:
            key: rate_limit_burst_size
            name: gateway-intencoes-config
      - name: RATE_LIMIT_FAIL_OPEN
        valueFrom:
          configMapKeyRef:
            key: rate_limit_fail_open
            name: gateway-intencoes-config
      - name: RATE_LIMIT_TENANT_OVERRIDES
        valueFrom:
          configMapKeyRef:
            key: rate_limit_tenant_overrides
            name: gateway-intencoes-config
      - name: RATE_LIMIT_USER_OVERRIDES
        valueFrom:
          configMapKeyRef:
            key: rate_limit_user_overrides
            name: gateway-intencoes-config
      - name: ALLOW_INSECURE_HTTP_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: allow_insecure_http_endpoints
            name: gateway-intencoes-config
      - name: POD_UID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.uid
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 25
        successThreshold: 1
        timeoutSeconds: 8
      name: gateway-intencoes
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      startupProbe:
        failureThreshold: 40
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/models
        name: model-cache
      - mountPath: /etc/ssl/certs/schema-registry-ca.crt
        name: schema-registry-ca-cert
        readOnly: true
        subPath: ca.crt
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cgt82
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: gateway-intencoes
    serviceAccountName: gateway-intencoes
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: model-cache
      persistentVolumeClaim:
        claimName: gateway-intencoes-models-pvc
    - name: schema-registry-ca-cert
      secret:
        defaultMode: 420
        items:
        - key: ca.crt
          path: ca.crt
        secretName: schema-registry-tls-secret
    - name: kube-api-access-cgt82
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:12:10Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:12:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:12:10Z"
      message: 'containers with unready status: [gateway-intencoes]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:12:10Z"
      message: 'containers with unready status: [gateway-intencoes]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:12:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
      imageID: ""
      lastState: {}
      name: gateway-intencoes
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: ContainerCreating
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Pending
    qosClass: Burstable
    startTime: "2026-02-13T21:12:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
      checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T14:36:59Z"
    generateName: gateway-intencoes-c95f49675-
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: c95f49675
    name: gateway-intencoes-c95f49675-pfgsf
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gateway-intencoes-c95f49675
      uid: b46c079b-8b9e-4705-846c-b71c691ade40
    resourceVersion: "29801223"
    uid: f6d45a1b-aab8-4bfd-a6aa-5546db340182
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - gateway-intencoes
            topologyKey: kubernetes.io/hostname
          weight: 100
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - gateway-intencoes
          topologyKey: topology.kubernetes.io/zone
    containers:
    - env:
      - name: ENVIRONMENT
        valueFrom:
          configMapKeyRef:
            key: environment
            name: gateway-intencoes-config
      - name: LOG_LEVEL
        valueFrom:
          configMapKeyRef:
            key: log_level
            name: gateway-intencoes-config
      - name: KAFKA_BOOTSTRAP_SERVERS
        valueFrom:
          configMapKeyRef:
            key: kafka_bootstrap_servers
            name: gateway-intencoes-config
      - name: SCHEMA_REGISTRY_URL
        valueFrom:
          configMapKeyRef:
            key: schema_registry_url
            name: gateway-intencoes-config
      - name: KAFKA_SECURITY_PROTOCOL
        valueFrom:
          configMapKeyRef:
            key: kafka_security_protocol
            name: gateway-intencoes-config
      - name: KAFKA_BATCH_SIZE
        valueFrom:
          configMapKeyRef:
            key: kafka_batch_size
            name: gateway-intencoes-config
      - name: KAFKA_LINGER_MS
        valueFrom:
          configMapKeyRef:
            key: kafka_linger_ms
            name: gateway-intencoes-config
      - name: KAFKA_COMPRESSION_TYPE
        valueFrom:
          configMapKeyRef:
            key: kafka_compression_type
            name: gateway-intencoes-config
      - name: KAFKA_SSL_CA_LOCATION
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_ca_location
            name: gateway-intencoes-config
      - name: KAFKA_SSL_CERTIFICATE_LOCATION
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_certificate_location
            name: gateway-intencoes-config
      - name: KAFKA_SSL_KEY_LOCATION
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_key_location
            name: gateway-intencoes-config
      - name: KAFKA_SSL_ENABLED
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_enabled
            name: gateway-intencoes-config
      - name: KAFKA_SSL_CA_VERIFY_MODE
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_ca_verify_mode
            name: gateway-intencoes-config
      - name: KAFKA_SSL_PROTOCOL
        valueFrom:
          configMapKeyRef:
            key: kafka_ssl_protocol
            name: gateway-intencoes-config
      - name: KAFKA_SASL_ENABLED
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_enabled
            name: gateway-intencoes-config
      - name: KAFKA_SASL_MECHANISM
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_mechanism
            name: gateway-intencoes-config
      - name: KAFKA_SASL_OAUTH2_ENABLED
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_oauth2_enabled
            name: gateway-intencoes-config
      - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_oauth2_token_endpoint
            name: gateway-intencoes-config
      - name: KAFKA_SASL_OAUTH2_SCOPE
        valueFrom:
          configMapKeyRef:
            key: kafka_sasl_oauth2_scope
            name: gateway-intencoes-config
      - name: REDIS_CLUSTER_NODES
        valueFrom:
          configMapKeyRef:
            key: redis_cluster_nodes
            name: gateway-intencoes-config
      - name: REDIS_DEFAULT_TTL
        valueFrom:
          configMapKeyRef:
            key: redis_default_ttl
            name: gateway-intencoes-config
      - name: REDIS_MAX_CONNECTIONS
        valueFrom:
          configMapKeyRef:
            key: redis_max_connections
            name: gateway-intencoes-config
      - name: REDIS_POOL_SIZE
        valueFrom:
          configMapKeyRef:
            key: redis_pool_size
            name: gateway-intencoes-config
      - name: REDIS_TIMEOUT
        valueFrom:
          configMapKeyRef:
            key: redis_timeout
            name: gateway-intencoes-config
      - name: REDIS_RETRY_ON_TIMEOUT
        valueFrom:
          configMapKeyRef:
            key: redis_retry_on_timeout
            name: gateway-intencoes-config
      - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
        valueFrom:
          configMapKeyRef:
            key: redis_connection_pool_max_connections
            name: gateway-intencoes-config
      - name: REDIS_SSL_ENABLED
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_enabled
            name: gateway-intencoes-config
      - name: REDIS_SSL_CERT_REQS
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_cert_reqs
            name: gateway-intencoes-config
      - name: REDIS_SSL_CA_CERTS
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_ca_certs
            name: gateway-intencoes-config
      - name: REDIS_SSL_CERTFILE
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_certfile
            name: gateway-intencoes-config
      - name: REDIS_SSL_KEYFILE
        valueFrom:
          configMapKeyRef:
            key: redis_ssl_keyfile
            name: gateway-intencoes-config
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: gateway-intencoes-secret
      - name: KEYCLOAK_URL
        valueFrom:
          configMapKeyRef:
            key: keycloak_url
            name: gateway-intencoes-config
      - name: KEYCLOAK_REALM
        valueFrom:
          configMapKeyRef:
            key: keycloak_realm
            name: gateway-intencoes-config
      - name: KEYCLOAK_CLIENT_ID
        valueFrom:
          configMapKeyRef:
            key: keycloak_client_id
            name: gateway-intencoes-config
      - name: JWKS_URI
        valueFrom:
          configMapKeyRef:
            key: jwks_uri
            name: gateway-intencoes-config
      - name: TOKEN_VALIDATION_ENABLED
        valueFrom:
          configMapKeyRef:
            key: token_validation_enabled
            name: gateway-intencoes-config
      - name: KEYCLOAK_CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            key: keycloak-client-secret
            name: gateway-intencoes-secret
      - name: SCHEMA_REGISTRY_TLS_ENABLED
        valueFrom:
          configMapKeyRef:
            key: schema_registry_tls_enabled
            name: gateway-intencoes-config
      - name: SCHEMA_REGISTRY_TLS_VERIFY
        valueFrom:
          configMapKeyRef:
            key: schema_registry_tls_verify
            name: gateway-intencoes-config
      - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
        valueFrom:
          configMapKeyRef:
            key: schema_registry_ssl_ca_location
            name: gateway-intencoes-config
      - name: JWT_SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: jwt-secret-key
            name: gateway-intencoes-secret
      - name: ASR_MODEL_NAME
        value: base
      - name: ASR_DEVICE
        value: cpu
      - name: ASR_LAZY_LOADING
        value: "true"
      - name: ASR_MODEL_CACHE_DIR
        value: /app/models/whisper
      - name: NLU_LANGUAGE_MODEL
        value: pt_core_news_sm
      - name: NLU_MODEL_CACHE_DIR
        value: /app/models/spacy
      - name: NLU_CONFIDENCE_THRESHOLD
        value: "0.6"
      - name: NLU_CONFIDENCE_THRESHOLD_STRICT
        value: "0.75"
      - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
        value: "false"
      - name: NLU_RULES_CONFIG_PATH
        value: /app/config/nlu_rules.yaml
      - name: NLU_ROUTING_THRESHOLD_HIGH
        valueFrom:
          configMapKeyRef:
            key: nlu_routing_threshold_high
            name: gateway-intencoes-config
      - name: NLU_ROUTING_THRESHOLD_LOW
        valueFrom:
          configMapKeyRef:
            key: nlu_routing_threshold_low
            name: gateway-intencoes-config
      - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
        valueFrom:
          configMapKeyRef:
            key: nlu_routing_use_adaptive_for_decisions
            name: gateway-intencoes-config
      - name: JAEGER_ENDPOINT
        value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
      - name: ENABLE_TRACING
        value: "true"
      - name: OTEL_ENABLED
        valueFrom:
          configMapKeyRef:
            key: otel_enabled
            name: gateway-intencoes-config
      - name: OTEL_ENDPOINT
        valueFrom:
          configMapKeyRef:
            key: otel_endpoint
            name: gateway-intencoes-config
      - name: RATE_LIMIT_ENABLED
        valueFrom:
          configMapKeyRef:
            key: rate_limit_enabled
            name: gateway-intencoes-config
      - name: RATE_LIMIT_REQUESTS_PER_MINUTE
        valueFrom:
          configMapKeyRef:
            key: rate_limit_requests_per_minute
            name: gateway-intencoes-config
      - name: RATE_LIMIT_BURST_SIZE
        valueFrom:
          configMapKeyRef:
            key: rate_limit_burst_size
            name: gateway-intencoes-config
      - name: RATE_LIMIT_FAIL_OPEN
        valueFrom:
          configMapKeyRef:
            key: rate_limit_fail_open
            name: gateway-intencoes-config
      - name: RATE_LIMIT_TENANT_OVERRIDES
        valueFrom:
          configMapKeyRef:
            key: rate_limit_tenant_overrides
            name: gateway-intencoes-config
      - name: RATE_LIMIT_USER_OVERRIDES
        valueFrom:
          configMapKeyRef:
            key: rate_limit_user_overrides
            name: gateway-intencoes-config
      - name: ALLOW_INSECURE_HTTP_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: allow_insecure_http_endpoints
            name: gateway-intencoes-config
      - name: POD_UID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.uid
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 25
        successThreshold: 1
        timeoutSeconds: 8
      name: gateway-intencoes
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      startupProbe:
        failureThreshold: 40
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/models
        name: model-cache
      - mountPath: /etc/ssl/certs/schema-registry-ca.crt
        name: schema-registry-ca-cert
        readOnly: true
        subPath: ca.crt
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mhzn5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911680
    nodeSelector:
      kubernetes.io/hostname: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: gateway-intencoes
    serviceAccountName: gateway-intencoes
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: model-cache
      persistentVolumeClaim:
        claimName: gateway-intencoes-models-pvc
    - name: schema-registry-ca-cert
      secret:
        defaultMode: 420
        items:
        - key: ca.crt
          path: ca.crt
        secretName: schema-registry-tls-secret
    - name: kube-api-access-mhzn5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:38:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:37:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:38:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:38:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:37:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2201f6967addec5fd5b77b0807b269a7780dff82fbc585065b79c83966192285
      image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes@sha256:47b41a940cf2fb397a32442cf10d1d298016b60c78fddda37e8c12a997f5ff81
      lastState: {}
      name: gateway-intencoes
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-13T14:38:24Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.73
    podIPs:
    - ip: 10.244.1.73
    qosClass: Burstable
    startTime: "2026-02-13T14:37:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 39da66c3b643acffb0000cabbc3dbe7e2337c017c9fb87f3c40f93e8d1072470
      kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:20+01:00"
    creationTimestamp: "2026-02-13T21:06:07Z"
    generateName: guard-agents-7d66b9cfdf-
    labels:
      app.kubernetes.io/instance: guard-agents
      app.kubernetes.io/name: guard-agents
      pod-template-hash: 7d66b9cfdf
    name: guard-agents-7d66b9cfdf-5xtr8
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: guard-agents-7d66b9cfdf
      uid: 8f6a54ec-6c44-44ff-9584-72836fd60332
    resourceVersion: "29939560"
    uid: ed35dab0-6bfa-4972-97fe-f8aa62408ac0
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - guard-agents
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ALLOW_INSECURE_HTTP_ENDPOINTS
        value: "true"
      envFrom:
      - configMapRef:
          name: guard-agents-config
      image: ghcr.io/albinojimy/neural-hive-mind/guard-agents:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: guard-agents
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/readiness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health/startup
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7qjjw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: guard-agents
    serviceAccountName: guard-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: guard-agents
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-7qjjw
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:15Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c1582cff1692dfd4da5e0a9c884c7e587544e00ea288694781f4b2d9e79c5d12
      image: ghcr.io/albinojimy/neural-hive-mind/guard-agents:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/guard-agents@sha256:e67a1d7325f22e5a9047570cb086bf12af5a87dfb2eed21f7155ef3e5634602b
      lastState:
        terminated:
          containerID: containerd://795ce915877b03370bb55d62bc612efefc359f3398b03051ca4cfbde09cc9da9
          exitCode: 137
          finishedAt: "2026-02-13T23:01:08Z"
          reason: Error
          startedAt: "2026-02-13T22:58:39Z"
      name: guard-agents
      ready: true
      restartCount: 27
      started: true
      state:
        running:
          startedAt: "2026-02-13T23:06:19Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.111
    podIPs:
    - ip: 10.244.2.111
    qosClass: Burstable
    startTime: "2026-02-13T21:06:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
      kubectl.kubernetes.io/restartedAt: "2026-02-13T22:06:48+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:06:50Z"
    generateName: optimizer-agents-698f8d4fbd-
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 698f8d4fbd
    name: optimizer-agents-698f8d4fbd-nksjz
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: optimizer-agents-698f8d4fbd
      uid: a3feaeb9-9e61-4412-9ea8-3964939fa02c
    resourceVersion: "29939722"
    uid: 4156ea4a-771a-4310-b340-bffad51110f2
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - optimizer-agents
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      envFrom:
      - configMapRef:
          name: optimizer-agents
      image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: optimizer-agents
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6r272
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: optimizer-agents
    serviceAccountName: optimizer-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-6r272
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:07:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:50Z"
      message: 'containers with unready status: [optimizer-agents]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:50Z"
      message: 'containers with unready status: [optimizer-agents]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://88246d3f5a1831dc411394390363f18fc47b4e43cfa9b828cfc80891f49fef9d
      image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents@sha256:7bd59de1012a5b990345f04b2b12b3ac9594dac1723aeb76a0fcd3184e0999b2
      lastState:
        terminated:
          containerID: containerd://88246d3f5a1831dc411394390363f18fc47b4e43cfa9b828cfc80891f49fef9d
          exitCode: 137
          finishedAt: "2026-02-13T23:07:06Z"
          reason: Error
          startedAt: "2026-02-13T23:06:35Z"
      name: optimizer-agents
      ready: false
      restartCount: 22
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=optimizer-agents pod=optimizer-agents-698f8d4fbd-nksjz_neural-hive(4156ea4a-771a-4310-b340-bffad51110f2)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.12
    podIPs:
    - ip: 10.244.4.12
    qosClass: Burstable
    startTime: "2026-02-13T21:06:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
      kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:19+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-12T09:20:06Z"
    generateName: optimizer-agents-7d5d5d7b5c-
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 7d5d5d7b5c
    name: optimizer-agents-7d5d5d7b5c-89hhk
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: optimizer-agents-7d5d5d7b5c
      uid: 7f357bc2-6d1e-476f-ba16-1db3c853e0dc
    resourceVersion: "29939035"
    uid: a0877a90-031b-4958-b45e-095a120ae814
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - optimizer-agents
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      envFrom:
      - configMapRef:
          name: optimizer-agents
      image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: optimizer-agents
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7f9wv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: optimizer-agents
    serviceAccountName: optimizer-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-7f9wv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:55Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      message: 'containers with unready status: [optimizer-agents]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      message: 'containers with unready status: [optimizer-agents]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f56c2caac7a5a1a33a041ffe608ab9feccf71a9bb1a69bb5bd35f9a18724b5de
      image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents@sha256:7bd59de1012a5b990345f04b2b12b3ac9594dac1723aeb76a0fcd3184e0999b2
      lastState:
        terminated:
          containerID: containerd://f56c2caac7a5a1a33a041ffe608ab9feccf71a9bb1a69bb5bd35f9a18724b5de
          exitCode: 137
          finishedAt: "2026-02-13T23:04:53Z"
          reason: Error
          startedAt: "2026-02-13T23:01:45Z"
      name: optimizer-agents
      ready: false
      restartCount: 405
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=optimizer-agents pod=optimizer-agents-7d5d5d7b5c-89hhk_neural-hive(a0877a90-031b-4958-b45e-095a120ae814)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.5
    podIPs:
    - ip: 10.244.4.5
    qosClass: Burstable
    startTime: "2026-02-12T09:20:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
      kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:19+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-12T09:20:06Z"
    generateName: optimizer-agents-7d5d5d7b5c-
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 7d5d5d7b5c
    name: optimizer-agents-7d5d5d7b5c-vdsvl
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: optimizer-agents-7d5d5d7b5c
      uid: 7f357bc2-6d1e-476f-ba16-1db3c853e0dc
    resourceVersion: "29938946"
    uid: bb18f869-8b6a-4c4c-aff9-3d3ce6a9f912
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - optimizer-agents
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      envFrom:
      - configMapRef:
          name: optimizer-agents
      image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: optimizer-agents
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5lhg9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: optimizer-agents
    serviceAccountName: optimizer-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-5lhg9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:04:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      message: 'containers with unready status: [optimizer-agents]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      message: 'containers with unready status: [optimizer-agents]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d1f5fc7b485c5e2c7f51e6b58e92a9a2af333eebbd77210aca5fc9810ae4d4f8
      image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents@sha256:7bd59de1012a5b990345f04b2b12b3ac9594dac1723aeb76a0fcd3184e0999b2
      lastState:
        terminated:
          containerID: containerd://d1f5fc7b485c5e2c7f51e6b58e92a9a2af333eebbd77210aca5fc9810ae4d4f8
          exitCode: 137
          finishedAt: "2026-02-13T23:04:33Z"
          reason: Error
          startedAt: "2026-02-13T23:04:02Z"
      name: optimizer-agents
      ready: false
      restartCount: 387
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=optimizer-agents pod=optimizer-agents-7d5d5d7b5c-vdsvl_neural-hive(bb18f869-8b6a-4c4c-aff9-3d3ce6a9f912)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.4
    podIPs:
    - ip: 10.244.4.4
    qosClass: Burstable
    startTime: "2026-02-12T09:20:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
      kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:19+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-12T09:20:06Z"
    generateName: optimizer-agents-7d5d5d7b5c-
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 7d5d5d7b5c
    name: optimizer-agents-7d5d5d7b5c-vvc8n
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: optimizer-agents-7d5d5d7b5c
      uid: 7f357bc2-6d1e-476f-ba16-1db3c853e0dc
    resourceVersion: "29938069"
    uid: a97ec611-2477-41a9-a80d-26658592d854
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - optimizer-agents
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      envFrom:
      - configMapRef:
          name: optimizer-agents
      image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: optimizer-agents
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "2"
          memory: 4Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c4rcl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: optimizer-agents
    serviceAccountName: optimizer-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-c4rcl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:11Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      message: 'containers with unready status: [optimizer-agents]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      message: 'containers with unready status: [optimizer-agents]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:20:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://eea93e1c9e7ba229a85b34749101399fe52fb993bdff81786717ea36ec38df47
      image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents@sha256:7bd59de1012a5b990345f04b2b12b3ac9594dac1723aeb76a0fcd3184e0999b2
      lastState:
        terminated:
          containerID: containerd://8fc94e818af04c2553a8a74afc04ea898f0be845f8a3842d247263d5bb59aa0c
          exitCode: 137
          finishedAt: "2026-02-13T23:01:27Z"
          reason: Error
          startedAt: "2026-02-13T22:57:31Z"
      name: optimizer-agents
      ready: false
      restartCount: 332
      started: false
      state:
        running:
          startedAt: "2026-02-13T23:01:28Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.144
    podIPs:
    - ip: 10.244.3.144
    qosClass: Burstable
    startTime: "2026-02-12T09:20:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 55470a93f8433c149c692760ef45163f223297b83f4d1ac982c01be161e3413c
      checksum/secret: 6057a0138a7af487f36fc3cf0a850586ee5e6ee65ff787027433029e9338eb64
      kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-12T07:51:51Z"
    generateName: orchestrator-dynamic-7596f58f7c-
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 7596f58f7c
    name: orchestrator-dynamic-7596f58f7c-tpg96
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: orchestrator-dynamic-7596f58f7c
      uid: 0e6a8227-3946-41dc-ae52-aecb632e1c38
    resourceVersion: "29304575"
    uid: bedcb54e-d71e-47b1-b8d3-754c0b219d89
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - orchestrator-dynamic
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: MONGODB_DATABASE
        value: neural_hive
      - name: EXECUTION_TICKET_SERVICE_URL
        value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
      - name: SLA_MANAGEMENT_HOST
        value: sla-management-system.neural-hive.svc.cluster.local
      - name: OPA_HOST
        value: opa.neural-hive.svc.cluster.local
      - name: KAFKA_SESSION_TIMEOUT_MS
        value: "30000"
      envFrom:
      - configMapRef:
          name: orchestrator-dynamic-config
      - secretRef:
          name: orchestrator-dynamic-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: orchestrator-dynamic
      ports:
      - containerPort: 50053
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 200m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/src/workflows/orchestration_workflow.py
        name: workflow-fix
        subPath: orchestration_workflow.py
      - mountPath: /tmp
        name: tmp
      - mountPath: /app/logs
        name: logs
      - mountPath: /app/src/activities/result_consolidation.py
        name: result-consolidation-fix
        subPath: result_consolidation.py
      - mountPath: /app/src/clients/mongodb_client.py
        name: mongodb-client-fix
        subPath: mongodb_client.py
      - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
        name: hotfix-volume
        subPath: execution_ticket_client.py
      - mountPath: /app/src/activities/ticket_generation.py
        name: ticket-generation-fix
        subPath: ticket_generation.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zxzm7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: orchestrator-dynamic
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - configMap:
        defaultMode: 420
        name: orchestration-workflow-fix
      name: workflow-fix
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: logs
    - configMap:
        defaultMode: 420
        name: result-consolidation-fix
      name: result-consolidation-fix
    - configMap:
        defaultMode: 420
        name: mongodb-client-fix
      name: mongodb-client-fix
    - configMap:
        defaultMode: 420
        name: execution-ticket-client-hotfix
      name: hotfix-volume
    - configMap:
        defaultMode: 420
        name: ticket-generation-hotfix
      name: ticket-generation-fix
    - name: kube-api-access-zxzm7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T07:51:54Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T07:51:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T07:52:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T07:52:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T07:51:51Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2a1b79c069fd3f634ec97a51e79e4c899b22e4faa13a4d8baa70024b560d335e
      image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic@sha256:2cc2de03469b703a944eda4699d20eddab1009f71beb98e92ceeee245b854599
      lastState: {}
      name: orchestrator-dynamic
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-12T07:51:54Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.57
    podIPs:
    - ip: 10.244.1.57
    qosClass: Burstable
    startTime: "2026-02-12T07:51:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 1e7c444f34d431cf0f1422f54bfd59651b5347fc2838858437c1f282fc8e24a0
      checksum/secret: cec7ee8d589c1c780ec94fa19000d2ebe1c14f5a587ce9608ba95e67293eabfd
      kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      vault.hashicorp.com/agent-inject: "true"
      vault.hashicorp.com/agent-inject-secret-kafka: secret/data/orchestrator/kafka
      vault.hashicorp.com/agent-inject-secret-mongodb: secret/data/orchestrator/mongodb
      vault.hashicorp.com/agent-inject-secret-postgres: database/creds/temporal-orchestrator
      vault.hashicorp.com/agent-inject-template-kafka: |-
        {{`{{- with secret "secret/data/orchestrator/kafka" -}}
        export KAFKA_SASL_USERNAME="{{ .Data.data.username }}"
        export KAFKA_SASL_PASSWORD="{{ .Data.data.password }}"
        {{- end }}`}}
      vault.hashicorp.com/agent-inject-template-mongodb: |-
        {{`{{- with secret "secret/data/orchestrator/mongodb" -}}
        export MONGODB_URI="{{ .Data.data.uri }}"
        {{- end }}`}}
      vault.hashicorp.com/agent-inject-template-postgres: |-
        {{`{{- with secret "database/creds/temporal-orchestrator" -}}
        export POSTGRES_USER="{{ .Data.username }}"
        export POSTGRES_PASSWORD="{{ .Data.password }}"
        {{- end }}`}}
      vault.hashicorp.com/role: orchestrator-dynamic
    creationTimestamp: "2026-02-13T21:09:46Z"
    generateName: orchestrator-dynamic-7fcb47fdc8-
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 7fcb47fdc8
    name: orchestrator-dynamic-7fcb47fdc8-4l2x7
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: orchestrator-dynamic-7fcb47fdc8
      uid: 2319e27d-fa14-464b-bf4e-18c9ab056927
    resourceVersion: "29939769"
    uid: 7eef6a53-d231-4d43-a23e-48010ec45f86
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - orchestrator-dynamic
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: MONGODB_DATABASE
        value: neural_hive
      - name: EXECUTION_TICKET_SERVICE_URL
        value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
      - name: SLA_MANAGEMENT_HOST
        value: sla-management-system.neural-hive.svc.cluster.local
      - name: OPA_HOST
        value: opa.neural-hive.svc.cluster.local
      - name: KAFKA_SESSION_TIMEOUT_MS
        value: "30000"
      - name: REQUESTS_CA_BUNDLE
        value: /etc/ssl/certs/ca-certificates.crt
      envFrom:
      - configMapRef:
          name: orchestrator-dynamic-config
      - secretRef:
          name: orchestrator-dynamic-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: orchestrator-dynamic
      ports:
      - containerPort: 50053
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      startupProbe:
        failureThreshold: 30
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/src/workflows/orchestration_workflow.py
        name: workflow-fix
        subPath: orchestration_workflow.py
      - mountPath: /tmp
        name: tmp
      - mountPath: /app/logs
        name: logs
      - mountPath: /run/spire/sockets
        name: spire-agent-socket
        readOnly: true
      - mountPath: /app/src/activities/result_consolidation.py
        name: result-consolidation-fix
        subPath: result_consolidation.py
      - mountPath: /app/src/clients/mongodb_client.py
        name: mongodb-client-fix
        subPath: mongodb_client.py
      - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
        name: hotfix-volume
        subPath: execution_ticket_client.py
      - mountPath: /app/src/activities/ticket_generation.py
        name: ticket-generation-fix
        subPath: ticket_generation.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-77g6c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: orchestrator-dynamic
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - configMap:
        defaultMode: 420
        name: orchestration-workflow-fix
      name: workflow-fix
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: logs
    - hostPath:
        path: /run/spire/sockets
        type: Directory
      name: spire-agent-socket
    - configMap:
        defaultMode: 420
        name: orchestrator-dynamic-spire-agent-config
      name: spire-agent-config
    - configMap:
        defaultMode: 420
        name: result-consolidation-fix
      name: result-consolidation-fix
    - configMap:
        defaultMode: 420
        name: mongodb-client-fix
      name: mongodb-client-fix
    - configMap:
        defaultMode: 420
        name: execution-ticket-client-hotfix
      name: hotfix-volume
    - configMap:
        defaultMode: 420
        name: ticket-generation-hotfix
      name: ticket-generation-fix
    - name: kube-api-access-77g6c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:09:49Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:09:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:09:47Z"
      message: 'containers with unready status: [orchestrator-dynamic]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:09:47Z"
      message: 'containers with unready status: [orchestrator-dynamic]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:09:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f9578706358fd74996885f4949d3d0c893a4abc188a20605063ca15bd6152c34
      image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic@sha256:2cc2de03469b703a944eda4699d20eddab1009f71beb98e92ceeee245b854599
      lastState:
        terminated:
          containerID: containerd://f9578706358fd74996885f4949d3d0c893a4abc188a20605063ca15bd6152c34
          exitCode: 3
          finishedAt: "2026-02-13T23:07:09Z"
          reason: Error
          startedAt: "2026-02-13T23:07:02Z"
      name: orchestrator-dynamic
      ready: false
      restartCount: 27
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=orchestrator-dynamic
            pod=orchestrator-dynamic-7fcb47fdc8-4l2x7_neural-hive(7eef6a53-d231-4d43-a23e-48010ec45f86)
          reason: CrashLoopBackOff
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.113
    podIPs:
    - ip: 10.244.2.113
    qosClass: Burstable
    startTime: "2026-02-13T21:09:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
      checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
      kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:06:11Z"
    generateName: queen-agent-5b96b4c956-
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 5b96b4c956
    name: queen-agent-5b96b4c956-wt2mm
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: queen-agent-5b96b4c956
      uid: 9464ccf0-a243-4300-b189-fe9c504277f7
    resourceVersion: "29939585"
    uid: c9975435-f77a-48c7-826a-d7294c6edc4d
  spec:
    containers:
    - command:
      - python
      - /app/fix/startup.py
      env:
      - name: ALLOW_INSECURE_ENDPOINTS
        value: "true"
      - name: ENVIRONMENT
        value: development
      envFrom:
      - configMapRef:
          name: queen-agent-config
      - secretRef:
          name: queen-agent-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      name: queen-agent
      ports:
      - containerPort: 50053
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 300m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 18
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /app/logs
        name: logs
      - mountPath: /app/fix
        name: grpc-fix
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9wnrn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: queen-agent
    serviceAccountName: queen-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: logs
    - configMap:
        defaultMode: 420
        name: grpc-context-fix
      name: grpc-fix
    - name: kube-api-access-9wnrn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:26Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:11Z"
      message: 'containers with unready status: [queen-agent]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:11Z"
      message: 'containers with unready status: [queen-agent]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b2a839143d0dd22b1015d8316fb3ec2ec903567b45afe28b60ca4182fb01823d
      image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/queen-agent@sha256:1e0e48eeb66402dd389e726b8c5af8c3894fa435d72c1a10974422c159799c2a
      lastState:
        terminated:
          containerID: containerd://b2a839143d0dd22b1015d8316fb3ec2ec903567b45afe28b60ca4182fb01823d
          exitCode: 3
          finishedAt: "2026-02-13T23:06:31Z"
          reason: Error
          startedAt: "2026-02-13T23:06:29Z"
      name: queen-agent
      ready: false
      restartCount: 28
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=queen-agent pod=queen-agent-5b96b4c956-wt2mm_neural-hive(c9975435-f77a-48c7-826a-d7294c6edc4d)
          reason: CrashLoopBackOff
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.151
    podIPs:
    - ip: 10.244.3.151
    qosClass: Burstable
    startTime: "2026-02-13T21:06:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
      checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
      kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-11T22:17:09Z"
    generateName: queen-agent-844d56d769-
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 844d56d769
    name: queen-agent-844d56d769-8w64m
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: queen-agent-844d56d769
      uid: e0dff4e4-7475-499a-ac03-1db8ab5446b4
    resourceVersion: "29132981"
    uid: 73c99edf-bb6f-49a5-bdf9-21f8169bf58b
  spec:
    containers:
    - command:
      - python
      - /app/fix/startup.py
      env:
      - name: ALLOW_INSECURE_ENDPOINTS
        value: "true"
      - name: ENVIRONMENT
        value: development
      envFrom:
      - configMapRef:
          name: queen-agent-config
      - secretRef:
          name: queen-agent-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      name: queen-agent
      ports:
      - containerPort: 50053
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 300m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 18
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /app/logs
        name: logs
      - mountPath: /app/fix
        name: grpc-fix
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tb26v
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: queen-agent
    serviceAccountName: queen-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: logs
    - configMap:
        defaultMode: 420
        name: grpc-context-fix
      name: grpc-fix
    - name: kube-api-access-tb26v
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T22:17:18Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T22:17:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T22:17:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T22:17:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T22:17:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://88f3ed8eaf67eeb0c6ed80a6b22d21965e17be763291da9992d4a014fd75b2f5
      image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/queen-agent@sha256:eddc903c171917f97c35b4d0bb3efc15c17542d6a9f2db4df8bf8e140f83881f
      lastState: {}
      name: queen-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-11T22:17:17Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.124
    podIPs:
    - ip: 10.244.3.124
    qosClass: Burstable
    startTime: "2026-02-11T22:17:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: dfe9128ad43e25ff61a0ea9ebb43571dd52ee8975ba58a152935f55a60d19d8a
      kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:05+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-12T09:21:03Z"
    generateName: scout-agents-575db6d7b7-
    labels:
      app.kubernetes.io/instance: scout-agents
      app.kubernetes.io/name: scout-agents
      pod-template-hash: 575db6d7b7
    name: scout-agents-575db6d7b7-766b5
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: scout-agents-575db6d7b7
      uid: c73cde10-86c1-4921-9591-1e6d635f560a
    resourceVersion: "29330589"
    uid: b52d7649-ea69-4f44-91ce-d693ea8ee9cb
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: scout-agents
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: POSTGRES_HOST
        value: postgres-sla.neural-hive-data.svc.cluster.local
      - name: POSTGRES_PORT
        value: "5432"
      - name: POSTGRES_USER
        value: sla_user
      - name: POSTGRES_DB
        value: sla_management
      - name: POSTGRES_PASSWORD
        value: neural_hive_sla_2024
      - name: POSTGRES_SSL_MODE
        value: disable
      envFrom:
      - configMapRef:
          name: scout-agents
      image: ghcr.io/albinojimy/neural-hive-mind/scout-agents:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/live
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: scout-agents
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 1500m
          memory: 2Gi
        requests:
          cpu: 400m
          memory: 768Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 18
        httpGet:
          path: /health/live
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dmjgr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: false
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: scout-agents
    serviceAccountName: scout-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: scout-agents
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - name: kube-api-access-dmjgr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:21:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:21:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:21:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:21:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T09:21:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://29051ef99f6fd1b83b12b30b596cf43c1bd7c210c2f22fb403cfc00d73b7753c
      image: ghcr.io/albinojimy/neural-hive-mind/scout-agents:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/scout-agents@sha256:d793dae46145cbbd2748821bd13dc4545e7ca6cae37474cce2c8fcd6b1e146a5
      lastState: {}
      name: scout-agents
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-12T09:21:04Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.101
    podIPs:
    - ip: 10.244.2.101
    qosClass: Burstable
    startTime: "2026-02-12T09:21:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 797d649f766885eaa7c8926451997da40a3aeab5dc4d0eb324951047fa246914
      checksum/secret: 247768ac8899c81bf336763f0488784ed9da5327a122af8ff4d9a0640f59b8fc
      kubectl.kubernetes.io/restartedAt: "2026-01-28T00:05:19+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:07:45Z"
    generateName: self-healing-engine-68b6f6f4cb-
    labels:
      app.kubernetes.io/component: self-healing
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/name: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
      pod-template-hash: 68b6f6f4cb
    name: self-healing-engine-68b6f6f4cb-jtfxc
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: self-healing-engine-68b6f6f4cb
      uid: 67926721-df7f-4e20-a68b-dbe9bc762d9e
    resourceVersion: "29905709"
    uid: b7d80267-f40a-48ae-bf25-888e13f31c77
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - self-healing-engine
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - envFrom:
      - configMapRef:
          name: self-healing-engine-config
      - secretRef:
          name: self-healing-engine-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/live
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: self-healing-engine
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 1200m
          memory: 1536Mi
        requests:
          cpu: 300m
          memory: 512Mi
      startupProbe:
        failureThreshold: 18
        httpGet:
          path: /health/live
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /app/logs
        name: logs
      - mountPath: /app/playbooks
        name: playbooks
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nqj9x
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: self-healing-engine
    serviceAccountName: self-healing-engine
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: self-healing-engine
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: logs
    - configMap:
        defaultMode: 420
        name: self-healing-engine-playbooks
        optional: true
      name: playbooks
    - name: kube-api-access-nqj9x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:54Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:45Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:08:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:08:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4cd0ec9d7e8306a0087ee0acd036e2ff7b090a5afb3ff9544bc601a3eb6e04a2
      image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine@sha256:13d95e27688a29a1183f47e33c8ab6bfcbe4b9a0b93c4672c73604e88711ee07
      lastState: {}
      name: self-healing-engine
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-13T21:07:54Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.79
    podIPs:
    - ip: 10.244.1.79
    qosClass: Burstable
    startTime: "2026-02-13T21:07:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 797d649f766885eaa7c8926451997da40a3aeab5dc4d0eb324951047fa246914
      checksum/secret: 247768ac8899c81bf336763f0488784ed9da5327a122af8ff4d9a0640f59b8fc
      kubectl.kubernetes.io/restartedAt: "2026-01-28T00:05:19+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:06:19Z"
    generateName: self-healing-engine-68b6f6f4cb-
    labels:
      app.kubernetes.io/component: self-healing
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/name: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
      pod-template-hash: 68b6f6f4cb
    name: self-healing-engine-68b6f6f4cb-psd69
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: self-healing-engine-68b6f6f4cb
      uid: 67926721-df7f-4e20-a68b-dbe9bc762d9e
    resourceVersion: "29939335"
    uid: 228d7e42-b676-4fe9-85dc-1f47734e32b1
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - self-healing-engine
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - envFrom:
      - configMapRef:
          name: self-healing-engine-config
      - secretRef:
          name: self-healing-engine-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/live
          port: http
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: self-healing-engine
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 1200m
          memory: 1536Mi
        requests:
          cpu: 300m
          memory: 512Mi
      startupProbe:
        failureThreshold: 18
        httpGet:
          path: /health/live
          port: http
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /app/logs
        name: logs
      - mountPath: /app/playbooks
        name: playbooks
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nr7vk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: self-healing-engine
    serviceAccountName: self-healing-engine
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: self-healing-engine
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: logs
    - configMap:
        defaultMode: 420
        name: self-healing-engine-playbooks
        optional: true
      name: playbooks
    - name: kube-api-access-nr7vk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:51Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:50Z"
      message: 'containers with unready status: [self-healing-engine]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:50Z"
      message: 'containers with unready status: [self-healing-engine]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bcc624c7c3392e670637643a7df901d32406a33f032df27a31e35c73eefe98df
      image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine@sha256:13d95e27688a29a1183f47e33c8ab6bfcbe4b9a0b93c4672c73604e88711ee07
      lastState:
        terminated:
          containerID: containerd://bcc624c7c3392e670637643a7df901d32406a33f032df27a31e35c73eefe98df
          exitCode: 137
          finishedAt: "2026-02-13T23:05:49Z"
          reason: Error
          startedAt: "2026-02-13T23:05:16Z"
      name: self-healing-engine
      ready: false
      restartCount: 24
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=self-healing-engine pod=self-healing-engine-68b6f6f4cb-psd69_neural-hive(228d7e42-b676-4fe9-85dc-1f47734e32b1)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.7
    podIPs:
    - ip: 10.244.4.7
    qosClass: Burstable
    startTime: "2026-02-13T21:06:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d784f022b946275705ff626e2ecc3df1484c46a832a48ba6ca8e6c1c463a1690
      checksum/secret: d7c7026787afcb0bf2ac9218b116581a2ba5999c5f97250c405b4c7e9b6dec21
      kubectl.kubernetes.io/restartedAt: "2026-02-01T11:13:20+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8000"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-11T20:50:27Z"
    generateName: semantic-translation-engine-775f4c454d-
    labels:
      app.kubernetes.io/component: semantic-translator
      app.kubernetes.io/instance: semantic-translation-engine
      app.kubernetes.io/name: semantic-translation-engine
      neural-hive.io/domain: plan-generation
      neural-hive.io/layer: cognitiva
      pod-template-hash: 775f4c454d
    name: semantic-translation-engine-775f4c454d-89bxp
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: semantic-translation-engine-775f4c454d
      uid: 5bf4837c-f822-4c01-8d3e-7bb097d27da3
    resourceVersion: "29106882"
    uid: 93502ecd-e1ca-4046-a359-92b0c9ae76ca
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: semantic-translation-engine
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: LOG_LEVEL
        value: DEBUG
      - name: SCHEMA_REGISTRY_URL
        value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
      - name: ENVIRONMENT
        value: development
      envFrom:
      - configMapRef:
          name: semantic-translation-engine-config
      - secretRef:
          name: semantic-translation-engine-secrets
      image: ghcr.io/albinojimy/neural-hive-mind/semantic-translation-engine:b4cd999
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: semantic-translation-engine
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8000
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 1500m
          memory: 3Gi
        requests:
          cpu: 450m
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      startupProbe:
        failureThreshold: 20
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /app/logs
        name: logs
      - mountPath: /app/src/main.py
        name: ste-hotfix
        subPath: main.py
      - mountPath: /app/src/services/risk_scorer.py
        name: risk-scorer-hotfix
        subPath: risk_scorer.py
      - mountPath: /app/src/services/dag_generator.py
        name: intent-decomposition
        subPath: dag_generator.py
      - mountPath: /app/src/services/intent_classifier.py
        name: intent-decomposition
        subPath: intent_classifier.py
      - mountPath: /app/src/services/decomposition_templates.py
        name: intent-decomposition
        subPath: decomposition_templates.py
      - mountPath: /app/src/services/semantic_parser.py
        name: semantic-parser
        subPath: semantic_parser.py
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-r5tfs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: semantic-translation-engine
    serviceAccountName: semantic-translation-engine
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: semantic-translation-engine
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: tmp
    - emptyDir: {}
      name: logs
    - configMap:
        defaultMode: 420
        name: ste-hotfix
      name: ste-hotfix
    - configMap:
        defaultMode: 420
        name: ste-risk-scorer-hotfix
      name: risk-scorer-hotfix
    - configMap:
        defaultMode: 420
        name: ste-intent-decomposition
      name: intent-decomposition
    - configMap:
        defaultMode: 420
        name: ste-semantic-parser
      name: semantic-parser
    - name: kube-api-access-r5tfs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T20:50:49Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T20:50:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T20:51:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T20:51:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T20:50:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d2bdf77b2680ee3733964e5f7222981f55e62e72e46eb0a95b6260cf48cffc94
      image: ghcr.io/albinojimy/neural-hive-mind/semantic-translation-engine:b4cd999
      imageID: ghcr.io/albinojimy/neural-hive-mind/semantic-translation-engine@sha256:cbb4de1e879cac19a0531f0c0c93d291083139995526f3d60b1306684faafed6
      lastState: {}
      name: semantic-translation-engine
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-11T20:50:48Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.252
    podIPs:
    - ip: 10.244.1.252
    qosClass: Burstable
    startTime: "2026-02-11T20:50:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-02-10T22:44:08+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:06:01Z"
    generateName: service-registry-867758cb55-
    labels:
      app.kubernetes.io/component: coordination
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/name: service-registry
      app.kubernetes.io/part-of: neural-hive-mind
      pod-template-hash: 867758cb55
    name: service-registry-867758cb55-zgjkq
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: service-registry-867758cb55
      uid: 26666e77-8085-41a4-b64f-63a36afa7521
    resourceVersion: "29904458"
    uid: e1adedae-0ee5-49fe-8d13-1cc5f33a605e
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - service-registry
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: SERVICE_NAME
        value: service-registry
      - name: SERVICE_VERSION
        value: 1.0.0
      - name: ENVIRONMENT
        value: development
      - name: LOG_LEVEL
        value: INFO
      - name: GRPC_PORT
        value: "50051"
      - name: METRICS_PORT
        value: "9090"
      - name: ETCD_ENDPOINTS
        value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
      - name: ETCD_PREFIX
        value: neural-hive:agents
      - name: ETCD_TIMEOUT_SECONDS
        value: "5"
      - name: HEALTH_CHECK_INTERVAL_SECONDS
        value: "60"
      - name: HEARTBEAT_TIMEOUT_SECONDS
        value: "120"
      - name: REDIS_CLUSTER_NODES
        value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
      - name: REDIS_PASSWORD
        valueFrom:
          secretKeyRef:
            key: redis-password
            name: service-registry-secret
      - name: OTEL_EXPORTER_ENDPOINT
        value: http://otel-collector:4317
      image: ghcr.io/albinojimy/neural-hive-mind/service-registry:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        grpc:
          port: 50051
          service: ""
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: service-registry
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        grpc:
          port: 50051
          service: ""
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 1200m
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 256Mi
      startupProbe:
        failureThreshold: 20
        grpc:
          port: 50051
          service: ""
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-56zk5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: service-registry
    serviceAccountName: service-registry
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: service-registry
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - name: kube-api-access-56zk5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7c919b5844d3d7b6b0d3a159bf8c9c851443482d6d1797052f119c0e369b7212
      image: ghcr.io/albinojimy/neural-hive-mind/service-registry:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/service-registry@sha256:57e0f015afd10a35e0d2b8f21f2880eb95c4dae04dc7854966f240350fc789b7
      lastState: {}
      name: service-registry
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-13T21:06:08Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.109
    podIPs:
    - ip: 10.244.2.109
    qosClass: Burstable
    startTime: "2026-02-13T21:06:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-02-10T22:06:19+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:06:23Z"
    generateName: sla-management-system-867c876fbd-
    labels:
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      neural-hive.io/layer: monitoring
      pod-template-hash: 867c876fbd
    name: sla-management-system-867c876fbd-b94fw
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sla-management-system-867c876fbd
      uid: 5cb866b6-e57c-48f7-9c0d-4310aa88f487
    resourceVersion: "29939601"
    uid: e942097d-36ec-463f-960f-1b1acb5cca12
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - sla-management-system
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: SERVICE_NAME
        value: sla-management-system
      - name: VERSION
        value: 1.0.0
      - name: ENVIRONMENT
        value: production
      - name: LOG_LEVEL
        value: INFO
      - name: PROMETHEUS__URL
        value: http://prometheus-server.monitoring.svc.cluster.local:9090
      - name: PROMETHEUS__TIMEOUT_SECONDS
        value: "30"
      - name: PROMETHEUS__MAX_RETRIES
        value: "3"
      - name: POSTGRESQL__HOST
        value: postgres-sla.neural-hive-data.svc.cluster.local
      - name: POSTGRESQL__PORT
        value: "5432"
      - name: POSTGRESQL__DATABASE
        value: sla_management
      - name: POSTGRESQL__USER
        valueFrom:
          secretKeyRef:
            key: POSTGRESQL__USER
            name: sla-management-system-secret
      - name: POSTGRESQL__PASSWORD
        valueFrom:
          secretKeyRef:
            key: POSTGRESQL__PASSWORD
            name: sla-management-system-secret
      - name: POSTGRESQL__POOL_MIN_SIZE
        value: "2"
      - name: POSTGRESQL__POOL_MAX_SIZE
        value: "10"
      - name: REDIS__CLUSTER_NODES
        value: '["redis-cluster.redis-cluster.svc.cluster.local:6379"]'
      - name: REDIS__PASSWORD
        valueFrom:
          secretKeyRef:
            key: REDIS__PASSWORD
            name: sla-management-system-secret
      - name: REDIS__CACHE_TTL_SECONDS
        value: "60"
      - name: KAFKA__BOOTSTRAP_SERVERS
        value: '["neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"]'
      - name: KAFKA__BUDGET_TOPIC
        value: sla.budgets
      - name: KAFKA__FREEZE_TOPIC
        value: sla.freeze.events
      - name: KAFKA__VIOLATIONS_TOPIC
        value: sla.violations
      - name: ALERTMANAGER__URL
        value: http://alertmanager.monitoring.svc.cluster.local:9093
      - name: ALERTMANAGER__WEBHOOK_PATH
        value: /webhooks/alertmanager
      - name: CALCULATOR__CALCULATION_INTERVAL_SECONDS
        value: "30"
      - name: CALCULATOR__ERROR_BUDGET_WINDOW_DAYS
        value: "30"
      - name: CALCULATOR__BURN_RATE_FAST_THRESHOLD
        value: "14.4"
      - name: CALCULATOR__BURN_RATE_SLOW_THRESHOLD
        value: "6"
      - name: POLICY__FREEZE_THRESHOLD_PERCENT
        value: "20"
      - name: POLICY__AUTO_UNFREEZE_ENABLED
        value: "true"
      - name: POLICY__UNFREEZE_THRESHOLD_PERCENT
        value: "50"
      - name: ALLOW_INSECURE_HTTP_ENDPOINTS
        value: "true"
      image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: sla-management-system
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 1200m
          memory: 2Gi
        requests:
          cpu: 300m
          memory: 640Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s2hvv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: sla-management-system
    serviceAccountName: sla-management-system
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-s2hvv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:01:25Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:43Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:43Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://09e783b5930e78e32528fd6664793403772b66715a21a9376580cab30943387d
      image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/sla-management-system@sha256:f72576241c0a4ae53ad94a528be35a3581cbf2f8f971ce3711db3e78b0369582
      lastState:
        terminated:
          containerID: containerd://6b4220b2117ed31fee1b928e0237fee6a6fd0ace1445280fc338e4bc45bee582
          exitCode: 137
          finishedAt: "2026-02-13T23:01:23Z"
          reason: Error
          startedAt: "2026-02-13T23:00:52Z"
      name: sla-management-system
      ready: true
      restartCount: 24
      started: true
      state:
        running:
          startedAt: "2026-02-13T23:06:23Z"
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.246
    podIPs:
    - ip: 10.244.4.246
    qosClass: Burstable
    startTime: "2026-02-13T21:06:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-02-10T22:06:19+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:07:14Z"
    generateName: sla-management-system-867c876fbd-
    labels:
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      neural-hive.io/layer: monitoring
      pod-template-hash: 867c876fbd
    name: sla-management-system-867c876fbd-hrhpn
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sla-management-system-867c876fbd
      uid: 5cb866b6-e57c-48f7-9c0d-4310aa88f487
    resourceVersion: "29937876"
    uid: 4d4f1ae8-e600-4fb4-9149-0c63d0a25575
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - sla-management-system
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: SERVICE_NAME
        value: sla-management-system
      - name: VERSION
        value: 1.0.0
      - name: ENVIRONMENT
        value: production
      - name: LOG_LEVEL
        value: INFO
      - name: PROMETHEUS__URL
        value: http://prometheus-server.monitoring.svc.cluster.local:9090
      - name: PROMETHEUS__TIMEOUT_SECONDS
        value: "30"
      - name: PROMETHEUS__MAX_RETRIES
        value: "3"
      - name: POSTGRESQL__HOST
        value: postgres-sla.neural-hive-data.svc.cluster.local
      - name: POSTGRESQL__PORT
        value: "5432"
      - name: POSTGRESQL__DATABASE
        value: sla_management
      - name: POSTGRESQL__USER
        valueFrom:
          secretKeyRef:
            key: POSTGRESQL__USER
            name: sla-management-system-secret
      - name: POSTGRESQL__PASSWORD
        valueFrom:
          secretKeyRef:
            key: POSTGRESQL__PASSWORD
            name: sla-management-system-secret
      - name: POSTGRESQL__POOL_MIN_SIZE
        value: "2"
      - name: POSTGRESQL__POOL_MAX_SIZE
        value: "10"
      - name: REDIS__CLUSTER_NODES
        value: '["redis-cluster.redis-cluster.svc.cluster.local:6379"]'
      - name: REDIS__PASSWORD
        valueFrom:
          secretKeyRef:
            key: REDIS__PASSWORD
            name: sla-management-system-secret
      - name: REDIS__CACHE_TTL_SECONDS
        value: "60"
      - name: KAFKA__BOOTSTRAP_SERVERS
        value: '["neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"]'
      - name: KAFKA__BUDGET_TOPIC
        value: sla.budgets
      - name: KAFKA__FREEZE_TOPIC
        value: sla.freeze.events
      - name: KAFKA__VIOLATIONS_TOPIC
        value: sla.violations
      - name: ALERTMANAGER__URL
        value: http://alertmanager.monitoring.svc.cluster.local:9093
      - name: ALERTMANAGER__WEBHOOK_PATH
        value: /webhooks/alertmanager
      - name: CALCULATOR__CALCULATION_INTERVAL_SECONDS
        value: "30"
      - name: CALCULATOR__ERROR_BUDGET_WINDOW_DAYS
        value: "30"
      - name: CALCULATOR__BURN_RATE_FAST_THRESHOLD
        value: "14.4"
      - name: CALCULATOR__BURN_RATE_SLOW_THRESHOLD
        value: "6"
      - name: POLICY__FREEZE_THRESHOLD_PERCENT
        value: "20"
      - name: POLICY__AUTO_UNFREEZE_ENABLED
        value: "true"
      - name: POLICY__UNFREEZE_THRESHOLD_PERCENT
        value: "50"
      - name: ALLOW_INSECURE_HTTP_ENDPOINTS
        value: "true"
      image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: sla-management-system
      ports:
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 1200m
          memory: 2Gi
        requests:
          cpu: 300m
          memory: 640Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xxz4z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1001
    serviceAccount: sla-management-system
    serviceAccountName: sla-management-system
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-xxz4z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T22:55:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:00:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:00:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ce3cf424a02474387065c95a3fe40dbda300f78ef270f15de728a2a3a0b4bf5f
      image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/sla-management-system@sha256:f72576241c0a4ae53ad94a528be35a3581cbf2f8f971ce3711db3e78b0369582
      lastState:
        terminated:
          containerID: containerd://c6b802ca818c827e39ae38b2c302d7c8cc580eda435b1f1816d54dee799c378b
          exitCode: 137
          finishedAt: "2026-02-13T22:55:21Z"
          reason: Error
          startedAt: "2026-02-13T22:54:51Z"
      name: sla-management-system
      ready: true
      restartCount: 23
      started: true
      state:
        running:
          startedAt: "2026-02-13T23:00:30Z"
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.225
    podIPs:
    - ip: 10.244.4.225
    qosClass: Burstable
    startTime: "2026-02-13T21:07:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d84b2e53cf70d2acea5239b51f3bfaf6347648f28a5e5fc1c898942f75ebff54
      kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:44+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-08T13:29:24Z"
    generateName: specialist-architecture-75f65497dc-
    labels:
      app.kubernetes.io/instance: specialist-architecture
      app.kubernetes.io/name: specialist-architecture
      neural-hive.io/component: specialist
      pod-template-hash: 75f65497dc
    name: specialist-architecture-75f65497dc-s7p8n
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: specialist-architecture-75f65497dc
      uid: c471f1bf-2304-435d-be8d-1d9adb46352b
    resourceVersion: "27880828"
    uid: 525805f3-27d6-4f05-859c-224aa2e7cc0f
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                neural-hive.io/component: specialist
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: ENVIRONMENT
        value: production
      - name: LOG_LEVEL
        value: DEBUG
      - name: ENABLE_JWT_AUTH
        value: "false"
      - name: JWT_SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: jwt_secret_key
            name: specialist-architecture-secrets
      - name: ENABLE_PII_DETECTION
        value: "false"
      - name: MLFLOW_TRACKING_URI
        value: http://mlflow.mlflow.svc.cluster.local:5000
      - name: MLFLOW_EXPERIMENT_NAME
        value: specialist-architecture
      - name: MLFLOW_MODEL_NAME
        value: architecture-evaluator
      - name: MLFLOW_MODEL_STAGE
        value: Production
      - name: MONGODB_URI
        valueFrom:
          secretKeyRef:
            key: mongodb_uri
            name: specialist-architecture-secrets
      - name: MONGODB_DATABASE
        value: neural_hive
      - name: NEO4J_URI
        value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
      - name: NEO4J_USER
        value: neo4j
      - name: NEO4J_PASSWORD
        valueFrom:
          secretKeyRef:
            key: neo4j_password
            name: specialist-architecture-secrets
      - name: NEO4J_DATABASE
        value: neo4j
      - name: REDIS_CLUSTER_NODES
        value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
      - name: REDIS_SSL_ENABLED
        value: "false"
      - name: OTEL_EXPORTER_OTLP_ENDPOINT
        value: http://opentelemetry-collector.observability.svc.cluster.local:4317
      - name: GRPC_PORT
        value: "50051"
      - name: HTTP_PORT
        value: "8000"
      - name: PROMETHEUS_PORT
        value: "8080"
      - name: ENABLE_LEDGER
        value: "true"
      - name: LEDGER_REQUIRED
        value: "false"
      - name: LEDGER_INIT_RETRY_ATTEMPTS
        value: "5"
      - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
        value: "30"
      - name: MODEL_REQUIRED
        value: "true"
      - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
        value: "true"
      - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
        value: "10"
      - name: ENABLE_TRACING
        value: "true"
      - name: USE_SEMANTIC_FALLBACK
        value: "true"
      - name: ENABLE_FEEDBACK_COLLECTION
        value: "true"
      - name: FEEDBACK_API_ENABLED
        value: "true"
      - name: FEEDBACK_REQUIRE_AUTHENTICATION
        value: "true"
      - name: FEEDBACK_ALLOWED_ROLES
        value: '["admin","specialist_reviewer","human_expert"]'
      - name: FEEDBACK_MONGODB_COLLECTION
        value: specialist_feedback
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-architecture:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: specialist-architecture
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      startupProbe:
        failureThreshold: 15
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/mlruns
        name: mlruns
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jwb2q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: specialist-architecture
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: mlruns
    - name: kube-api-access-jwb2q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:28Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5cbb5de83340c16bcf6628588976c55c7a99b33c704a9ea68525abbc51fbfed1
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-architecture:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/specialist-architecture@sha256:849a10a2012b26bafc87bab3dcca6664977ecdd2f58e63088f0f5dedfc70ea53
      lastState: {}
      name: specialist-architecture
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-08T13:29:28Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.158
    podIPs:
    - ip: 10.244.2.158
    qosClass: Burstable
    startTime: "2026-02-08T13:29:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 5a3489da92e79aaf70bd34e56dd2871d8351f15bfafe0d18a026b0e4ab74a418
      kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:46+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-08T13:29:11Z"
    generateName: specialist-behavior-68c57f76bd-
    labels:
      app.kubernetes.io/instance: specialist-behavior
      app.kubernetes.io/name: specialist-behavior
      neural-hive.io/component: specialist
      pod-template-hash: 68c57f76bd
    name: specialist-behavior-68c57f76bd-w4ms9
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: specialist-behavior-68c57f76bd
      uid: 4919cbb9-9214-499b-bba9-6e7b9380361b
    resourceVersion: "27880879"
    uid: 6d3450c2-1731-4665-80fc-7dad3cd8e8d2
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                neural-hive.io/component: specialist
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: ENVIRONMENT
        value: production
      - name: LOG_LEVEL
        value: DEBUG
      - name: ENABLE_JWT_AUTH
        value: "false"
      - name: JWT_SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: jwt_secret_key
            name: specialist-behavior-secrets
      - name: ENABLE_PII_DETECTION
        value: "false"
      - name: MLFLOW_TRACKING_URI
        value: http://mlflow.mlflow.svc.cluster.local:5000
      - name: MLFLOW_EXPERIMENT_NAME
        value: specialist-behavior
      - name: MLFLOW_MODEL_NAME
        value: behavior-evaluator
      - name: MLFLOW_MODEL_STAGE
        value: Production
      - name: MONGODB_URI
        valueFrom:
          secretKeyRef:
            key: mongodb_uri
            name: specialist-behavior-secrets
      - name: MONGODB_DATABASE
        value: neural_hive
      - name: NEO4J_URI
        value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
      - name: NEO4J_USER
        value: neo4j
      - name: NEO4J_PASSWORD
        valueFrom:
          secretKeyRef:
            key: neo4j_password
            name: specialist-behavior-secrets
      - name: NEO4J_DATABASE
        value: neo4j
      - name: REDIS_CLUSTER_NODES
        value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
      - name: REDIS_SSL_ENABLED
        value: "false"
      - name: OTEL_EXPORTER_OTLP_ENDPOINT
        value: http://opentelemetry-collector.observability.svc.cluster.local:4317
      - name: GRPC_PORT
        value: "50051"
      - name: HTTP_PORT
        value: "8000"
      - name: PROMETHEUS_PORT
        value: "8080"
      - name: ENABLE_LEDGER
        value: "true"
      - name: LEDGER_REQUIRED
        value: "false"
      - name: LEDGER_INIT_RETRY_ATTEMPTS
        value: "5"
      - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
        value: "30"
      - name: MODEL_REQUIRED
        value: "true"
      - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
        value: "true"
      - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
        value: "10"
      - name: ENABLE_TRACING
        value: "true"
      - name: USE_SEMANTIC_FALLBACK
        value: "true"
      - name: ENABLE_FEEDBACK_COLLECTION
        value: "true"
      - name: FEEDBACK_API_ENABLED
        value: "true"
      - name: FEEDBACK_REQUIRE_AUTHENTICATION
        value: "true"
      - name: FEEDBACK_ALLOWED_ROLES
        value: '["admin","specialist_reviewer","human_expert"]'
      - name: FEEDBACK_MONGODB_COLLECTION
        value: specialist_feedback
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-behavior:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: specialist-behavior
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      startupProbe:
        failureThreshold: 15
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/mlruns
        name: mlruns
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gxnmj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: specialist-behavior
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: mlruns
    - name: kube-api-access-gxnmj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:16Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:30:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:30:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6df0b127920344d0fb649caaf072662644b8fb4497cdec75e03ca939ac4cac58
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-behavior:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/specialist-behavior@sha256:8aface2ab726752b6d2df35ec04e1c3595d14f7b7469cf7d359ae33a6b64482d
      lastState: {}
      name: specialist-behavior
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-08T13:29:15Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.206
    podIPs:
    - ip: 10.244.3.206
    qosClass: Burstable
    startTime: "2026-02-08T13:29:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 8acc665d6304eaa2fcce6e7ddc07cdbee4970465ed81e2b6ce5e456151f195f3
      kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:48+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-08T13:28:57Z"
    generateName: specialist-business-689d656dc4-
    labels:
      app.kubernetes.io/instance: specialist-business
      app.kubernetes.io/name: specialist-business
      neural-hive.io/component: specialist
      pod-template-hash: 689d656dc4
    name: specialist-business-689d656dc4-f2w52
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: specialist-business-689d656dc4
      uid: 66229b2a-cbc5-43b0-9f64-a09e49c80da9
    resourceVersion: "27880791"
    uid: 76a51498-0939-4a2a-8912-b3412627d78c
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                neural-hive.io/component: specialist
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: ENVIRONMENT
        value: production
      - name: LOG_LEVEL
        value: DEBUG
      - name: ENABLE_JWT_AUTH
        value: "false"
      - name: JWT_SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: jwt_secret_key
            name: specialist-business-secrets
      - name: ENABLE_PII_DETECTION
        value: "false"
      - name: MLFLOW_TRACKING_URI
        value: http://mlflow.mlflow.svc.cluster.local:5000
      - name: MLFLOW_EXPERIMENT_NAME
        value: specialist-business
      - name: MLFLOW_MODEL_NAME
        value: business-evaluator
      - name: MLFLOW_MODEL_STAGE
        value: Production
      - name: MONGODB_URI
        valueFrom:
          secretKeyRef:
            key: mongodb_uri
            name: specialist-business-secrets
      - name: MONGODB_DATABASE
        value: neural_hive
      - name: NEO4J_URI
        value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
      - name: NEO4J_USER
        value: neo4j
      - name: NEO4J_PASSWORD
        valueFrom:
          secretKeyRef:
            key: neo4j_password
            name: specialist-business-secrets
      - name: NEO4J_DATABASE
        value: neo4j
      - name: REDIS_CLUSTER_NODES
        value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
      - name: REDIS_SSL_ENABLED
        value: "false"
      - name: OTEL_EXPORTER_OTLP_ENDPOINT
        value: http://opentelemetry-collector.observability.svc.cluster.local:4317
      - name: GRPC_PORT
        value: "50051"
      - name: HTTP_PORT
        value: "8000"
      - name: PROMETHEUS_PORT
        value: "8080"
      - name: ENABLE_LEDGER
        value: "true"
      - name: LEDGER_REQUIRED
        value: "false"
      - name: LEDGER_INIT_RETRY_ATTEMPTS
        value: "5"
      - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
        value: "30"
      - name: MODEL_REQUIRED
        value: "true"
      - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
        value: "true"
      - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
        value: "10"
      - name: ENABLE_TRACING
        value: "true"
      - name: USE_SEMANTIC_FALLBACK
        value: "true"
      - name: ENABLE_FEEDBACK_COLLECTION
        value: "true"
      - name: FEEDBACK_API_ENABLED
        value: "true"
      - name: FEEDBACK_REQUIRE_AUTHENTICATION
        value: "true"
      - name: FEEDBACK_ALLOWED_ROLES
        value: '["admin","specialist_reviewer","human_expert"]'
      - name: FEEDBACK_MONGODB_COLLECTION
        value: specialist_feedback
      - name: FEATURE_CACHE_ENABLED
        value: "true"
      - name: FEATURE_CACHE_TTL_SECONDS
        value: "3600"
      - name: ENABLE_BATCH_INFERENCE
        value: "true"
      - name: BATCH_INFERENCE_SIZE
        value: "32"
      - name: BATCH_INFERENCE_MAX_WORKERS
        value: "8"
      - name: ENABLE_GPU_ACCELERATION
        value: "false"
      - name: GPU_DEVICE
        value: auto
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-business:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: specialist-business
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      startupProbe:
        failureThreshold: 15
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/mlruns
        name: mlruns
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b7zv8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: specialist-business
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: mlruns
    - name: kube-api-access-b7zv8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:02Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://701d99934ef051cc00a7b86a4e45c4f39746564b10660ebc2cdb6bffb83948eb
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-business:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/specialist-business@sha256:da0dad104456596877c45a7e14a9c0849d38f9224451b191cb0469dfc4065e9c
      lastState: {}
      name: specialist-business
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-08T13:29:01Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.75
    podIPs:
    - ip: 10.244.1.75
    qosClass: Burstable
    startTime: "2026-02-08T13:29:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: c6c47581f315a6fd65b5b6530d5f156139862aff0d72f692b99f9370daf76752
      kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:50+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-08T13:29:17Z"
    generateName: specialist-evolution-5f6b789f48-
    labels:
      app.kubernetes.io/instance: specialist-evolution
      app.kubernetes.io/name: specialist-evolution
      neural-hive.io/component: specialist
      pod-template-hash: 5f6b789f48
    name: specialist-evolution-5f6b789f48-kwdfg
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: specialist-evolution-5f6b789f48
      uid: b05836bc-6a2f-431f-b694-86c92e16e241
    resourceVersion: "27880857"
    uid: 7877b0e7-028d-4a3d-b0b6-2294f258691a
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                neural-hive.io/component: specialist
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: ENVIRONMENT
        value: production
      - name: LOG_LEVEL
        value: DEBUG
      - name: ENABLE_JWT_AUTH
        value: "false"
      - name: JWT_SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: jwt_secret_key
            name: specialist-evolution-secrets
      - name: ENABLE_PII_DETECTION
        value: "false"
      - name: MLFLOW_TRACKING_URI
        value: http://mlflow.mlflow.svc.cluster.local:5000
      - name: MLFLOW_EXPERIMENT_NAME
        value: specialist-evolution
      - name: MLFLOW_MODEL_NAME
        value: evolution-evaluator
      - name: MLFLOW_MODEL_STAGE
        value: Production
      - name: MONGODB_URI
        valueFrom:
          secretKeyRef:
            key: mongodb_uri
            name: specialist-evolution-secrets
      - name: MONGODB_DATABASE
        value: neural_hive
      - name: NEO4J_URI
        value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
      - name: NEO4J_USER
        value: neo4j
      - name: NEO4J_PASSWORD
        valueFrom:
          secretKeyRef:
            key: neo4j_password
            name: specialist-evolution-secrets
      - name: NEO4J_DATABASE
        value: neo4j
      - name: REDIS_CLUSTER_NODES
        value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
      - name: REDIS_SSL_ENABLED
        value: "false"
      - name: OTEL_EXPORTER_OTLP_ENDPOINT
        value: http://opentelemetry-collector.observability.svc.cluster.local:4317
      - name: GRPC_PORT
        value: "50051"
      - name: HTTP_PORT
        value: "8000"
      - name: PROMETHEUS_PORT
        value: "8080"
      - name: ENABLE_LEDGER
        value: "true"
      - name: LEDGER_REQUIRED
        value: "false"
      - name: LEDGER_INIT_RETRY_ATTEMPTS
        value: "5"
      - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
        value: "30"
      - name: MODEL_REQUIRED
        value: "true"
      - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
        value: "true"
      - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
        value: "10"
      - name: ENABLE_TRACING
        value: "true"
      - name: USE_SEMANTIC_FALLBACK
        value: "true"
      - name: ENABLE_FEEDBACK_COLLECTION
        value: "true"
      - name: FEEDBACK_API_ENABLED
        value: "true"
      - name: FEEDBACK_REQUIRE_AUTHENTICATION
        value: "true"
      - name: FEEDBACK_ALLOWED_ROLES
        value: '["admin","specialist_reviewer","human_expert"]'
      - name: FEEDBACK_MONGODB_COLLECTION
        value: specialist_feedback
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-evolution:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: specialist-evolution
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      startupProbe:
        failureThreshold: 15
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/mlruns
        name: mlruns
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zwqmb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: specialist-evolution
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: mlruns
    - name: kube-api-access-zwqmb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:22Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:30:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:30:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b8bd79352c292a085ff231055c0ec9c6c241b80ab76a8f0dd0ed8d2621904784
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-evolution:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/specialist-evolution@sha256:f3529f02945607143d35fabcc83989f64fb6b5b27443dcaa96d5c0e08c24ae35
      lastState: {}
      name: specialist-evolution
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-08T13:29:21Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.207
    podIPs:
    - ip: 10.244.3.207
    qosClass: Burstable
    startTime: "2026-02-08T13:29:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 96e2e5b50df5f53c3c4cd828b33ca93fdb45f04dd60e799b43c2e0f2c975ac8e
      kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:51+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-08T13:29:05Z"
    generateName: specialist-technical-fcb8bc9f8-
    labels:
      app.kubernetes.io/instance: specialist-technical
      app.kubernetes.io/name: specialist-technical
      neural-hive.io/component: specialist
      pod-template-hash: fcb8bc9f8
    name: specialist-technical-fcb8bc9f8-wnzmj
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: specialist-technical-fcb8bc9f8
      uid: aa4c628a-2973-480c-ad44-9905b583c268
    resourceVersion: "27880776"
    uid: ee189b8c-d9ad-4988-bc17-729a2f60f280
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                neural-hive.io/component: specialist
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - env:
      - name: ENVIRONMENT
        value: production
      - name: LOG_LEVEL
        value: DEBUG
      - name: ENABLE_JWT_AUTH
        value: "false"
      - name: JWT_SECRET_KEY
        valueFrom:
          secretKeyRef:
            key: jwt_secret_key
            name: specialist-technical-secrets
      - name: ENABLE_PII_DETECTION
        value: "false"
      - name: MLFLOW_TRACKING_URI
        value: http://mlflow.mlflow.svc.cluster.local:5000
      - name: MLFLOW_EXPERIMENT_NAME
        value: specialist-technical
      - name: MLFLOW_MODEL_NAME
        value: technical-evaluator
      - name: MLFLOW_MODEL_STAGE
        value: Production
      - name: MONGODB_URI
        valueFrom:
          secretKeyRef:
            key: mongodb_uri
            name: specialist-technical-secrets
      - name: MONGODB_DATABASE
        value: neural_hive
      - name: NEO4J_URI
        value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
      - name: NEO4J_USER
        value: neo4j
      - name: NEO4J_PASSWORD
        valueFrom:
          secretKeyRef:
            key: neo4j_password
            name: specialist-technical-secrets
      - name: NEO4J_DATABASE
        value: neo4j
      - name: REDIS_CLUSTER_NODES
        value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
      - name: REDIS_SSL_ENABLED
        value: "false"
      - name: OTEL_EXPORTER_OTLP_ENDPOINT
        value: http://opentelemetry-collector.observability.svc.cluster.local:4317
      - name: GRPC_PORT
        value: "50051"
      - name: HTTP_PORT
        value: "8000"
      - name: PROMETHEUS_PORT
        value: "8080"
      - name: ENABLE_LEDGER
        value: "true"
      - name: LEDGER_REQUIRED
        value: "false"
      - name: LEDGER_INIT_RETRY_ATTEMPTS
        value: "5"
      - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
        value: "30"
      - name: MODEL_REQUIRED
        value: "true"
      - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
        value: "true"
      - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
        value: "10"
      - name: ENABLE_TRACING
        value: "true"
      - name: USE_SEMANTIC_FALLBACK
        value: "true"
      - name: ENABLE_FEEDBACK_COLLECTION
        value: "true"
      - name: FEEDBACK_API_ENABLED
        value: "true"
      - name: FEEDBACK_REQUIRE_AUTHENTICATION
        value: "true"
      - name: FEEDBACK_ALLOWED_ROLES
        value: '["admin","specialist_reviewer","human_expert"]'
      - name: FEEDBACK_MONGODB_COLLECTION
        value: specialist_feedback
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-technical:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: specialist-technical
      ports:
      - containerPort: 50051
        name: grpc
        protocol: TCP
      - containerPort: 8000
        name: http
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 250m
          memory: 512Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: false
      startupProbe:
        failureThreshold: 15
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/mlruns
        name: mlruns
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4v6fq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: specialist-technical
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: mlruns
    - name: kube-api-access-4v6fq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:08Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:08Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-08T13:29:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://096eff21694bd34c0df69ffefe241c407d5547d89cab5c54f3cfa98a806f46ad
      image: ghcr.io/albinojimy/neural-hive-mind/specialist-technical:latest
      imageID: ghcr.io/albinojimy/neural-hive-mind/specialist-technical@sha256:eedbfbe0ecc2b652d076ee50eb5fcbc94df2bf66c7ffd6c98c0dd051d6e6ef44
      lastState: {}
      name: specialist-technical
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-08T13:29:08Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.76
    podIPs:
    - ip: 10.244.1.76
    qosClass: Burstable
    startTime: "2026-02-08T13:29:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 8ac2bd0d9fdec2265af9bdd10b674d703f35d020860160fa4577d18160f593e8
      kubectl.kubernetes.io/restartedAt: "2026-02-13T22:06:51+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:07:18Z"
    generateName: worker-agents-cbfcc57b9-
    labels:
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/name: worker-agents
      pod-template-hash: cbfcc57b9
    name: worker-agents-cbfcc57b9-9x2xz
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: worker-agents-cbfcc57b9
      uid: c0bc85ad-f67a-4d4b-a477-35550a99a96c
    resourceVersion: "29939516"
    uid: f80e8589-6a5f-4875-ba1d-2b25d6ede639
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - worker-agents
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POSTGRES_USER
        value: sla_user
      - name: POSTGRES_PASSWORD
        value: neural_hive_sla_2024
      - name: POSTGRES_DB
        value: sla_management
      - name: POSTGRES_HOST
        value: postgres-sla.neural-hive-data.svc.cluster.local
      envFrom:
      - configMapRef:
          name: worker-agents
      image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:3a3e661
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 5
      name: worker-agents
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 400m
          memory: 640Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 20
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sj5bg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    - name: ghcr-secret-fixed
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: worker-agents
    serviceAccountName: worker-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: worker-agents
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-sj5bg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:01:18Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:18Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b1803c387480d4fc55f5e233dadb5f0cd8c07ff827f3af4f4fa358536f8ddea8
      image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:3a3e661
      imageID: ghcr.io/albinojimy/neural-hive-mind/worker-agents@sha256:41eca7054e7ff40d27dac1412ca99be4f77d3a773db78e33c1a81647da463211
      lastState:
        terminated:
          containerID: containerd://49069c9c67fe98d30fe8fa7e931265e66225c65de03e5c62b31e1c338ddd9407
          exitCode: 137
          finishedAt: "2026-02-13T23:01:15Z"
          reason: Error
          startedAt: "2026-02-13T23:00:45Z"
      name: worker-agents
      ready: true
      restartCount: 21
      started: true
      state:
        running:
          startedAt: "2026-02-13T23:06:24Z"
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.244
    podIPs:
    - ip: 10.244.4.244
    qosClass: Burstable
    startTime: "2026-02-13T21:07:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 8ac2bd0d9fdec2265af9bdd10b674d703f35d020860160fa4577d18160f593e8
      kubectl.kubernetes.io/restartedAt: "2026-02-13T22:06:51+01:00"
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-13T21:06:56Z"
    generateName: worker-agents-cbfcc57b9-
    labels:
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/name: worker-agents
      pod-template-hash: cbfcc57b9
    name: worker-agents-cbfcc57b9-cd4qh
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: worker-agents-cbfcc57b9
      uid: c0bc85ad-f67a-4d4b-a477-35550a99a96c
    resourceVersion: "29905157"
    uid: 46762d1e-fec2-434c-a03f-463eb797aa55
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - worker-agents
            topologyKey: topology.kubernetes.io/zone
          weight: 100
    containers:
    - env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POSTGRES_USER
        value: sla_user
      - name: POSTGRES_PASSWORD
        value: neural_hive_sla_2024
      - name: POSTGRES_DB
        value: sla_management
      - name: POSTGRES_HOST
        value: postgres-sla.neural-hive-data.svc.cluster.local
      envFrom:
      - configMapRef:
          name: worker-agents
      image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:3a3e661
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 5
      name: worker-agents
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 1500m
          memory: 1536Mi
        requests:
          cpu: 400m
          memory: 640Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      startupProbe:
        failureThreshold: 20
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-dslvm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    imagePullSecrets:
    - name: ghcr-secret
    - name: ghcr-secret-fixed
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsGroup: 1001
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: worker-agents
    serviceAccountName: worker-agents
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/name: worker-agents
      maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-dslvm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:07:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:06:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f29a76803509c4900e8c3a932fd0159d3e5fb306c22da47681fae13e48ace76d
      image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:3a3e661
      imageID: ghcr.io/albinojimy/neural-hive-mind/worker-agents@sha256:41eca7054e7ff40d27dac1412ca99be4f77d3a773db78e33c1a81647da463211
      lastState: {}
      name: worker-agents
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-13T21:07:06Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.78
    podIPs:
    - ip: 10.244.1.78
    qosClass: Burstable
    startTime: "2026-02-13T21:06:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
    creationTimestamp: "2026-01-27T16:30:04Z"
    generateName: alertmanager-neural-hive-prometheus-kub-alertmanager-
    labels:
      alertmanager: neural-hive-prometheus-kub-alertmanager
      app.kubernetes.io/instance: neural-hive-prometheus-kub-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.30.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: alertmanager-neural-hive-prometheus-kub-alertmanager-6fc8599c46
      statefulset.kubernetes.io/pod-name: alertmanager-neural-hive-prometheus-kub-alertmanager-0
    name: alertmanager-neural-hive-prometheus-kub-alertmanager-0
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-neural-hive-prometheus-kub-alertmanager
      uid: 25852239-9890-47ce-972b-d29c3cd914ab
    resourceVersion: "26435304"
    uid: 067b2b7e-adb8-4aeb-8403-2d677db8acd0
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - alertmanager
              - key: alertmanager
                operator: In
                values:
                - neural-hive-prometheus-kub-alertmanager
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://neural-hive-prometheus-kub-alertmanager.observability:9093
      - --web.route-prefix=/
      - --cluster.label=observability/neural-hive-prometheus-kub-alertmanager
      - --cluster.peer=alertmanager-neural-hive-prometheus-kub-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.30.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 100m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-neural-hive-prometheus-kub-alertmanager-db
        subPath: alertmanager-db
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
        name: cluster-tls-config
        readOnly: true
        subPath: cluster-tls-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xcsxh
        readOnly: true
    - args:
      - --listen-address=:8080
      - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xcsxh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-neural-hive-prometheus-kub-alertmanager-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
      - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-init
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /etc/alertmanager/config_out
        name: config-out
      - mountPath: /etc/alertmanager/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xcsxh
        readOnly: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: neural-hive-prometheus-kub-alertmanager
    serviceAccountName: neural-hive-prometheus-kub-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: alertmanager-neural-hive-prometheus-kub-alertmanager-db
      persistentVolumeClaim:
        claimName: alertmanager-neural-hive-prometheus-kub-alertmanager-db-alertmanager-neural-hive-prometheus-kub-alertmanager-0
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-neural-hive-prometheus-kub-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-neural-hive-prometheus-kub-alertmanager-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - name: web-config
      secret:
        defaultMode: 420
        secretName: alertmanager-neural-hive-prometheus-kub-alertmanager-web-config
    - name: cluster-tls-config
      secret:
        defaultMode: 420
        secretName: alertmanager-neural-hive-prometheus-kub-alertmanager-cluster-tls-config
    - name: kube-api-access-xcsxh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:02:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:02:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0b8b70767a55bd385e3ba0c9a69a33b7d3939e0cf81a0f5162e8771c62b8e51d
      image: quay.io/prometheus/alertmanager:v0.30.0
      imageID: quay.io/prometheus/alertmanager@sha256:abb750ac7b63116761c16dd481ae92496fbe04721686c0920f0fa4d0728cd4a6
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-27T16:30:07Z"
    - containerID: containerd://c35b6dd141e0cd453c1df3300c9ad4e400db42770801a60ef123a5bf10f54609
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-27T16:30:08Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    initContainerStatuses:
    - containerID: containerd://243a054d09edfbac52729bbfe8062232b8da2916b9c3d96171837dacabb3b044
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://243a054d09edfbac52729bbfe8062232b8da2916b9c3d96171837dacabb3b044
          exitCode: 0
          finishedAt: "2026-01-27T16:30:06Z"
          reason: Completed
          startedAt: "2026-01-27T16:30:06Z"
    phase: Running
    podIP: 10.244.2.226
    podIPs:
    - ip: 10.244.2.226
    qosClass: Burstable
    startTime: "2026-01-27T16:30:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 8513c6dec898b2da52aae0a2d13648bfac3d3b2b459d63734e38aa1503dae05c
      prometheus.io/port: http-metrics
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-27T16:30:00Z"
    generateName: loki-
    labels:
      app: loki
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: loki-84997d8d68
      name: loki
      release: loki
      statefulset.kubernetes.io/pod-name: loki-0
    name: loki-0
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: loki
      uid: 9a7b0fd0-ef08-42ed-8560-44df8c5fc544
    resourceVersion: "26435453"
    uid: f7c7ffbc-7508-48d4-b1d4-ebf1d439fd27
  spec:
    affinity: {}
    containers:
    - args:
      - -config.file=/etc/loki/loki.yaml
      image: grafana/loki:2.6.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      - containerPort: 9095
        name: grpc
        protocol: TCP
      - containerPort: 7946
        name: memberlist-port
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 512Mi
        requests:
          cpu: 50m
          memory: 256Mi
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7kvqs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: loki-0
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: loki
    serviceAccountName: loki
    subdomain: loki-headless
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-loki-0
    - emptyDir: {}
      name: tmp
    - name: config
      secret:
        defaultMode: 420
        secretName: loki
    - name: kube-api-access-7kvqs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:02Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:30:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://52433b7d24d9e542d0b119eb4c4cfdc0eff8318e2dc25f190b46939f00340cc6
      image: docker.io/grafana/loki:2.6.1
      imageID: docker.io/grafana/loki@sha256:1ee60f980950b00e505bd564b40f720132a0653b110e993043bb5940673d060a
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-27T16:30:02Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.225
    podIPs:
    - ip: 10.244.2.225
    qosClass: Burstable
    startTime: "2026-01-27T16:30:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0f49fcd7a8fab642f9644e0a4d67b9f2bf9ce3e2cbf1f2ebfa7a301dbd59a7e0
    creationTimestamp: "2025-12-29T10:22:09Z"
    generateName: loki-promtail-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: 664cbbf7cf
      pod-template-generation: "1"
    name: loki-promtail-9rf4x
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: 37aa15c7-b45d-4b08-bd82-35dd8a7546b9
    resourceVersion: "26435426"
    uid: 40ed72bd-cabf-4747-8c84-0198f53bc110
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-45k2w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-45k2w
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:22:14Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:22:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:03:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:22:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ac64068f8defb83be24be31b6921351724bcb2cad1d704a95bad224aa37d8b6a
      image: docker.io/grafana/promtail:3.5.1
      imageID: docker.io/grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-29T10:22:13Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.230
    podIPs:
    - ip: 10.244.2.230
    qosClass: Burstable
    startTime: "2025-12-29T10:22:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0f49fcd7a8fab642f9644e0a4d67b9f2bf9ce3e2cbf1f2ebfa7a301dbd59a7e0
    creationTimestamp: "2026-02-09T23:43:50Z"
    generateName: loki-promtail-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: 664cbbf7cf
      pod-template-generation: "1"
    name: loki-promtail-kvr48
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: 37aa15c7-b45d-4b08-bd82-35dd8a7546b9
    resourceVersion: "29938488"
    uid: a5f32180-a800-440f-a94e-19342566dd43
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-b6f7g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-b6f7g
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:03:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T22:47:42Z"
      message: 'containers with unready status: [promtail]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T22:47:42Z"
      message: 'containers with unready status: [promtail]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://827eb5672a041bfa5a693122d55cbcc46950351bbb3302fadc0865626dc2782f
      image: docker.io/grafana/promtail:3.5.1
      imageID: docker.io/grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState:
        terminated:
          containerID: containerd://827eb5672a041bfa5a693122d55cbcc46950351bbb3302fadc0865626dc2782f
          exitCode: 2
          finishedAt: "2026-02-13T23:03:03Z"
          reason: Error
          startedAt: "2026-02-13T23:03:01Z"
      name: promtail
      ready: false
      restartCount: 1140
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=promtail pod=loki-promtail-kvr48_observability(a5f32180-a800-440f-a94e-19342566dd43)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.250
    podIPs:
    - ip: 10.244.4.250
    qosClass: Burstable
    startTime: "2026-02-09T23:43:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0f49fcd7a8fab642f9644e0a4d67b9f2bf9ce3e2cbf1f2ebfa7a301dbd59a7e0
    creationTimestamp: "2025-12-29T10:22:09Z"
    generateName: loki-promtail-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: 664cbbf7cf
      pod-template-generation: "1"
    name: loki-promtail-rq5lb
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: 37aa15c7-b45d-4b08-bd82-35dd8a7546b9
    resourceVersion: "26438735"
    uid: d1ef009f-b9ec-4421-9833-937e81771a5a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5rz2q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-5rz2q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:22:13Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:22:10Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:22:10Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://abe5f7668ab8a91098cf528c0fae3df9f01a1e237bc8711ae438e35ba34c2b70
      image: docker.io/grafana/promtail:3.5.1
      imageID: docker.io/grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-29T10:22:13Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.4
    podIPs:
    - ip: 10.244.1.4
    qosClass: Burstable
    startTime: "2025-12-29T10:22:10Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0f49fcd7a8fab642f9644e0a4d67b9f2bf9ce3e2cbf1f2ebfa7a301dbd59a7e0
    creationTimestamp: "2026-01-14T09:38:37Z"
    generateName: loki-promtail-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: 664cbbf7cf
      pod-template-generation: "1"
    name: loki-promtail-st7cv
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: 37aa15c7-b45d-4b08-bd82-35dd8a7546b9
    resourceVersion: "29795441"
    uid: 32647e2a-24fc-41e3-907c-fd44d92eee5e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rhk5b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-rhk5b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:39:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:38:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:18:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T14:18:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:38:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bdd67ad50e6ca618d5738fe6d27a3ac757d330067038cbb35322c99c3993d6cf
      image: docker.io/grafana/promtail:3.5.1
      imageID: docker.io/grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-14T09:39:38Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.48
    podIPs:
    - ip: 10.244.0.48
    qosClass: Burstable
    startTime: "2026-01-14T09:38:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0f49fcd7a8fab642f9644e0a4d67b9f2bf9ce3e2cbf1f2ebfa7a301dbd59a7e0
    creationTimestamp: "2026-01-03T20:54:42Z"
    generateName: loki-promtail-
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/name: promtail
      controller-revision-hash: 664cbbf7cf
      pod-template-generation: "1"
    name: loki-promtail-v9kmr
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: loki-promtail
      uid: 37aa15c7-b45d-4b08-bd82-35dd8a7546b9
    resourceVersion: "26774936"
    uid: f1d68b68-88b2-475e-9035-a24920a317e9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    automountServiceAccountToken: true
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:3.5.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/lib/docker/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-f2nr9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: loki-promtail
    serviceAccountName: loki-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: loki-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-f2nr9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:55:07Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ceafe628bec8c52fd7c079e2c829165453c68d8044ffd71b4db3add68b69cca8
      image: docker.io/grafana/promtail:3.5.1
      imageID: docker.io/grafana/promtail@sha256:65bfae480b572854180c78f7dc567a4ad2ba548b0c410e696baa1e0fa6381299
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:55:07Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.3
    podIPs:
    - ip: 10.244.3.3
    qosClass: Burstable
    startTime: "2026-01-03T20:54:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "8888"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-12T13:55:06Z"
    generateName: neural-hive-jaeger-5fbd6fffcc-
    labels:
      app.kubernetes.io/component: all-in-one
      app.kubernetes.io/instance: neural-hive-jaeger
      app.kubernetes.io/name: jaeger
      pod-template-hash: 5fbd6fffcc
    name: neural-hive-jaeger-5fbd6fffcc-6v6tz
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: neural-hive-jaeger-5fbd6fffcc
      uid: 63680798-b8fe-48f4-b384-ae2390bf9615
    resourceVersion: "29939405"
    uid: f0a9652f-6e98-42f6-9e24-d429e53dffdf
  spec:
    containers:
    - env:
      - name: SPAN_STORAGE_TYPE
        value: memory
      - name: COLLECTOR_ZIPKIN_HOST_PORT
        value: :9411
      - name: JAEGER_DISABLED
        value: "false"
      - name: COLLECTOR_OTLP_ENABLED
        value: "true"
      image: jaegertracing/jaeger:2.13.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /status
          port: 13133
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 1
      name: jaeger
      ports:
      - containerPort: 5775
        protocol: UDP
      - containerPort: 6831
        protocol: UDP
      - containerPort: 6832
        protocol: UDP
      - containerPort: 5778
        protocol: TCP
      - containerPort: 16686
        protocol: TCP
      - containerPort: 16685
        protocol: TCP
      - containerPort: 9411
        protocol: TCP
      - containerPort: 4317
        protocol: TCP
      - containerPort: 4318
        protocol: TCP
      - containerPort: 13133
        protocol: TCP
      - containerPort: 8888
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /status
          port: 13133
          scheme: HTTP
        initialDelaySeconds: 1
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tckr7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsUser: 10001
    serviceAccount: neural-hive-jaeger
    serviceAccountName: neural-hive-jaeger
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-tckr7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:05Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T13:55:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:04Z"
      message: 'containers with unready status: [jaeger]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:04Z"
      message: 'containers with unready status: [jaeger]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T13:55:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8a5bef6334990b80a9defcf4e37a7589c7351feeae2d24d8830306e09e1700b2
      image: docker.io/jaegertracing/jaeger:2.13.0
      imageID: docker.io/jaegertracing/jaeger@sha256:d5e5fe1c2b2f663b07c28c6f7cc5a6e61aef076752dab8df8eeb6b0f5494c94b
      lastState:
        terminated:
          containerID: containerd://8a5bef6334990b80a9defcf4e37a7589c7351feeae2d24d8830306e09e1700b2
          exitCode: 0
          finishedAt: "2026-02-13T23:06:03Z"
          reason: Completed
          startedAt: "2026-02-13T23:06:02Z"
      name: jaeger
      ready: false
      restartCount: 380
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=jaeger pod=neural-hive-jaeger-5fbd6fffcc-6v6tz_observability(f0a9652f-6e98-42f6-9e24-d429e53dffdf)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.9
    podIPs:
    - ip: 10.244.4.9
    qosClass: BestEffort
    startTime: "2026-02-12T13:55:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-01-10T01:45:30+01:00"
    creationTimestamp: "2026-01-10T00:45:30Z"
    generateName: neural-hive-prometheus-kub-operator-5fc4d75679-
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      pod-template-hash: 5fc4d75679
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-operator-5fc4d75679-2sgh5
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: neural-hive-prometheus-kub-operator-5fc4d75679
      uid: 7bfdeaab-5acf-4b5c-b000-6cf32ffce4f1
    resourceVersion: "26774988"
    uid: bd243f41-3811-4510-8365-eac182460c07
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --kubelet-service=kube-system/neural-hive-prometheus-kub-kubelet
      - --kubelet-endpoints=true
      - --kubelet-endpointslice=false
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      - --config-reloader-cpu-request=0
      - --config-reloader-cpu-limit=0
      - --config-reloader-memory-request=0
      - --config-reloader-memory-limit=0
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
      - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      env:
      - name: GOGC
        value: "30"
      image: quay.io/prometheus-operator/prometheus-operator:v0.87.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ddvbg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: neural-hive-prometheus-kub-operator
    serviceAccountName: neural-hive-prometheus-kub-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: neural-hive-prometheus-kub-admission
    - name: kube-api-access-ddvbg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-10T00:45:37Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-10T00:45:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-10T00:45:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://51db37abe7eedf7dbf5dd0cff435dd870586e826482196ac1a7bf6f9b401f3fe
      image: quay.io/prometheus-operator/prometheus-operator:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:6dbbbbeca6d7b94aa30723bfaa55262e41b9f8c15304c484611c696503840aac
      lastState: {}
      name: kube-prometheus-stack
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-10T00:45:37Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.120
    podIPs:
    - ip: 10.244.3.120
    qosClass: Burstable
    startTime: "2026-01-10T00:45:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-12-29T10:18:46Z"
    generateName: neural-hive-prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 5f46476465
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: neural-hive-prometheus
    name: neural-hive-prometheus-prometheus-node-exporter-2bvj9
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: neural-hive-prometheus-prometheus-node-exporter
      uid: 39952527-5ea7-40b3-8c2e-4c91b41f9825
    resourceVersion: "26435333"
    uid: 10093917-3616-4508-bfbf-14e724739635
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: vmi2911681
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: neural-hive-prometheus-prometheus-node-exporter
    serviceAccountName: neural-hive-prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:18:47Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:18:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:02:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:02:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:18:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a5befe1bde76861e3dab1930ba2bf6f8eb72af9186ee97b69c86b16d76501507
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState:
        terminated:
          containerID: containerd://b169644a27f8b914a86e4829c18552194c0ab88d678397580caf5d564239bcab
          exitCode: 143
          finishedAt: "2026-01-15T13:33:09Z"
          reason: Error
          startedAt: "2025-12-29T10:18:47Z"
      name: node-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-01-15T13:33:13Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 84.247.138.35
    podIPs:
    - ip: 84.247.138.35
    qosClass: BestEffort
    startTime: "2025-12-29T10:18:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2026-01-14T09:39:23Z"
    generateName: neural-hive-prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 5f46476465
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: neural-hive-prometheus
    name: neural-hive-prometheus-prometheus-node-exporter-8rhnk
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: neural-hive-prometheus-prometheus-node-exporter
      uid: 39952527-5ea7-40b3-8c2e-4c91b41f9825
    resourceVersion: "29906578"
    uid: 29732bde-7708-4a80-95f1-2fe79fce9948
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: vmi2092350.contaboserver.net
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: neural-hive-prometheus-prometheus-node-exporter
    serviceAccountName: neural-hive-prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:39:49Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:10:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T21:10:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b7fe1ae219605abaa6ceda0affcc4cd88e7d3cb508112980e54292990657f8e8
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState:
        terminated:
          containerID: containerd://99aab6cf37a8e7faddaa26ce637180046f7d98e7d753df2e18823fba8fdc6ebe
          exitCode: 143
          finishedAt: "2026-02-13T17:47:18Z"
          reason: Error
          startedAt: "2026-02-13T17:17:27Z"
      name: node-exporter
      ready: true
      restartCount: 23
      started: true
      state:
        running:
          startedAt: "2026-02-13T17:47:22Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 37.60.241.150
    podIPs:
    - ip: 37.60.241.150
    qosClass: BestEffort
    startTime: "2026-01-14T09:39:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2025-12-29T10:18:46Z"
    generateName: neural-hive-prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 5f46476465
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: neural-hive-prometheus
    name: neural-hive-prometheus-prometheus-node-exporter-dglrj
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: neural-hive-prometheus-prometheus-node-exporter
      uid: 39952527-5ea7-40b3-8c2e-4c91b41f9825
    resourceVersion: "26438719"
    uid: d90262c7-3af9-4dc3-a9df-af1e68b9bd85
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: vmi2911680
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: neural-hive-prometheus-prometheus-node-exporter
    serviceAccountName: neural-hive-prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:18:47Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:18:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-29T10:18:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4cd6aedb9bb33777bb14aca78785c0e52be24820fe8ac7c3a1f7b4e892fcca72
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState:
        terminated:
          containerID: containerd://d6fa896c8809c9518c828f5a469068c1eada88dea6a7ed0a220da06a215c92d2
          exitCode: 143
          finishedAt: "2026-01-22T16:57:51Z"
          reason: Error
          startedAt: "2026-01-15T13:30:43Z"
      name: node-exporter
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:57:54Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 158.220.101.216
    podIPs:
    - ip: 158.220.101.216
    qosClass: BestEffort
    startTime: "2025-12-29T10:18:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2026-01-03T20:54:29Z"
    generateName: neural-hive-prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 5f46476465
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: neural-hive-prometheus
    name: neural-hive-prometheus-prometheus-node-exporter-hl9cn
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: neural-hive-prometheus-prometheus-node-exporter
      uid: 39952527-5ea7-40b3-8c2e-4c91b41f9825
    resourceVersion: "26775006"
    uid: fd7b47e5-04da-4bc8-bd83-a90ccfe32162
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: vmi3002938
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: neural-hive-prometheus-prometheus-node-exporter
    serviceAccountName: neural-hive-prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:38Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://58a9517f578b05c9b36b63a76c74c40732a3ac7acb5b5eac974716a96a1ab63b
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState:
        terminated:
          containerID: containerd://34627a6990137ce2d45ffb5564ff71c4f985877f1a555aefa74140fec858f14a
          exitCode: 143
          finishedAt: "2026-02-01T13:46:49Z"
          reason: Error
          startedAt: "2026-01-03T20:54:37Z"
      name: node-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2026-02-01T13:46:50Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 89.117.60.74
    podIPs:
    - ip: 89.117.60.74
    qosClass: BestEffort
    startTime: "2026-01-03T20:54:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    creationTimestamp: "2026-02-09T23:43:34Z"
    generateName: neural-hive-prometheus-prometheus-node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      controller-revision-hash: 5f46476465
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      pod-template-generation: "1"
      release: neural-hive-prometheus
    name: neural-hive-prometheus-prometheus-node-exporter-t945j
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: neural-hive-prometheus-prometheus-node-exporter
      uid: 39952527-5ea7-40b3-8c2e-4c91b41f9825
    resourceVersion: "29939194"
    uid: 5ee8ad20-16f7-4164-b823-5a4d255da1d9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --path.udev.data=/host/root/run/udev/data
      - --web.listen-address=[$(HOST_IP)]:9100
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.10.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: http-metrics
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: vmi3075398
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: neural-hive-prometheus-prometheus-node-exporter
    serviceAccountName: neural-hive-prometheus-prometheus-node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:26Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:25Z"
      message: 'containers with unready status: [node-exporter]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:05:25Z"
      message: 'containers with unready status: [node-exporter]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://97974fc06e98a167a86954c85b149c6131a0d2a40b895ff0a299e9a3bc868250
      image: quay.io/prometheus/node-exporter:v1.10.2
      imageID: quay.io/prometheus/node-exporter@sha256:337ff1d356b68d39cef853e8c6345de11ce7556bb34cda8bd205bcf2ed30b565
      lastState:
        terminated:
          containerID: containerd://97974fc06e98a167a86954c85b149c6131a0d2a40b895ff0a299e9a3bc868250
          exitCode: 143
          finishedAt: "2026-02-13T23:05:24Z"
          reason: Error
          startedAt: "2026-02-13T23:05:22Z"
      name: node-exporter
      ready: false
      restartCount: 1081
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=node-exporter pod=neural-hive-prometheus-prometheus-node-exporter-t945j_observability(5ee8ad20-16f7-4164-b823-5a4d255da1d9)
          reason: CrashLoopBackOff
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 144.91.115.90
    podIPs:
    - ip: 144.91.115.90
    qosClass: BestEffort
    startTime: "2026-02-09T23:43:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
      kubectl.kubernetes.io/restartedAt: "2026-02-12T14:55:23+01:00"
      neural.hive/metrics: enabled
      prometheus.io/path: /metrics
      prometheus.io/port: "8888"
      prometheus.io/scrape: "true"
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2026-02-12T13:55:26Z"
    generateName: otel-collector-neural-hive-otel-collector-64c9989c49-
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 64c9989c49
    name: otel-collector-neural-hive-otel-collector-64c9989c49-56jtb
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: otel-collector-neural-hive-otel-collector-64c9989c49
      uid: 0d27455a-43e2-4ce4-a884-14ebd776ef23
    resourceVersion: "29402594"
    uid: 9a4fc9f2-98a4-4e48-a0d3-c9e843f3e912
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - neural-hive-otel-collector
            topologyKey: kubernetes.io/hostname
          weight: 100
    containers:
    - command:
      - /otelcol-contrib
      - --config=/conf/otelcol.yaml
      env:
      - name: MY_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: MY_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: GOMEMLIMIT
        value: 2GiB
      - name: GOGC
        value: "80"
      image: otel/opentelemetry-collector-contrib:0.89.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: health
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 5
      name: neural-hive-otel-collector
      ports:
      - containerPort: 13133
        name: health
        protocol: TCP
      - containerPort: 14250
        name: jaeger-grpc
        protocol: TCP
      - containerPort: 14268
        name: jaeger-http
        protocol: TCP
      - containerPort: 8888
        name: metrics
        protocol: TCP
      - containerPort: 4317
        name: otlp-grpc
        protocol: TCP
      - containerPort: 4318
        name: otlp-http
        protocol: TCP
      - containerPort: 1777
        name: pprof
        protocol: TCP
      - containerPort: 9411
        name: zipkin
        protocol: TCP
      - containerPort: 55679
        name: zpages
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: health
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /conf
        name: config
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8tg7z
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: otel-collector-neural-hive-otel-collector
    serviceAccountName: otel-collector-neural-hive-otel-collector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: otelcol.yaml
          path: otelcol.yaml
        name: otel-collector-neural-hive-otel-collector-config
      name: config
    - name: kube-api-access-8tg7z
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T13:55:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T13:55:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T13:55:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T13:55:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-12T13:55:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3b01e6d69afc1e8832dcd9ebc2391d79b3a7b4e156cc9571258aac12cb32a884
      image: docker.io/otel/opentelemetry-collector-contrib:0.89.0
      imageID: docker.io/otel/opentelemetry-collector-contrib@sha256:995f17004231249d2c7d775849bf100b209508448919ca2077f8023ba49e3a95
      lastState: {}
      name: neural-hive-otel-collector
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-12T13:55:34Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.146
    podIPs:
    - ip: 10.244.3.146
    qosClass: Burstable
    startTime: "2026-02-12T13:55:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
    creationTimestamp: "2026-01-22T16:58:04Z"
    generateName: prometheus-neural-hive-prometheus-kub-prometheus-
    labels:
      app.kubernetes.io/instance: neural-hive-prometheus-kub-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 3.8.1
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: prometheus-neural-hive-prometheus-kub-prometheus-7c9b4d888
      operator.prometheus.io/name: neural-hive-prometheus-kub-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: neural-hive-prometheus-kub-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-neural-hive-prometheus-kub-prometheus-0
    name: prometheus-neural-hive-prometheus-kub-prometheus-0
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-neural-hive-prometheus-kub-prometheus
      uid: d0693d67-ae99-4d2b-aa89-fac4b6ec7ada
    resourceVersion: "26438786"
    uid: 4c6202f5-f7ed-40db-b523-287db3601718
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - neural-hive-prometheus-kub-prometheus
            topologyKey: kubernetes.io/hostname
          weight: 100
    automountServiceAccountToken: true
    containers:
    - args:
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --web.enable-lifecycle
      - --web.external-url=http://neural-hive-prometheus-kub-prometheus.observability:9090
      - --web.route-prefix=/
      - --storage.tsdb.retention.time=7d
      - --storage.tsdb.retention.size=10GB
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.wal-compression
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v3.8.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: 500m
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 1Gi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-neural-hive-prometheus-kub-prometheus-db
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
        readOnly: true
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
        readOnly: true
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
        readOnly: true
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-442jb
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
      - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
      - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-442jb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-neural-hive-prometheus-kub-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8081
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
      - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
      - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8081
        name: reloader-init
        protocol: TCP
      resources: {}
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
      - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-442jb
        readOnly: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    serviceAccount: neural-hive-prometheus-kub-prometheus
    serviceAccountName: neural-hive-prometheus-kub-prometheus
    shareProcessNamespace: false
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: prometheus-neural-hive-prometheus-kub-prometheus-db
      persistentVolumeClaim:
        claimName: prometheus-neural-hive-prometheus-kub-prometheus-db-prometheus-neural-hive-prometheus-kub-prometheus-0
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-neural-hive-prometheus-kub-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-neural-hive-prometheus-kub-prometheus-tls-assets-0
    - emptyDir:
        medium: Memory
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
        optional: true
      name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
    - configMap:
        defaultMode: 420
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
        optional: true
      name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
    - configMap:
        defaultMode: 420
        name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
        optional: true
      name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-neural-hive-prometheus-kub-prometheus-web-config
    - name: kube-api-access-442jb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f428c3ad5a5106332b8bd873c92a06cfcef824b5b38263dbfc07d2b737709192
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:11Z"
    - containerID: containerd://9c133a10671a26d91b177db38430582b64c95b16734967fabeff8e5cefa8c278
      image: quay.io/prometheus/prometheus:v3.8.1
      imageID: quay.io/prometheus/prometheus@sha256:2b6f734e372c1b4717008f7d0a0152316aedd4d13ae17ef1e3268dbfaf68041b
      lastState: {}
      name: prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:11Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    initContainerStatuses:
    - containerID: containerd://32582222161c1d04c89c1cff669be8e5306f40845212e269079a74ce02c7e638
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:257ce3543e8f45c059dab488bcf897d8cc598cabd78afba12f41d6dbe36e22d2
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://32582222161c1d04c89c1cff669be8e5306f40845212e269079a74ce02c7e638
          exitCode: 0
          finishedAt: "2026-01-22T16:58:09Z"
          reason: Completed
          startedAt: "2026-01-22T16:58:09Z"
    phase: Running
    podIP: 10.244.1.32
    podIPs:
    - ip: 10.244.1.32
    qosClass: Burstable
    startTime: "2026-01-22T16:58:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-01-11T14:15:29+01:00"
    creationTimestamp: "2026-01-11T13:24:13Z"
    generateName: portainer-d6fcbdd48-
    labels:
      app.kubernetes.io/instance: portainer
      app.kubernetes.io/name: portainer
      pod-template-hash: d6fcbdd48
    name: portainer-d6fcbdd48-vx9vc
    namespace: portainer
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: portainer-d6fcbdd48
      uid: e0fcaab3-650a-45da-b03d-9e3d7e06c09c
    resourceVersion: "26775173"
    uid: 0913b561-4a22-4343-8797-76363eabdf2d
  spec:
    containers:
    - args:
      - --tunnel-port=30776
      image: portainer/portainer-ce:latest
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /
          port: 9443
          scheme: HTTPS
        initialDelaySeconds: 45
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      name: portainer
      ports:
      - containerPort: 9000
        name: http
        protocol: TCP
      - containerPort: 9443
        name: https
        protocol: TCP
      - containerPort: 8000
        name: tcp-edge
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /
          port: 9443
          scheme: HTTPS
        initialDelaySeconds: 45
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xtlbb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: portainer-sa-clusteradmin
    serviceAccountName: portainer-sa-clusteradmin
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: portainer
    - name: kube-api-access-xtlbb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-11T13:24:24Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-11T13:24:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-11T13:24:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ca25d293a59afec57e49ec47acfaba7d51ba1374eb50f45caf3c42a1fe184391
      image: docker.io/portainer/portainer-ce:latest
      imageID: docker.io/portainer/portainer-ce@sha256:4786931dc7c588ff1c242696fe1eb3f7f9c5dafb136b6c713aff7745dd5bd407
      lastState: {}
      name: portainer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-11T13:24:23Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.134
    podIPs:
    - ip: 10.244.3.134
    qosClass: BestEffort
    startTime: "2026-01-11T13:24:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:33+01:00"
      sidecar.istio.io/inject: "false"
    creationTimestamp: "2026-02-11T11:10:12Z"
    generateName: redis-66b84474ff-
    labels:
      app: redis
      pod-template-hash: 66b84474ff
      version: v1
    name: redis-66b84474ff-cb6cc
    namespace: redis-cluster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-66b84474ff
      uid: 7dbcfb33-43e3-44c6-8b07-6837b82868a9
    resourceVersion: "28939229"
    uid: 47ade23a-496c-43e5-b94c-68d768fb6fb4
  spec:
    containers:
    - command:
      - redis-server
      - /usr/local/etc/redis/redis.conf
      image: redis:7-alpine
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 6379
        timeoutSeconds: 1
      name: redis
      ports:
      - containerPort: 6379
        name: redis
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - redis-cli
          - ping
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 250m
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/local/etc/redis
        name: config
      - mountPath: /data
        name: data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-ps548
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: redis-config
      name: config
    - emptyDir: {}
      name: data
    - name: kube-api-access-ps548
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:10:13Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:10:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:10:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:10:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T11:10:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bb94998bbf18467c04358c57d2f38de5deb44ef87daab80f1f56c346d29154fb
      image: docker.io/library/redis:7-alpine
      imageID: docker.io/library/redis@sha256:ee64a64eaab618d88051c3ade8f6352d11531fcf79d9a4818b9b183d8c1d18ba
      lastState: {}
      name: redis
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-11T11:10:13Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.106
    podIPs:
    - ip: 10.244.3.106
    qosClass: Burstable
    startTime: "2026-02-11T11:10:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cert-manager.io/inject-ca-from: redis-operator/serving-cert
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:37+01:00"
    creationTimestamp: "2026-01-22T16:38:25Z"
    generateName: redis-operator-5b7d784c5-
    labels:
      name: redis-operator
      pod-template-hash: 5b7d784c5
    name: redis-operator-5b7d784c5-th97z
    namespace: redis-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-operator-5b7d784c5
      uid: b8bdeac5-82e1-40ea-b95d-a92ea4b94984
    resourceVersion: "26438742"
    uid: 82bc24e8-c14e-473c-a8af-72d2cc4f9c3a
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - --leader-elect
      - --metrics-bind-address=:8080
      - --kube-client-timeout=60s
      command:
      - /operator
      - manager
      env:
      - name: OPERATOR_IMAGE
        value: quay.io/opstree/redis-operator:v0.22.2
      - name: ENABLE_WEBHOOKS
        value: "false"
      - name: FEATURE_GATES
        value: GenerateConfigInInitContainer=false
      image: quay.io/opstree/redis-operator:v0.22.2
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: probe
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: redis-operator
      ports:
      - containerPort: 8081
        name: probe
        protocol: TCP
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: probe
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 500Mi
        requests:
          cpu: 500m
          memory: 500Mi
      securityContext: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cq9hx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: redis-operator
    serviceAccountName: redis-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-cq9hx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:10Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c58c9b628583a637bfe3ef72f27d585752c9d1033e277450ff835445fd469fc1
      image: quay.io/opstree/redis-operator:v0.22.2
      imageID: quay.io/opstree/redis-operator@sha256:464ac61a6eb44f009e076011f8fbac67248ddfb8d284b3727dfc962b8c54748f
      lastState: {}
      name: redis-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:09Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.27
    podIPs:
    - ip: 10.244.1.27
    qosClass: Guaranteed
    startTime: "2026-01-22T16:58:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2026-01-13T14:48:10+01:00"
    creationTimestamp: "2026-01-13T14:37:10Z"
    generateName: docker-registry-6b5c9fffcb-
    labels:
      app.kubernetes.io/name: docker-registry
      pod-template-hash: 6b5c9fffcb
    name: docker-registry-6b5c9fffcb-6gmzz
    namespace: registry
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: docker-registry-6b5c9fffcb
      uid: 2a26bc08-8178-4fc2-9bd6-145a4b15f138
    resourceVersion: "28851338"
    uid: 846c1284-15c6-44ce-adad-86471c346f42
  spec:
    containers:
    - env:
      - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
        value: /var/lib/registry
      - name: REGISTRY_HTTP_ADDR
        value: :5000
      - name: REGISTRY_STORAGE_DELETE_ENABLED
        value: "true"
      image: registry:2.8.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 5000
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: registry
      ports:
      - containerPort: 5000
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 5000
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/registry
        name: registry-data
      - mountPath: /etc/docker/registry
        name: registry-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-km7zv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    nodeSelector:
      node-role.kubernetes.io/control-plane: ""
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: registry-data
      persistentVolumeClaim:
        claimName: docker-registry-data
    - configMap:
        defaultMode: 420
        name: docker-registry-config
      name: registry-config
    - name: kube-api-access-km7zv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:30:45Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:30:20Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T05:42:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-11T05:42:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-14T09:30:20Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e3ebfd099f788700c49720596de8cf2f5ba9489e1e73c0317e91dc190f2d3b86
      image: docker.io/library/registry:2.8.3
      imageID: docker.io/library/registry@sha256:a3d8aaa63ed8681a604f1dea0aa03f100d5895b6a58ace528858a7b332415373
      lastState:
        terminated:
          containerID: containerd://c2570d50f6edcfb49f822f3bfadf0f6beadd92bb1aa72cae599e7d536c2c97bc
          exitCode: 2
          finishedAt: "2026-02-11T05:42:35Z"
          reason: Error
          startedAt: "2026-02-11T05:40:46Z"
      name: registry
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2026-02-11T05:42:37Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.47
    podIPs:
    - ip: 10.244.0.47
    qosClass: Burstable
    startTime: "2026-01-14T09:30:20Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
    creationTimestamp: "2026-01-22T16:38:23Z"
    generateName: temporal-admintools-78c898f5f9-
    labels:
      app.kubernetes.io/component: admintools
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 78c898f5f9
    name: temporal-admintools-78c898f5f9-n9drh
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: temporal-admintools-78c898f5f9
      uid: 9fff258a-7374-470d-91aa-77c76f47692f
    resourceVersion: "25557808"
    uid: 4336da85-29ed-48b5-b955-8acac7a1fd55
  spec:
    containers:
    - env:
      - name: TEMPORAL_CLI_ADDRESS
        value: temporal-frontend:7233
      - name: TEMPORAL_ADDRESS
        value: temporal-frontend:7233
      image: temporalio/admin-tools:1.29.1-tctl-1.18.4-cli-1.5.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - ls
          - /
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: admin-tools
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-xwh5l
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-xwh5l
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:39:19Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:39:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:39:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5d557d11da59e1b5e32a90166981b4c81282375b06df1f3c6c8dd7412c1e7538
      image: docker.io/temporalio/admin-tools:1.29.1-tctl-1.18.4-cli-1.5.0
      imageID: docker.io/temporalio/admin-tools@sha256:a3a52e6ca122fe47cc69dc1039ab82ac59fbf6d8b84003eb2557b5b1340e82e0
      lastState: {}
      name: admin-tools
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:39:19Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.69
    podIPs:
    - ip: 10.244.3.69
    qosClass: Burstable
    startTime: "2026-01-22T16:38:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
      prometheus.io/job: temporal-frontend
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-27T16:23:33Z"
    generateName: temporal-frontend-bc8b49c8d-
    labels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: bc8b49c8d
    name: temporal-frontend-bc8b49c8d-g7m5k
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: temporal-frontend-bc8b49c8d
      uid: d9e32e1a-6719-42cc-8a7c-a271584cc0d7
    resourceVersion: "25557773"
    uid: 67ef5039-bf31-445c-ab63-1e12ad5d4ec5
  spec:
    containers:
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: SERVICES
        value: frontend
      - name: TEMPORAL_STORE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: temporal-default-store
      - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: temporal-visibility-store
      image: temporalio/server:1.29.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 150
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: rpc
        timeoutSeconds: 1
      name: temporal-frontend
      ports:
      - containerPort: 7233
        name: rpc
        protocol: TCP
      - containerPort: 6933
        name: membership
        protocol: TCP
      - containerPort: 7243
        name: http
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/temporal/config/config_template.yaml
        name: config
        subPath: config_template.yaml
      - mountPath: /etc/temporal/dynamic_config
        name: dynamic-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-klbfp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: temporal-config
      name: config
    - configMap:
        defaultMode: 420
        items:
        - key: dynamic_config.yaml
          path: dynamic_config.yaml
        name: temporal-dynamic-config
      name: dynamic-config
    - name: kube-api-access-klbfp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:23:35Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:23:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:23:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:23:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-27T16:23:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c1f8c8f1d138227f0d4b77279fb559b129254365c2310a2b9d2f1e837a0f18d1
      image: docker.io/temporalio/server:1.29.1
      imageID: docker.io/temporalio/server@sha256:c1e3326b2ce1472fdc07bae53812ed97624a03f41f72b7da0e6792dd9464ceb0
      lastState: {}
      name: temporal-frontend
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-27T16:23:35Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.99
    podIPs:
    - ip: 10.244.3.99
    qosClass: Burstable
    startTime: "2026-01-27T16:23:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
      prometheus.io/job: temporal-history
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-22T16:38:22Z"
    generateName: temporal-history-86c95ccf9b-
    labels:
      app.kubernetes.io/component: history
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 86c95ccf9b
    name: temporal-history-86c95ccf9b-nqg4g
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: temporal-history-86c95ccf9b
      uid: 2139c077-7f01-44a6-b209-783178c546c8
    resourceVersion: "25557671"
    uid: 459bd320-3119-4eac-9c7b-dce9bd4c7c3e
  spec:
    containers:
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: SERVICES
        value: history
      - name: TEMPORAL_STORE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: temporal-default-store
      - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: temporal-visibility-store
      image: temporalio/server:1.29.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 150
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: rpc
        timeoutSeconds: 1
      name: temporal-history
      ports:
      - containerPort: 7234
        name: rpc
        protocol: TCP
      - containerPort: 6934
        name: membership
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/temporal/config/config_template.yaml
        name: config
        subPath: config_template.yaml
      - mountPath: /etc/temporal/dynamic_config
        name: dynamic-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tjcqs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: temporal-config
      name: config
    - configMap:
        defaultMode: 420
        items:
        - key: dynamic_config.yaml
          path: dynamic_config.yaml
        name: temporal-dynamic-config
      name: dynamic-config
    - name: kube-api-access-tjcqs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:57Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:59:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:59:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9c3af184308001c55a16f16cdf8a3c7437fbfcc01543ba0e1c48aabb16e240a7
      image: docker.io/temporalio/server:1.29.1
      imageID: docker.io/temporalio/server@sha256:c1e3326b2ce1472fdc07bae53812ed97624a03f41f72b7da0e6792dd9464ceb0
      lastState:
        terminated:
          containerID: containerd://cd097c30cf2b756e4eb9d655edb9b59f68e17b95e5231245e6c3d341a4756ec9
          exitCode: 1
          finishedAt: "2026-01-22T16:54:49Z"
          reason: Error
          startedAt: "2026-01-22T16:54:49Z"
      name: temporal-history
      ready: true
      restartCount: 9
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:59:53Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.66
    podIPs:
    - ip: 10.244.3.66
    qosClass: Burstable
    startTime: "2026-01-22T16:38:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
    creationTimestamp: "2026-01-22T16:38:21Z"
    generateName: temporal-matching-5fb946b9c5-
    labels:
      app.kubernetes.io/component: matching
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 5fb946b9c5
    name: temporal-matching-5fb946b9c5-rsdjz
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: temporal-matching-5fb946b9c5
      uid: a3fd5474-597f-45d9-9d98-7b5cacf7aa46
    resourceVersion: "25557847"
    uid: 79089d76-d204-440a-b697-00f67427a292
  spec:
    containers:
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: SERVICES
        value: matching
      - name: TEMPORAL_STORE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: temporal-default-store
      - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: temporal-visibility-store
      image: temporalio/server:1.29.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 150
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: rpc
        timeoutSeconds: 1
      name: temporal-matching
      ports:
      - containerPort: 7235
        name: rpc
        protocol: TCP
      - containerPort: 6935
        name: membership
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/temporal/config/config_template.yaml
        name: config
        subPath: config_template.yaml
      - mountPath: /etc/temporal/dynamic_config
        name: dynamic-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c67sv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: temporal-config
      name: config
    - configMap:
        defaultMode: 420
        items:
        - key: dynamic_config.yaml
          path: dynamic_config.yaml
        name: temporal-dynamic-config
      name: dynamic-config
    - name: kube-api-access-c67sv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:37Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:59:46Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:59:46Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://85b83871a28b5880b2cb986dcc46d7cc178e4d4b1dc04c580457e9859bfe8a56
      image: docker.io/temporalio/server:1.29.1
      imageID: docker.io/temporalio/server@sha256:c1e3326b2ce1472fdc07bae53812ed97624a03f41f72b7da0e6792dd9464ceb0
      lastState:
        terminated:
          containerID: containerd://79875431b80db83270ef7595cea35c224c14b1fc3b03ebce136341b1f7c75a17
          exitCode: 1
          finishedAt: "2026-01-22T16:54:38Z"
          reason: Error
          startedAt: "2026-01-22T16:54:37Z"
      name: temporal-matching
      ready: true
      restartCount: 9
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:59:46Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.63
    podIPs:
    - ip: 10.244.3.63
    qosClass: Burstable
    startTime: "2026-01-22T16:38:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2026-01-22T16:58:00Z"
    generateName: temporal-postgresql-
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: temporal-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 18.1.0
      apps.kubernetes.io/pod-index: "0"
      controller-revision-hash: temporal-postgresql-64485f6d9
      helm.sh/chart: postgresql-18.1.13
      statefulset.kubernetes.io/pod-name: temporal-postgresql-0
    name: temporal-postgresql-0
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: temporal-postgresql
      uid: dfca87f5-8976-4cec-8a96-a43edfe2f80b
    resourceVersion: "26438845"
    uid: b5facf3a-34d0-4e81-96a1-df8a2af8bae0
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: primary
                app.kubernetes.io/instance: temporal-postgresql
                app.kubernetes.io/name: postgresql
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: false
    containers:
    - env:
      - name: BITNAMI_DEBUG
        value: "false"
      - name: POSTGRESQL_PORT_NUMBER
        value: "5432"
      - name: POSTGRESQL_VOLUME_DIR
        value: /bitnami/postgresql
      - name: OPENSSL_FIPS
        value: "yes"
      - name: PGDATA
        value: /bitnami/postgresql/data
      - name: POSTGRES_USER
        value: temporal
      - name: POSTGRES_PASSWORD_FILE
        value: /opt/bitnami/postgresql/secrets/password
      - name: POSTGRES_POSTGRES_PASSWORD_FILE
        value: /opt/bitnami/postgresql/secrets/postgres-password
      - name: POSTGRES_DATABASE
        value: temporal
      - name: POSTGRESQL_ENABLE_LDAP
        value: "no"
      - name: POSTGRESQL_ENABLE_TLS
        value: "no"
      - name: POSTGRESQL_LOG_HOSTNAME
        value: "false"
      - name: POSTGRESQL_LOG_CONNECTIONS
        value: "false"
      - name: POSTGRESQL_LOG_DISCONNECTIONS
        value: "false"
      - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
        value: "off"
      - name: POSTGRESQL_CLIENT_MIN_MESSAGES
        value: error
      - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
        value: pgaudit
      image: registry-1.docker.io/bitnami/postgresql:latest
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "temporal" -d "dbname=temporal" -h 127.0.0.1 -p 5432
        failureThreshold: 6
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: tcp-postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - -e
          - |
            exec pg_isready -U "temporal" -d "dbname=temporal" -h 127.0.0.1 -p 5432
            [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
        failureThreshold: 6
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
        seLinuxOptions: {}
        seccompProfile:
          type: RuntimeDefault
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: empty-dir
        subPath: tmp-dir
      - mountPath: /opt/bitnami/postgresql/conf
        name: empty-dir
        subPath: app-conf-dir
      - mountPath: /opt/bitnami/postgresql/tmp
        name: empty-dir
        subPath: app-tmp-dir
      - mountPath: /opt/bitnami/postgresql/secrets/
        name: postgresql-password
      - mountPath: /dev/shm
        name: dshm
      - mountPath: /bitnami/postgresql
        name: data
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: temporal-postgresql-0
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
      fsGroupChangePolicy: Always
    serviceAccount: temporal-postgresql
    serviceAccountName: temporal-postgresql
    subdomain: temporal-postgresql-hl
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-temporal-postgresql-0
    - emptyDir: {}
      name: empty-dir
    - name: postgresql-password
      secret:
        defaultMode: 420
        secretName: temporal-postgresql
    - emptyDir:
        medium: Memory
      name: dshm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:01Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-04T05:15:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7fd718a27d322106c0ef73b6c7be4959a962c48e51e9e39f2d3db764dd676513
      image: registry-1.docker.io/bitnami/postgresql:latest
      imageID: registry-1.docker.io/bitnami/postgresql@sha256:9dc1c74aff55144030fb0307666dfa97aefadf5914749696a348fb8f36eb4d7d
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:58:01Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.22
    podIPs:
    - ip: 10.244.1.22
    qosClass: Burstable
    startTime: "2026-01-22T16:58:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
    creationTimestamp: "2026-01-22T16:38:22Z"
    generateName: temporal-web-bfbd4b558-
    labels:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: bfbd4b558
    name: temporal-web-bfbd4b558-x6cl8
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: temporal-web-bfbd4b558
      uid: ddfedcbb-ff8a-487e-965d-d14af99224c0
    resourceVersion: "25557852"
    uid: 45edd87b-2f97-4aeb-9737-41b682808173
  spec:
    containers:
    - env:
      - name: TEMPORAL_ADDRESS
        value: temporal-frontend.temporal.svc:7233
      image: temporalio/ui:2.42.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: http
        timeoutSeconds: 1
      name: temporal-web
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-6hnvp
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-6hnvp
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:56Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:38:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a6d0573c0fcfa02cb3a28ff0c628560ea0aa739ba158b063946f133943d8f3e8
      image: docker.io/temporalio/ui:2.42.1
      imageID: docker.io/temporalio/ui@sha256:64e8d7d3cb24072373034186cd2be41338b75c02cdaba8fec68de9d76ed3b60a
      lastState: {}
      name: temporal-web
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:38:55Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.65
    podIPs:
    - ip: 10.244.3.65
    qosClass: Burstable
    startTime: "2026-01-22T16:38:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
      kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
      prometheus.io/job: temporal-worker
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-22T16:44:39Z"
    generateName: temporal-worker-77df5b85f6-
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 77df5b85f6
    name: temporal-worker-77df5b85f6-54g96
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: temporal-worker-77df5b85f6
      uid: 70e60f59-ca2e-4c39-8c6e-8645b4023edc
    resourceVersion: "22401876"
    uid: 75d70bb3-8ad0-4f34-b88f-7fecc8273c06
  spec:
    containers:
    - env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: SERVICES
        value: worker
      - name: TEMPORAL_STORE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: temporal-default-store
      - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
        valueFrom:
          secretKeyRef:
            key: password
            name: temporal-visibility-store
      image: temporalio/server:1.29.1
      imagePullPolicy: IfNotPresent
      name: temporal-worker
      ports:
      - containerPort: 6939
        name: membership
        protocol: TCP
      - containerPort: 9090
        name: metrics
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/temporal/config/config_template.yaml
        name: config
        subPath: config_template.yaml
      - mountPath: /etc/temporal/dynamic_config
        name: dynamic-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zw8mg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: temporal-config
      name: config
    - configMap:
        defaultMode: 420
        items:
        - key: dynamic_config.yaml
          path: dynamic_config.yaml
        name: temporal-dynamic-config
      name: dynamic-config
    - name: kube-api-access-zw8mg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:09Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:59:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:59:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:58:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://26162976ab2e83d3c8e83ded3639243bbb951b3baa94aded95aabbb43a4bbbbf
      image: docker.io/temporalio/server:1.29.1
      imageID: docker.io/temporalio/server@sha256:c1e3326b2ce1472fdc07bae53812ed97624a03f41f72b7da0e6792dd9464ceb0
      lastState:
        terminated:
          containerID: containerd://5852795dc8a2b88f54937f9b5174075b67616618ca39192254726feee1aee22a
          exitCode: 1
          finishedAt: "2026-01-22T16:59:22Z"
          reason: Error
          startedAt: "2026-01-22T16:58:21Z"
      name: temporal-worker
      ready: true
      restartCount: 3
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:59:48Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.28
    podIPs:
    - ip: 10.244.1.28
    qosClass: Burstable
    startTime: "2026-01-22T16:58:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 80e7cc85a7dc2cac261932d8c7c6611e6dbfdb0879e63cce3bd67d7c6b340946
      checksum/secrets: 184cec76b907d0ce15c3e1d11f7c5e20c066057a49968de890f5bba4de52f63a
    creationTimestamp: "2026-01-22T16:44:38Z"
    generateName: minio-5c5464c875-
    labels:
      app: minio
      pod-template-hash: 5c5464c875
      release: minio
    name: minio-5c5464c875-4zbzx
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: minio-5c5464c875
      uid: 185ced4e-92e7-432f-a848-d4c2a13c726b
    resourceVersion: "22400330"
    uid: afb60a72-0db5-4e3b-b79b-679915b6ce16
  spec:
    containers:
    - command:
      - /bin/sh
      - -ce
      - /usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/ --address
        :9000 --console-address :9001
      env:
      - name: MINIO_ROOT_USER
        valueFrom:
          secretKeyRef:
            key: rootUser
            name: minio
      - name: MINIO_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            key: rootPassword
            name: minio
      - name: MINIO_PROMETHEUS_AUTH_TYPE
        value: public
      image: quay.io/minio/minio:RELEASE.2024-12-18T13-15-44Z
      imagePullPolicy: IfNotPresent
      name: minio
      ports:
      - containerPort: 9000
        name: http
        protocol: TCP
      - containerPort: 9001
        name: http-console
        protocol: TCP
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 256Mi
      securityContext:
        readOnlyRootFilesystem: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/credentials
        name: minio-user
        readOnly: true
      - mountPath: /export
        name: export
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rcfdg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      fsGroupChangePolicy: OnRootMismatch
      runAsGroup: 1000
      runAsUser: 1000
    serviceAccount: minio-sa
    serviceAccountName: minio-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: export
      persistentVolumeClaim:
        claimName: minio
    - name: minio-user
      secret:
        defaultMode: 420
        secretName: minio
    - name: kube-api-access-rcfdg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:58Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-22T16:57:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9a1f99c29b86b495e22a8af2bb4baf654c35f24aae4272273f5f76f48b1d21c8
      image: quay.io/minio/minio:RELEASE.2024-12-18T13-15-44Z
      imageID: quay.io/minio/minio@sha256:1dce27c494a16bae114774f1cec295493f3613142713130c2d22dd5696be6ad3
      lastState: {}
      name: minio
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-22T16:57:57Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.21
    podIPs:
    - ip: 10.244.1.21
    qosClass: Burstable
    startTime: "2026-01-22T16:57:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
      prometheus.io/path: /metrics
      prometheus.io/port: "8085"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-01T12:54:58Z"
    generateName: node-agent-
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      controller-revision-hash: 7554fb4d4
      helm.sh/chart: velero-11.2.0
      name: node-agent
      pod-template-generation: "1"
      role: node-agent
    name: node-agent-2lq5c
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-agent
      uid: 42f5c37b-8e4a-4f71-8512-5cf1f66e6212
    resourceVersion: "22400104"
    uid: a7f222d9-ca93-4ae8-9bb7-3a7bd747bedc
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911680
    automountServiceAccountToken: true
    containers:
    - args:
      - node-agent
      - server
      command:
      - /velero
      env:
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      - name: AWS_SHARED_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /credentials/cloud
      - name: AZURE_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: ALIBABA_CLOUD_CREDENTIALS_FILE
        value: /credentials/cloud
      image: velero/velero:v1.17.1
      imagePullPolicy: IfNotPresent
      name: node-agent
      ports:
      - containerPort: 8085
        name: http-monitoring
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /credentials
        name: cloud-credentials
      - mountPath: /host_pods
        mountPropagation: HostToContainer
        name: host-pods
      - mountPath: /host_plugins
        mountPropagation: HostToContainer
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jgjzj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911680
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: velero-server
    serviceAccountName: velero-server
    terminationGracePeriodSeconds: 3600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: cloud-credentials
      secret:
        defaultMode: 420
        secretName: velero
    - hostPath:
        path: /var/lib/kubelet/pods
        type: ""
      name: host-pods
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: ""
      name: host-plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-jgjzj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T12:55:31Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T12:55:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T12:55:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T12:55:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-01T12:55:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f7883e3cff4de26e4af870e814bab300b9042d8285448d844c804fd1a50fa3dd
      image: docker.io/velero/velero:v1.17.1
      imageID: docker.io/velero/velero@sha256:58f551f0d89a9ef1919771bdc7475c08bc1795a50a51a57b5f14316d68fc3baa
      lastState: {}
      name: node-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-01T12:55:31Z"
    hostIP: 158.220.101.216
    hostIPs:
    - ip: 158.220.101.216
    phase: Running
    podIP: 10.244.1.97
    podIPs:
    - ip: 10.244.1.97
    qosClass: Burstable
    startTime: "2026-01-01T12:55:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
      prometheus.io/path: /metrics
      prometheus.io/port: "8085"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-03T20:54:42Z"
    generateName: node-agent-
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      controller-revision-hash: 7554fb4d4
      helm.sh/chart: velero-11.2.0
      name: node-agent
      pod-template-generation: "1"
      role: node-agent
    name: node-agent-7cftr
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-agent
      uid: 42f5c37b-8e4a-4f71-8512-5cf1f66e6212
    resourceVersion: "25557612"
    uid: 1e02776b-a3e6-45ad-a674-7c4e03054547
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3002938
    automountServiceAccountToken: true
    containers:
    - args:
      - node-agent
      - server
      command:
      - /velero
      env:
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      - name: AWS_SHARED_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /credentials/cloud
      - name: AZURE_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: ALIBABA_CLOUD_CREDENTIALS_FILE
        value: /credentials/cloud
      image: velero/velero:v1.17.1
      imagePullPolicy: IfNotPresent
      name: node-agent
      ports:
      - containerPort: 8085
        name: http-monitoring
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /credentials
        name: cloud-credentials
      - mountPath: /host_pods
        mountPropagation: HostToContainer
        name: host-pods
      - mountPath: /host_plugins
        mountPropagation: HostToContainer
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-crcz7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: velero-server
    serviceAccountName: velero-server
    terminationGracePeriodSeconds: 3600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: cloud-credentials
      secret:
        defaultMode: 420
        secretName: velero
    - hostPath:
        path: /var/lib/kubelet/pods
        type: ""
      name: host-pods
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: ""
      name: host-plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-crcz7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:55:01Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:55:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:55:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-03T20:54:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://44e23d14f9217294f115c7496c851b1ffca7118b443eef37948c56f0d35d5ac9
      image: docker.io/velero/velero:v1.17.1
      imageID: docker.io/velero/velero@sha256:58f551f0d89a9ef1919771bdc7475c08bc1795a50a51a57b5f14316d68fc3baa
      lastState: {}
      name: node-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-03T20:55:00Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    phase: Running
    podIP: 10.244.3.2
    podIPs:
    - ip: 10.244.3.2
    qosClass: Burstable
    startTime: "2026-01-03T20:54:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
      prometheus.io/path: /metrics
      prometheus.io/port: "8085"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-09T23:43:50Z"
    generateName: node-agent-
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      controller-revision-hash: 7554fb4d4
      helm.sh/chart: velero-11.2.0
      name: node-agent
      pod-template-generation: "1"
      role: node-agent
    name: node-agent-8bnkg
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-agent
      uid: 42f5c37b-8e4a-4f71-8512-5cf1f66e6212
    resourceVersion: "29939649"
    uid: db9fa694-8692-466b-983f-46b6cf777fdd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi3075398
    automountServiceAccountToken: true
    containers:
    - args:
      - node-agent
      - server
      command:
      - /velero
      env:
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      - name: AWS_SHARED_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /credentials/cloud
      - name: AZURE_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: ALIBABA_CLOUD_CREDENTIALS_FILE
        value: /credentials/cloud
      image: velero/velero:v1.17.1
      imagePullPolicy: IfNotPresent
      name: node-agent
      ports:
      - containerPort: 8085
        name: http-monitoring
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /credentials
        name: cloud-credentials
      - mountPath: /host_pods
        mountPropagation: HostToContainer
        name: host-pods
      - mountPath: /host_plugins
        mountPropagation: HostToContainer
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7rrdb
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi3075398
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: velero-server
    serviceAccountName: velero-server
    terminationGracePeriodSeconds: 3600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: cloud-credentials
      secret:
        defaultMode: 420
        secretName: velero
    - hostPath:
        path: /var/lib/kubelet/pods
        type: ""
      name: host-pods
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: ""
      name: host-plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-7rrdb
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-13T23:06:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-09T23:43:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6144dd8dacf2f64ef835b1368b4fc4dd3e784fbe2fa82b943da5911934845cfc
      image: docker.io/velero/velero:v1.17.1
      imageID: docker.io/velero/velero@sha256:58f551f0d89a9ef1919771bdc7475c08bc1795a50a51a57b5f14316d68fc3baa
      lastState:
        terminated:
          containerID: containerd://c17423f948ef305499f1ec1c0fb48c543bb4ce0c6dbc08c7d691489998e08e2b
          exitCode: 0
          finishedAt: "2026-02-13T23:06:03Z"
          reason: Completed
          startedAt: "2026-02-13T23:03:37Z"
      name: node-agent
      ready: true
      restartCount: 1020
      started: true
      state:
        running:
          startedAt: "2026-02-13T23:06:50Z"
    hostIP: 144.91.115.90
    hostIPs:
    - ip: 144.91.115.90
    phase: Running
    podIP: 10.244.4.8
    podIPs:
    - ip: 10.244.4.8
    qosClass: Burstable
    startTime: "2026-02-09T23:43:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
      prometheus.io/path: /metrics
      prometheus.io/port: "8085"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-10T21:25:04Z"
    generateName: node-agent-
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      controller-revision-hash: 7554fb4d4
      helm.sh/chart: velero-11.2.0
      name: node-agent
      pod-template-generation: "1"
      role: node-agent
    name: node-agent-99vrq
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-agent
      uid: 42f5c37b-8e4a-4f71-8512-5cf1f66e6212
    resourceVersion: "23972087"
    uid: ec8cafea-072b-457a-b7cf-dc871a984b1b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2911681
    automountServiceAccountToken: true
    containers:
    - args:
      - node-agent
      - server
      command:
      - /velero
      env:
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      - name: AWS_SHARED_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /credentials/cloud
      - name: AZURE_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: ALIBABA_CLOUD_CREDENTIALS_FILE
        value: /credentials/cloud
      image: velero/velero:v1.17.1
      imagePullPolicy: IfNotPresent
      name: node-agent
      ports:
      - containerPort: 8085
        name: http-monitoring
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /credentials
        name: cloud-credentials
      - mountPath: /host_pods
        mountPropagation: HostToContainer
        name: host-pods
      - mountPath: /host_plugins
        mountPropagation: HostToContainer
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zm5nq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2911681
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: velero-server
    serviceAccountName: velero-server
    terminationGracePeriodSeconds: 3600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: cloud-credentials
      secret:
        defaultMode: 420
        secretName: velero
    - hostPath:
        path: /var/lib/kubelet/pods
        type: ""
      name: host-pods
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: ""
      name: host-plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-zm5nq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-12-10T21:26:33Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-12-10T21:26:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-12-10T21:26:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-12-10T21:26:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-12-10T21:26:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://de7d1f7e72252137cf2e439f4c9f6e28e912da5d52f05f5dbe9800c355823713
      image: docker.io/velero/velero:v1.17.1
      imageID: docker.io/velero/velero@sha256:58f551f0d89a9ef1919771bdc7475c08bc1795a50a51a57b5f14316d68fc3baa
      lastState: {}
      name: node-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-12-10T21:26:32Z"
    hostIP: 84.247.138.35
    hostIPs:
    - ip: 84.247.138.35
    phase: Running
    podIP: 10.244.2.51
    podIPs:
    - ip: 10.244.2.51
    qosClass: Burstable
    startTime: "2025-12-10T21:26:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
      prometheus.io/path: /metrics
      prometheus.io/port: "8085"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-02-07T23:49:24Z"
    generateName: node-agent-
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      controller-revision-hash: 7554fb4d4
      helm.sh/chart: velero-11.2.0
      name: node-agent
      pod-template-generation: "1"
      role: node-agent
    name: node-agent-fb67q
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-agent
      uid: 42f5c37b-8e4a-4f71-8512-5cf1f66e6212
    resourceVersion: "27680869"
    uid: 4182eb9e-33e7-47c0-b5b8-9b7cab2a19d5
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - vmi2092350.contaboserver.net
    automountServiceAccountToken: true
    containers:
    - args:
      - node-agent
      - server
      command:
      - /velero
      env:
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      - name: AWS_SHARED_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /credentials/cloud
      - name: AZURE_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: ALIBABA_CLOUD_CREDENTIALS_FILE
        value: /credentials/cloud
      image: velero/velero:v1.17.1
      imagePullPolicy: IfNotPresent
      name: node-agent
      ports:
      - containerPort: 8085
        name: http-monitoring
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /credentials
        name: cloud-credentials
      - mountPath: /host_pods
        mountPropagation: HostToContainer
        name: host-pods
      - mountPath: /host_plugins
        mountPropagation: HostToContainer
        name: host-plugins
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pc8x2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: vmi2092350.contaboserver.net
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 0
    serviceAccount: velero-server
    serviceAccountName: velero-server
    terminationGracePeriodSeconds: 3600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: cloud-credentials
      secret:
        defaultMode: 420
        secretName: velero
    - hostPath:
        path: /var/lib/kubelet/pods
        type: ""
      name: host-pods
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: ""
      name: host-plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-pc8x2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:50:57Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:49:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:50:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:50:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-02-07T23:49:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bde444c802bc67f9dca4539ac6d30834764a84a8a154c729300ae4df2a61e994
      image: docker.io/velero/velero:v1.17.1
      imageID: docker.io/velero/velero@sha256:58f551f0d89a9ef1919771bdc7475c08bc1795a50a51a57b5f14316d68fc3baa
      lastState: {}
      name: node-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-02-07T23:50:56Z"
    hostIP: 37.60.241.150
    hostIPs:
    - ip: 37.60.241.150
    phase: Running
    podIP: 10.244.0.53
    podIPs:
    - ip: 10.244.0.53
    qosClass: Burstable
    startTime: "2026-02-07T23:49:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
      prometheus.io/path: /metrics
      prometheus.io/port: "8085"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-15T13:29:43Z"
    generateName: velero-8445566485-
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      app.kubernetes.io/version: 1.17.1
      helm.sh/chart: velero-11.2.0
      name: velero
      pod-template-hash: "8445566485"
    name: velero-8445566485-lwclg
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: velero-8445566485
      uid: d8a8b611-dbfd-4861-8099-4e497d7cd368
    resourceVersion: "26775055"
    uid: 17be24cd-9621-493b-9a2e-54434ef0721c
  spec:
    automountServiceAccountToken: true
    containers:
    - args:
      - server
      - --uploader-type=kopia
      - --repo-maintenance-job-configmap=velero-repo-maintenance
      command:
      - /velero
      env:
      - name: VELERO_SCRATCH_DIR
        value: /scratch
      - name: VELERO_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: LD_LIBRARY_PATH
        value: /plugins
      - name: AWS_SHARED_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /credentials/cloud
      - name: AZURE_CREDENTIALS_FILE
        value: /credentials/cloud
      - name: ALIBABA_CLOUD_CREDENTIALS_FILE
        value: /credentials/cloud
      image: velero/velero:v1.17.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: http-monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: velero
      ports:
      - containerPort: 8085
        name: http-monitoring
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: http-monitoring
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /plugins
        name: plugins
      - mountPath: /credentials
        name: cloud-credentials
      - mountPath: /scratch
        name: scratch
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n6h2c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: velero/velero-plugin-for-aws:v1.10.0
      imagePullPolicy: IfNotPresent
      name: velero-plugin-for-aws
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /target
        name: plugins
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-n6h2c
        readOnly: true
    nodeName: vmi3002938
    preemptionPolicy: PreemptLowerPriority
    priority: 10000
    priorityClassName: normal-priority
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: velero-server
    serviceAccountName: velero-server
    terminationGracePeriodSeconds: 3600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: cloud-credentials
      secret:
        defaultMode: 420
        secretName: velero
    - emptyDir: {}
      name: plugins
    - emptyDir: {}
      name: scratch
    - name: kube-api-access-n6h2c
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:50Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2026-02-05T05:50:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2026-01-15T13:29:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://085ca0b4fcf2bad33cd59cffd3e9816ec2d4df9e972f5ae3a84dcc7b85a67792
      image: docker.io/velero/velero:v1.17.1
      imageID: docker.io/velero/velero@sha256:58f551f0d89a9ef1919771bdc7475c08bc1795a50a51a57b5f14316d68fc3baa
      lastState: {}
      name: velero
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2026-01-15T13:29:50Z"
    hostIP: 89.117.60.74
    hostIPs:
    - ip: 89.117.60.74
    initContainerStatuses:
    - containerID: containerd://8be53ac402020f476da648c6f46b8976236fea697e7a1fdc2b46cf6152159fa5
      image: docker.io/velero/velero-plugin-for-aws:v1.10.0
      imageID: docker.io/velero/velero-plugin-for-aws@sha256:80f84902d9f644aecce043908867a6629cc89b7ddb8c6633ad0e8bbe2c364e7e
      lastState: {}
      name: velero-plugin-for-aws
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://8be53ac402020f476da648c6f46b8976236fea697e7a1fdc2b46cf6152159fa5
          exitCode: 0
          finishedAt: "2026-01-15T13:29:49Z"
          reason: Completed
          startedAt: "2026-01-15T13:29:49Z"
    phase: Running
    podIP: 10.244.3.247
    podIPs:
    - ip: 10.244.3.247
    qosClass: Burstable
    startTime: "2026-01-15T13:29:43Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"approval-service","component":"governance"},"name":"approval-service","namespace":"approval"},"spec":{"ports":[{"name":"http","port":8080,"protocol":"TCP","targetPort":8080},{"name":"metrics","port":8000,"protocol":"TCP","targetPort":8000}],"selector":{"app":"approval-service"},"type":"ClusterIP"}}
    creationTimestamp: "2026-01-19T13:58:56Z"
    labels:
      app: approval-service
      component: governance
    name: approval-service
    namespace: approval
    resourceVersion: "21400294"
    uid: b5d78b55-9ae8-47dd-b970-afb9d2a82eca
  spec:
    clusterIP: 10.106.73.226
    clusterIPs:
    - 10.106.73.226
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: metrics
      port: 8000
      protocol: TCP
      targetPort: 8000
    selector:
      app: approval-service
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:48Z"
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "8235607"
    uid: f623c72e-92d7-40f0-8349-399b2408d5af
  spec:
    clusterIP: 10.110.26.158
    clusterIPs:
    - 10.110.26.158
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-prometheus-servicemonitor
      port: 9402
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cert-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:48Z"
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "8235604"
    uid: 81d87634-e075-498b-a140-3e8825076ede
  spec:
    clusterIP: 10.106.105.117
    clusterIPs:
    - 10.106.105.117
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: cainjector
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:48Z"
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "8235610"
    uid: 2d2b5b72-a44f-42b3-a6a5-1238832e6250
  spec:
    clusterIP: 10.99.53.217
    clusterIPs:
    - 10.99.53.217
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    - name: metrics
      port: 9402
      protocol: TCP
      targetPort: http-metrics
    selector:
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/name: webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      clickhouse.altinity.com/ready: "yes"
    creationTimestamp: "2026-01-14T21:33:22Z"
    labels:
      clickhouse.altinity.com/Service: host
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chi: neural-hive-clickhouse
      clickhouse.altinity.com/cluster: cluster
      clickhouse.altinity.com/namespace: clickhouse-operator
      clickhouse.altinity.com/object-version: 31de030f12dbfe7b57491a38a5c08bbd94cb47ae
      clickhouse.altinity.com/replica: "0"
      clickhouse.altinity.com/shard: "0"
    name: chi-neural-hive-clickhouse-cluster-0-0
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: clickhouse.altinity.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: ClickHouseInstallation
      name: neural-hive-clickhouse
      uid: 2d65c783-5365-4760-9540-577f8c11aefc
    resourceVersion: "19891139"
    uid: 000b3929-31eb-49d1-b052-7f120a58ea11
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp
      port: 9000
      protocol: TCP
      targetPort: 9000
    - name: http
      port: 8123
      protocol: TCP
      targetPort: 8123
    - name: interserver
      port: 9009
      protocol: TCP
      targetPort: 9009
    publishNotReadyAddresses: true
    selector:
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chi: neural-hive-clickhouse
      clickhouse.altinity.com/cluster: cluster
      clickhouse.altinity.com/namespace: clickhouse-operator
      clickhouse.altinity.com/replica: "0"
      clickhouse.altinity.com/shard: "0"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2026-01-14T21:34:34Z"
    labels:
      clickhouse.altinity.com/Service: chi
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chi: neural-hive-clickhouse
      clickhouse.altinity.com/namespace: clickhouse-operator
      clickhouse.altinity.com/object-version: 04f48295867b95901c5a1651560537b1b9a600e5
    name: clickhouse-neural-hive-clickhouse
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: clickhouse.altinity.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: ClickHouseInstallation
      name: neural-hive-clickhouse
      uid: 2d65c783-5365-4760-9540-577f8c11aefc
    resourceVersion: "19891146"
    uid: a7c29468-74d5-4d02-b152-60c339ca5f98
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8123
      protocol: TCP
      targetPort: http
    - name: tcp
      port: 9000
      protocol: TCP
      targetPort: tcp
    selector:
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chi: neural-hive-clickhouse
      clickhouse.altinity.com/namespace: clickhouse-operator
      clickhouse.altinity.com/ready: "yes"
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: clickhouse-operator
      meta.helm.sh/release-namespace: clickhouse-operator
    creationTimestamp: "2025-11-20T13:27:41Z"
    labels:
      app.kubernetes.io/instance: clickhouse-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/version: 0.25.5
      helm.sh/chart: altinity-clickhouse-operator-0.25.5
    name: clickhouse-operator-altinity-clickhouse-operator-metrics
    namespace: clickhouse-operator
    resourceVersion: "3732147"
    uid: 45f369bb-b996-4f8b-a1b5-63c540888ed3
  spec:
    clusterIP: 10.100.108.197
    clusterIPs:
    - 10.100.108.197
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: ch-metrics
      port: 8888
      protocol: TCP
      targetPort: 8888
    - name: op-metrics
      port: 9999
      protocol: TCP
      targetPort: 9999
    selector:
      app.kubernetes.io/instance: clickhouse-operator
      app.kubernetes.io/name: altinity-clickhouse-operator
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"clickhouse","namespace":"clickhouse"},"spec":{"ports":[{"name":"http","port":8123,"targetPort":8123},{"name":"native","port":9000,"targetPort":9000}],"selector":{"app":"clickhouse"}}}
    creationTimestamp: "2026-01-01T21:45:45Z"
    name: clickhouse
    namespace: clickhouse
    resourceVersion: "15623545"
    uid: 5c762ab1-650c-4692-bd2d-c29a5145694e
  spec:
    clusterIP: 10.105.91.43
    clusterIPs:
    - 10.105.91.43
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8123
      protocol: TCP
      targetPort: 8123
    - name: native
      port: 9000
      protocol: TCP
      targetPort: 9000
    selector:
      app: clickhouse
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-10-29T10:54:36Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "233"
    uid: 9c79c17a-5468-4371-99b3-3c1f74e30d29
  spec:
    clusterIP: 10.96.0.1
    clusterIPs:
    - 10.96.0.1
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-12-09T20:55:04Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "8312386"
    uid: ddb90841-4d02-438e-b3ee-81b9db56d65d
  spec:
    clusterIP: 10.97.13.110
    clusterIPs:
    - 10.97.13.110
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      port: 80
      protocol: TCP
      targetPort: http
    - appProtocol: https
      name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-12-09T20:55:04Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
    name: ingress-nginx-controller-admission
    namespace: ingress-nginx
    resourceVersion: "8052068"
    uid: 0039ec4a-450f-46fd-89ca-bd4acb2824eb
  spec:
    clusterIP: 10.107.196.210
    clusterIPs:
    - 10.107.196.210
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: https
      name: https-webhook
      port: 443
      protocol: TCP
      targetPort: webhook
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-12-09T20:55:04Z"
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
    name: ingress-nginx-controller-metrics
    namespace: ingress-nginx
    resourceVersion: "8052060"
    uid: a6bf94ba-55ae-459d-8756-61a49610e31c
  spec:
    clusterIP: 10.99.73.252
    clusterIPs:
    - 10.99.73.252
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 10254
      protocol: TCP
      targetPort: metrics
    selector:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      strimzi.io/discovery: |-
        [ {
          "port" : 9092,
          "tls" : false,
          "protocol" : "kafka",
          "auth" : "none"
        }, {
          "port" : 9093,
          "tls" : true,
          "protocol" : "kafka",
          "auth" : "none"
        } ]
    creationTimestamp: "2025-11-19T07:56:03Z"
    labels:
      app.kubernetes.io/instance: neural-hive-kafka
      app.kubernetes.io/managed-by: strimzi-cluster-operator
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: strimzi-neural-hive-kafka
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/component-type: kafka
      strimzi.io/discovery: "true"
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-kafka
    name: neural-hive-kafka-kafka-bootstrap
    namespace: kafka
    ownerReferences:
    - apiVersion: kafka.strimzi.io/v1beta2
      blockOwnerDeletion: false
      controller: false
      kind: Kafka
      name: neural-hive-kafka
      uid: 2e1371f6-7e80-48f3-aac2-828e0f1c91f4
    resourceVersion: "3451829"
    uid: 2d0e4abb-c139-4626-ac68-9de659c01d41
  spec:
    clusterIP: 10.99.11.200
    clusterIPs:
    - 10.99.11.200
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-replication
      port: 9091
      protocol: TCP
      targetPort: tcp-replication
    - name: tcp-clients
      port: 9092
      protocol: TCP
      targetPort: tcp-clients
    - name: tcp-clientstls
      port: 9093
      protocol: TCP
      targetPort: tcp-clientstls
    selector:
      strimzi.io/broker-role: "true"
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-11-19T07:56:03Z"
    labels:
      app.kubernetes.io/instance: neural-hive-kafka
      app.kubernetes.io/managed-by: strimzi-cluster-operator
      app.kubernetes.io/name: kafka
      app.kubernetes.io/part-of: strimzi-neural-hive-kafka
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/component-type: kafka
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-kafka
    name: neural-hive-kafka-kafka-brokers
    namespace: kafka
    ownerReferences:
    - apiVersion: kafka.strimzi.io/v1beta2
      blockOwnerDeletion: false
      controller: false
      kind: Kafka
      name: neural-hive-kafka
      uid: 2e1371f6-7e80-48f3-aac2-828e0f1c91f4
    resourceVersion: "3451830"
    uid: 1d2fe185-ed93-4410-ba97-926dd7ebc3cd
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-ctrlplane
      port: 9090
      protocol: TCP
      targetPort: tcp-ctrlplane
    - name: tcp-replication
      port: 9091
      protocol: TCP
      targetPort: tcp-replication
    - name: tcp-kafkaagent
      port: 8443
      protocol: TCP
      targetPort: tcp-kafkaagent
    - name: tcp-clients
      port: 9092
      protocol: TCP
      targetPort: tcp-clients
    - name: tcp-clientstls
      port: 9093
      protocol: TCP
      targetPort: tcp-clientstls
    publishNotReadyAddresses: true
    selector:
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-kafka
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"service.beta.kubernetes.io/tls-enabled":"true"},"labels":{"app":"apicurio-registry","component":"kafka"},"name":"schema-registry","namespace":"kafka"},"spec":{"ports":[{"name":"https","port":8081,"protocol":"TCP","targetPort":8443},{"name":"health","port":8090,"protocol":"TCP","targetPort":8090}],"publishNotReadyAddresses":true,"selector":{"app":"apicurio-registry"},"type":"ClusterIP"}}
      service.beta.kubernetes.io/tls-enabled: "true"
    creationTimestamp: "2025-10-30T09:00:56Z"
    labels:
      app: apicurio-registry
      component: kafka
    name: schema-registry
    namespace: kafka
    resourceVersion: "29823608"
    uid: dd2495a0-7e25-444e-834a-40f30d5c8d09
  spec:
    clusterIP: 10.105.33.30
    clusterIPs:
    - 10.105.33.30
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 8081
      protocol: TCP
      targetPort: 8443
    - name: health
      port: 8090
      protocol: TCP
      targetPort: 8090
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    publishNotReadyAddresses: true
    selector:
      app: apicurio-registry
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"neural-hive.io/service-type":"auth"},"labels":{"app.kubernetes.io/component":"auth","app.kubernetes.io/instance":"keycloak","app.kubernetes.io/managed-by":"helm","app.kubernetes.io/name":"keycloak","app.kubernetes.io/part-of":"neural-hive-mind","app.kubernetes.io/version":"22.0.5","component":"metrics","helm.sh/chart":"keycloak-0.1.0","neural-hive.io/component":"auth","neural-hive.io/data-owner":"team-security","neural-hive.io/layer":"security","neural-hive.io/sla-tier":"platinum","neural.hive/metrics":"enabled"},"name":"keycloak","namespace":"keycloak"},"spec":{"ports":[{"name":"http","port":8080,"protocol":"TCP","targetPort":8080},{"name":"https","port":8443,"protocol":"TCP","targetPort":8443},{"name":"management","port":9000,"protocol":"TCP","targetPort":9000},{"name":"metrics","port":9001,"protocol":"TCP","targetPort":9000}],"selector":{"app.kubernetes.io/instance":"keycloak","app.kubernetes.io/name":"keycloak"},"type":"ClusterIP"}}
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
      neural-hive.io/service-type: auth
    creationTimestamp: "2026-01-17T10:46:36Z"
    labels:
      app.kubernetes.io/component: auth
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/managed-by: helm
      app.kubernetes.io/name: keycloak
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 22.0.5
      component: metrics
      helm.sh/chart: keycloak-0.1.0
      neural-hive.io/component: auth
      neural-hive.io/data-owner: team-security
      neural-hive.io/layer: security
      neural-hive.io/sla-tier: platinum
      neural.hive/metrics: enabled
    name: keycloak
    namespace: keycloak
    resourceVersion: "20856691"
    uid: 69b1f73d-fdbd-47b5-9255-4f92dbe4a664
  spec:
    clusterIP: 10.107.204.19
    clusterIPs:
    - 10.107.204.19
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: https
      port: 8443
      protocol: TCP
      targetPort: 8443
    - name: management
      port: 9000
      protocol: TCP
      targetPort: 9000
    - name: metrics
      port: 9001
      protocol: TCP
      targetPort: 9000
    selector:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"headless","app.kubernetes.io/instance":"keycloak","app.kubernetes.io/managed-by":"helm","app.kubernetes.io/name":"keycloak","app.kubernetes.io/part-of":"neural-hive-mind","app.kubernetes.io/version":"22.0.5","helm.sh/chart":"keycloak-0.1.0","neural-hive.io/component":"auth","neural-hive.io/data-owner":"team-security","neural-hive.io/layer":"security","neural-hive.io/sla-tier":"platinum"},"name":"keycloak-headless","namespace":"keycloak"},"spec":{"clusterIP":"None","ports":[{"name":"http","port":8080,"protocol":"TCP","targetPort":"http"}],"selector":{"app.kubernetes.io/instance":"keycloak","app.kubernetes.io/name":"keycloak"}}}
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T10:46:36Z"
    labels:
      app.kubernetes.io/component: headless
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/managed-by: helm
      app.kubernetes.io/name: keycloak
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 22.0.5
      helm.sh/chart: keycloak-0.1.0
      neural-hive.io/component: auth
      neural-hive.io/data-owner: team-security
      neural-hive.io/layer: security
      neural-hive.io/sla-tier: platinum
    name: keycloak-headless
    namespace: keycloak
    resourceVersion: "20856697"
    uid: 124b215c-5758-48e3-b434-aee4491c62b9
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"9000","prometheus.io/scrape":"true"},"labels":{"app.kubernetes.io/component":"metrics","app.kubernetes.io/instance":"keycloak","app.kubernetes.io/managed-by":"helm","app.kubernetes.io/name":"keycloak","app.kubernetes.io/part-of":"neural-hive-mind","app.kubernetes.io/version":"22.0.5","helm.sh/chart":"keycloak-0.1.0","neural-hive.io/component":"auth","neural-hive.io/data-owner":"team-security","neural-hive.io/layer":"security","neural-hive.io/sla-tier":"platinum"},"name":"keycloak-metrics","namespace":"keycloak"},"spec":{"ports":[{"name":"metrics","port":9000,"protocol":"TCP","targetPort":"management"}],"selector":{"app.kubernetes.io/instance":"keycloak","app.kubernetes.io/name":"keycloak"},"type":"ClusterIP"}}
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
      prometheus.io/path: /metrics
      prometheus.io/port: "9000"
      prometheus.io/scrape: "true"
    creationTimestamp: "2026-01-17T10:46:36Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/managed-by: helm
      app.kubernetes.io/name: keycloak
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 22.0.5
      helm.sh/chart: keycloak-0.1.0
      neural-hive.io/component: auth
      neural-hive.io/data-owner: team-security
      neural-hive.io/layer: security
      neural-hive.io/sla-tier: platinum
    name: keycloak-metrics
    namespace: keycloak
    resourceVersion: "20856703"
    uid: 0b407dd9-bf11-47b7-a7fb-e0cf0eb38e3e
  spec:
    clusterIP: 10.111.99.242
    clusterIPs:
    - 10.111.99.242
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: metrics
      port: 9000
      protocol: TCP
      targetPort: management
    selector:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"primary","app.kubernetes.io/instance":"keycloak","app.kubernetes.io/managed-by":"Helm","app.kubernetes.io/name":"postgresql","app.kubernetes.io/version":"15.4.0","helm.sh/chart":"postgresql-12.12.10"},"name":"keycloak-postgresql","namespace":"keycloak"},"spec":{"ports":[{"name":"tcp-postgresql","nodePort":null,"port":5432,"targetPort":"tcp-postgresql"}],"selector":{"app.kubernetes.io/component":"primary","app.kubernetes.io/instance":"keycloak","app.kubernetes.io/name":"postgresql"},"sessionAffinity":"None","type":"ClusterIP"}}
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T10:46:36Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 15.4.0
      helm.sh/chart: postgresql-12.12.10
    name: keycloak-postgresql
    namespace: keycloak
    resourceVersion: "20856689"
    uid: a537c946-d8b0-4a41-974f-8c66f43ddf90
  spec:
    clusterIP: 10.110.244.87
    clusterIPs:
    - 10.110.244.87
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"service.alpha.kubernetes.io/tolerate-unready-endpoints":"true"},"labels":{"app.kubernetes.io/component":"primary","app.kubernetes.io/instance":"keycloak","app.kubernetes.io/managed-by":"Helm","app.kubernetes.io/name":"postgresql","app.kubernetes.io/version":"15.4.0","helm.sh/chart":"postgresql-12.12.10"},"name":"keycloak-postgresql-hl","namespace":"keycloak"},"spec":{"clusterIP":"None","ports":[{"name":"tcp-postgresql","port":5432,"targetPort":"tcp-postgresql"}],"publishNotReadyAddresses":true,"selector":{"app.kubernetes.io/component":"primary","app.kubernetes.io/instance":"keycloak","app.kubernetes.io/name":"postgresql"},"type":"ClusterIP"}}
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2026-01-17T10:46:36Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 15.4.0
      helm.sh/chart: postgresql-12.12.10
    name: keycloak-postgresql-hl
    namespace: keycloak
    resourceVersion: "20856688"
    uid: 1b4e2971-96a8-4bf7-9d7e-6512a6518889
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: external-dns
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-12-10T08:53:21Z"
    labels:
      app.kubernetes.io/instance: external-dns
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: external-dns
      app.kubernetes.io/version: 0.19.0
      helm.sh/chart: external-dns-1.19.0
    name: external-dns
    namespace: kube-system
    resourceVersion: "8220480"
    uid: 5336fc7c-801f-4a37-b2ce-230e3b402fb1
  spec:
    clusterIP: 10.111.124.139
    clusterIPs:
    - 10.111.124.139
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 7979
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: external-dns
      app.kubernetes.io/name: external-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-10-29T10:54:38Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: CoreDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "288"
    uid: de2ec4d5-2249-4adf-b744-e06a6b8a3806
  spec:
    clusterIP: 10.96.0.10
    clusterIPs:
    - 10.96.0.10
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"ports":[{"appProtocol":"https","name":"https","port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"k8s-app":"metrics-server"}}}
    creationTimestamp: "2025-11-26T09:48:15Z"
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "5048887"
    uid: 5309634d-4da1-4706-8cc8-f2381eddc97c
  spec:
    clusterIP: 10.106.53.135
    clusterIPs:
    - 10.106.53.135
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: https
      name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app: kube-prometheus-stack-coredns
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      jobLabel: coredns
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-coredns
    namespace: kube-system
    resourceVersion: "14489676"
    uid: dff71657-d655-4da1-bb26-392bb54a4747
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app: kube-prometheus-stack-kube-controller-manager
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      jobLabel: kube-controller-manager
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-kube-controller-manager
    namespace: kube-system
    resourceVersion: "14489674"
    uid: 93a882e6-0884-4b05-81d9-3de265ba234b
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10257
      protocol: TCP
      targetPort: 10257
    selector:
      component: kube-controller-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app: kube-prometheus-stack-kube-etcd
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      jobLabel: kube-etcd
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-kube-etcd
    namespace: kube-system
    resourceVersion: "14489675"
    uid: 406d5e46-4ecc-49d9-b6f1-e307f0a34b88
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 2381
      protocol: TCP
      targetPort: 2381
    selector:
      component: etcd
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app: kube-prometheus-stack-kube-proxy
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      jobLabel: kube-proxy
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-kube-proxy
    namespace: kube-system
    resourceVersion: "14489673"
    uid: bba499c4-779f-49b3-a1c4-2b26ac093626
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10249
      protocol: TCP
      targetPort: 10249
    selector:
      k8s-app: kube-proxy
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app: kube-prometheus-stack-kube-scheduler
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      jobLabel: kube-scheduler
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-kube-scheduler
    namespace: kube-system
    resourceVersion: "14489677"
    uid: b994ab71-2996-4ee6-98a5-64d947c7cc87
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 10259
      protocol: TCP
      targetPort: 10259
    selector:
      component: kube-scheduler
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-11-20T13:33:37Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: kubelet
      k8s-app: kubelet
    name: neural-hive-prometheus-kub-kubelet
    namespace: kube-system
    resourceVersion: "3733368"
    uid: ec173004-f7ff-4973-aa44-8993d1637b0d
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    - IPv6
    ipFamilyPolicy: RequireDualStack
    ports:
    - name: https-metrics
      port: 10250
      protocol: TCP
      targetPort: 10250
    - name: http-metrics
      port: 10255
      protocol: TCP
      targetPort: 10255
    - name: cadvisor
      port: 4194
      protocol: TCP
      targetPort: 4194
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:05Z"
    labels:
      app: longhorn-admission-webhook
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
    name: longhorn-admission-webhook
    namespace: longhorn-system
    resourceVersion: "8057370"
    uid: 5e5ef6f3-b78b-458c-b191-43bac198fa80
  spec:
    clusterIP: 10.105.35.140
    clusterIPs:
    - 10.105.35.140
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: admission-webhook
      port: 9502
      protocol: TCP
      targetPort: admission-wh
    selector:
      longhorn.io/admission-webhook: longhorn-admission-webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:05Z"
    labels:
      app: longhorn-manager
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
    name: longhorn-backend
    namespace: longhorn-system
    resourceVersion: "8057359"
    uid: 11636b25-e1cc-490d-b522-ae61203168e7
  spec:
    clusterIP: 10.111.81.28
    clusterIPs:
    - 10.111.81.28
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: manager
      port: 9500
      protocol: TCP
      targetPort: manager
    selector:
      app: longhorn-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:05Z"
    labels:
      app: longhorn-ui
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
    name: longhorn-frontend
    namespace: longhorn-system
    resourceVersion: "8057356"
    uid: cd82ed48-70dd-4b42-972a-8a8630263f65
  spec:
    clusterIP: 10.111.26.198
    clusterIPs:
    - 10.111.26.198
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app: longhorn-ui
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:05Z"
    labels:
      app: longhorn-recovery-backend
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
    name: longhorn-recovery-backend
    namespace: longhorn-system
    resourceVersion: "8057352"
    uid: e3144348-6502-42ed-ba77-118f45b17dc5
  spec:
    clusterIP: 10.98.1.240
    clusterIPs:
    - 10.98.1.240
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: recovery-backend
      port: 9503
      protocol: TCP
      targetPort: recov-backend
    selector:
      longhorn.io/recovery-backend: longhorn-recovery-backend
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"mlflow","namespace":"mlflow"},"spec":{"ports":[{"name":"http","port":5000,"protocol":"TCP","targetPort":5000}],"selector":{"app":"mlflow"},"type":"ClusterIP"}}
    creationTimestamp: "2025-10-30T13:28:31Z"
    name: mlflow
    namespace: mlflow
    resourceVersion: "176516"
    uid: a57dd8d0-c66f-446d-87e3-f5c49743d0de
  spec:
    clusterIP: 10.101.158.48
    clusterIPs:
    - 10.101.158.48
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 5000
      protocol: TCP
      targetPort: 5000
    selector:
      app: mlflow
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: mlflow-postgresql
      meta.helm.sh/release-namespace: mlflow
    creationTimestamp: "2026-01-03T19:26:03Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: mlflow-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 18.1.0
      helm.sh/chart: postgresql-18.2.0
    name: mlflow-postgresql
    namespace: mlflow
    resourceVersion: "16233466"
    uid: 5495ee1f-8fad-4705-a128-135230b670ed
  spec:
    clusterIP: 10.99.195.108
    clusterIPs:
    - 10.99.195.108
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: mlflow-postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: mlflow-postgresql
      meta.helm.sh/release-namespace: mlflow
    creationTimestamp: "2026-01-03T19:26:03Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: mlflow-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 18.1.0
      helm.sh/chart: postgresql-18.2.0
    name: mlflow-postgresql-hl
    namespace: mlflow
    resourceVersion: "16233464"
    uid: 1b85e015-8189-4955-ad39-69142728450f
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: mlflow-postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: mongodb
      meta.helm.sh/release-namespace: mongodb-cluster
    creationTimestamp: "2025-11-21T08:26:32Z"
    labels:
      app.kubernetes.io/component: mongodb
      app.kubernetes.io/instance: mongodb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: mongodb-18.1.10
    name: mongodb
    namespace: mongodb-cluster
    resourceVersion: "3918368"
    uid: acc2574c-2d79-4746-855c-00e4f0b4725e
  spec:
    clusterIP: 10.99.254.86
    clusterIPs:
    - 10.99.254.86
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: mongodb
      port: 27017
      protocol: TCP
      targetPort: mongodb
    selector:
      app.kubernetes.io/component: mongodb
      app.kubernetes.io/instance: mongodb
      app.kubernetes.io/name: mongodb
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: mongodb
      meta.helm.sh/release-namespace: mongodb-cluster
      prometheus.io/path: /metrics
      prometheus.io/port: "9216"
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-11-21T08:26:32Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: mongodb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: mongodb-18.1.10
    name: mongodb-metrics
    namespace: mongodb-cluster
    resourceVersion: "3918364"
    uid: 1732869a-36d6-4700-975f-7ca2e792ca52
  spec:
    clusterIP: 10.111.247.124
    clusterIPs:
    - 10.111.247.124
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9216
      protocol: TCP
      targetPort: metrics
    selector:
      app.kubernetes.io/component: mongodb
      app.kubernetes.io/instance: mongodb
      app.kubernetes.io/name: mongodb
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neo4j
      meta.helm.sh/release-namespace: neo4j-cluster
    creationTimestamp: "2025-11-21T08:14:58Z"
    labels:
      app: neo4j
      app.kubernetes.io/managed-by: Helm
      helm.neo4j.com/instance: neo4j
      helm.neo4j.com/neo4j.name: neo4j
      helm.neo4j.com/service: default
    name: neo4j
    namespace: neo4j-cluster
    resourceVersion: "3916410"
    uid: 5fb351c2-0b24-490d-ac77-ef0120eb1092
  spec:
    clusterIP: 10.97.44.47
    clusterIPs:
    - 10.97.44.47
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-bolt
      port: 7687
      protocol: TCP
      targetPort: 7687
    - name: tcp-http
      port: 7474
      protocol: TCP
      targetPort: 7474
    selector:
      app: neo4j
      helm.neo4j.com/instance: neo4j
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neo4j
      meta.helm.sh/release-namespace: neo4j-cluster
    creationTimestamp: "2025-11-21T08:14:58Z"
    labels:
      app: neo4j
      app.kubernetes.io/managed-by: Helm
      helm.neo4j.com/instance: neo4j
      helm.neo4j.com/neo4j.name: neo4j
      helm.neo4j.com/service: admin
    name: neo4j-admin
    namespace: neo4j-cluster
    resourceVersion: "3916406"
    uid: 681b7133-4df7-4f4b-9f4e-cd8eab7b722f
  spec:
    clusterIP: 10.101.181.85
    clusterIPs:
    - 10.101.181.85
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-bolt
      port: 7687
      protocol: TCP
      targetPort: 7687
    - name: tcp-http
      port: 7474
      protocol: TCP
      targetPort: 7474
    publishNotReadyAddresses: true
    selector:
      app: neo4j
      helm.neo4j.com/instance: neo4j
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neo4j
      meta.helm.sh/release-namespace: neo4j-cluster
    creationTimestamp: "2025-11-21T08:14:58Z"
    labels:
      app: neo4j
      app.kubernetes.io/managed-by: Helm
      helm.neo4j.com/neo4j.name: neo4j
      helm.neo4j.com/service: neo4j
    name: neo4j-lb-neo4j
    namespace: neo4j-cluster
    resourceVersion: "3916418"
    uid: 6731b8f6-f652-437a-883e-87025ff5c640
  spec:
    allocateLoadBalancerNodePorts: true
    clusterIP: 10.110.139.194
    clusterIPs:
    - 10.110.139.194
    externalTrafficPolicy: Local
    healthCheckNodePort: 32009
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30205
      port: 7474
      protocol: TCP
      targetPort: 7474
    - name: https
      nodePort: 32665
      port: 7473
      protocol: TCP
      targetPort: 7473
    - name: tcp-bolt
      nodePort: 31518
      port: 7687
      protocol: TCP
      targetPort: 7687
    selector:
      app: neo4j
      helm.neo4j.com/clustering: "false"
      helm.neo4j.com/neo4j.loadbalancer: include
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"postgres-sla","namespace":"neural-hive-data"},"spec":{"ports":[{"port":5432,"targetPort":5432}],"selector":{"app":"postgres-sla"},"type":"ClusterIP"}}
    creationTimestamp: "2026-01-01T13:02:32Z"
    name: postgres-sla
    namespace: neural-hive-data
    resourceVersion: "15493779"
    uid: f4753fad-0b14-4091-b785-09fa885fb7f3
  spec:
    clusterIP: 10.106.234.80
    clusterIPs:
    - 10.106.234.80
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: postgres-sla
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: consensus-engine
      meta.helm.sh/release-namespace: neural-hive-staging
    creationTimestamp: "2026-01-31T22:37:29Z"
    labels:
      app.kubernetes.io/component: consensus-aggregator
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: consensus-engine
      app.kubernetes.io/version: 1.0.8
      component: metrics
      helm.sh/chart: consensus-engine-0.1.8
      neural-hive.io/component: consensus-engine
      neural-hive.io/domain: consensus
      neural-hive.io/layer: cognitiva
      neural.hive/metrics: enabled
    name: consensus-engine
    namespace: neural-hive-staging
    resourceVersion: "25346064"
    uid: 2edfef04-7417-4a79-999d-a1593163b0cc
  spec:
    clusterIP: 10.98.231.90
    clusterIPs:
    - 10.98.231.90
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/name: consensus-engine
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive-staging
      neural-hive-mind.org/component: gateway-intencoes
    creationTimestamp: "2026-01-30T14:24:47Z"
    labels:
      app.kubernetes.io/component: gateway-intencoes
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: gateway-intencoes
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: gateway-intencoes-0.1.0
      neural-hive.io/component: gateway-intencoes
      neural-hive.io/layer: application
      neural.hive/metrics: enabled
    name: gateway-intencoes
    namespace: neural-hive-staging
    resourceVersion: "25149608"
    uid: c2ce646b-cc11-4d1b-b031-29814c9613ab
  spec:
    clusterIP: 10.111.69.133
    clusterIPs:
    - 10.111.69.133
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive-staging
    creationTimestamp: "2026-02-02T00:33:55Z"
    labels:
      app: opa
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: policy-engine
      helm.sh/chart: orchestrator-dynamic-1.0.0
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: opa
    namespace: neural-hive-staging
    resourceVersion: "25708849"
    uid: 37198ead-c33b-41a4-86a0-c57594041088
  spec:
    clusterIP: 10.109.251.85
    clusterIPs:
    - 10.109.251.85
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8181
      protocol: TCP
      targetPort: 8181
    selector:
      app: opa
      component: policy-engine
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive-staging
    creationTimestamp: "2026-02-02T00:33:55Z"
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: orchestrator-dynamic-1.0.0
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
      neural.hive/metrics: enabled
    name: orchestrator-dynamic
    namespace: neural-hive-staging
    resourceVersion: "25708850"
    uid: e0ef99b0-262f-44e4-bf21-15e6ef7b5955
  spec:
    clusterIP: 10.103.175.139
    clusterIPs:
    - 10.103.175.139
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50053
      protocol: TCP
      targetPort: 50053
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: analyst-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:49Z"
    labels:
      app.kubernetes.io/component: analyst-agents
      app.kubernetes.io/instance: analyst-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: analyst-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.2.0
      component: metrics
      helm.sh/chart: analyst-agents-1.0.0
      neural-hive.io/component: analyst-agents
      neural-hive.io/domain: insight-generation
      neural-hive.io/layer: analise
      neural.hive/metrics: enabled
    name: analyst-agents
    namespace: neural-hive
    resourceVersion: "27655422"
    uid: cf95767c-df35-45ca-b02e-ba331c670359
  spec:
    clusterIP: 10.103.211.30
    clusterIPs:
    - 10.103.211.30
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: analyst-agents
      app.kubernetes.io/name: analyst-agents
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"approval-service"},"name":"approval-service","namespace":"neural-hive"},"spec":{"ports":[{"name":"http","port":8080,"targetPort":8080}],"selector":{"app.kubernetes.io/name":"approval-service"}}}
    creationTimestamp: "2026-01-25T18:46:28Z"
    labels:
      app.kubernetes.io/name: approval-service
    name: approval-service
    namespace: neural-hive
    resourceVersion: "23366942"
    uid: 79060ecb-0e4c-4a68-ac69-587909aa7964
  spec:
    clusterIP: 10.98.167.149
    clusterIPs:
    - 10.98.167.149
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/name: approval-service
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: code-forge
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:13:04Z"
    labels:
      app.kubernetes.io/component: code-forge
      app.kubernetes.io/instance: code-forge
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: code-forge
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: code-forge-0.1.0
      neural-hive.io/component: code-forge
      neural-hive.io/domain: code-generation
      neural-hive.io/layer: execution
      neural.hive/metrics: enabled
    name: code-forge
    namespace: neural-hive
    resourceVersion: "18259591"
    uid: 68a43064-1137-4f8e-95f5-03f0d56284af
  spec:
    clusterIP: 10.104.1.188
    clusterIPs:
    - 10.104.1.188
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: code-forge
      app.kubernetes.io/name: code-forge
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: consensus-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:07:46Z"
    labels:
      app.kubernetes.io/component: consensus-aggregator
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: consensus-engine
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      component: metrics
      helm.sh/chart: consensus-engine-0.1.8
      neural-hive.io/component: consensus-engine
      neural-hive.io/domain: consensus
      neural-hive.io/layer: cognitiva
      neural.hive/metrics: enabled
    name: consensus-engine
    namespace: neural-hive
    resourceVersion: "25354877"
    uid: e0af54b1-f035-4c02-bf3f-bf316ad4a1f3
  spec:
    clusterIP: 10.103.220.249
    clusterIPs:
    - 10.103.220.249
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/name: consensus-engine
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: execution-ticket-service
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-26T09:58:20Z"
    labels:
      app.kubernetes.io/component: execution-ticket-service
      app.kubernetes.io/instance: execution-ticket-service
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: execution-ticket-service
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: execution-ticket-service-1.0.0
      neural-hive.io/component: execution-ticket-service
      neural-hive.io/domain: ticket-management
      neural-hive.io/layer: orchestration
      neural.hive/metrics: enabled
    name: execution-ticket-service
    namespace: neural-hive
    resourceVersion: "27655432"
    uid: 1e240158-33f6-4c1d-a6f1-0159404e31a7
  spec:
    clusterIP: 10.105.85.223
    clusterIPs:
    - 10.105.85.223
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50052
      protocol: TCP
      targetPort: 50052
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: execution-ticket-service
      app.kubernetes.io/name: execution-ticket-service
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: explainability-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:17Z"
    labels:
      app.kubernetes.io/component: explainability-api
      app.kubernetes.io/instance: explainability-api
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: explainability-api
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: explainability-api-1.0.0
      neural-hive.io/component: explainability-api
      neural-hive.io/domain: audit-transparency
      neural-hive.io/layer: transparencia
      neural.hive/metrics: enabled
    name: explainability-api
    namespace: neural-hive
    resourceVersion: "28702806"
    uid: 1a2fb438-b50d-49c9-b100-7a67f39d8f0c
  spec:
    clusterIP: 10.106.172.31
    clusterIPs:
    - 10.106.172.31
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: explainability-api
      app.kubernetes.io/name: explainability-api
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"feedback-collection-service","namespace":"neural-hive"},"spec":{"ports":[{"port":8080,"targetPort":8080}],"selector":{"app":"feedback-collection"},"type":"ClusterIP"}}
    creationTimestamp: "2026-02-08T14:39:33Z"
    name: feedback-collection-service
    namespace: neural-hive
    resourceVersion: "27896952"
    uid: b7f30fd6-f3c7-47d1-8272-bddde57ee170
  spec:
    clusterIP: 10.100.29.166
    clusterIPs:
    - 10.100.29.166
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: feedback-collection
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"feedback-collection-service-external","namespace":"neural-hive"},"spec":{"ports":[{"nodePort":30080,"port":8080,"targetPort":8080}],"selector":{"app":"feedback-collection"},"type":"NodePort"}}
    creationTimestamp: "2026-02-08T14:41:42Z"
    name: feedback-collection-service-external
    namespace: neural-hive
    resourceVersion: "27897507"
    uid: 0ad618b2-39fb-4fc0-b3ba-debad83d45ad
  spec:
    clusterIP: 10.97.106.67
    clusterIPs:
    - 10.97.106.67
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - nodePort: 30080
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: feedback-collection
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
      neural-hive-mind.org/component: gateway-intencoes
    creationTimestamp: "2026-02-13T14:13:19Z"
    labels:
      app.kubernetes.io/component: gateway-intencoes
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: gateway-intencoes
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: gateway-intencoes-0.1.0
      neural-hive.io/component: gateway-intencoes
      neural-hive.io/layer: application
      neural.hive/metrics: enabled
    name: gateway-intencoes
    namespace: neural-hive
    resourceVersion: "29794073"
    uid: d5ac83f8-4db8-403e-88b1-c7f8df94b7be
  spec:
    clusterIP: 10.97.177.17
    clusterIPs:
    - 10.97.177.17
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: guard-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:42Z"
    labels:
      app.kubernetes.io/component: guard-agents
      app.kubernetes.io/instance: guard-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: guard-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: guard-agents-1.0.0
      neural-hive.io/component: guard-agents
      neural-hive.io/domain: security-validation
      neural-hive.io/layer: resilience
    name: guard-agents
    namespace: neural-hive
    resourceVersion: "28371355"
    uid: af86e696-39e9-47e1-9692-d7605a5673a9
  spec:
    clusterIP: 10.110.49.133
    clusterIPs:
    - 10.110.49.133
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: guard-agents
      app.kubernetes.io/name: guard-agents
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: mcp-tool-catalog
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:14:52Z"
    labels:
      app.kubernetes.io/component: mcp-tool-catalog
      app.kubernetes.io/instance: mcp-tool-catalog
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mcp-tool-catalog
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: mcp-tool-catalog-1.0.0
      neural-hive.io/component: mcp-tool-catalog
      neural-hive.io/domain: tool-management
      neural-hive.io/layer: infrastructure
      neural.hive/metrics: enabled
    name: mcp-tool-catalog
    namespace: neural-hive
    resourceVersion: "28708653"
    uid: b498dd0a-95a5-4498-92b0-9eb9064f9d4c
  spec:
    clusterIP: 10.108.70.62
    clusterIPs:
    - 10.108.70.62
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: metrics
      port: 9091
      protocol: TCP
      targetPort: 9091
    selector:
      app.kubernetes.io/instance: mcp-tool-catalog
      app.kubernetes.io/name: mcp-tool-catalog
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
      neural-hive-mind.org/component: memory-layer-api
    creationTimestamp: "2026-02-11T23:18:03Z"
    labels:
      app.kubernetes.io/component: memory-layer-api
      app.kubernetes.io/instance: memory-layer-api
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: memory-layer-api
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: memory-layer-api-0.1.0
      neural-hive.io/component: memory-layer-api
      neural-hive.io/domain: memory-management
      neural-hive.io/layer: conhecimento-dados
      neural.hive/metrics: enabled
    name: memory-layer-api
    namespace: neural-hive
    resourceVersion: "29151938"
    uid: 0f7bfc6a-eb3e-4a03-a62b-3fd167688e6c
  spec:
    clusterIP: 10.102.19.150
    clusterIPs:
    - 10.102.19.150
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: memory-layer-api
      app.kubernetes.io/name: memory-layer-api
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"opa"},"name":"opa","namespace":"neural-hive"},"spec":{"ports":[{"name":"http","port":8181,"targetPort":8181}],"selector":{"app":"opa"},"type":"ClusterIP"}}
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-28T10:32:35Z"
    labels:
      app: opa
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: policy-engine
      helm.sh/chart: orchestrator-dynamic-1.0.0
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: opa
    namespace: neural-hive
    resourceVersion: "29896123"
    uid: 3be3904e-7577-4ff5-892c-ff35a177a107
  spec:
    clusterIP: 10.98.54.94
    clusterIPs:
    - 10.98.54.94
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8181
      protocol: TCP
      targetPort: 8181
    selector:
      app: opa
      component: policy-engine
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: optimizer-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:57Z"
    labels:
      app.kubernetes.io/component: optimizer-agents
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: optimizer-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.13
      component: metrics
      helm.sh/chart: optimizer-agents-1.0.0
      neural-hive.io/component: optimizer-agents
      neural-hive.io/domain: continuous-improvement
      neural-hive.io/layer: otimizacao
      neural.hive/metrics: enabled
    name: optimizer-agents
    namespace: neural-hive
    resourceVersion: "18258389"
    uid: efd5e147-96c5-4329-9a2e-1f8901eafddb
  spec:
    clusterIP: 10.105.230.162
    clusterIPs:
    - 10.105.230.162
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T22:58:05Z"
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: orchestrator-dynamic-1.0.0
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic
    namespace: neural-hive
    resourceVersion: "29896133"
    uid: 35ef66f1-de15-44b8-9ade-add25d1ffe84
  spec:
    clusterIP: 10.101.90.228
    clusterIPs:
    - 10.101.90.228
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50053
      protocol: TCP
      targetPort: 50053
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:22Z"
    labels:
      app.kubernetes.io/component: queen-agent
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: queen-agent
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: queen-agent-0.1.0
      neural-hive.io/component: queen-agent
      neural-hive.io/domain: hive-coordination
      neural-hive.io/layer: coordination
      neural.hive/metrics: enabled
    name: queen-agent
    namespace: neural-hive
    resourceVersion: "18258133"
    uid: 495cbafc-4122-4565-9128-7b4578606541
  spec:
    clusterIP: 10.107.87.7
    clusterIPs:
    - 10.107.87.7
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50053
      protocol: TCP
      targetPort: 50053
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: scout-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:31Z"
    labels:
      app.kubernetes.io/component: scout-agents
      app.kubernetes.io/instance: scout-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: scout-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: scout-agents-0.1.0
      neural-hive.io/component: scout-agents
      neural-hive.io/domain: reconnaissance
      neural-hive.io/layer: exploration
      neural.hive/metrics: enabled
    name: scout-agents
    namespace: neural-hive
    resourceVersion: "27682318"
    uid: e9ab60c7-cc7e-4832-b1fc-868c169d7ca4
  spec:
    clusterIP: 10.106.110.30
    clusterIPs:
    - 10.106.110.30
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: scout-agents
      app.kubernetes.io/name: scout-agents
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: self-healing-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:43Z"
    labels:
      app.kubernetes.io/component: self-healing-engine
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: self-healing-engine
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: self-healing-engine-1.0.0
      neural-hive.io/component: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
      neural.hive/metrics: enabled
    name: self-healing-engine
    namespace: neural-hive
    resourceVersion: "18260649"
    uid: 61c55960-8397-4b7d-9e68-02bc1c0ba74a
  spec:
    clusterIP: 10.103.182.225
    clusterIPs:
    - 10.103.182.225
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/name: self-healing-engine
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: semantic-translation-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:06Z"
    labels:
      app.kubernetes.io/component: semantic-translation-engine
      app.kubernetes.io/instance: semantic-translation-engine
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: semantic-translation-engine
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: semantic-translation-engine-0.1.0
      neural-hive.io/component: semantic-translator
      neural-hive.io/domain: plan-generation
      neural-hive.io/layer: cognitiva
      neural.hive/metrics: enabled
    name: semantic-translation-engine
    namespace: neural-hive
    resourceVersion: "25363543"
    uid: 290bdbd8-7c7b-4d07-885e-772345f1cccc
  spec:
    clusterIP: 10.110.198.204
    clusterIPs:
    - 10.110.198.204
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: semantic-translation-engine
      app.kubernetes.io/name: semantic-translation-engine
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: service-registry
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T22:57:13Z"
    labels:
      app.kubernetes.io/component: service-registry
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: service-registry
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: service-registry-1.0.0
      neural-hive.io/component: service-registry
      neural-hive.io/domain: service-discovery
      neural-hive.io/layer: infrastructure
      neural.hive/metrics: enabled
    name: service-registry
    namespace: neural-hive
    resourceVersion: "26993494"
    uid: 2f25af14-0dd1-4c62-bb00-6204bfe30e61
  spec:
    clusterIP: 10.98.9.69
    clusterIPs:
    - 10.98.9.69
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/name: service-registry
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:32Z"
    labels:
      app.kubernetes.io/component: sla-management-system
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: sla-management-system-1.0.0
      neural-hive.io/component: sla-management-system
      neural-hive.io/domain: sla-management
      neural-hive.io/layer: monitoring
      neural.hive/metrics: enabled
    name: sla-management-system
    namespace: neural-hive
    resourceVersion: "27974815"
    uid: 490a2a55-51ac-4241-b599-0ffff97c2a99
  spec:
    clusterIP: 10.110.54.29
    clusterIPs:
    - 10.110.54.29
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: specialist-architecture
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:11:19Z"
    labels:
      app.kubernetes.io/component: architecture-specialist
      app.kubernetes.io/instance: specialist-architecture
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-architecture
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      component: metrics
      helm.sh/chart: specialist-architecture-0.1.8
      neural-hive.io/component: architecture-specialist
      neural-hive.io/domain: architecture-analysis
      neural-hive.io/layer: cognitiva
      neural.hive/metrics: enabled
    name: specialist-architecture
    namespace: neural-hive
    resourceVersion: "18259084"
    uid: 41cadc5b-ec98-4305-89ae-f7c7d4b3970d
  spec:
    clusterIP: 10.106.65.102
    clusterIPs:
    - 10.106.65.102
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: specialist-architecture
      app.kubernetes.io/name: specialist-architecture
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: specialist-behavior
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:09:21Z"
    labels:
      app.kubernetes.io/component: behavior-specialist
      app.kubernetes.io/instance: specialist-behavior
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-behavior
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      component: metrics
      helm.sh/chart: specialist-behavior-0.1.8
      neural-hive.io/component: behavior-specialist
      neural-hive.io/domain: behavior-analysis
      neural-hive.io/layer: cognitiva
      neural.hive/metrics: enabled
    name: specialist-behavior
    namespace: neural-hive
    resourceVersion: "20729900"
    uid: 2ff57941-725f-46fd-b862-9cb5a000267f
  spec:
    clusterIP: 10.109.150.75
    clusterIPs:
    - 10.109.150.75
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: specialist-behavior
      app.kubernetes.io/name: specialist-behavior
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: specialist-business
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:11:09Z"
    labels:
      app.kubernetes.io/component: business-specialist
      app.kubernetes.io/instance: specialist-business
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-business
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      component: metrics
      helm.sh/chart: specialist-business-0.1.8
      neural-hive.io/component: business-specialist
      neural-hive.io/domain: business-analysis
      neural-hive.io/layer: cognitiva
      neural.hive/metrics: enabled
    name: specialist-business
    namespace: neural-hive
    resourceVersion: "20729974"
    uid: 4c152c9e-9521-4bc8-8ce0-bc0b27e21006
  spec:
    clusterIP: 10.103.16.201
    clusterIPs:
    - 10.103.16.201
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: specialist-business
      app.kubernetes.io/name: specialist-business
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: specialist-evolution
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:11:29Z"
    labels:
      app.kubernetes.io/component: evolution-specialist
      app.kubernetes.io/instance: specialist-evolution
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-evolution
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      component: metrics
      helm.sh/chart: specialist-evolution-0.1.8
      neural-hive.io/component: evolution-specialist
      neural-hive.io/domain: evolution-analysis
      neural-hive.io/layer: cognitiva
      neural.hive/metrics: enabled
    name: specialist-evolution
    namespace: neural-hive
    resourceVersion: "18259153"
    uid: 6fcf711b-394b-4b81-be21-625bb1d64645
  spec:
    clusterIP: 10.100.202.15
    clusterIPs:
    - 10.100.202.15
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: specialist-evolution
      app.kubernetes.io/name: specialist-evolution
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: specialist-technical
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:10:10Z"
    labels:
      app.kubernetes.io/component: technical-specialist
      app.kubernetes.io/instance: specialist-technical
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-technical
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      component: metrics
      helm.sh/chart: specialist-technical-0.1.8
      neural-hive.io/component: technical-specialist
      neural-hive.io/domain: technical-analysis
      neural-hive.io/layer: cognitiva
      neural.hive/metrics: enabled
    name: specialist-technical
    namespace: neural-hive
    resourceVersion: "20730099"
    uid: 89625cef-ce43-4433-8ef8-1628e69d1175
  spec:
    clusterIP: 10.111.174.11
    clusterIPs:
    - 10.111.174.11
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    - name: http
      port: 8000
      protocol: TCP
      targetPort: 8000
    - name: metrics
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app.kubernetes.io/instance: specialist-technical
      app.kubernetes.io/name: specialist-technical
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: worker-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-03T14:53:56Z"
    labels:
      app.kubernetes.io/component: worker-agents
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: worker-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: metrics
      helm.sh/chart: worker-agents-1.0.0
      neural-hive.io/component: worker-agents
      neural-hive.io/domain: task-execution
      neural-hive.io/layer: execution
      neural.hive/metrics: enabled
    name: worker-agents
    namespace: neural-hive
    resourceVersion: "27816413"
    uid: b3343775-7022-485e-9f91-d77e7e7a451d
  spec:
    clusterIP: 10.107.93.197
    clusterIPs:
    - 10.107.93.197
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: metrics
      port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/name: worker-agents
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-29T10:18:55Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      managed-by: prometheus-operator
      operated-alertmanager: "true"
    name: alertmanager-operated
    namespace: observability
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: neural-hive-prometheus-kub-alertmanager
      uid: a52784b6-9e19-4523-ac26-c697ae33348d
    resourceVersion: "14489912"
    uid: c023b64f-8756-42e5-83d9-c78ff8b2e4c6
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: http-web
    - name: tcp-mesh
      port: 9094
      protocol: TCP
      targetPort: mesh-tcp
    - name: udp-mesh
      port: 9094
      protocol: UDP
      targetPort: mesh-udp
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:22:09Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki
    namespace: observability
    resourceVersion: "14490887"
    uid: 9490d0da-24ff-4b62-96a0-d211a7a5cb79
  spec:
    clusterIP: 10.105.212.11
    clusterIPs:
    - 10.105.212.11
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:22:09Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
      variant: headless
    name: loki-headless
    namespace: observability
    resourceVersion: "14490881"
    uid: 4c826526-2bf3-4b44-965f-bb8e38ba8e1f
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: http-metrics
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:22:09Z"
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki-memberlist
    namespace: observability
    resourceVersion: "14490880"
    uid: 78481882-c899-4ac1-b8dd-6b225a4fd48d
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 7946
      protocol: TCP
      targetPort: memberlist-port
    publishNotReadyAddresses: true
    selector:
      app: loki
      release: loki
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-jaeger
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:21:49Z"
    labels:
      app.kubernetes.io/component: all-in-one
      app.kubernetes.io/instance: neural-hive-jaeger
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/version: 2.13.0
      helm.sh/chart: jaeger-4.2.2
    name: neural-hive-jaeger
    namespace: observability
    resourceVersion: "14490765"
    uid: 6ebe38f8-8f44-4b98-8b51-ae1359056629
  spec:
    clusterIP: 10.103.198.246
    clusterIPs:
    - 10.103.198.246
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: zk-compact-trft
      port: 5775
      protocol: UDP
      targetPort: 5775
    - name: config-rest
      port: 5778
      protocol: TCP
      targetPort: 5778
    - name: jg-compact-trft
      port: 6831
      protocol: UDP
      targetPort: 6831
    - name: jg-binary-trft
      port: 6832
      protocol: UDP
      targetPort: 6832
    - appProtocol: http
      name: http-zipkin
      port: 9411
      protocol: TCP
      targetPort: 9411
    - appProtocol: grpc
      name: grpc-http
      port: 14250
      protocol: TCP
      targetPort: 14250
    - name: c-tchan-trft
      port: 14267
      protocol: TCP
      targetPort: 14267
    - appProtocol: http
      name: http-c-binary-trft
      port: 14268
      protocol: TCP
      targetPort: 14268
    - appProtocol: grpc
      name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - appProtocol: http
      name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: http-query
      port: 16686
      protocol: TCP
      targetPort: 16686
    - name: grpc-query
      port: 16685
      protocol: TCP
      targetPort: 16685
    selector:
      app.kubernetes.io/component: all-in-one
      app.kubernetes.io/instance: neural-hive-jaeger
      app.kubernetes.io/name: jaeger
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.3.0
      helm.sh/chart: grafana-10.4.0
    name: neural-hive-prometheus-grafana
    namespace: observability
    resourceVersion: "14489712"
    uid: 3cd797ae-df3e-426a-bd08-b5c4210a11c6
  spec:
    clusterIP: 10.109.61.204
    clusterIPs:
    - 10.109.61.204
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 80
      protocol: TCP
      targetPort: grafana
    selector:
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/name: grafana
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      release: neural-hive-prometheus
      self-monitor: "true"
    name: neural-hive-prometheus-kub-alertmanager
    namespace: observability
    resourceVersion: "14489704"
    uid: c06418cd-ee08-4faf-8a91-fc17828f5eda
  spec:
    clusterIP: 10.111.114.196
    clusterIPs:
    - 10.111.114.196
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9093
      protocol: TCP
      targetPort: 9093
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      alertmanager: neural-hive-prometheus-kub-alertmanager
      app.kubernetes.io/name: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-operator
    namespace: observability
    resourceVersion: "14489699"
    uid: 049e016b-2381-4032-8f97-4281665790b6
  spec:
    clusterIP: 10.105.20.37
    clusterIPs:
    - 10.105.20.37
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
    selector:
      app: kube-prometheus-stack-operator
      release: neural-hive-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      release: neural-hive-prometheus
      self-monitor: "true"
    name: neural-hive-prometheus-kub-prometheus
    namespace: observability
    resourceVersion: "14489708"
    uid: 042c3344-db19-4256-8f7b-0cb95b00cc91
  spec:
    clusterIP: 10.97.49.115
    clusterIPs:
    - 10.97.49.115
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: 9090
    - appProtocol: http
      name: reloader-web
      port: 8080
      protocol: TCP
      targetPort: reloader-web
    selector:
      app.kubernetes.io/name: prometheus
      operator.prometheus.io/name: neural-hive-prometheus-kub-prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-7.0.0
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kube-state-metrics
    namespace: observability
    resourceVersion: "14489680"
    uid: 9c8d8656-3211-45b2-93e3-ccfb95285708
  spec:
    clusterIP: 10.108.126.140
    clusterIPs:
    - 10.108.126.140
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/name: kube-state-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
      prometheus.io/scrape: "true"
    creationTimestamp: "2025-12-29T10:18:45Z"
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      helm.sh/chart: prometheus-node-exporter-4.49.2
      jobLabel: node-exporter
      release: neural-hive-prometheus
    name: neural-hive-prometheus-prometheus-node-exporter
    namespace: observability
    resourceVersion: "14489693"
    uid: ceb580db-550f-4d8b-8b4e-43ac790d42cc
  spec:
    clusterIP: 10.105.121.96
    clusterIPs:
    - 10.105.121.96
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/name: prometheus-node-exporter
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"opentelemetry-collector","namespace":"observability"},"spec":{"ports":[{"name":"otlp-grpc","port":4317,"targetPort":4317},{"name":"otlp-http","port":4318,"targetPort":4318},{"name":"jaeger-grpc","port":14250,"targetPort":14250}],"selector":{"app.kubernetes.io/instance":"otel-collector","app.kubernetes.io/name":"opentelemetry-collector"},"type":"ClusterIP"}}
    creationTimestamp: "2026-01-01T03:38:03Z"
    name: opentelemetry-collector
    namespace: observability
    resourceVersion: "15369453"
    uid: 2d0d5139-dbdb-4ad5-bcef-40e76c37fa0c
  spec:
    clusterIP: 10.107.201.134
    clusterIPs:
    - 10.107.201.134
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: jaeger-grpc
      port: 14250
      protocol: TCP
      targetPort: 14250
    selector:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: opentelemetry-collector
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
      neural.hive/metrics: enabled
      prometheus.io/path: /metrics
      prometheus.io/port: "8888"
      prometheus.io/scrape: "true"
      service.beta.kubernetes.io/aws-load-balancer-type: nlb
    creationTimestamp: "2026-01-01T03:36:57Z"
    labels:
      app.kubernetes.io/component: telemetry-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: neural-hive-otel-collector
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 0.89.0
      component: metrics
      helm.sh/chart: neural-hive-otel-collector-1.0.0
      neural-hive.io/component: telemetry-collector
      neural-hive.io/layer: observabilidade
      neural.hive/component: telemetry-collector
      neural.hive/layer: observabilidade
      neural.hive/metrics: enabled
    name: otel-collector-neural-hive-otel-collector
    namespace: observability
    resourceVersion: "15369155"
    uid: a26713ff-b5c0-4a5e-a884-a333d88d075a
  spec:
    clusterIP: 10.109.128.252
    clusterIPs:
    - 10.109.128.252
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: health
      port: 13133
      protocol: TCP
      targetPort: 13133
    - name: jaeger-grpc
      port: 14250
      protocol: TCP
      targetPort: 14250
    - name: jaeger-http
      port: 14268
      protocol: TCP
      targetPort: 14268
    - name: metrics
      port: 8888
      protocol: TCP
      targetPort: 8888
    - name: otlp-grpc
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: pprof
      port: 1777
      protocol: TCP
      targetPort: 1777
    - name: zipkin
      port: 9411
      protocol: TCP
      targetPort: 9411
    - name: zpages
      port: 55679
      protocol: TCP
      targetPort: 55679
    selector:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-01-01T03:36:57Z"
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/version: 0.83.0
      component: standalone-collector
      helm.sh/chart: opentelemetry-collector-0.66.0
    name: otel-collector-opentelemetry-collector
    namespace: observability
    resourceVersion: "15369153"
    uid: 1a578d8d-9a82-4aac-967f-f78bcd3288db
  spec:
    clusterIP: 10.104.142.181
    clusterIPs:
    - 10.104.142.181
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: jaeger-compact
      port: 6831
      protocol: UDP
      targetPort: 6831
    - name: jaeger-grpc
      port: 14250
      protocol: TCP
      targetPort: 14250
    - name: jaeger-thrift
      port: 14268
      protocol: TCP
      targetPort: 14268
    - appProtocol: grpc
      name: otlp
      port: 4317
      protocol: TCP
      targetPort: 4317
    - name: otlp-http
      port: 4318
      protocol: TCP
      targetPort: 4318
    - name: zipkin
      port: 9411
      protocol: TCP
      targetPort: 9411
    selector:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: opentelemetry-collector
      component: standalone-collector
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2025-12-29T10:18:56Z"
    labels:
      app.kubernetes.io/managed-by: prometheus-operator
      managed-by: prometheus-operator
      operated-prometheus: "true"
    name: prometheus-operated
    namespace: observability
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: neural-hive-prometheus-kub-prometheus
      uid: c1cfb238-91e2-441e-baf6-f695020acdd7
    resourceVersion: "14489949"
    uid: 8081fb75-6edf-4656-9f08-b43e7d727a0e
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-web
      port: 9090
      protocol: TCP
      targetPort: http-web
    selector:
      app.kubernetes.io/name: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: portainer
      meta.helm.sh/release-namespace: portainer
    creationTimestamp: "2025-11-08T11:51:26Z"
    labels:
      app.kubernetes.io/instance: portainer
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: portainer
      app.kubernetes.io/version: ce-latest-ee-2.33.6
      helm.sh/chart: portainer-2.33.6
      io.portainer.kubernetes.application.stack: portainer
    name: portainer
    namespace: portainer
    resourceVersion: "18770062"
    uid: 3becec2a-fa89-449b-8ed1-e058bffd8649
  spec:
    clusterIP: 10.102.38.184
    clusterIPs:
    - 10.102.38.184
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30777
      port: 9000
      protocol: TCP
      targetPort: 9000
    - name: https
      nodePort: 30779
      port: 9443
      protocol: TCP
      targetPort: 9443
    - name: edge
      nodePort: 30776
      port: 30776
      protocol: TCP
      targetPort: 30776
    selector:
      app.kubernetes.io/instance: portainer
      app.kubernetes.io/name: portainer
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"redis"},"name":"neural-hive-cache","namespace":"redis-cluster"},"spec":{"ports":[{"name":"redis","port":6379,"protocol":"TCP","targetPort":6379}],"selector":{"app":"redis"},"type":"ClusterIP"}}
    creationTimestamp: "2025-11-20T10:57:45Z"
    labels:
      app: redis
    name: neural-hive-cache
    namespace: redis-cluster
    resourceVersion: "3707904"
    uid: f33abbac-b13c-4565-b59e-ad34b020ef2f
  spec:
    clusterIP: 10.109.171.3
    clusterIPs:
    - 10.109.171.3
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: redis
      port: 6379
      protocol: TCP
      targetPort: 6379
    selector:
      app: redis
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: registry
      meta.helm.sh/release-namespace: registry
    creationTimestamp: "2025-12-04T14:17:34Z"
    labels:
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: docker-registry
    name: docker-registry
    namespace: registry
    resourceVersion: "6898825"
    uid: 0e29e7da-bdc4-41be-9715-39c50164ebde
  spec:
    clusterIP: 10.96.186.213
    clusterIPs:
    - 10.96.186.213
    externalTrafficPolicy: Cluster
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      nodePort: 30500
      port: 5000
      protocol: TCP
      targetPort: 5000
    selector:
      app.kubernetes.io/name: docker-registry
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    labels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-frontend
    namespace: temporal
    resourceVersion: "5258141"
    uid: 17e0205f-940a-4ff4-ba43-626c8a181c16
  spec:
    clusterIP: 10.111.254.133
    clusterIPs:
    - 10.111.254.133
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: tcp
      name: grpc-rpc
      port: 7233
      protocol: TCP
      targetPort: rpc
    - appProtocol: http
      name: http
      port: 7243
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/name: temporal
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
      prometheus.io/job: temporal-frontend
      prometheus.io/port: "9090"
      prometheus.io/scheme: http
      prometheus.io/scrape: "true"
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2025-11-27T08:16:51Z"
    labels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/headless: "true"
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-frontend-headless
    namespace: temporal
    resourceVersion: "5258125"
    uid: f53766b6-1c2d-484a-9610-8ad2d3257f1e
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: tcp
      name: grpc-rpc
      port: 7233
      protocol: TCP
      targetPort: rpc
    - appProtocol: tcp
      name: grpc-membership
      port: 6933
      protocol: TCP
      targetPort: membership
    - appProtocol: http
      name: metrics
      port: 9090
      protocol: TCP
      targetPort: metrics
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/name: temporal
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
      prometheus.io/job: temporal-history
      prometheus.io/port: "9090"
      prometheus.io/scheme: http
      prometheus.io/scrape: "true"
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2025-11-27T08:16:51Z"
    labels:
      app.kubernetes.io/component: history
      app.kubernetes.io/headless: "true"
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-history-headless
    namespace: temporal
    resourceVersion: "5258124"
    uid: 34ea874a-864c-4868-9688-cd49b75a4d27
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: tcp
      name: grpc-rpc
      port: 7234
      protocol: TCP
      targetPort: rpc
    - appProtocol: tcp
      name: grpc-membership
      port: 6934
      protocol: TCP
      targetPort: membership
    - appProtocol: http
      name: metrics
      port: 9090
      protocol: TCP
      targetPort: metrics
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: history
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/name: temporal
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    labels:
      app.kubernetes.io/component: internal-frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-internal-frontend
    namespace: temporal
    resourceVersion: "5258145"
    uid: 14a19632-de05-4539-b165-a78ce572df6c
  spec:
    clusterIP: 10.97.186.37
    clusterIPs:
    - 10.97.186.37
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: tcp
      name: grpc-rpc
      port: 7236
      protocol: TCP
      targetPort: rpc
    - appProtocol: http
      name: http
      port: 7246
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/component: internal-frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/name: temporal
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2025-11-27T08:16:51Z"
    labels:
      app.kubernetes.io/component: matching
      app.kubernetes.io/headless: "true"
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-matching-headless
    namespace: temporal
    resourceVersion: "5258123"
    uid: 248d3ff4-17f3-4955-b1d9-84adbbe2de56
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: tcp
      name: grpc-rpc
      port: 7235
      protocol: TCP
      targetPort: rpc
    - appProtocol: tcp
      name: grpc-membership
      port: 6935
      protocol: TCP
      targetPort: membership
    - appProtocol: http
      name: metrics
      port: 9090
      protocol: TCP
      targetPort: metrics
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: matching
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/name: temporal
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal-postgresql
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:12:57Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: temporal-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 18.1.0
      helm.sh/chart: postgresql-18.1.13
    name: temporal-postgresql
    namespace: temporal
    resourceVersion: "5257468"
    uid: dc7e21d4-174d-465f-bf25-32515238f519
  spec:
    clusterIP: 10.100.123.90
    clusterIPs:
    - 10.100.123.90
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: temporal-postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal-postgresql
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:12:57Z"
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: temporal-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 18.1.0
      helm.sh/chart: postgresql-18.1.13
    name: temporal-postgresql-hl
    namespace: temporal
    resourceVersion: "5257464"
    uid: b4129a6e-d918-483b-8042-5c5bdacb3c39
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: tcp-postgresql
      port: 5432
      protocol: TCP
      targetPort: tcp-postgresql
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: temporal-postgresql
      app.kubernetes.io/name: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    labels:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-web
    namespace: temporal
    resourceVersion: "5258130"
    uid: a351a53a-cc81-4fa8-9777-69a7397cff08
  spec:
    clusterIP: 10.103.20.174
    clusterIPs:
    - 10.103.20.174
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: http
      name: http
      port: 8080
      protocol: TCP
      targetPort: http
    selector:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/name: temporal
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
      prometheus.io/job: temporal-worker
      prometheus.io/port: "9090"
      prometheus.io/scheme: http
      prometheus.io/scrape: "true"
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    creationTimestamp: "2025-11-27T08:16:51Z"
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/headless: "true"
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-worker-headless
    namespace: temporal
    resourceVersion: "5258126"
    uid: 21a0c460-c57f-441c-8d96-74836928c01a
  spec:
    clusterIP: None
    clusterIPs:
    - None
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - appProtocol: tcp
      name: grpc-rpc
      port: 7239
      protocol: TCP
      targetPort: rpc
    - appProtocol: tcp
      name: grpc-membership
      port: 6939
      protocol: TCP
      targetPort: membership
    - appProtocol: http
      name: metrics
      port: 9090
      protocol: TCP
      targetPort: metrics
    publishNotReadyAddresses: true
    selector:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/name: temporal
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: minio
      meta.helm.sh/release-namespace: velero
    creationTimestamp: "2025-12-09T20:59:11Z"
    labels:
      app: minio
      app.kubernetes.io/managed-by: Helm
      chart: minio-5.4.0
      heritage: Helm
      monitoring: "true"
      release: minio
    name: minio
    namespace: velero
    resourceVersion: "8052923"
    uid: 545d57b0-51f3-4f73-b8d8-d66736fb9e1e
  spec:
    clusterIP: 10.103.131.143
    clusterIPs:
    - 10.103.131.143
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
    selector:
      app: minio
      release: minio
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: minio
      meta.helm.sh/release-namespace: velero
    creationTimestamp: "2025-12-09T20:59:11Z"
    labels:
      app: minio
      app.kubernetes.io/managed-by: Helm
      chart: minio-5.4.0
      heritage: Helm
      release: minio
    name: minio-console
    namespace: velero
    resourceVersion: "8052918"
    uid: 921bf753-b5d0-42d3-8dca-ae7cac40ef5f
  spec:
    clusterIP: 10.103.160.124
    clusterIPs:
    - 10.103.160.124
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http
      port: 9001
      protocol: TCP
      targetPort: 9001
    selector:
      app: minio
      release: minio
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      meta.helm.sh/release-name: velero
      meta.helm.sh/release-namespace: velero
    creationTimestamp: "2025-12-09T21:02:38Z"
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      helm.sh/chart: velero-11.2.0
    name: velero
    namespace: velero
    resourceVersion: "8053671"
    uid: 0eae3d55-87f6-4f26-ae48-f533522548d1
  spec:
    clusterIP: 10.100.90.42
    clusterIPs:
    - 10.100.90.42
    internalTrafficPolicy: Cluster
    ipFamilies:
    - IPv4
    ipFamilyPolicy: SingleStack
    ports:
    - name: http-monitoring
      port: 8085
      protocol: TCP
      targetPort: http-monitoring
    selector:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/name: velero
      name: velero
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "4"
      meta.helm.sh/release-name: ingress-nginx
      meta.helm.sh/release-namespace: ingress-nginx
    creationTimestamp: "2025-12-09T20:55:04Z"
    generation: 4
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
      app.kubernetes.io/version: 1.14.1
      helm.sh/chart: ingress-nginx-4.14.1
    name: ingress-nginx-controller
    namespace: ingress-nginx
    resourceVersion: "29938827"
    uid: 85482b5e-ff51-46a9-a591-3f06673347eb
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-15T09:25:58+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: ingress-nginx
          app.kubernetes.io/part-of: ingress-nginx
          app.kubernetes.io/version: 1.14.1
          helm.sh/chart: ingress-nginx-4.14.1
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - /nginx-ingress-controller
          - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
          - --election-id=ingress-nginx-leader
          - --controller-class=k8s.io/ingress-nginx
          - --ingress-class=nginx
          - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
          - --validating-webhook=:8443
          - --validating-webhook-certificate=/usr/local/certificates/cert
          - --validating-webhook-key=/usr/local/certificates/key
          - --enable-metrics=true
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_PRELOAD
            value: /usr/local/lib/libmimalloc.so
          image: registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /wait-shutdown
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: controller
          ports:
          - containerPort: 80
            hostPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            hostPort: 443
            name: https
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 8443
            name: webhook
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 82
            runAsNonRoot: true
            runAsUser: 101
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/certificates/
            name: webhook-cert
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ingress-nginx
        serviceAccountName: ingress-nginx
        terminationGracePeriodSeconds: 300
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: webhook-cert
          secret:
            defaultMode: 420
            secretName: ingress-nginx-admission
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 5
    desiredNumberScheduled: 5
    numberAvailable: 5
    numberMisscheduled: 0
    numberReady: 5
    observedGeneration: 4
    updatedNumberScheduled: 5
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"app":"flannel","k8s-app":"flannel","tier":"node"},"name":"kube-flannel-ds","namespace":"kube-flannel"},"spec":{"selector":{"matchLabels":{"app":"flannel","k8s-app":"flannel"}},"template":{"metadata":{"labels":{"app":"flannel","k8s-app":"flannel","tier":"node"}},"spec":{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"kubernetes.io/os","operator":"In","values":["linux"]}]}]}}},"containers":[{"args":["--ip-masq","--kube-subnet-mgr"],"command":["/opt/bin/flanneld"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"EVENT_QUEUE_DEPTH","value":"5000"},{"name":"CONT_WHEN_CACHE_NOT_READY","value":"false"}],"image":"ghcr.io/flannel-io/flannel:v0.27.4","name":"kube-flannel","resources":{"requests":{"cpu":"100m","memory":"50Mi"}},"securityContext":{"capabilities":{"add":["NET_ADMIN","NET_RAW"]},"privileged":false},"volumeMounts":[{"mountPath":"/run/flannel","name":"run"},{"mountPath":"/etc/kube-flannel/","name":"flannel-cfg"},{"mountPath":"/run/xtables.lock","name":"xtables-lock"}]}],"hostNetwork":true,"initContainers":[{"args":["-f","/flannel","/opt/cni/bin/flannel"],"command":["cp"],"image":"ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1","name":"install-cni-plugin","volumeMounts":[{"mountPath":"/opt/cni/bin","name":"cni-plugin"}]},{"args":["-f","/etc/kube-flannel/cni-conf.json","/etc/cni/net.d/10-flannel.conflist"],"command":["cp"],"image":"ghcr.io/flannel-io/flannel:v0.27.4","name":"install-cni","volumeMounts":[{"mountPath":"/etc/cni/net.d","name":"cni"},{"mountPath":"/etc/kube-flannel/","name":"flannel-cfg"}]}],"priorityClassName":"system-node-critical","serviceAccountName":"flannel","tolerations":[{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/run/flannel"},"name":"run"},{"hostPath":{"path":"/opt/cni/bin"},"name":"cni-plugin"},{"hostPath":{"path":"/etc/cni/net.d"},"name":"cni"},{"configMap":{"name":"kube-flannel-cfg"},"name":"flannel-cfg"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"xtables-lock"}]}}}}
    creationTimestamp: "2025-10-29T10:55:17Z"
    generation: 1
    labels:
      app: flannel
      k8s-app: flannel
      tier: node
    name: kube-flannel-ds
    namespace: kube-flannel
    resourceVersion: "29939217"
    uid: dc2f3b90-3449-443a-b297-6a85f1f02101
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: flannel
        k8s-app: flannel
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: flannel
          k8s-app: flannel
          tier: node
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/os
                  operator: In
                  values:
                  - linux
        containers:
        - args:
          - --ip-masq
          - --kube-subnet-mgr
          command:
          - /opt/bin/flanneld
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: EVENT_QUEUE_DEPTH
            value: "5000"
          - name: CONT_WHEN_CACHE_NOT_READY
            value: "false"
          image: ghcr.io/flannel-io/flannel:v0.27.4
          imagePullPolicy: IfNotPresent
          name: kube-flannel
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
              - NET_RAW
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /run/flannel
            name: run
          - mountPath: /etc/kube-flannel/
            name: flannel-cfg
          - mountPath: /run/xtables.lock
            name: xtables-lock
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - args:
          - -f
          - /flannel
          - /opt/cni/bin/flannel
          command:
          - cp
          image: ghcr.io/flannel-io/flannel-cni-plugin:v1.8.0-flannel1
          imagePullPolicy: IfNotPresent
          name: install-cni-plugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/cni/bin
            name: cni-plugin
        - args:
          - -f
          - /etc/kube-flannel/cni-conf.json
          - /etc/cni/net.d/10-flannel.conflist
          command:
          - cp
          image: ghcr.io/flannel-io/flannel:v0.27.4
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cni/net.d
            name: cni
          - mountPath: /etc/kube-flannel/
            name: flannel-cfg
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: flannel
        serviceAccountName: flannel
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /run/flannel
            type: ""
          name: run
        - hostPath:
            path: /opt/cni/bin
            type: ""
          name: cni-plugin
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni
        - configMap:
            defaultMode: 420
            name: kube-flannel-cfg
          name: flannel-cfg
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 5
    desiredNumberScheduled: 5
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 5
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2025-10-29T10:54:38Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "29939302"
    uid: 8d9f43df-3467-418c-b6d0-13ba7748664e
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: registry.k8s.io/kube-proxy:v1.29.15
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 5
    desiredNumberScheduled: 5
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 5
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:22:47Z"
    generation: 1
    labels:
      longhorn.io/component: engine-image
      longhorn.io/engine-image: ei-3154f3aa
      longhorn.io/managed-by: longhorn-manager
    name: engine-image-ei-3154f3aa
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: longhorn.io/v1beta2
      blockOwnerDeletion: true
      kind: EngineImage
      name: ei-3154f3aa
      uid: 0557e962-23e3-4bbc-a000-7fb1ad26b95c
    resourceVersion: "29938495"
    uid: 6d65259d-8d6c-4a6e-bc7b-0be4f036838b
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        longhorn.io/component: engine-image
        longhorn.io/engine-image: ei-3154f3aa
    template:
      metadata:
        creationTimestamp: null
        labels:
          longhorn.io/component: engine-image
          longhorn.io/engine-image: ei-3154f3aa
        name: engine-image-ei-3154f3aa
        ownerReferences:
        - apiVersion: longhorn.io/v1beta2
          blockOwnerDeletion: true
          kind: EngineImage
          name: ei-3154f3aa
          uid: 0557e962-23e3-4bbc-a000-7fb1ad26b95c
      spec:
        containers:
        - args:
          - -c
          - diff /usr/local/bin/longhorn /data/longhorn > /dev/null 2>&1; if [ $?
            -ne 0 ]; then cp -p /usr/local/bin/longhorn /data/ && echo installed;
            fi && trap 'rm /data/longhorn* && echo cleaned up' EXIT && sleep infinity
          command:
          - /bin/bash
          image: longhornio/longhorn-engine:v1.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - /data/longhorn version --client-only
            failureThreshold: 3
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          name: engine-image-ei-3154f3aa
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - ls /data/longhorn && /data/longhorn version --client-only
            failureThreshold: 3
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data/
            name: data
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/longhorn/engine-binaries/longhornio-longhorn-engine-v1.10.1
            type: ""
          name: data
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 100%
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 3
    numberMisscheduled: 1
    numberReady: 3
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:24Z"
    generation: 1
    labels:
      longhorn.io/managed-by: longhorn-manager
    name: longhorn-csi-plugin
    namespace: longhorn-system
    resourceVersion: "29877344"
    uid: b37e2ff1-0cce-471e-85ac-03298aaf4f13
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: longhorn-csi-plugin
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: longhorn-csi-plugin
      spec:
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --kubelet-registration-path=/var/lib/kubelet/plugins/driver.longhorn.io/csi.sock
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          image: longhornio/csi-node-driver-registrar:v2.15.0-20251030
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/sh
                - -c
                - rm -rf /registration/driver.longhorn.io /registration/driver.longhorn.io-reg.sock
                  /csi//*
          name: node-driver-registrar
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
          - mountPath: /registration
            name: registration-dir
        - args:
          - --v=4
          - --csi-address=/csi/csi.sock
          image: longhornio/livenessprobe:v2.17.0-20251030
          imagePullPolicy: IfNotPresent
          name: longhorn-liveness-probe
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        - args:
          - longhorn-manager
          - -d
          - csi
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --drivername=driver.longhorn.io
          - --manager-url=http://longhorn-backend:9500/v1
          env:
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: longhornio/longhorn-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/sh
                - -c
                - rm -f /csi//*
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 9808
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          name: longhorn-csi-plugin
          ports:
          - containerPort: 9808
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          startupProbe:
            failureThreshold: 36
            httpGet:
              path: /healthz
              port: 9808
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 4
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
          - mountPath: /var/lib/kubelet/plugins/kubernetes.io/csi
            mountPropagation: Bidirectional
            name: kubernetes-csi-dir
          - mountPath: /var/lib/kubelet/pods
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /host/proc
            name: host-proc
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/kubernetes.io/csi
            type: DirectoryOrCreate
          name: kubernetes-csi-dir
        - hostPath:
            path: /var/lib/kubelet/plugins_registry
            type: DirectoryOrCreate
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
        - hostPath:
            path: /var/lib/kubelet/pods
            type: DirectoryOrCreate
          name: pods-mount-dir
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /proc
            type: ""
          name: host-proc
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 3
    numberMisscheduled: 1
    numberReady: 3
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:05Z"
    generation: 1
    labels:
      app: longhorn-manager
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
    name: longhorn-manager
    namespace: longhorn-system
    resourceVersion: "29932912"
    uid: 3978df76-cbfa-4bf8-808f-f13074ac837d
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: longhorn-manager
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: longhorn-manager
          app.kubernetes.io/instance: longhorn
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: longhorn
          app.kubernetes.io/version: v1.10.1
          helm.sh/chart: longhorn-1.10.1
      spec:
        containers:
        - command:
          - longhorn-manager
          - -d
          - daemon
          - --engine-image
          - longhornio/longhorn-engine:v1.10.1
          - --instance-manager-image
          - longhornio/longhorn-instance-manager:v1.10.1
          - --share-manager-image
          - longhornio/longhorn-share-manager:v1.10.1
          - --backing-image-manager-image
          - longhornio/backing-image-manager:v1.10.1
          - --support-bundle-manager-image
          - longhornio/support-bundle-kit:v0.0.71
          - --manager-image
          - longhornio/longhorn-manager:v1.10.1
          - --service-account
          - longhorn-service-account
          - --upgrade-version-check
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: longhornio/longhorn-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          name: longhorn-manager
          ports:
          - containerPort: 9500
            name: manager
            protocol: TCP
          - containerPort: 9502
            name: admission-wh
            protocol: TCP
          - containerPort: 9503
            name: recov-backend
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /v1/healthz
              port: 9502
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/boot/
            name: boot
            readOnly: true
          - mountPath: /host/dev/
            name: dev
          - mountPath: /host/proc/
            name: proc
            readOnly: true
          - mountPath: /host/etc/
            name: etc
            readOnly: true
          - mountPath: /var/lib/longhorn/
            mountPropagation: Bidirectional
            name: longhorn
          - mountPath: /tls-files/
            name: longhorn-grpc-tls
        - command:
          - sh
          - -c
          - echo share-manager image pulled && sleep infinity
          image: longhornio/longhorn-share-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          name: pre-pull-share-manager-image
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /boot/
            type: ""
          name: boot
        - hostPath:
            path: /dev/
            type: ""
          name: dev
        - hostPath:
            path: /proc/
            type: ""
          name: proc
        - hostPath:
            path: /etc/
            type: ""
          name: etc
        - hostPath:
            path: /var/lib/longhorn/
            type: ""
          name: longhorn
        - name: longhorn-grpc-tls
          secret:
            defaultMode: 420
            optional: true
            secretName: longhorn-grpc-tls
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 100%
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 3
    numberMisscheduled: 1
    numberReady: 3
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:22:09Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: loki
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: promtail
      app.kubernetes.io/version: 3.5.1
      helm.sh/chart: promtail-6.17.0
    name: loki-promtail
    namespace: observability
    resourceVersion: "29934077"
    uid: 37aa15c7-b45d-4b08-bd82-35dd8a7546b9
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: loki
        app.kubernetes.io/name: promtail
    template:
      metadata:
        annotations:
          checksum/config: 0f49fcd7a8fab642f9644e0a4d67b9f2bf9ce3e2cbf1f2ebfa7a301dbd59a7e0
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: loki
          app.kubernetes.io/name: promtail
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - -config.file=/etc/promtail/promtail.yaml
          env:
          - name: HOSTNAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: docker.io/grafana/promtail:3.5.1
          imagePullPolicy: IfNotPresent
          name: promtail
          ports:
          - containerPort: 3101
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/promtail
            name: config
          - mountPath: /run/promtail
            name: run
          - mountPath: /var/lib/docker/containers
            name: containers
            readOnly: true
          - mountPath: /var/log/pods
            name: pods
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsGroup: 0
          runAsUser: 0
        serviceAccount: loki-promtail
        serviceAccountName: loki-promtail
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: loki-promtail
        - hostPath:
            path: /run/promtail
            type: ""
          name: run
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: containers
        - hostPath:
            path: /var/log/pods
            type: ""
          name: pods
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 5
    desiredNumberScheduled: 5
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 5
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:46Z"
    generation: 1
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/part-of: prometheus-node-exporter
      app.kubernetes.io/version: 1.10.2
      helm.sh/chart: prometheus-node-exporter-4.49.2
      release: neural-hive-prometheus
    name: neural-hive-prometheus-prometheus-node-exporter
    namespace: observability
    resourceVersion: "29939184"
    uid: 39952527-5ea7-40b3-8c2e-4c91b41f9825
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: neural-hive-prometheus
        app.kubernetes.io/name: prometheus-node-exporter
    template:
      metadata:
        annotations:
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: neural-hive-prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: prometheus-node-exporter
          app.kubernetes.io/part-of: prometheus-node-exporter
          app.kubernetes.io/version: 1.10.2
          helm.sh/chart: prometheus-node-exporter-4.49.2
          jobLabel: node-exporter
          release: neural-hive-prometheus
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                  - fargate
                - key: type
                  operator: NotIn
                  values:
                  - virtual-kubelet
        automountServiceAccountToken: false
        containers:
        - args:
          - --path.procfs=/host/proc
          - --path.sysfs=/host/sys
          - --path.rootfs=/host/root
          - --path.udev.data=/host/root/run/udev/data
          - --web.listen-address=[$(HOST_IP)]:9100
          - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run/containerd/.+|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
          - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$
          env:
          - name: HOST_IP
            value: 0.0.0.0
          image: quay.io/prometheus/node-exporter:v1.10.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: node-exporter
          ports:
          - containerPort: 9100
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http-metrics
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/proc
            name: proc
            readOnly: true
          - mountPath: /host/sys
            name: sys
            readOnly: true
          - mountPath: /host/root
            mountPropagation: HostToContainer
            name: root
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        hostPID: true
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: neural-hive-prometheus-prometheus-node-exporter
        serviceAccountName: neural-hive-prometheus-prometheus-node-exporter
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /proc
            type: ""
          name: proc
        - hostPath:
            path: /sys
            type: ""
          name: sys
        - hostPath:
            path: /
            type: ""
          name: root
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 5
    desiredNumberScheduled: 5
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    numberUnavailable: 1
    observedGeneration: 1
    updatedNumberScheduled: 5
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      meta.helm.sh/release-name: velero
      meta.helm.sh/release-namespace: velero
    creationTimestamp: "2025-12-09T21:02:38Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      helm.sh/chart: velero-11.2.0
    name: node-agent
    namespace: velero
    resourceVersion: "29939650"
    uid: 42f5c37b-8e4a-4f71-8512-5cf1f66e6212
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: node-agent
    template:
      metadata:
        annotations:
          checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
          prometheus.io/path: /metrics
          prometheus.io/port: "8085"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: velero
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: velero
          helm.sh/chart: velero-11.2.0
          name: node-agent
          role: node-agent
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - node-agent
          - server
          command:
          - /velero
          env:
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: AWS_SHARED_CREDENTIALS_FILE
            value: /credentials/cloud
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /credentials/cloud
          - name: AZURE_CREDENTIALS_FILE
            value: /credentials/cloud
          - name: ALIBABA_CLOUD_CREDENTIALS_FILE
            value: /credentials/cloud
          image: velero/velero:v1.17.1
          imagePullPolicy: IfNotPresent
          name: node-agent
          ports:
          - containerPort: 8085
            name: http-monitoring
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /credentials
            name: cloud-credentials
          - mountPath: /host_pods
            mountPropagation: HostToContainer
            name: host-pods
          - mountPath: /host_plugins
            mountPropagation: HostToContainer
            name: host-plugins
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        serviceAccount: velero-server
        serviceAccountName: velero-server
        terminationGracePeriodSeconds: 3600
        volumes:
        - name: cloud-credentials
          secret:
            defaultMode: 420
            secretName: velero
        - hostPath:
            path: /var/lib/kubelet/pods
            type: ""
          name: host-pods
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: ""
          name: host-plugins
        - emptyDir: {}
          name: scratch
    updateStrategy:
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 1
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "11"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"approval-service","component":"governance","neuralhive/layer":"governance"},"name":"approval-service","namespace":"approval"},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"approval-service"}},"template":{"metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"8000","prometheus.io/scrape":"true"},"labels":{"app":"approval-service","component":"governance"}},"spec":{"containers":[{"envFrom":[{"configMapRef":{"name":"approval-service-config"}},{"secretRef":{"name":"approval-service-secrets"}}],"image":"ghcr.io/albinojimy/neural-hive-mind/approval-service:1.0.0","imagePullPolicy":"IfNotPresent","livenessProbe":{"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":30,"periodSeconds":30,"timeoutSeconds":10},"name":"approval-service","ports":[{"containerPort":8080,"name":"http","protocol":"TCP"},{"containerPort":8000,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/ready","port":8080},"initialDelaySeconds":20,"periodSeconds":10,"timeoutSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"200m","memory":"256Mi"}},"volumeMounts":[{"mountPath":"/app/schemas","name":"schemas","readOnly":true}]}],"volumes":[{"configMap":{"name":"cognitive-plan-schema"},"name":"schemas"}]}}}}
    creationTimestamp: "2026-01-19T13:58:55Z"
    generation: 14
    labels:
      app: approval-service
      app.kubernetes.io/managed-by: manual
      app.kubernetes.io/part-of: neural-hive-mind
      component: governance
      neural-hive.io/domain: core
      neural-hive.io/layer: cognitiva
      neuralhive/layer: governance
    name: approval-service
    namespace: approval
    resourceVersion: "29939301"
    uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: approval-service
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-02T16:38:00+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:10d7a42
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    conditions:
    - lastTransitionTime: "2026-02-02T15:29:10Z"
      lastUpdateTime: "2026-02-08T14:36:53Z"
      message: ReplicaSet "approval-service-586bb5bd7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T23:05:46Z"
      lastUpdateTime: "2026-02-13T23:05:46Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 14
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "5"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:49Z"
    generation: 5
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
    name: cert-manager
    namespace: cert-manager
    resourceVersion: "25557399"
    uid: d8af4579-4e68-406b-8534-7101364e6c31
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.2
          - --dns01-recursive-nameservers-only
          - --dns01-recursive-nameservers=162.159.44.25:53,108.162.194.2:53
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.19.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T20:42:49Z"
      lastUpdateTime: "2025-12-27T23:31:15Z"
      message: ReplicaSet "cert-manager-69c44b659d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:46:50Z"
      lastUpdateTime: "2026-02-01T13:46:50Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 5
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:49Z"
    generation: 4
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
    name: cert-manager-cainjector
    namespace: cert-manager
    resourceVersion: "22400308"
    uid: 1598322e-1468-4a27-a0ff-5f825d563505
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.19.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T20:42:49Z"
      lastUpdateTime: "2025-12-27T23:31:11Z"
      message: ReplicaSet "cert-manager-cainjector-6b8d568764" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-01-22T16:58:00Z"
      lastUpdateTime: "2026-01-22T16:58:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:49Z"
    generation: 4
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
    name: cert-manager-webhook
    namespace: cert-manager
    resourceVersion: "26775001"
    uid: fd995350-7df8-41d9-96cb-64b4efe208fb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.19.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T20:42:49Z"
      lastUpdateTime: "2025-12-27T23:32:04Z"
      message: ReplicaSet "cert-manager-webhook-8546d8cdff" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-05T05:50:29Z"
      lastUpdateTime: "2026-02-05T05:50:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: clickhouse-operator
      meta.helm.sh/release-namespace: clickhouse-operator
    creationTimestamp: "2025-11-20T13:27:41Z"
    generation: 4
    labels:
      app.kubernetes.io/instance: clickhouse-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/version: 0.25.5
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chop: 0.25.5
      clickhouse.altinity.com/chop-commit: 9ab22d8
      clickhouse.altinity.com/chop-date: 2025-10-24T08.40.12
      helm.sh/chart: altinity-clickhouse-operator-0.25.5
    name: clickhouse-operator-altinity-clickhouse-operator
    namespace: clickhouse-operator
    resourceVersion: "25557681"
    uid: a800c3b2-36ad-41ff-b287-c65b7bbc9d52
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: clickhouse-operator
        app.kubernetes.io/name: altinity-clickhouse-operator
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          checksum/confd-files: 5edfe9bdcd34fb98956d853ad29d712abd4894854a8135746b003d2b9c133577
          checksum/configd-files: c17d6658e0c99f87a8d73e095c45603c62ded9d66a0041aa588d83f1d66caeee
          checksum/files: e715c2eb8d54de48fe1a0b1ea2b2a5ee693cc53b6bc089374b49ac1e4e7a9095
          checksum/keeper-confd-files: 21795302d930e9eb1b9cb5b1020199582189134766632e46c162b3ada2649a92
          checksum/keeper-configd-files: d93fa1304ddab91942cfa7de911ada5fb02306b234bf5328938efbd78563f7bd
          checksum/keeper-templatesd-files: 6c0b64e4c96322864592ff1c6eb84ed9ddc4f96880c55878f3d15036e6d6b424
          checksum/keeper-usersd-files: 9b0fb56434072a9301b625c5b5857152d637e519ace77e54e0059f20df789e52
          checksum/templatesd-files: f32867861e9a45819ac6bf9ef21943190fcff1e00ff39f5045e98b98b37fdfb3
          checksum/usersd-files: a6af808e6d3a67c93fa43ab9443a22d35cd71fc1429a41dee8314a656b2577c6
          clickhouse-operator-metrics/port: "9999"
          clickhouse-operator-metrics/scrape: "true"
          kubectl.kubernetes.io/restartedAt: "2026-01-14T22:19:21+01:00"
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: clickhouse-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: altinity-clickhouse-operator
          app.kubernetes.io/version: 0.25.5
          helm.sh/chart: altinity-clickhouse-operator-0.25.5
      spec:
        affinity: {}
        containers:
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/clickhouse-operator:0.25.5
          imagePullPolicy: IfNotPresent
          name: altinity-clickhouse-operator
          ports:
          - containerPort: 9999
            name: op-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/metrics-exporter:0.25.5
          imagePullPolicy: IfNotPresent
          name: metrics-exporter
          ports:
          - containerPort: 8888
            name: ch-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: clickhouse-operator-altinity-clickhouse-operator
        serviceAccountName: clickhouse-operator-altinity-clickhouse-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-files
          name: etc-clickhouse-operator-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-confd-files
          name: etc-clickhouse-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-configd-files
          name: etc-clickhouse-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-templatesd-files
          name: etc-clickhouse-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-usersd-files
          name: etc-clickhouse-operator-usersd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-confd-files
          name: etc-keeper-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-configd-files
          name: etc-keeper-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-templatesd-files
          name: etc-keeper-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-usersd-files
          name: etc-keeper-operator-usersd-folder
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-20T13:27:41Z"
      lastUpdateTime: "2026-01-14T21:20:12Z"
      message: ReplicaSet "clickhouse-operator-altinity-clickhouse-operator-6599ffbd4c"
        has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:00Z"
      lastUpdateTime: "2026-02-01T13:47:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "9"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"github-runner"},"name":"github-runner","namespace":"github-runner"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"github-runner"}},"template":{"metadata":{"labels":{"app":"github-runner"}},"spec":{"containers":[{"env":[{"name":"DOCKER_TLS_CERTDIR","value":""}],"image":"docker:24-dind","name":"dind","resources":{"limits":{"cpu":"1","memory":"2Gi"},"requests":{"cpu":"250m","memory":"512Mi"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/docker","name":"docker-storage"},{"mountPath":"/runner/_work","name":"runner-work"},{"mountPath":"/etc/docker","name":"docker-config"}]},{"env":[{"name":"REPO_URL","valueFrom":{"configMapKeyRef":{"key":"RUNNER_REPOSITORY_URL","name":"github-runner-config"}}},{"name":"RUNNER_NAME","valueFrom":{"configMapKeyRef":{"key":"RUNNER_NAME","name":"github-runner-config"}}},{"name":"RUNNER_TOKEN","valueFrom":{"secretKeyRef":{"key":"RUNNER_TOKEN","name":"github-runner-secret"}}},{"name":"LABELS","valueFrom":{"configMapKeyRef":{"key":"RUNNER_LABELS","name":"github-runner-config"}}},{"name":"RUNNER_WORKDIR","valueFrom":{"configMapKeyRef":{"key":"RUNNER_WORKDIR","name":"github-runner-config"}}},{"name":"DISABLE_AUTO_UPDATE","value":"true"},{"name":"DOCKER_HOST","value":"tcp://localhost:2375"}],"image":"myoung34/github-runner:latest","name":"runner","resources":{"limits":{"cpu":"1","memory":"2Gi"},"requests":{"cpu":"250m","memory":"512Mi"}},"volumeMounts":[{"mountPath":"/runner/_work","name":"runner-work"}]}],"initContainers":[{"command":["sh","-c","mkdir -p /etc/docker\ncat \u003e /etc/docker/daemon.json \u003c\u003c 'EOF'\n{\n  \"insecure-registries\": [\"37.60.241.150:30500\"]\n}\nEOF\n"],"image":"busybox:1.36","name":"init-docker-config","volumeMounts":[{"mountPath":"/etc/docker","name":"docker-config"}]}],"serviceAccountName":"github-runner","volumes":[{"emptyDir":{"sizeLimit":"30Gi"},"name":"docker-storage"},{"emptyDir":{"sizeLimit":"20Gi"},"name":"runner-work"},{"emptyDir":{},"name":"docker-config"}]}}}}
    creationTimestamp: "2026-01-27T00:36:19Z"
    generation: 13
    labels:
      app: github-runner
    name: github-runner
    namespace: github-runner
    resourceVersion: "28896597"
    uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: github-runner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-02T22:37:23+01:00"
        creationTimestamp: null
        labels:
          app: github-runner
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: ACCESS_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    conditions:
    - lastTransitionTime: "2026-02-03T10:48:16Z"
      lastUpdateTime: "2026-02-03T11:34:34Z"
      message: ReplicaSet "github-runner-6f6bd8f49d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-11T08:36:46Z"
      lastUpdateTime: "2026-02-11T08:36:46Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 13
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "7"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"apicurio-registry","component":"kafka"},"name":"apicurio-registry","namespace":"kafka"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"apicurio-registry"}},"template":{"metadata":{"labels":{"app":"apicurio-registry","component":"kafka"}},"spec":{"containers":[{"env":[{"name":"REGISTRY_KAFKASQL_BOOTSTRAP_SERVERS","value":"neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"},{"name":"ENABLE_CCOMPAT_LEGACY_ID_MODE","value":"true"},{"name":"QUARKUS_HTTP_SSL_CERTIFICATE_FILES","value":"/etc/tls/tls.crt"},{"name":"QUARKUS_HTTP_SSL_CERTIFICATE_KEY_FILES","value":"/etc/tls/tls.key"},{"name":"QUARKUS_HTTP_SSL_PORT","value":"8443"},{"name":"QUARKUS_HTTP_INSECURE_REQUESTS","value":"disabled"},{"name":"QUARKUS_OTEL_ENABLED","value":"true"},{"name":"QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT","value":"http://otel-collector.observability.svc.cluster.local:4317"},{"name":"QUARKUS_MICROMETER_EXPORT_PROMETHEUS_ENABLED","value":"true"},{"name":"QUARKUS_MICROMETER_BINDER_JVM_ENABLED","value":"true"},{"name":"QUARKUS_MICROMETER_BINDER_HTTP_SERVER_ENABLED","value":"true"}],"image":"apicurio/apicurio-registry-kafkasql:2.5.11.Final","livenessProbe":{"httpGet":{"path":"/health/live","port":8443,"scheme":"HTTPS"},"initialDelaySeconds":30,"periodSeconds":30,"timeoutSeconds":10},"name":"apicurio-registry","ports":[{"containerPort":8443,"name":"https","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/health/ready","port":8443,"scheme":"HTTPS"},"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"100m","memory":"256Mi"}},"volumeMounts":[{"mountPath":"/etc/tls","name":"tls-certs","readOnly":true}]},{"command":["/bin/sh","-c","# Instalar dependencias minimas\npip install --quiet --no-cache-dir requests urllib3\n\n# Criar servidor HTTP de health check\ncat \u003e /tmp/health_server.py \u003c\u003c 'PYTHON_EOF'\nimport http.server\nimport socketserver\nimport json\nimport requests\nimport threading\nimport time\nimport urllib3\n\n# Disable SSL verification for localhost (self-signed cert)\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n# Estado global do health check\nhealth_state = {\n    \"schemas_ok\": False,\n    \"registry_ready\": False,\n    \"missing_schemas\": [],\n    \"last_check\": None\n}\n\nCRITICAL_SCHEMAS = [\"plans.ready-value\", \"execution.tickets-value\"]\nREGISTRY_URL = \"https://localhost:8443\"\n\ndef check_schemas():\n    \"\"\"Verifica schemas criticos periodicamente\"\"\"\n    global health_state\n    while True:\n        try:\n            # Verificar se registry esta pronto\n            try:\n                resp = requests.get(f\"{REGISTRY_URL}/health/ready\", timeout=5, verify=False)\n                health_state[\"registry_ready\"] = resp.status_code == 200\n            except:\n                health_state[\"registry_ready\"] = False\n                health_state[\"schemas_ok\"] = False\n                time.sleep(30)\n                continue\n\n            # Verificar schemas criticos\n            missing = []\n            for subject in CRITICAL_SCHEMAS:\n                try:\n                    resp = requests.get(\n                        f\"{REGISTRY_URL}/apis/ccompat/v6/subjects/{subject}/versions/latest\",\n                        timeout=5,\n                        verify=False\n                    )\n                    if resp.status_code != 200:\n                        missing.append(subject)\n                except:\n                    missing.append(subject)\n\n            health_state[\"missing_schemas\"] = missing\n            health_state[\"schemas_ok\"] = len(missing) == 0\n            health_state[\"last_check\"] = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n\n            status = \"OK\" if health_state[\"schemas_ok\"] else \"DEGRADED\"\n            print(f\"[{health_state['last_check']}] Schema check: {status}, missing: {missing}\")\n\n        except Exception as e:\n            print(f\"Error checking schemas: {e}\")\n\n        time.sleep(60)\n\nclass HealthHandler(http.server.BaseHTTPRequestHandler):\n    def log_message(self, format, *args):\n        pass  # Silenciar logs de requisicao\n\n    def do_GET(self):\n        if self.path == \"/health/schemas\":\n            if health_state[\"schemas_ok\"]:\n                self.send_response(200)\n                status = \"healthy\"\n            else:\n                self.send_response(503)\n                status = \"unhealthy\"\n\n            self.send_header(\"Content-Type\", \"application/json\")\n            self.end_headers()\n            response = {\n                \"status\": status,\n                \"registry_ready\": health_state[\"registry_ready\"],\n                \"schemas_ok\": health_state[\"schemas_ok\"],\n                \"missing_schemas\": health_state[\"missing_schemas\"],\n                \"critical_schemas\": CRITICAL_SCHEMAS,\n                \"last_check\": health_state[\"last_check\"]\n            }\n            self.wfile.write(json.dumps(response).encode())\n\n        elif self.path == \"/health/live\":\n            self.send_response(200)\n            self.send_header(\"Content-Type\", \"text/plain\")\n            self.end_headers()\n            self.wfile.write(b\"OK\")\n\n        else:\n            self.send_response(404)\n            self.end_headers()\n\nif __name__ == \"__main__\":\n    # Iniciar thread de verificacao de schemas\n    checker_thread = threading.Thread(target=check_schemas, daemon=True)\n    checker_thread.start()\n\n    # Iniciar servidor HTTP\n    PORT = 8090\n    with socketserver.TCPServer((\"\", PORT), HealthHandler) as httpd:\n        print(f\"Schema health server running on port {PORT}\")\n        httpd.serve_forever()\nPYTHON_EOF\n\npython /tmp/health_server.py\n"],"image":"python:3.11-alpine","livenessProbe":{"httpGet":{"path":"/health/live","port":8090},"initialDelaySeconds":10,"periodSeconds":30,"timeoutSeconds":5},"name":"schema-health-checker","ports":[{"containerPort":8090,"name":"health","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/health/schemas","port":8090},"initialDelaySeconds":30,"periodSeconds":30,"successThreshold":1,"timeoutSeconds":10},"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"20m","memory":"64Mi"}}}],"volumes":[{"name":"tls-certs","secret":{"secretName":"schema-registry-tls-secret"}}]}}}}
    creationTimestamp: "2025-10-30T09:23:25Z"
    generation: 9
    labels:
      app: apicurio-registry
      component: kafka
    name: apicurio-registry
    namespace: kafka
    resourceVersion: "27243901"
    uid: 03048f81-3343-47a1-b9da-86bc5e0615c2
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: apicurio-registry
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-30T23:42:13+01:00"
        creationTimestamp: null
        labels:
          app: apicurio-registry
          component: kafka
      spec:
        containers:
        - env:
          - name: REGISTRY_KAFKASQL_BOOTSTRAP_SERVERS
            value: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
          - name: ENABLE_CCOMPAT_LEGACY_ID_MODE
            value: "true"
          - name: QUARKUS_HTTP_SSL_CERTIFICATE_FILES
            value: /etc/tls/tls.crt
          - name: QUARKUS_HTTP_SSL_CERTIFICATE_KEY_FILES
            value: /etc/tls/tls.key
          - name: QUARKUS_HTTP_SSL_PORT
            value: "8443"
          - name: QUARKUS_HTTP_INSECURE_REQUESTS
            value: enabled
          - name: QUARKUS_OTEL_ENABLED
            value: "true"
          - name: QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://otel-collector.observability.svc.cluster.local:4317
          - name: QUARKUS_MICROMETER_EXPORT_PROMETHEUS_ENABLED
            value: "true"
          - name: QUARKUS_MICROMETER_BINDER_JVM_ENABLED
            value: "true"
          - name: QUARKUS_MICROMETER_BINDER_HTTP_SERVER_ENABLED
            value: "true"
          - name: QUARKUS_HTTP_PORT
            value: "8080"
          - name: REGISTRY_CCOMPAT_ENABLED
            value: "true"
          - name: REGISTRY_CCOMPAT-storage-enabled
            value: "true"
          image: apicurio/apicurio-registry-kafkasql:2.5.11.Final
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: apicurio-registry
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls
            name: tls-certs
            readOnly: true
        - command:
          - /bin/sh
          - -c
          - |
            # Instalar dependencias minimas
            pip install --quiet --no-cache-dir requests urllib3

            # Criar servidor HTTP de health check
            cat > /tmp/health_server.py << 'PYTHON_EOF'
            import http.server
            import socketserver
            import json
            import requests
            import threading
            import time
            import urllib3

            # Disable SSL verification for localhost (self-signed cert)
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

            # Estado global do health check
            health_state = {
                "schemas_ok": False,
                "registry_ready": False,
                "missing_schemas": [],
                "last_check": None
            }

            CRITICAL_SCHEMAS = ["plans.ready-value", "execution.tickets-value"]
            REGISTRY_URL = "https://localhost:8443"

            def check_schemas():
                """Verifica schemas criticos periodicamente"""
                global health_state
                while True:
                    try:
                        # Verificar se registry esta pronto
                        try:
                            resp = requests.get(f"{REGISTRY_URL}/health/ready", timeout=5, verify=False)
                            health_state["registry_ready"] = resp.status_code == 200
                        except:
                            health_state["registry_ready"] = False
                            health_state["schemas_ok"] = False
                            time.sleep(30)
                            continue

                        # Verificar schemas criticos
                        missing = []
                        for subject in CRITICAL_SCHEMAS:
                            try:
                                resp = requests.get(
                                    f"{REGISTRY_URL}/apis/ccompat/v6/subjects/{subject}/versions/latest",
                                    timeout=5,
                                    verify=False
                                )
                                if resp.status_code != 200:
                                    missing.append(subject)
                            except:
                                missing.append(subject)

                        health_state["missing_schemas"] = missing
                        health_state["schemas_ok"] = len(missing) == 0
                        health_state["last_check"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

                        status = "OK" if health_state["schemas_ok"] else "DEGRADED"
                        print(f"[{health_state['last_check']}] Schema check: {status}, missing: {missing}")

                    except Exception as e:
                        print(f"Error checking schemas: {e}")

                    time.sleep(60)

            class HealthHandler(http.server.BaseHTTPRequestHandler):
                def log_message(self, format, *args):
                    pass  # Silenciar logs de requisicao

                def do_GET(self):
                    if self.path == "/health/schemas":
                        if health_state["schemas_ok"]:
                            self.send_response(200)
                            status = "healthy"
                        else:
                            self.send_response(503)
                            status = "unhealthy"

                        self.send_header("Content-Type", "application/json")
                        self.end_headers()
                        response = {
                            "status": status,
                            "registry_ready": health_state["registry_ready"],
                            "schemas_ok": health_state["schemas_ok"],
                            "missing_schemas": health_state["missing_schemas"],
                            "critical_schemas": CRITICAL_SCHEMAS,
                            "last_check": health_state["last_check"]
                        }
                        self.wfile.write(json.dumps(response).encode())

                    elif self.path == "/health/live":
                        self.send_response(200)
                        self.send_header("Content-Type", "text/plain")
                        self.end_headers()
                        self.wfile.write(b"OK")

                    else:
                        self.send_response(404)
                        self.end_headers()

            if __name__ == "__main__":
                # Iniciar thread de verificacao de schemas
                checker_thread = threading.Thread(target=check_schemas, daemon=True)
                checker_thread.start()

                # Iniciar servidor HTTP
                PORT = 8090
                with socketserver.TCPServer(("", PORT), HealthHandler) as httpd:
                    print(f"Schema health server running on port {PORT}")
                    httpd.serve_forever()
            PYTHON_EOF

            python /tmp/health_server.py
          env:
          - name: QUARKUS_HTTP_INSECURE_REQUESTS
            value: enabled
          - name: QUARKUS_HTTP_PORT
            value: "8080"
          - name: REGISTRY_CCOMPAT_ENABLED
            value: "true"
          - name: REGISTRY_CCOMPAT-storage-enabled
            value: "true"
          image: python:3.11-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: 8090
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: schema-health-checker
          ports:
          - containerPort: 8090
            name: health
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 20m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-certs
          secret:
            defaultMode: 420
            secretName: schema-registry-tls-secret
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-05T05:50:36Z"
      lastUpdateTime: "2026-02-05T05:50:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-10-30T09:23:25Z"
      lastUpdateTime: "2026-02-06T15:58:21Z"
      message: ReplicaSet "apicurio-registry-69fbd98587" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 9
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-19T07:57:04Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: neural-hive-kafka
      app.kubernetes.io/managed-by: strimzi-cluster-operator
      app.kubernetes.io/name: entity-operator
      app.kubernetes.io/part-of: strimzi-neural-hive-kafka
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/component-type: entity-operator
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-entity-operator
    name: neural-hive-kafka-entity-operator
    namespace: kafka
    ownerReferences:
    - apiVersion: kafka.strimzi.io/v1beta2
      blockOwnerDeletion: false
      controller: false
      kind: Kafka
      name: neural-hive-kafka
      uid: 2e1371f6-7e80-48f3-aac2-828e0f1c91f4
    resourceVersion: "26775200"
    uid: bad3b5b4-a881-44ab-9416-3d5bec7567a3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        strimzi.io/cluster: neural-hive-kafka
        strimzi.io/kind: Kafka
        strimzi.io/name: neural-hive-kafka-entity-operator
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          strimzi.io/cluster-ca-cert-generation: "0"
          strimzi.io/cluster-ca-key-generation: "0"
          strimzi.io/server-cert-hash: 37244d10
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: neural-hive-kafka
          app.kubernetes.io/managed-by: strimzi-cluster-operator
          app.kubernetes.io/name: entity-operator
          app.kubernetes.io/part-of: strimzi-neural-hive-kafka
          strimzi.io/cluster: neural-hive-kafka
          strimzi.io/component-type: entity-operator
          strimzi.io/kind: Kafka
          strimzi.io/name: neural-hive-kafka-entity-operator
      spec:
        containers:
        - args:
          - /opt/strimzi/bin/topic_operator_run.sh
          env:
          - name: STRIMZI_RESOURCE_LABELS
            value: strimzi.io/cluster=neural-hive-kafka
          - name: STRIMZI_KAFKA_BOOTSTRAP_SERVERS
            value: neural-hive-kafka-kafka-bootstrap:9091
          - name: STRIMZI_NAMESPACE
            value: kafka
          - name: STRIMZI_SECURITY_PROTOCOL
            value: SSL
          - name: STRIMZI_TLS_ENABLED
            value: "true"
          - name: STRIMZI_GC_LOG_ENABLED
            value: "false"
          image: quay.io/strimzi/operator:0.48.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthy
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: topic-operator
          ports:
          - containerPort: 8080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          startupProbe:
            failureThreshold: 12
            httpGet:
              path: /healthy
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: strimzi-to-tmp
          - mountPath: /opt/topic-operator/custom-config/
            name: entity-topic-operator-metrics-and-logging
          - mountPath: /etc/eto-certs/
            name: eto-certs
          - mountPath: /etc/tls-sidecar/cluster-ca-certs/
            name: cluster-ca-certs
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: neural-hive-kafka-entity-operator
        serviceAccountName: neural-hive-kafka-entity-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: neural-hive-kafka-entity-topic-operator-config
          name: entity-topic-operator-metrics-and-logging
        - emptyDir:
            medium: Memory
            sizeLimit: 5Mi
          name: strimzi-to-tmp
        - name: eto-certs
          secret:
            defaultMode: 292
            secretName: neural-hive-kafka-entity-topic-operator-certs
        - name: cluster-ca-certs
          secret:
            defaultMode: 292
            secretName: neural-hive-kafka-cluster-ca-cert
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-26T09:44:02Z"
      lastUpdateTime: "2025-11-26T09:47:32Z"
      message: ReplicaSet "neural-hive-kafka-entity-operator-67df88987b" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-05T05:50:41Z"
      lastUpdateTime: "2026-02-05T05:50:41Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2025-10-29T11:22:05Z"
    generation: 3
    labels:
      app: strimzi
    name: strimzi-cluster-operator
    namespace: kafka
    resourceVersion: "26775259"
    uid: fa0cc085-65a7-46ee-99e9-e285ab9e5f4f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: strimzi-cluster-operator
        strimzi.io/kind: cluster-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: strimzi-cluster-operator
          strimzi.io/kind: cluster-operator
      spec:
        containers:
        - args:
          - /opt/strimzi/bin/cluster_operator_run.sh
          env:
          - name: STRIMZI_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS
            value: "120000"
          - name: STRIMZI_OPERATION_TIMEOUT_MS
            value: "300000"
          - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE
            value: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE
            value: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_KAFKA_IMAGES
            value: |
              4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
              4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_KAFKA_CONNECT_IMAGES
            value: |
              4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
              4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES
            value: |
              4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
              4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE
            value: quay.io/strimzi/operator:0.48.0
          - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE
            value: quay.io/strimzi/operator:0.48.0
          - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE
            value: quay.io/strimzi/operator:0.48.0
          - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE
            value: quay.io/strimzi/kafka-bridge:0.33.1
          - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE
            value: quay.io/strimzi/kaniko-executor:0.48.0
          - name: STRIMZI_DEFAULT_MAVEN_BUILDER
            value: quay.io/strimzi/maven-builder:0.48.0
          - name: STRIMZI_OPERATOR_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: STRIMZI_FEATURE_GATES
          - name: STRIMZI_LEADER_ELECTION_ENABLED
            value: "true"
          - name: STRIMZI_LEADER_ELECTION_LEASE_NAME
            value: strimzi-cluster-operator
          - name: STRIMZI_LEADER_ELECTION_LEASE_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: STRIMZI_LEADER_ELECTION_IDENTITY
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: quay.io/strimzi/operator:0.48.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthy
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: strimzi-cluster-operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 384Mi
            requests:
              cpu: 200m
              memory: 384Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: strimzi-tmp
          - mountPath: /opt/strimzi/custom-config/
            name: co-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: strimzi-cluster-operator
        serviceAccountName: strimzi-cluster-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 1Mi
          name: strimzi-tmp
        - configMap:
            defaultMode: 420
            name: strimzi-cluster-operator
          name: co-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-30T15:24:04Z"
      lastUpdateTime: "2025-12-30T15:25:17Z"
      message: ReplicaSet "strimzi-cluster-operator-fd565f467" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-05T05:50:54Z"
      lastUpdateTime: "2026-02-05T05:50:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "7"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"auth","app.kubernetes.io/instance":"keycloak","app.kubernetes.io/managed-by":"helm","app.kubernetes.io/name":"keycloak","app.kubernetes.io/part-of":"neural-hive-mind","app.kubernetes.io/version":"22.0.5","helm.sh/chart":"keycloak-0.1.0","neural-hive.io/component":"auth","neural-hive.io/data-owner":"team-security","neural-hive.io/layer":"security","neural-hive.io/sla-tier":"platinum"},"name":"keycloak","namespace":"keycloak"},"spec":{"replicas":2,"selector":{"matchLabels":{"app.kubernetes.io/instance":"keycloak","app.kubernetes.io/name":"keycloak"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":0},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"checksum/config":"7206792e292638a83360af2cccaf0cf0710804f5815079b6f460602bfcdb47f6","checksum/secret":"3859a7c073bbb858b1d0db6e1eb821fefc09594aadca3657309caed214199e81","neural-hive.io/data-classification":"confidential","neural-hive.io/monitoring":"enabled","prometheus.io/path":"/metrics","prometheus.io/port":"9000","prometheus.io/scrape":"true","sidecar.istio.io/inject":"true"},"labels":{"app.kubernetes.io/instance":"keycloak","app.kubernetes.io/name":"keycloak","neural-hive.io/component":"auth","neural-hive.io/layer":"security"}},"spec":{"affinity":{"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"podAffinityTerm":{"labelSelector":{"matchLabels":{"app.kubernetes.io/name":"keycloak"}},"topologyKey":"topology.kubernetes.io/zone"},"weight":100}]}},"containers":[{"args":["start","--import-realm"],"env":[{"name":"KEYCLOAK_ADMIN","valueFrom":{"secretKeyRef":{"key":"admin-user","name":"keycloak-secret"}}},{"name":"KEYCLOAK_ADMIN_PASSWORD","valueFrom":{"secretKeyRef":{"key":"admin-password","name":"keycloak-secret"}}},{"name":"KC_DB_PASSWORD","valueFrom":{"secretKeyRef":{"key":"password","name":"keycloak-postgresql"}}},{"name":"KC_HEALTH_ENABLED","value":"true"},{"name":"KC_METRICS_ENABLED","value":"true"},{"name":"KC_LOG_LEVEL","value":"INFO"},{"name":"JAVA_OPTS_APPEND","value":"-Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local"}],"image":"quay.io/keycloak/keycloak:22.0.5","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health/live","port":"http"},"initialDelaySeconds":300,"periodSeconds":30,"timeoutSeconds":5},"name":"keycloak","ports":[{"containerPort":8080,"name":"http","protocol":"TCP"},{"containerPort":8443,"name":"https","protocol":"TCP"},{"containerPort":9000,"name":"management","protocol":"TCP"},{"containerPort":9000,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/health/ready","port":"http"},"initialDelaySeconds":30,"periodSeconds":10,"timeoutSeconds":5},"resources":{"limits":{"cpu":2,"memory":"2Gi"},"requests":{"cpu":"500m","memory":"1Gi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":false},"startupProbe":{"failureThreshold":30,"httpGet":{"path":"/health","port":"http"},"initialDelaySeconds":30,"periodSeconds":10,"timeoutSeconds":5},"volumeMounts":[{"mountPath":"/opt/keycloak/conf/keycloak.conf","name":"keycloak-config","readOnly":true,"subPath":"keycloak.conf"},{"mountPath":"/opt/keycloak/data/import/realm.json","name":"realm-import","readOnly":true,"subPath":"realm.json"}]}],"initContainers":[{"command":["sh","-c","until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...'; sleep 2; done; echo 'Database is ready!'"],"image":"busybox:1.35","name":"wait-for-db","securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":false}}],"securityContext":{"fsGroup":1000,"runAsGroup":1000,"runAsNonRoot":true,"runAsUser":1000},"serviceAccountName":"keycloak","volumes":[{"configMap":{"name":"keycloak-config"},"name":"keycloak-config"},{"configMap":{"name":"keycloak-realm"},"name":"realm-import"}]}}}}
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T10:46:37Z"
    generation: 10
    labels:
      app.kubernetes.io/component: auth
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/managed-by: helm
      app.kubernetes.io/name: keycloak
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 22.0.5
      helm.sh/chart: keycloak-0.1.0
      neural-hive.io/component: auth
      neural-hive.io/data-owner: team-security
      neural-hive.io/layer: security
      neural-hive.io/sla-tier: platinum
    name: keycloak
    namespace: keycloak
    resourceVersion: "29939599"
    uid: 149d590b-3743-4f01-8e90-f52003e4766c
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: keycloak
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 7206792e292638a83360af2cccaf0cf0710804f5815079b6f460602bfcdb47f6
          checksum/secret: 3859a7c073bbb858b1d0db6e1eb821fefc09594aadca3657309caed214199e81
          neural-hive.io/data-classification: confidential
          neural-hive.io/monitoring: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "9000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: keycloak
          app.kubernetes.io/name: keycloak
          neural-hive.io/component: auth
          neural-hive.io/layer: security
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: keycloak
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - start
          - --import-realm
          env:
          - name: KEYCLOAK_ADMIN
            value: admin
          - name: KEYCLOAK_ADMIN_PASSWORD
            value: Admin123\!
          - name: KC_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: keycloak-postgresql
          - name: KC_HEALTH_ENABLED
            value: "true"
          - name: KC_METRICS_ENABLED
            value: "true"
          - name: KC_LOG_LEVEL
            value: INFO
          - name: JAVA_OPTS_APPEND
            value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
          image: quay.io/keycloak/keycloak:22.0.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: keycloak
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 9000
            name: management
            protocol: TCP
          - containerPort: 9000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/keycloak/conf/keycloak.conf
            name: keycloak-config
            readOnly: true
            subPath: keycloak.conf
          - mountPath: /opt/keycloak/data/import/realm.json
            name: realm-import
            readOnly: true
            subPath: realm.json
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...';
            sleep 2; done; echo 'Database is ready!'
          env:
          - name: KEYCLOAK_ADMIN
            value: admin
          - name: KEYCLOAK_ADMIN_PASSWORD
            value: Admin123\!
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          name: wait-for-db
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: keycloak
        serviceAccountName: keycloak
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: keycloak-config
          name: keycloak-config
        - configMap:
            defaultMode: 420
            name: keycloak-realm
          name: realm-import
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-06T20:50:00Z"
      lastUpdateTime: "2026-02-06T20:50:01Z"
      message: ReplicaSet "keycloak-5cdb4cbfd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T23:06:43Z"
      lastUpdateTime: "2026-02-13T23:06:43Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 10
    readyReplicas: 1
    replicas: 2
    unavailableReplicas: 1
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-29T10:54:38Z"
    generation: 1
    labels:
      k8s-app: kube-dns
    name: coredns
    namespace: kube-system
    resourceVersion: "29800170"
    uid: 8964e6ea-902c-498b-91ff-683f73cc79be
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2025-10-29T10:55:39Z"
      lastUpdateTime: "2025-10-29T10:55:39Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-10-29T10:54:50Z"
      lastUpdateTime: "2025-10-29T10:55:39Z"
      message: ReplicaSet "coredns-76f75df574" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: external-dns
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-12-10T08:53:21Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: external-dns
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: external-dns
      app.kubernetes.io/version: 0.19.0
      helm.sh/chart: external-dns-1.19.0
    name: external-dns
    namespace: kube-system
    resourceVersion: "29903092"
    uid: 740e3866-6e4d-4d6f-a4b5-235bbff9ca8c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: external-dns
        app.kubernetes.io/name: external-dns
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: external-dns
          app.kubernetes.io/name: external-dns
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --log-level=info
          - --log-format=text
          - --interval=1m
          - --source=ingress
          - --source=service
          - --policy=sync
          - --registry=txt
          - --txt-owner-id=neural-hive-k8s
          - --domain-filter=elysiumii.site
          - --provider=cloudflare
          env:
          - name: CF_API_TOKEN
            valueFrom:
              secretKeyRef:
                key: api-token
                name: cloudflare-api-token
          image: registry.k8s.io/external-dns/external-dns:v0.19.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 2
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: external-dns
          ports:
          - containerPort: 7979
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: external-dns
        serviceAccountName: external-dns
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-10T08:53:21Z"
      lastUpdateTime: "2025-12-10T14:00:21Z"
      message: ReplicaSet "external-dns-55c5bfbf97" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T21:01:26Z"
      lastUpdateTime: "2026-02-13T21:01:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"metrics-server"}},"strategy":{"rollingUpdate":{"maxUnavailable":0}},"template":{"metadata":{"labels":{"k8s-app":"metrics-server"}},"spec":{"containers":[{"args":["--cert-dir=/tmp","--secure-port=10250","--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname","--kubelet-use-node-status-port","--metric-resolution=15s"],"image":"registry.k8s.io/metrics-server/metrics-server:v0.8.0","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/livez","port":"https","scheme":"HTTPS"},"periodSeconds":10},"name":"metrics-server","ports":[{"containerPort":10250,"name":"https","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/readyz","port":"https","scheme":"HTTPS"},"initialDelaySeconds":20,"periodSeconds":10},"resources":{"requests":{"cpu":"100m","memory":"200Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":1000,"seccompProfile":{"type":"RuntimeDefault"}},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-dir"}]}],"nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","serviceAccountName":"metrics-server","volumes":[{"emptyDir":{},"name":"tmp-dir"}]}}}}
    creationTimestamp: "2025-11-26T09:48:15Z"
    generation: 2
    labels:
      k8s-app: metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "25557964"
    uid: 50027e81-9fd9-46bc-9b07-9bf36bf95ceb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: metrics-server
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --kubelet-insecure-tls
          image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-26T09:48:15Z"
      lastUpdateTime: "2025-11-26T09:48:50Z"
      message: ReplicaSet "metrics-server-59d465df9f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:17Z"
      lastUpdateTime: "2026-02-01T13:47:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"local-path-provisioner","namespace":"local-path-storage"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"local-path-provisioner"}},"template":{"metadata":{"labels":{"app":"local-path-provisioner"}},"spec":{"containers":[{"command":["local-path-provisioner","--debug","start","--config","/etc/config/config.json"],"env":[{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"rancher/local-path-provisioner:v0.0.24","imagePullPolicy":"IfNotPresent","name":"local-path-provisioner","volumeMounts":[{"mountPath":"/etc/config/","name":"config-volume"}]}],"serviceAccountName":"local-path-provisioner-service-account","volumes":[{"configMap":{"name":"local-path-config"},"name":"config-volume"}]}}}}
    creationTimestamp: "2025-10-29T11:00:49Z"
    generation: 2
    name: local-path-provisioner
    namespace: local-path-storage
    resourceVersion: "25557846"
    uid: a4326b79-278f-4d45-a631-27cb706c9e4f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: local-path-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.24
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-29T11:00:49Z"
      lastUpdateTime: "2025-11-21T08:30:02Z"
      message: ReplicaSet "local-path-provisioner-844bd8758f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:03Z"
      lastUpdateTime: "2026-02-01T13:47:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:23Z"
    generation: 2
    labels:
      app: csi-attacher
      longhorn.io/managed-by: longhorn-manager
    name: csi-attacher
    namespace: longhorn-system
    resourceVersion: "25557803"
    uid: b79461e0-0dd5-44e6-95b9-5a2a4da7b5ba
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-attacher
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: csi-attacher
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-attacher
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-attacher:v4.10.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          ports:
          - containerPort: 8000
            name: csi-attacher
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T21:23:23Z"
      lastUpdateTime: "2025-12-27T23:29:22Z"
      message: ReplicaSet "csi-attacher-57f8656cc6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:02Z"
      lastUpdateTime: "2026-02-01T13:47:02Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:23Z"
    generation: 2
    labels:
      app: csi-provisioner
      longhorn.io/managed-by: longhorn-manager
    name: csi-provisioner
    namespace: longhorn-system
    resourceVersion: "25557797"
    uid: 1b9e324c-9989-45b5-98be-6b5900f7e877
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-provisioner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: csi-provisioner
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-provisioner
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --default-fstype=ext4
          - --enable-capacity
          - --capacity-ownerref-level=2
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-provisioner:v5.3.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          ports:
          - containerPort: 8000
            name: csi-provisioner
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T21:23:23Z"
      lastUpdateTime: "2025-12-27T23:29:33Z"
      message: ReplicaSet "csi-provisioner-6f58d758d9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:02Z"
      lastUpdateTime: "2026-02-01T13:47:02Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:23Z"
    generation: 2
    labels:
      app: csi-resizer
      longhorn.io/managed-by: longhorn-manager
    name: csi-resizer
    namespace: longhorn-system
    resourceVersion: "25557663"
    uid: bead408c-7be4-483d-bcca-7af0e4b535d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-resizer
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: csi-resizer
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-resizer
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=false
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-resizer:v1.14.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          ports:
          - containerPort: 8000
            name: csi-resizer
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T21:23:24Z"
      lastUpdateTime: "2025-12-27T23:29:29Z"
      message: ReplicaSet "csi-resizer-755f48c7c9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:00Z"
      lastUpdateTime: "2026-02-01T13:47:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:24Z"
    generation: 2
    labels:
      app: csi-snapshotter
      longhorn.io/managed-by: longhorn-manager
    name: csi-snapshotter
    namespace: longhorn-system
    resourceVersion: "25557684"
    uid: 3feadb0f-876c-4ac3-ad47-23e95fea5d25
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-snapshotter
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: csi-snapshotter
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-snapshotter
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-snapshotter:v8.4.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          ports:
          - containerPort: 8000
            name: csi-snapshotter
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T21:23:24Z"
      lastUpdateTime: "2025-12-27T23:29:37Z"
      message: ReplicaSet "csi-snapshotter-7959bd58bb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:00Z"
      lastUpdateTime: "2026-02-01T13:47:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:06Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
    name: longhorn-driver-deployer
    namespace: longhorn-system
    resourceVersion: "25557828"
    uid: 05740e92-28a4-46a3-af32-61972d9760ad
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: longhorn-driver-deployer
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: longhorn-driver-deployer
          app.kubernetes.io/instance: longhorn
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: longhorn
          app.kubernetes.io/version: v1.10.1
          helm.sh/chart: longhorn-1.10.1
      spec:
        containers:
        - command:
          - longhorn-manager
          - -d
          - deploy-driver
          - --manager-image
          - longhornio/longhorn-manager:v1.10.1
          - --manager-url
          - http://longhorn-backend:9500/v1
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: CSI_ATTACHER_IMAGE
            value: longhornio/csi-attacher:v4.10.0-20251030
          - name: CSI_PROVISIONER_IMAGE
            value: longhornio/csi-provisioner:v5.3.0-20251030
          - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE
            value: longhornio/csi-node-driver-registrar:v2.15.0-20251030
          - name: CSI_RESIZER_IMAGE
            value: longhornio/csi-resizer:v1.14.0-20251030
          - name: CSI_SNAPSHOTTER_IMAGE
            value: longhornio/csi-snapshotter:v8.4.0-20251030
          - name: CSI_LIVENESS_PROBE_IMAGE
            value: longhornio/livenessprobe:v2.17.0-20251030
          - name: CSI_ATTACHER_REPLICA_COUNT
            value: "1"
          - name: CSI_PROVISIONER_REPLICA_COUNT
            value: "1"
          - name: CSI_RESIZER_REPLICA_COUNT
            value: "1"
          - name: CSI_SNAPSHOTTER_REPLICA_COUNT
            value: "1"
          image: longhornio/longhorn-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          name: longhorn-driver-deployer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - while [ $(curl -m 1 -s -o /dev/null -w "%{http_code}" http://longhorn-backend:9500/v1)
            != "200" ]; do echo waiting; sleep 2; done
          image: longhornio/longhorn-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          name: wait-longhorn-manager
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T21:22:06Z"
      lastUpdateTime: "2025-12-27T23:29:19Z"
      message: ReplicaSet "longhorn-driver-deployer-6bcdb9864b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:03Z"
      lastUpdateTime: "2026-02-01T13:47:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:06Z"
    generation: 3
    labels:
      app: longhorn-ui
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
    name: longhorn-ui
    namespace: longhorn-system
    resourceVersion: "25571929"
    uid: 77765e7f-80ec-4430-bde4-f4033b178234
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: longhorn-ui
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: longhorn-ui
          app.kubernetes.io/instance: longhorn
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: longhorn
          app.kubernetes.io/version: v1.10.1
          helm.sh/chart: longhorn-1.10.1
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - longhorn-ui
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: LONGHORN_MANAGER_IP
            value: http://longhorn-backend:9500
          - name: LONGHORN_UI_PORT
            value: "8000"
          image: longhornio/longhorn-ui:v1.10.1
          imagePullPolicy: IfNotPresent
          name: longhorn-ui
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/cache/nginx/
            name: nginx-cache
          - mountPath: /var/config/nginx/
            name: nginx-config
          - mountPath: /var/run/
            name: var-run
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-ui-service-account
        serviceAccountName: longhorn-ui-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: nginx-cache
        - emptyDir: {}
          name: nginx-config
        - emptyDir: {}
          name: var-run
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T21:22:06Z"
      lastUpdateTime: "2025-12-27T23:29:44Z"
      message: ReplicaSet "longhorn-ui-56bc6dd9cb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:03Z"
      lastUpdateTime: "2026-02-01T13:47:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "14"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"mlflow","namespace":"mlflow"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"mlflow"}},"template":{"metadata":{"labels":{"app":"mlflow"}},"spec":{"containers":[{"command":["mlflow","server","--host","0.0.0.0","--port","5000","--backend-store-uri","postgresql://mlflow:$(POSTGRES_PASSWORD)@mlflow-postgresql:5432/mlflow","--default-artifact-root","/mlflow/artifacts"],"env":[{"name":"POSTGRES_PASSWORD","valueFrom":{"secretKeyRef":{"key":"password","name":"mlflow-postgres-secret"}}}],"image":"ghcr.io/mlflow/mlflow:v2.9.2","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":5000},"initialDelaySeconds":0,"periodSeconds":30,"timeoutSeconds":5},"name":"mlflow","ports":[{"containerPort":5000,"name":"http"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":5000},"initialDelaySeconds":0,"periodSeconds":10,"timeoutSeconds":5},"resources":{"limits":{"cpu":"1000m","memory":"2Gi"},"requests":{"cpu":"200m","memory":"1Gi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]}},"startupProbe":{"failureThreshold":15,"httpGet":{"path":"/health","port":5000},"initialDelaySeconds":45,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":5},"volumeMounts":[{"mountPath":"/mlflow","name":"mlflow-data"}]}],"securityContext":{"fsGroup":1000,"runAsNonRoot":true,"runAsUser":1000},"volumes":[{"name":"mlflow-data","persistentVolumeClaim":{"claimName":"mlflow-artifacts-pvc"}}]}}}}
    creationTimestamp: "2025-10-30T13:28:31Z"
    generation: 21
    name: mlflow
    namespace: mlflow
    resourceVersion: "29742340"
    uid: 49476224-6d57-4990-86de-c482954529d1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: mlflow
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-10T22:39:04+01:00"
        creationTimestamp: null
        labels:
          app: mlflow
      spec:
        containers:
        - command:
          - mlflow
          - server
          - --host
          - 0.0.0.0
          - --port
          - "5000"
          - --backend-store-uri
          - sqlite:////mlflow/mlflow.db
          - --default-artifact-root
          - mlflow-artifacts:/
          - --artifacts-destination
          - /mlflow/artifacts
          - --serve-artifacts
          image: ghcr.io/mlflow/mlflow:v2.9.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mlflow
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mlflow
            name: mlflow-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - name: mlflow-data
          persistentVolumeClaim:
            claimName: mlflow-artifacts-pvc
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-01-09T22:17:34Z"
      lastUpdateTime: "2026-01-09T22:17:34Z"
      message: ReplicaSet "mlflow-6f55659b89" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T10:59:11Z"
      lastUpdateTime: "2026-02-13T10:59:11Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 21
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: mongodb
      meta.helm.sh/release-namespace: mongodb-cluster
    creationTimestamp: "2025-11-21T08:26:32Z"
    generation: 4
    labels:
      app.kubernetes.io/component: mongodb
      app.kubernetes.io/instance: mongodb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: mongodb-18.1.10
    name: mongodb
    namespace: mongodb-cluster
    resourceVersion: "26435412"
    uid: b704676e-047e-47de-908b-64c64e2d7441
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: mongodb
        app.kubernetes.io/instance: mongodb
        app.kubernetes.io/name: mongodb
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:28:42+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: mongodb
          app.kubernetes.io/instance: mongodb
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: mongodb
          app.kubernetes.io/version: 8.2.2
          helm.sh/chart: mongodb-18.1.10
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: mongodb
                    app.kubernetes.io/instance: mongodb
                    app.kubernetes.io/name: mongodb
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: MONGODB_ROOT_USER
            value: root
          - name: MONGODB_ROOT_PASSWORD_FILE
            value: /opt/bitnami/mongodb/secrets/mongodb-root-password
          - name: OPENSSL_FIPS
            value: "yes"
          - name: ALLOW_EMPTY_PASSWORD
            value: "no"
          - name: MONGODB_SYSTEM_LOG_VERBOSITY
            value: "0"
          - name: MONGODB_DISABLE_SYSTEM_LOG
            value: "no"
          - name: MONGODB_DISABLE_JAVASCRIPT
            value: "no"
          - name: MONGODB_ENABLE_JOURNAL
            value: "yes"
          - name: MONGODB_PORT_NUMBER
            value: "27017"
          - name: MONGODB_ENABLE_IPV6
            value: "no"
          - name: MONGODB_ENABLE_DIRECTORY_PER_DB
            value: "no"
          image: registry-1.docker.io/bitnami/mongodb:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bitnami/scripts/ping-mongodb.sh
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10
          name: mongodb
          ports:
          - containerPort: 27017
            name: mongodb
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bitnami/scripts/readiness-probe.sh
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/mongodb/conf
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/mongodb/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /opt/bitnami/mongodb/logs
            name: empty-dir
            subPath: app-logs-dir
          - mountPath: /.mongodb
            name: empty-dir
            subPath: mongosh-home
          - mountPath: /bitnami/mongodb
            name: datadir
          - mountPath: /bitnami/scripts
            name: common-scripts
          - mountPath: /opt/bitnami/mongodb/secrets
            name: mongodb-secrets
        - args:
          - "export MONGODB_ROOT_PASSWORD=\"$(< $MONGODB_ROOT_PASSWORD_FILE)\"\n/bin/mongodb_exporter
            \ --collector.diagnosticdata --collector.replicasetstatus --compatible-mode
            --mongodb.direct-connect --mongodb.global-conn-pool --web.listen-address
            \":9216\" --mongodb.uri \"mongodb://$MONGODB_ROOT_USER:$(echo $MONGODB_ROOT_PASSWORD
            | sed -r \"s/@/%40/g;s/:/%3A/g\")@$(hostname -s):27017/admin?\" \n"
          command:
          - /bin/bash
          - -ec
          env:
          - name: MONGODB_ROOT_USER
            value: root
          - name: MONGODB_ROOT_PASSWORD_FILE
            value: /opt/bitnami/mongodb/secrets/mongodb-root-password
          - name: OPENSSL_FIPS
            value: "yes"
          - name: GODEBUG
            value: fips140=on
          image: registry-1.docker.io/bitnami/mongodb-exporter:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: metrics
            timeoutSeconds: 10
          name: metrics
          ports:
          - containerPort: 9216
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/mongodb/secrets
            name: mongodb-secrets
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - args:
          - -ec
          - |
            ln -sf /dev/stdout "/opt/bitnami/mongodb/logs/mongodb.log"
          command:
          - /bin/bash
          env:
          - name: OPENSSL_FIPS
            value: "yes"
          image: registry-1.docker.io/bitnami/mongodb:latest
          imagePullPolicy: IfNotPresent
          name: log-dir
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/mongodb/logs
            name: empty-dir
            subPath: app-logs-dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: mongodb
        serviceAccountName: mongodb
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            defaultMode: 360
            name: mongodb-common-scripts
          name: common-scripts
        - name: mongodb-secrets
          secret:
            defaultMode: 420
            secretName: mongodb
        - name: datadir
          persistentVolumeClaim:
            claimName: mongodb
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-21T08:26:32Z"
      lastUpdateTime: "2025-12-27T23:35:51Z"
      message: ReplicaSet "mongodb-677c7746c4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-04T05:02:59Z"
      lastUpdateTime: "2026-02-04T05:02:59Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      email: support@mongodb.com
      meta.helm.sh/release-name: mongodb-operator
      meta.helm.sh/release-namespace: mongodb-operator
    creationTimestamp: "2025-11-19T08:11:07Z"
    generation: 1
    labels:
      app.kubernetes.io/managed-by: Helm
      owner: mongodb
    name: mongodb-kubernetes-operator
    namespace: mongodb-operator
    resourceVersion: "23970748"
    uid: 0c462e29-9327-492b-9b6d-27a7dd9eab1e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: mongodb-kubernetes-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: mongodb-kubernetes-operator
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: name
                  operator: In
                  values:
                  - mongodb-kubernetes-operator
              topologyKey: kubernetes.io/hostname
        containers:
        - command:
          - /usr/local/bin/entrypoint
          env:
          - name: WATCH_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_NAME
            value: mongodb-kubernetes-operator
          - name: AGENT_IMAGE
            value: quay.io/mongodb/mongodb-agent-ubi:108.0.6.8796-1
          - name: VERSION_UPGRADE_HOOK_IMAGE
            value: quay.io/mongodb/mongodb-kubernetes-operator-version-upgrade-post-start-hook:1.0.10
          - name: READINESS_PROBE_IMAGE
            value: quay.io/mongodb/mongodb-kubernetes-readinessprobe:1.0.23
          - name: MONGODB_IMAGE
            value: mongodb-community-server
          - name: MONGODB_REPO_URL
            value: docker.io/mongodb
          - name: MDB_IMAGE_TYPE
            value: ubi8
          image: quay.io/mongodb/mongodb-kubernetes-operator:0.13.0
          imagePullPolicy: Always
          name: mongodb-kubernetes-operator
          resources:
            limits:
              cpu: 1100m
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 2000
        serviceAccount: mongodb-kubernetes-operator
        serviceAccountName: mongodb-kubernetes-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-19T08:11:08Z"
      lastUpdateTime: "2025-11-19T08:11:08Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2025-11-19T08:11:08Z"
      lastUpdateTime: "2025-11-19T08:11:16Z"
      message: ReplicaSet "mongodb-kubernetes-operator-7669bd584" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"postgres-sla"},"name":"postgres-sla","namespace":"neural-hive-data"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"postgres-sla"}},"template":{"metadata":{"labels":{"app":"postgres-sla"}},"spec":{"containers":[{"envFrom":[{"secretRef":{"name":"postgres-sla-credentials"}}],"image":"postgres:15-alpine","livenessProbe":{"exec":{"command":["pg_isready","-U","sla_user","-d","sla_management"]},"initialDelaySeconds":30,"periodSeconds":10},"name":"postgres","ports":[{"containerPort":5432}],"readinessProbe":{"exec":{"command":["pg_isready","-U","sla_user","-d","sla_management"]},"initialDelaySeconds":5,"periodSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"100m","memory":"256Mi"}},"volumeMounts":[{"mountPath":"/var/lib/postgresql/data","name":"postgres-data"}]}],"volumes":[{"name":"postgres-data","persistentVolumeClaim":{"claimName":"postgres-sla-data"}}]}}}}
    creationTimestamp: "2026-01-01T13:02:31Z"
    generation: 3
    labels:
      app: postgres-sla
    name: postgres-sla
    namespace: neural-hive-data
    resourceVersion: "28924659"
    uid: d4d2f739-4592-4152-8cdc-006654a9206e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: postgres-sla
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgres-sla
      spec:
        containers:
        - envFrom:
          - secretRef:
              name: postgres-sla-credentials
          image: postgres:15-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - pg_isready
              - -U
              - sla_user
              - -d
              - sla_management
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: postgres
          ports:
          - containerPort: 5432
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - pg_isready
              - -U
              - sla_user
              - -d
              - sla_management
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data
            name: postgres-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: postgres-sla-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-01-01T13:02:31Z"
      lastUpdateTime: "2026-01-01T13:03:17Z"
      message: ReplicaSet "postgres-sla-6d998b77dd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-11T10:25:13Z"
      lastUpdateTime: "2026-02-11T10:25:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "80"
      kubernetes.io/change-cause: kubectl set image deployment/analyst-agents analyst-agents=ghcr.io/albinojimy/neural-hive-mind/analyst-agents:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: analyst-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:50Z"
    generation: 104
    labels:
      app.kubernetes.io/component: analyst-agents
      app.kubernetes.io/instance: analyst-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: analyst-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.2.0
      helm.sh/chart: analyst-agents-1.0.0
      neural-hive.io/component: analyst-agents
      neural-hive.io/domain: insight-generation
      neural-hive.io/layer: analise
    name: analyst-agents
    namespace: neural-hive
    resourceVersion: "29550646"
    uid: 547f60a6-d8cf-4938-bcef-7c8f66273e39
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: analyst-agents
        app.kubernetes.io/name: analyst-agents
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 2fcd3a724c65b72bb9230ccc74c77ede86a18c8082703be8cd7739eaabaef9ed
          kubectl.kubernetes.io/restartedAt: "2026-02-11T12:11:44+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: analyst-agents
          app.kubernetes.io/name: analyst-agents
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - analyst-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: analyst-agents
          image: ghcr.io/albinojimy/neural-hive-mind/analyst-agents:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: analyst-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: analyst-agents
        serviceAccountName: analyst-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-11T11:15:46Z"
      lastUpdateTime: "2026-02-11T11:15:46Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-12T23:03:02Z"
      lastUpdateTime: "2026-02-12T23:04:39Z"
      message: ReplicaSet "analyst-agents-5fb8dfd548" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 104
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"approval-service","app.kubernetes.io/name":"approval-service","app.kubernetes.io/part-of":"neural-hive-mind","neural-hive.io/layer":"governance"},"name":"approval-service","namespace":"neural-hive"},"spec":{"replicas":1,"selector":{"matchLabels":{"app.kubernetes.io/name":"approval-service"}},"template":{"metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"8080","prometheus.io/scrape":"true"},"labels":{"app.kubernetes.io/name":"approval-service"}},"spec":{"containers":[{"env":[{"name":"KAFKA_BOOTSTRAP_SERVERS","value":"neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"},{"name":"MONGODB_URI","value":"mongodb://mongodb.mongodb.svc.cluster.local:27017"},{"name":"ENVIRONMENT","value":"production"},{"name":"LOG_LEVEL","value":"INFO"}],"image":"37.60.241.150:30500/approval-service:1.0.0","livenessProbe":{"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":40,"periodSeconds":30,"timeoutSeconds":10},"name":"approval-service","ports":[{"containerPort":8080,"name":"http"}],"readinessProbe":{"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":20,"periodSeconds":10,"timeoutSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"512Mi"},"requests":{"cpu":"100m","memory":"256Mi"}}}]}}}}
    creationTimestamp: "2026-01-25T18:46:27Z"
    generation: 18
    labels:
      app.kubernetes.io/component: approval-service
      app.kubernetes.io/managed-by: manual
      app.kubernetes.io/name: approval-service
      app.kubernetes.io/part-of: neural-hive-mind
      neural-hive.io/domain: core
      neural-hive.io/layer: cognitiva
    name: approval-service
    namespace: neural-hive
    resourceVersion: "28728478"
    uid: 80dc1fc5-763b-4368-ac9b-bc4a892f80f5
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/name: approval-service
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-02T13:35:28+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: approval-service
      spec:
        containers:
        - env:
          - name: KAFKA_BOOTSTRAP_SERVERS
            value: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
          - name: KAFKA_SECURITY_PROTOCOL
            value: PLAINTEXT
          - name: MONGODB_URI
            value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017
          - name: ENVIRONMENT
            value: development
          - name: LOG_LEVEL
            value: INFO
          - name: APPROVAL_SERVICE_REQUIRE_AUTH
            value: "false"
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 40
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-01-25T18:59:41Z"
      lastUpdateTime: "2026-02-08T20:18:16Z"
      message: ReplicaSet "approval-service-77d49c4bd8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-10T11:42:59Z"
      lastUpdateTime: "2026-02-10T11:42:59Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 18
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "23"
      kubernetes.io/change-cause: kubectl set image deployment/code-forge code-forge=ghcr.io/albinojimy/neural-hive-mind/code-forge:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: code-forge
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:13:05Z"
    generation: 84
    labels:
      app.kubernetes.io/component: code-forge
      app.kubernetes.io/instance: code-forge
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: code-forge
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: code-forge-0.1.0
      neural-hive.io/component: code-forge
      neural-hive.io/domain: code-generation
      neural-hive.io/layer: execution
    name: code-forge
    namespace: neural-hive
    resourceVersion: "29938797"
    uid: 421a6526-c232-4ad5-8b56-0434aad00e85
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: code-forge
        app.kubernetes.io/name: code-forge
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 04a5ce5560e16ceba234402ba0d53ac1085437c75b520af6e7f9fad989923a22
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:57:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: code-forge
          app.kubernetes.io/name: code-forge
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - code-forge
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: code-forge-config
          - secretRef:
              name: code-forge-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/code-forge:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: code-forge
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 2200m
              memory: 3584Mi
            requests:
              cpu: 800m
              memory: 1536Mi
          startupProbe:
            failureThreshold: 25
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: code-forge
        serviceAccountName: code-forge
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: code-forge
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastTransitionTime: "2026-02-13T21:06:34Z"
      lastUpdateTime: "2026-02-13T21:07:37Z"
      message: ReplicaSet "code-forge-5b5cb4f857" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T23:04:07Z"
      lastUpdateTime: "2026-02-13T23:04:07Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 84
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "86"
      meta.helm.sh/release-name: consensus-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:07:47Z"
    generation: 113
    labels:
      app.kubernetes.io/component: consensus-aggregator
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: consensus-engine
      app.kubernetes.io/version: 1.0.8
      helm.sh/chart: consensus-engine-0.1.8
      neural-hive.io/component: consensus-engine
      neural-hive.io/domain: consensus
      neural-hive.io/layer: cognitiva
    name: consensus-engine
    namespace: neural-hive
    resourceVersion: "29319755"
    uid: bf70ceed-d792-46a9-81bd-6fe61640872b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: consensus-engine
        app.kubernetes.io/name: consensus-engine
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: ac2b81a4a6b9013bfc630ab07ea38dedf7dab374bf25c85afb35c19760cc8b22
          checksum/secret: c53e1323967446c56b88efd346e3f29681d2227762fc8bfcd3c4e2f8efc1074f
          kubectl.kubernetes.io/restartedAt: "2026-02-08T12:22:06+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: consensus-aggregator
          app.kubernetes.io/instance: consensus-engine
          app.kubernetes.io/name: consensus-engine
          neural-hive.io/domain: consensus
          neural-hive.io/layer: cognitiva
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - consensus-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: PYTHONUNBUFFERED
            value: "1"
          envFrom:
          - configMapRef:
              name: consensus-engine-config
          - secretRef:
              name: consensus-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/consensus-engine:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: consensus-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 1500m
              memory: 1280Mi
            requests:
              cpu: 400m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/models/consolidated_decision.py
            name: decision-hotfix
            subPath: consolidated_decision.py
          - mountPath: /app/src/services/consensus_orchestrator.py
            name: consensus-orchestrator-hotfix
            subPath: consensus_orchestrator.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: consensus-engine
        serviceAccountName: consensus-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: consensus-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: consensus-engine-decision-hotfix
          name: decision-hotfix
        - configMap:
            defaultMode: 420
            name: consensus-orchestrator-hotfix
          name: consensus-orchestrator-hotfix
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-11T11:15:35Z"
      lastUpdateTime: "2026-02-12T07:24:00Z"
      message: ReplicaSet "consensus-engine-6fbd8d768f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-12T08:40:35Z"
      lastUpdateTime: "2026-02-12T08:40:35Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 113
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "60"
      kubernetes.io/change-cause: kubectl set image deployment/execution-ticket-service
        execution-ticket-service=ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: execution-ticket-service
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-26T09:58:21Z"
    generation: 68
    labels:
      app.kubernetes.io/component: execution-ticket-service
      app.kubernetes.io/instance: execution-ticket-service
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: execution-ticket-service
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: execution-ticket-service-1.0.0
      neural-hive.io/component: execution-ticket-service
      neural-hive.io/domain: ticket-management
      neural-hive.io/layer: orchestration
    name: execution-ticket-service
    namespace: neural-hive
    resourceVersion: "29905091"
    uid: aa96a2cf-93b3-4c80-aef5-cf9bf104dc52
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: execution-ticket-service
        app.kubernetes.io/name: execution-ticket-service
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: e723785aafd84b52629c9ca2364d1dc4f4a8026ebfdf1e80f2a4949c98d47679
          kubectl.kubernetes.io/restartedAt: "2026-02-06T09:07:47+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: execution-ticket-service
          app.kubernetes.io/name: execution-ticket-service
          neural-hive.io/layer: orchestration
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - execution-ticket-service
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRES_PASSWORD
                name: execution-ticket-service-secrets
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: MONGODB_URI
                name: execution-ticket-service-secrets
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: JWT_SECRET_KEY
                name: execution-ticket-service-secrets
          - name: MAX_CONNECTION_RETRIES
            value: "5"
          - name: INITIAL_RETRY_DELAY_SECONDS
            value: "1"
          envFrom:
          - configMapRef:
              name: execution-ticket-service-config
          image: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: execution-ticket-service
          ports:
          - containerPort: 50052
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: execution-ticket-service
        serviceAccountName: execution-ticket-service
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: execution-ticket-service
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-11T11:04:55Z"
      lastUpdateTime: "2026-02-11T11:04:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-13T21:06:28Z"
      lastUpdateTime: "2026-02-13T21:07:12Z"
      message: ReplicaSet "execution-ticket-service-54988fd44f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 68
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "30"
      kubernetes.io/change-cause: kubectl set image deployment/explainability-api
        explainability-api=ghcr.io/albinojimy/neural-hive-mind/explainability-api:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: explainability-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:17Z"
    generation: 51
    labels:
      app.kubernetes.io/component: explainability-api
      app.kubernetes.io/instance: explainability-api
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: explainability-api
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: explainability-api-1.0.0
      neural-hive.io/component: explainability-api
      neural-hive.io/domain: audit-transparency
      neural-hive.io/layer: transparencia
    name: explainability-api
    namespace: neural-hive
    resourceVersion: "29904796"
    uid: 045477db-d366-4841-9d55-7728e40ff35f
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: explainability-api
        app.kubernetes.io/name: explainability-api
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: e3837d2b6b9dc131551798941b991c7d75898ac1636b0a9fd69e738edec79648
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:30+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: explainability-api
          app.kubernetes.io/name: explainability-api
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - explainability-api
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: explainability-api
          image: ghcr.io/albinojimy/neural-hive-mind/explainability-api:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: explainability-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: explainability-api
        serviceAccountName: explainability-api
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    conditions:
    - lastTransitionTime: "2026-02-12T08:29:54Z"
      lastUpdateTime: "2026-02-12T08:29:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-12T06:49:30Z"
      lastUpdateTime: "2026-02-13T21:06:38Z"
      message: ReplicaSet "explainability-api-55bf67c995" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 51
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"feedback-collection-service","namespace":"neural-hive"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"feedback-collection"}},"template":{"metadata":{"labels":{"app":"feedback-collection"}},"spec":{"containers":[{"args":["/scripts/feedback_service.py"],"command":["python3"],"image":"python:3.11-slim","name":"feedback-service","ports":[{"containerPort":8080}],"volumeMounts":[{"mountPath":"/scripts","name":"scripts"}]}],"volumes":[{"configMap":{"name":"feedback-script"},"name":"scripts"}]}}}}
    creationTimestamp: "2026-02-08T14:39:31Z"
    generation: 4
    name: feedback-collection-service
    namespace: neural-hive
    resourceVersion: "27898430"
    uid: 5dafa6f1-e370-4852-a6c6-f3147c03cb91
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: feedback-collection
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-08T15:44:50+01:00"
        creationTimestamp: null
        labels:
          app: feedback-collection
      spec:
        containers:
        - args:
          - /scripts/feedback_service.py
          command:
          - python3
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: feedback-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /scripts
            name: scripts
          - mountPath: /app/ui
            name: ui
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: feedback-script
          name: scripts
        - configMap:
            defaultMode: 420
            name: feedback-ui
          name: ui
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-08T14:40:26Z"
      lastUpdateTime: "2026-02-08T14:40:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-08T14:39:32Z"
      lastUpdateTime: "2026-02-08T14:45:22Z"
      message: ReplicaSet "feedback-collection-service-db8c9f98b" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "8"
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T14:13:19Z"
    generation: 8
    labels:
      app.kubernetes.io/component: gateway-intencoes
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: gateway-intencoes
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: gateway-intencoes-0.1.0
      neural-hive.io/component: gateway-intencoes
      neural-hive.io/layer: application
    name: gateway-intencoes
    namespace: neural-hive
    resourceVersion: "29910193"
    uid: cebf7b2b-588d-4ba8-919f-2ef3e85ac583
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: gateway-intencoes
        app.kubernetes.io/name: gateway-intencoes
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
          checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
          kubectl.kubernetes.io/restartedAt: "2026-02-13T22:12:06+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: gateway-intencoes
          app.kubernetes.io/name: gateway-intencoes
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - gateway-intencoes
                topologyKey: kubernetes.io/hostname
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - gateway-intencoes
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: gateway-intencoes-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: gateway-intencoes-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_URL
            valueFrom:
              configMapKeyRef:
                key: schema_registry_url
                name: gateway-intencoes-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: gateway-intencoes-config
          - name: KAFKA_BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                key: kafka_batch_size
                name: gateway-intencoes-config
          - name: KAFKA_LINGER_MS
            valueFrom:
              configMapKeyRef:
                key: kafka_linger_ms
                name: gateway-intencoes-config
          - name: KAFKA_COMPRESSION_TYPE
            valueFrom:
              configMapKeyRef:
                key: kafka_compression_type
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CERTIFICATE_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_certificate_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_KEY_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_key_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_VERIFY_MODE
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_verify_mode
                name: gateway-intencoes-config
          - name: KAFKA_SSL_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_protocol
                name: gateway-intencoes-config
          - name: KAFKA_SASL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_MECHANISM
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_mechanism
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_token_endpoint
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_SCOPE
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_scope
                name: gateway-intencoes-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: gateway-intencoes-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: gateway-intencoes-config
          - name: REDIS_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_max_connections
                name: gateway-intencoes-config
          - name: REDIS_POOL_SIZE
            valueFrom:
              configMapKeyRef:
                key: redis_pool_size
                name: gateway-intencoes-config
          - name: REDIS_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_timeout
                name: gateway-intencoes-config
          - name: REDIS_RETRY_ON_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_retry_on_timeout
                name: gateway-intencoes-config
          - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_connection_pool_max_connections
                name: gateway-intencoes-config
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERT_REQS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_cert_reqs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CA_CERTS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_ca_certs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERTFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_certfile
                name: gateway-intencoes-config
          - name: REDIS_SSL_KEYFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_keyfile
                name: gateway-intencoes-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: gateway-intencoes-secret
          - name: KEYCLOAK_URL
            valueFrom:
              configMapKeyRef:
                key: keycloak_url
                name: gateway-intencoes-config
          - name: KEYCLOAK_REALM
            valueFrom:
              configMapKeyRef:
                key: keycloak_realm
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                key: keycloak_client_id
                name: gateway-intencoes-config
          - name: JWKS_URI
            valueFrom:
              configMapKeyRef:
                key: jwks_uri
                name: gateway-intencoes-config
          - name: TOKEN_VALIDATION_ENABLED
            valueFrom:
              configMapKeyRef:
                key: token_validation_enabled
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: keycloak-client-secret
                name: gateway-intencoes-secret
          - name: SCHEMA_REGISTRY_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_enabled
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_TLS_VERIFY
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_verify
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: schema_registry_ssl_ca_location
                name: gateway-intencoes-config
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt-secret-key
                name: gateway-intencoes-secret
          - name: ASR_MODEL_NAME
            value: base
          - name: ASR_DEVICE
            value: cpu
          - name: ASR_LAZY_LOADING
            value: "true"
          - name: ASR_MODEL_CACHE_DIR
            value: /app/models/whisper
          - name: NLU_LANGUAGE_MODEL
            value: pt_core_news_sm
          - name: NLU_MODEL_CACHE_DIR
            value: /app/models/spacy
          - name: NLU_CONFIDENCE_THRESHOLD
            value: "0.6"
          - name: NLU_CONFIDENCE_THRESHOLD_STRICT
            value: "0.75"
          - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
            value: "false"
          - name: NLU_RULES_CONFIG_PATH
            value: /app/config/nlu_rules.yaml
          - name: NLU_ROUTING_THRESHOLD_HIGH
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_high
                name: gateway-intencoes-config
          - name: NLU_ROUTING_THRESHOLD_LOW
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_low
                name: gateway-intencoes-config
          - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_use_adaptive_for_decisions
                name: gateway-intencoes-config
          - name: JAEGER_ENDPOINT
            value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
          - name: ENABLE_TRACING
            value: "true"
          - name: OTEL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: otel_enabled
                name: gateway-intencoes-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: gateway-intencoes-config
          - name: RATE_LIMIT_ENABLED
            valueFrom:
              configMapKeyRef:
                key: rate_limit_enabled
                name: gateway-intencoes-config
          - name: RATE_LIMIT_REQUESTS_PER_MINUTE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_requests_per_minute
                name: gateway-intencoes-config
          - name: RATE_LIMIT_BURST_SIZE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_burst_size
                name: gateway-intencoes-config
          - name: RATE_LIMIT_FAIL_OPEN
            valueFrom:
              configMapKeyRef:
                key: rate_limit_fail_open
                name: gateway-intencoes-config
          - name: RATE_LIMIT_TENANT_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_tenant_overrides
                name: gateway-intencoes-config
          - name: RATE_LIMIT_USER_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_user_overrides
                name: gateway-intencoes-config
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: allow_insecure_http_endpoints
                name: gateway-intencoes-config
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 25
            successThreshold: 1
            timeoutSeconds: 8
          name: gateway-intencoes
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 40
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/models
            name: model-cache
          - mountPath: /etc/ssl/certs/schema-registry-ca.crt
            name: schema-registry-ca-cert
            readOnly: true
            subPath: ca.crt
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: gateway-intencoes
        serviceAccountName: gateway-intencoes
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gateway-intencoes-models-pvc
        - name: schema-registry-ca-cert
          secret:
            defaultMode: 420
            items:
            - key: ca.crt
              path: ca.crt
            secretName: schema-registry-tls-secret
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-13T14:38:53Z"
      lastUpdateTime: "2026-02-13T14:38:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-13T21:22:13Z"
      lastUpdateTime: "2026-02-13T21:22:13Z"
      message: ReplicaSet "gateway-intencoes-644bd4fd7d" has timed out progressing.
      reason: ProgressDeadlineExceeded
      status: "False"
      type: Progressing
    observedGeneration: 8
    readyReplicas: 1
    replicas: 2
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "65"
      kubernetes.io/change-cause: kubectl set image deployment/guard-agents guard-agents=ghcr.io/albinojimy/neural-hive-mind/guard-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: guard-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:42Z"
    generation: 85
    labels:
      app.kubernetes.io/component: guard-agents
      app.kubernetes.io/instance: guard-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: guard-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: guard-agents-1.0.0
      neural-hive.io/component: guard-agents
      neural-hive.io/domain: security-validation
      neural-hive.io/layer: resilience
    name: guard-agents
    namespace: neural-hive
    resourceVersion: "29939564"
    uid: 49650100-d08b-42a9-abba-338560021c7f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: guard-agents
        app.kubernetes.io/name: guard-agents
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 39da66c3b643acffb0000cabbc3dbe7e2337c017c9fb87f3c40f93e8d1072470
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:20+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: guard-agents
          app.kubernetes.io/name: guard-agents
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - guard-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          envFrom:
          - configMapRef:
              name: guard-agents-config
          image: ghcr.io/albinojimy/neural-hive-mind/guard-agents:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: guard-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/readiness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health/startup
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: guard-agents
        serviceAccountName: guard-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: guard-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-13T21:06:07Z"
      lastUpdateTime: "2026-02-13T21:06:39Z"
      message: ReplicaSet "guard-agents-7d66b9cfdf" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T23:06:38Z"
      lastUpdateTime: "2026-02-13T23:06:38Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 85
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "13"
      kubernetes.io/change-cause: kubectl set image deployment/memory-layer-api memory-layer-api=ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T23:18:03Z"
    generation: 16
    labels:
      app.kubernetes.io/component: memory-layer-api
      app.kubernetes.io/instance: memory-layer-api
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: memory-layer-api
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: memory-layer-api-0.1.0
      neural-hive.io/component: memory-layer-api
      neural-hive.io/domain: memory-management
      neural-hive.io/layer: conhecimento-dados
    name: memory-layer-api
    namespace: neural-hive
    resourceVersion: "29904853"
    uid: 0dd57997-ddf1-4813-bcb3-8cc17eb101b7
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: memory-layer-api
        app.kubernetes.io/name: memory-layer-api
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 1a18eae495b8c1273517645f9937d85d2194a61a4be3f628fa7c2f92a1b83ea8
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
          kubectl.kubernetes.io/restartedAt: "2026-02-12T08:50:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: memory-layer-api
          app.kubernetes.io/name: memory-layer-api
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - memory-layer-api
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          - name: DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: memory-layer-api-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: memory-layer-api-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis_password
                name: memory-layer-api-secrets
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: memory-layer-api-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: memory-layer-api-config
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: memory-layer-api-secrets
          - name: MONGODB_DATABASE
            valueFrom:
              configMapKeyRef:
                key: mongodb_database
                name: memory-layer-api-config
          - name: MONGODB_RETENTION_DAYS
            valueFrom:
              configMapKeyRef:
                key: mongodb_retention_days
                name: memory-layer-api-config
          - name: NEO4J_URI
            valueFrom:
              configMapKeyRef:
                key: neo4j_uri
                name: memory-layer-api-config
          - name: NEO4J_USER
            valueFrom:
              configMapKeyRef:
                key: neo4j_user
                name: memory-layer-api-config
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: memory-layer-api-secrets
          - name: NEO4J_DATABASE
            valueFrom:
              configMapKeyRef:
                key: neo4j_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_RETENTION_MONTHS
            valueFrom:
              configMapKeyRef:
                key: clickhouse_retention_months
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: PROMETHEUS_PORT
            valueFrom:
              configMapKeyRef:
                key: prometheus_port
                name: memory-layer-api-config
          - name: ENABLE_CACHE
            valueFrom:
              configMapKeyRef:
                key: enable_cache
                name: memory-layer-api-config
          - name: ENABLE_LINEAGE_TRACKING
            valueFrom:
              configMapKeyRef:
                key: enable_lineage_tracking
                name: memory-layer-api-config
          - name: ENABLE_QUALITY_MONITORING
            valueFrom:
              configMapKeyRef:
                key: enable_quality_monitoring
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            valueFrom:
              configMapKeyRef:
                key: enable_realtime_sync
                name: memory-layer-api-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_DLQ_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_dlq_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: memory-layer-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: memory-layer-api
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    conditions:
    - lastTransitionTime: "2026-02-12T08:29:59Z"
      lastUpdateTime: "2026-02-12T08:29:59Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-12T08:18:40Z"
      lastUpdateTime: "2026-02-13T21:06:43Z"
      message: ReplicaSet "memory-layer-api-b6cdc88d6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 16
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T23:18:03Z"
    generation: 8
    labels:
      app.kubernetes.io/component: sync-consumer
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: memory-layer-api-sync-consumer
      app.kubernetes.io/part-of: memory-layer-api
      neural-hive.io/component: memory-sync-consumer
      neural-hive.io/layer: conhecimento-dados
    name: memory-layer-api-sync-consumer
    namespace: neural-hive
    resourceVersion: "29331310"
    uid: 59aaf4d4-0d1e-4e40-8c1e-f80b82eabc6e
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/name: memory-layer-api-sync-consumer
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 1a18eae495b8c1273517645f9937d85d2194a61a4be3f628fa7c2f92a1b83ea8
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
          kubectl.kubernetes.io/restartedAt: "2026-02-12T00:19:22+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: sync-consumer
          app.kubernetes.io/name: memory-layer-api-sync-consumer
          neural-hive.io/component: memory-sync-consumer
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - memory-layer-api-sync-consumer
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - command:
          - python
          - -m
          - src.consumers.sync_event_consumer
          env:
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            value: "true"
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:7f171c7
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - python
              - -c
              - import sys; sys.exit(0)
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sync-consumer
          readinessProbe:
            exec:
              command:
              - python
              - -c
              - import sys; sys.exit(0)
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2026-02-12T08:18:41Z"
      lastUpdateTime: "2026-02-12T08:18:41Z"
      message: ReplicaSet "memory-layer-api-sync-consumer-599669c84b" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-12T09:23:27Z"
      lastUpdateTime: "2026-02-12T09:23:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 8
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T20:37:10Z"
    generation: 2
    labels:
      app: opa
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      component: policy-engine
      helm.sh/chart: orchestrator-dynamic-1.0.0
      layer: orchestration
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: opa
    namespace: neural-hive
    resourceVersion: "29905814"
    uid: 9fbb2c75-8492-4886-b64d-77a4541cce02
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: opa
        component: policy-engine
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: opa
          component: policy-engine
          layer: orchestration
      spec:
        containers:
        - args:
          - run
          - --server
          - --addr=0.0.0.0:8181
          - --config-file=/config/config.yaml
          - --log-level=info
          - --log-format=json
          - --bundle
          - /policies
          image: openpolicyagent/opa:0.60.0-rootless
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8181
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: opa
          ports:
          - containerPort: 8181
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health?bundle=true
              port: 8181
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /policies
            name: policies
            readOnly: true
          - mountPath: /config
            name: opa-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: opa-policies
          name: policies
        - configMap:
            defaultMode: 420
            name: orchestrator-dynamic-opa-config
          name: opa-config
  status:
    conditions:
    - lastTransitionTime: "2026-02-13T21:08:27Z"
      lastUpdateTime: "2026-02-13T21:08:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-13T21:08:27Z"
      lastUpdateTime: "2026-02-13T21:08:27Z"
      message: ReplicaSet "opa-5bd4887759" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "9"
      meta.helm.sh/release-name: optimizer-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:58Z"
    generation: 48
    labels:
      app.kubernetes.io/component: optimizer-agents
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: optimizer-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.13
      helm.sh/chart: optimizer-agents-1.0.0
      neural-hive.io/component: optimizer-agents
      neural-hive.io/domain: continuous-improvement
      neural-hive.io/layer: otimizacao
    name: optimizer-agents
    namespace: neural-hive
    resourceVersion: "29908643"
    uid: 47c66b7c-7c5a-4e23-b2d4-c7f24762f29a
  spec:
    progressDeadlineSeconds: 600
    replicas: 3
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: optimizer-agents
        app.kubernetes.io/name: optimizer-agents
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
          kubectl.kubernetes.io/restartedAt: "2026-02-13T22:06:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: optimizer-agents
          app.kubernetes.io/name: optimizer-agents
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - optimizer-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: optimizer-agents
          image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: optimizer-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: optimizer-agents
        serviceAccountName: optimizer-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    conditions:
    - lastTransitionTime: "2026-02-12T09:20:05Z"
      lastUpdateTime: "2026-02-12T09:20:05Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    - lastTransitionTime: "2026-02-13T21:16:51Z"
      lastUpdateTime: "2026-02-13T21:16:51Z"
      message: ReplicaSet "optimizer-agents-698f8d4fbd" has timed out progressing.
      reason: ProgressDeadlineExceeded
      status: "False"
      type: Progressing
    observedGeneration: 48
    replicas: 4
    unavailableReplicas: 4
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "145"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T22:58:06Z"
    generation: 169
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: orchestrator-dynamic-1.0.0
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic
    namespace: neural-hive
    resourceVersion: "29909503"
    uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 1e7c444f34d431cf0f1422f54bfd59651b5347fc2838858437c1f282fc8e24a0
          checksum/secret: cec7ee8d589c1c780ec94fa19000d2ebe1c14f5a587ce9608ba95e67293eabfd
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          vault.hashicorp.com/agent-inject: "true"
          vault.hashicorp.com/agent-inject-secret-kafka: secret/data/orchestrator/kafka
          vault.hashicorp.com/agent-inject-secret-mongodb: secret/data/orchestrator/mongodb
          vault.hashicorp.com/agent-inject-secret-postgres: database/creds/temporal-orchestrator
          vault.hashicorp.com/agent-inject-template-kafka: |-
            {{`{{- with secret "secret/data/orchestrator/kafka" -}}
            export KAFKA_SASL_USERNAME="{{ .Data.data.username }}"
            export KAFKA_SASL_PASSWORD="{{ .Data.data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-mongodb: |-
            {{`{{- with secret "secret/data/orchestrator/mongodb" -}}
            export MONGODB_URI="{{ .Data.data.uri }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-postgres: |-
            {{`{{- with secret "database/creds/temporal-orchestrator" -}}
            export POSTGRES_USER="{{ .Data.username }}"
            export POSTGRES_PASSWORD="{{ .Data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/role: orchestrator-dynamic
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          - name: REQUESTS_CA_BUNDLE
            value: /etc/ssl/certs/ca-certificates.crt
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /run/spire/sockets
            name: spire-agent-socket
            readOnly: true
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - hostPath:
            path: /run/spire/sockets
            type: Directory
          name: spire-agent-socket
        - configMap:
            defaultMode: 420
            name: orchestrator-dynamic-spire-agent-config
          name: spire-agent-config
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-12T07:52:25Z"
      lastUpdateTime: "2026-02-12T07:52:25Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-13T21:19:48Z"
      lastUpdateTime: "2026-02-13T21:19:48Z"
      message: ReplicaSet "orchestrator-dynamic-7fcb47fdc8" has timed out progressing.
      reason: ProgressDeadlineExceeded
      status: "False"
      type: Progressing
    observedGeneration: 169
    readyReplicas: 1
    replicas: 2
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "68"
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:22Z"
    generation: 80
    labels:
      app.kubernetes.io/component: queen-agent
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: queen-agent
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: queen-agent-0.1.0
      neural-hive.io/component: queen-agent
      neural-hive.io/domain: hive-coordination
      neural-hive.io/layer: coordination
    name: queen-agent
    namespace: neural-hive
    resourceVersion: "29908446"
    uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-11T22:17:51Z"
      lastUpdateTime: "2026-02-11T22:17:51Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-13T21:16:13Z"
      lastUpdateTime: "2026-02-13T21:16:13Z"
      message: ReplicaSet "queen-agent-5b96b4c956" has timed out progressing.
      reason: ProgressDeadlineExceeded
      status: "False"
      type: Progressing
    observedGeneration: 80
    readyReplicas: 1
    replicas: 2
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "23"
      kubernetes.io/change-cause: kubectl set image deployment/scout-agents scout-agents=ghcr.io/albinojimy/neural-hive-mind/scout-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: scout-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:08:31Z"
    generation: 92
    labels:
      app.kubernetes.io/component: scout-agents
      app.kubernetes.io/instance: scout-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: scout-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: scout-agents-0.1.0
      neural-hive.io/component: scout-agents
      neural-hive.io/domain: reconnaissance
      neural-hive.io/layer: exploration
    name: scout-agents
    namespace: neural-hive
    resourceVersion: "29551105"
    uid: b2ad9658-b88a-4626-9af2-d7e1102329de
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: scout-agents
        app.kubernetes.io/name: scout-agents
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: dfe9128ad43e25ff61a0ea9ebb43571dd52ee8975ba58a152935f55a60d19d8a
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: scout-agents
          app.kubernetes.io/name: scout-agents
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: scout-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRES_PORT
            value: "5432"
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_SSL_MODE
            value: disable
          envFrom:
          - configMapRef:
              name: scout-agents
          image: ghcr.io/albinojimy/neural-hive-mind/scout-agents:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: scout-agents
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 2Gi
            requests:
              cpu: 400m
              memory: 768Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: scout-agents
        serviceAccountName: scout-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: scout-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-12T09:21:24Z"
      lastUpdateTime: "2026-02-12T09:21:24Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-12T23:06:14Z"
      lastUpdateTime: "2026-02-12T23:06:14Z"
      message: ReplicaSet "scout-agents-575db6d7b7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 92
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "23"
      kubernetes.io/change-cause: kubectl set image deployment/self-healing-engine
        self-healing-engine=ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: self-healing-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:43Z"
    generation: 82
    labels:
      app.kubernetes.io/component: self-healing-engine
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: self-healing-engine
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: self-healing-engine-1.0.0
      neural-hive.io/component: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
    name: self-healing-engine
    namespace: neural-hive
    resourceVersion: "29939327"
    uid: eb06ab5b-6b26-4240-8ab6-7b871959d7ce
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: self-healing-engine
        app.kubernetes.io/name: self-healing-engine
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 797d649f766885eaa7c8926451997da40a3aeab5dc4d0eb324951047fa246914
          checksum/secret: 247768ac8899c81bf336763f0488784ed9da5327a122af8ff4d9a0640f59b8fc
          kubectl.kubernetes.io/restartedAt: "2026-01-28T00:05:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: self-healing
          app.kubernetes.io/instance: self-healing-engine
          app.kubernetes.io/name: self-healing-engine
          neural-hive.io/domain: remediation
          neural-hive.io/layer: resilience
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - self-healing-engine
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: self-healing-engine-config
          - secretRef:
              name: self-healing-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: self-healing-engine
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 1536Mi
            requests:
              cpu: 300m
              memory: 512Mi
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/playbooks
            name: playbooks
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: self-healing-engine
        serviceAccountName: self-healing-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: self-healing-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: self-healing-engine-playbooks
            optional: true
          name: playbooks
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-13T21:06:18Z"
      lastUpdateTime: "2026-02-13T21:08:35Z"
      message: ReplicaSet "self-healing-engine-68b6f6f4cb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T23:05:50Z"
      lastUpdateTime: "2026-02-13T23:05:50Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 82
    readyReplicas: 1
    replicas: 2
    unavailableReplicas: 1
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "86"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"34","meta.helm.sh/release-name":"semantic-translation-engine","meta.helm.sh/release-namespace":"neural-hive"},"creationTimestamp":"2026-01-09T23:15:07Z","generation":36,"labels":{"app.kubernetes.io/component":"semantic-translation-engine","app.kubernetes.io/instance":"semantic-translation-engine","app.kubernetes.io/managed-by":"Helm","app.kubernetes.io/name":"semantic-translation-engine","app.kubernetes.io/part-of":"neural-hive-mind","app.kubernetes.io/version":"1.0.0","helm.sh/chart":"semantic-translation-engine-0.1.0","neural-hive.io/component":"semantic-translator","neural-hive.io/domain":"plan-generation","neural-hive.io/layer":"cognitiva"},"name":"semantic-translation-engine","namespace":"neural-hive","resourceVersion":"21502947","uid":"69c48d4d-9204-4fcd-b298-9414bdf44afb"},"spec":{"progressDeadlineSeconds":600,"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app.kubernetes.io/instance":"semantic-translation-engine","app.kubernetes.io/name":"semantic-translation-engine"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":0},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"checksum/config":"19bb4dfafc71443df1ad9f92e2168ef3cd0965ae7ff3630cc4d2f0dcca66dc21","checksum/secret":"eccdb2c6d1574f67d3897324952fce671e1a52af85447f25134399afcf7cdcd1","kubectl.kubernetes.io/restartedAt":"2026-01-19T22:36:00+01:00","prometheus.io/path":"/metrics","prometheus.io/port":"8000","prometheus.io/scrape":"true"},"creationTimestamp":null,"labels":{"app.kubernetes.io/component":"semantic-translator","app.kubernetes.io/instance":"semantic-translation-engine","app.kubernetes.io/name":"semantic-translation-engine","neural-hive.io/domain":"plan-generation","neural-hive.io/layer":"cognitiva"}},"spec":{"affinity":{"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"podAffinityTerm":{"labelSelector":{"matchLabels":{"app.kubernetes.io/name":"semantic-translation-engine"}},"topologyKey":"topology.kubernetes.io/zone"},"weight":100}]}},"containers":[{"envFrom":[{"configMapRef":{"name":"semantic-translation-engine-config"}},{"secretRef":{"name":"semantic-translation-engine-secrets"}}],"image":"37.60.241.150:30500/semantic-translation-engine:1.2.8","imagePullPolicy":"Always","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/health","port":8000,"scheme":"HTTP"},"periodSeconds":10,"successThreshold":1,"timeoutSeconds":5},"name":"semantic-translation-engine","ports":[{"containerPort":8000,"name":"http","protocol":"TCP"},{"containerPort":9090,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"failureThreshold":3,"httpGet":{"path":"/ready","port":8000,"scheme":"HTTP"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":3},"resources":{"limits":{"cpu":"50m","memory":"2Gi"},"requests":{"cpu":"50m","memory":"512Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":false},"startupProbe":{"failureThreshold":30,"httpGet":{"path":"/health","port":8000,"scheme":"HTTP"},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":5},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/tmp","name":"tmp"},{"mountPath":"/app/logs","name":"logs"},{"mountPath":"/app/src/main.py","name":"ste-hotfix","subPath":"main.py"},{"mountPath":"/app/src/services/risk_scorer.py","name":"risk-scorer-hotfix","subPath":"risk_scorer.py"},{"mountPath":"/app/src/services/dag_generator.py","name":"intent-decomposition","subPath":"dag_generator.py"},{"mountPath":"/app/src/services/intent_classifier.py","name":"intent-decomposition","subPath":"intent_classifier.py"},{"mountPath":"/app/src/services/decomposition_templates.py","name":"intent-decomposition","subPath":"decomposition_templates.py"}]}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{"fsGroup":1000,"runAsNonRoot":true,"runAsUser":1000},"serviceAccount":"default","serviceAccountName":"default","terminationGracePeriodSeconds":30,"topologySpreadConstraints":[{"labelSelector":{"matchLabels":{"app.kubernetes.io/name":"semantic-translation-engine"}},"maxSkew":1,"topologyKey":"topology.kubernetes.io/zone","whenUnsatisfiable":"ScheduleAnyway"}],"volumes":[{"emptyDir":{},"name":"tmp"},{"emptyDir":{},"name":"logs"},{"configMap":{"defaultMode":420,"name":"ste-hotfix"},"name":"ste-hotfix"},{"configMap":{"defaultMode":420,"name":"ste-risk-scorer-hotfix"},"name":"risk-scorer-hotfix"},{"configMap":{"defaultMode":420,"name":"ste-intent-decomposition"},"name":"intent-decomposition"}]}}},"status":{"availableReplicas":1,"conditions":[{"lastTransitionTime":"2026-01-19T15:04:25Z","lastUpdateTime":"2026-01-19T15:04:25Z","message":"Deployment has minimum availability.","reason":"MinimumReplicasAvailable","status":"True","type":"Available"},{"lastTransitionTime":"2026-01-18T10:57:56Z","lastUpdateTime":"2026-01-19T21:39:55Z","message":"ReplicaSet \"semantic-translation-engine-6d468b9c86\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"}],"observedGeneration":36,"readyReplicas":1,"replicas":1,"updatedReplicas":1}}
      meta.helm.sh/release-name: semantic-translation-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:07Z"
    generation: 109
    labels:
      app.kubernetes.io/component: semantic-translation-engine
      app.kubernetes.io/instance: semantic-translation-engine
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: semantic-translation-engine
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: semantic-translation-engine-0.1.0
      neural-hive.io/component: semantic-translator
      neural-hive.io/domain: plan-generation
      neural-hive.io/layer: cognitiva
    name: semantic-translation-engine
    namespace: neural-hive
    resourceVersion: "29106887"
    uid: 69c48d4d-9204-4fcd-b298-9414bdf44afb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: semantic-translation-engine
        app.kubernetes.io/name: semantic-translation-engine
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: d784f022b946275705ff626e2ecc3df1484c46a832a48ba6ca8e6c1c463a1690
          checksum/secret: d7c7026787afcb0bf2ac9218b116581a2ba5999c5f97250c405b4c7e9b6dec21
          kubectl.kubernetes.io/restartedAt: "2026-02-01T11:13:20+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: semantic-translator
          app.kubernetes.io/instance: semantic-translation-engine
          app.kubernetes.io/name: semantic-translation-engine
          neural-hive.io/domain: plan-generation
          neural-hive.io/layer: cognitiva
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: semantic-translation-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: LOG_LEVEL
            value: DEBUG
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: semantic-translation-engine-config
          - secretRef:
              name: semantic-translation-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/semantic-translation-engine:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: semantic-translation-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 3Gi
            requests:
              cpu: 450m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/main.py
            name: ste-hotfix
            subPath: main.py
          - mountPath: /app/src/services/risk_scorer.py
            name: risk-scorer-hotfix
            subPath: risk_scorer.py
          - mountPath: /app/src/services/dag_generator.py
            name: intent-decomposition
            subPath: dag_generator.py
          - mountPath: /app/src/services/intent_classifier.py
            name: intent-decomposition
            subPath: intent_classifier.py
          - mountPath: /app/src/services/decomposition_templates.py
            name: intent-decomposition
            subPath: decomposition_templates.py
          - mountPath: /app/src/services/semantic_parser.py
            name: semantic-parser
            subPath: semantic_parser.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: semantic-translation-engine
        serviceAccountName: semantic-translation-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: semantic-translation-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: ste-hotfix
          name: ste-hotfix
        - configMap:
            defaultMode: 420
            name: ste-risk-scorer-hotfix
          name: risk-scorer-hotfix
        - configMap:
            defaultMode: 420
            name: ste-intent-decomposition
          name: intent-decomposition
        - configMap:
            defaultMode: 420
            name: ste-semantic-parser
          name: semantic-parser
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-11T11:00:01Z"
      lastUpdateTime: "2026-02-11T11:15:44Z"
      message: ReplicaSet "semantic-translation-engine-775f4c454d" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-11T20:51:08Z"
      lastUpdateTime: "2026-02-11T20:51:08Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 109
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "45"
      kubernetes.io/change-cause: kubectl set image deployment/service-registry service-registry=ghcr.io/albinojimy/neural-hive-mind/service-registry:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: service-registry
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-05T21:56:02Z"
    generation: 53
    labels:
      app.kubernetes.io/component: service-registry
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: service-registry
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: service-registry-1.0.0
      neural-hive.io/component: service-registry
      neural-hive.io/domain: service-discovery
      neural-hive.io/layer: infrastructure
    name: service-registry
    namespace: neural-hive
    resourceVersion: "29904463"
    uid: a32a2883-5ebb-42d4-950d-32c57bd2c7de
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: service-registry
        app.kubernetes.io/name: service-registry
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:44:08+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordination
          app.kubernetes.io/instance: service-registry
          app.kubernetes.io/name: service-registry
          app.kubernetes.io/part-of: neural-hive-mind
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - service-registry
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: service-registry
          - name: SERVICE_VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: development
          - name: LOG_LEVEL
            value: INFO
          - name: GRPC_PORT
            value: "50051"
          - name: METRICS_PORT
            value: "9090"
          - name: ETCD_ENDPOINTS
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: ETCD_PREFIX
            value: neural-hive:agents
          - name: ETCD_TIMEOUT_SECONDS
            value: "5"
          - name: HEALTH_CHECK_INTERVAL_SECONDS
            value: "60"
          - name: HEARTBEAT_TIMEOUT_SECONDS
            value: "120"
          - name: REDIS_CLUSTER_NODES
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: service-registry-secret
          - name: OTEL_EXPORTER_ENDPOINT
            value: http://otel-collector:4317
          image: ghcr.io/albinojimy/neural-hive-mind/service-registry:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: service-registry
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          startupProbe:
            failureThreshold: 20
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: service-registry
        serviceAccountName: service-registry
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: service-registry
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-10T23:18:56Z"
      lastUpdateTime: "2026-02-10T23:18:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-13T21:06:01Z"
      lastUpdateTime: "2026-02-13T21:06:12Z"
      message: ReplicaSet "service-registry-867758cb55" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 53
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "28"
      kubernetes.io/change-cause: kubectl set image deployment/sla-management-system
        sla-management-system=ghcr.io/albinojimy/neural-hive-mind/sla-management-system:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:32Z"
    generation: 68
    labels:
      app.kubernetes.io/component: sla-management-system
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: sla-management-system-1.0.0
      neural-hive.io/component: sla-management-system
      neural-hive.io/domain: sla-management
      neural-hive.io/layer: monitoring
    name: sla-management-system
    namespace: neural-hive
    resourceVersion: "29939606"
    uid: fea9d645-6081-4ddd-8c49-4ed38fd1b907
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:06:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system
          app.kubernetes.io/part-of: neural-hive-mind
          neural-hive.io/layer: monitoring
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - sla-management-system
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: sla-management-system
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: '["redis-cluster.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: KAFKA__BOOTSTRAP_SERVERS
            value: '["neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"]'
          - name: KAFKA__BUDGET_TOPIC
            value: sla.budgets
          - name: KAFKA__FREEZE_TOPIC
            value: sla.freeze.events
          - name: KAFKA__VIOLATIONS_TOPIC
            value: sla.violations
          - name: ALERTMANAGER__URL
            value: http://alertmanager.monitoring.svc.cluster.local:9093
          - name: ALERTMANAGER__WEBHOOK_PATH
            value: /webhooks/alertmanager
          - name: CALCULATOR__CALCULATION_INTERVAL_SECONDS
            value: "30"
          - name: CALCULATOR__ERROR_BUDGET_WINDOW_DAYS
            value: "30"
          - name: CALCULATOR__BURN_RATE_FAST_THRESHOLD
            value: "14.4"
          - name: CALCULATOR__BURN_RATE_SLOW_THRESHOLD
            value: "6"
          - name: POLICY__FREEZE_THRESHOLD_PERCENT
            value: "20"
          - name: POLICY__AUTO_UNFREEZE_ENABLED
            value: "true"
          - name: POLICY__UNFREEZE_THRESHOLD_PERCENT
            value: "50"
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sla-management-system
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 300m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: sla-management-system
        serviceAccountName: sla-management-system
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2026-02-13T21:06:22Z"
      lastUpdateTime: "2026-02-13T21:07:45Z"
      message: ReplicaSet "sla-management-system-867c876fbd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T23:06:44Z"
      lastUpdateTime: "2026-02-13T23:06:44Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 68
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "33"
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:15:33Z"
    generation: 45
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: sla-management-system-1.0.0
      neural-hive.io/component: sla-management-system
      neural-hive.io/domain: sla-management
      neural-hive.io/layer: monitoring
    name: sla-management-system-operator
    namespace: neural-hive
    resourceVersion: "28728023"
    uid: fe4e7014-a879-459e-be33-b920ed6c3119
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system-operator
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-06T11:28:03+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system-operator
          neural-hive.io/layer: monitoring
      spec:
        containers:
        - command:
          - kopf
          - run
          - /app/src/operator/main.py
          - --verbose
          env:
          - name: PYTHONPATH
            value: /app
          - name: SERVICE_NAME
            value: sla-management-system-operator
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: redis-cluster.redis-cluster.svc.cluster.local:6379
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: RECONCILIATION_INTERVAL
            value: "300"
          - name: REDIS_CLUSTER_NODES
            value: redis-cluster.neural-hive-cache.svc.cluster.local:6379
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: sla-management-system-operator
        serviceAccountName: sla-management-system-operator
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2026-02-10T22:10:22Z"
      lastUpdateTime: "2026-02-10T22:10:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-02-10T22:10:22Z"
      lastUpdateTime: "2026-02-10T22:10:22Z"
      message: ReplicaSet "sla-management-system-operator-df556c858" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 45
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "26"
      meta.helm.sh/release-name: specialist-architecture
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:11:19Z"
    generation: 32
    labels:
      app.kubernetes.io/component: architecture-specialist
      app.kubernetes.io/instance: specialist-architecture
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-architecture
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      helm.sh/chart: specialist-architecture-0.1.8
      neural-hive.io/component: architecture-specialist
      neural-hive.io/domain: architecture-analysis
      neural-hive.io/layer: cognitiva
    name: specialist-architecture
    namespace: neural-hive
    resourceVersion: "27880833"
    uid: 45ba4e9a-2490-4fcb-b614-caddaf46fc80
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-architecture
        app.kubernetes.io/name: specialist-architecture
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: d84b2e53cf70d2acea5239b51f3bfaf6347648f28a5e5fc1c898942f75ebff54
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:44+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-architecture
          app.kubernetes.io/name: specialist-architecture
          neural-hive.io/component: specialist
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-architecture-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-architecture
          - name: MLFLOW_MODEL_NAME
            value: architecture-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-architecture-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-architecture-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-architecture:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-architecture
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-architecture
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-07T23:44:32Z"
      lastUpdateTime: "2026-02-08T08:50:42Z"
      message: ReplicaSet "specialist-architecture-75f65497dc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-08T13:29:58Z"
      lastUpdateTime: "2026-02-08T13:29:58Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 32
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "27"
      meta.helm.sh/release-name: specialist-behavior
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:09:22Z"
    generation: 32
    labels:
      app.kubernetes.io/component: behavior-specialist
      app.kubernetes.io/instance: specialist-behavior
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-behavior
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      helm.sh/chart: specialist-behavior-0.1.8
      neural-hive.io/component: behavior-specialist
      neural-hive.io/domain: behavior-analysis
      neural-hive.io/layer: cognitiva
    name: specialist-behavior
    namespace: neural-hive
    resourceVersion: "27880883"
    uid: 8e6d9351-36c4-41c7-a22b-594b273cb422
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-behavior
        app.kubernetes.io/name: specialist-behavior
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 5a3489da92e79aaf70bd34e56dd2871d8351f15bfafe0d18a026b0e4ab74a418
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:46+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-behavior
          app.kubernetes.io/name: specialist-behavior
          neural-hive.io/component: specialist
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-behavior-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-behavior
          - name: MLFLOW_MODEL_NAME
            value: behavior-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-behavior-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-behavior-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-behavior:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-behavior
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-behavior
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-07T23:27:43Z"
      lastUpdateTime: "2026-02-08T08:50:48Z"
      message: ReplicaSet "specialist-behavior-68c57f76bd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-08T13:30:05Z"
      lastUpdateTime: "2026-02-08T13:30:05Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 32
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "27"
      meta.helm.sh/release-name: specialist-business
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:11:09Z"
    generation: 32
    labels:
      app.kubernetes.io/component: business-specialist
      app.kubernetes.io/instance: specialist-business
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-business
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      helm.sh/chart: specialist-business-0.1.8
      neural-hive.io/component: business-specialist
      neural-hive.io/domain: business-analysis
      neural-hive.io/layer: cognitiva
    name: specialist-business
    namespace: neural-hive
    resourceVersion: "27880795"
    uid: 88b587cd-9c77-472e-8e37-28936035e833
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-business
        app.kubernetes.io/name: specialist-business
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 8acc665d6304eaa2fcce6e7ddc07cdbee4970465ed81e2b6ce5e456151f195f3
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-business
          app.kubernetes.io/name: specialist-business
          neural-hive.io/component: specialist
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-business-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-business
          - name: MLFLOW_MODEL_NAME
            value: business-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-business-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-business-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          - name: FEATURE_CACHE_ENABLED
            value: "true"
          - name: FEATURE_CACHE_TTL_SECONDS
            value: "3600"
          - name: ENABLE_BATCH_INFERENCE
            value: "true"
          - name: BATCH_INFERENCE_SIZE
            value: "32"
          - name: BATCH_INFERENCE_MAX_WORKERS
            value: "8"
          - name: ENABLE_GPU_ACCELERATION
            value: "false"
          - name: GPU_DEVICE
            value: auto
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-business:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-business
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-business
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-07T23:27:43Z"
      lastUpdateTime: "2026-02-08T08:51:37Z"
      message: ReplicaSet "specialist-business-689d656dc4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-08T13:29:51Z"
      lastUpdateTime: "2026-02-08T13:29:51Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 32
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "26"
      meta.helm.sh/release-name: specialist-evolution
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:11:29Z"
    generation: 34
    labels:
      app.kubernetes.io/component: evolution-specialist
      app.kubernetes.io/instance: specialist-evolution
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-evolution
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      helm.sh/chart: specialist-evolution-0.1.8
      neural-hive.io/component: evolution-specialist
      neural-hive.io/domain: evolution-analysis
      neural-hive.io/layer: cognitiva
    name: specialist-evolution
    namespace: neural-hive
    resourceVersion: "27880861"
    uid: 34746f1d-77cc-4082-b649-11aa626847d3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-evolution
        app.kubernetes.io/name: specialist-evolution
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: c6c47581f315a6fd65b5b6530d5f156139862aff0d72f692b99f9370daf76752
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:50+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-evolution
          app.kubernetes.io/name: specialist-evolution
          neural-hive.io/component: specialist
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-evolution-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-evolution
          - name: MLFLOW_MODEL_NAME
            value: evolution-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-evolution-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-evolution-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-evolution:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-evolution
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-evolution
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-07T23:27:44Z"
      lastUpdateTime: "2026-02-08T08:52:17Z"
      message: ReplicaSet "specialist-evolution-5f6b789f48" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-08T13:30:01Z"
      lastUpdateTime: "2026-02-08T13:30:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 34
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "27"
      meta.helm.sh/release-name: specialist-technical
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T23:10:11Z"
    generation: 30
    labels:
      app.kubernetes.io/component: technical-specialist
      app.kubernetes.io/instance: specialist-technical
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: specialist-technical
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.8
      helm.sh/chart: specialist-technical-0.1.8
      neural-hive.io/component: technical-specialist
      neural-hive.io/domain: technical-analysis
      neural-hive.io/layer: cognitiva
    name: specialist-technical
    namespace: neural-hive
    resourceVersion: "27880780"
    uid: 1e25852a-a17d-40fa-8349-1b454c4a908c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-technical
        app.kubernetes.io/name: specialist-technical
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 96e2e5b50df5f53c3c4cd828b33ca93fdb45f04dd60e799b43c2e0f2c975ac8e
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:51+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-technical
          app.kubernetes.io/name: specialist-technical
          neural-hive.io/component: specialist
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-technical-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-technical
          - name: MLFLOW_MODEL_NAME
            value: technical-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-technical-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-technical-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-technical:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-technical
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-technical
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-07T23:27:43Z"
      lastUpdateTime: "2026-02-08T09:01:19Z"
      message: ReplicaSet "specialist-technical-fcb8bc9f8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-08T13:29:48Z"
      lastUpdateTime: "2026-02-08T13:29:48Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 30
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "39"
      kubernetes.io/change-cause: kubectl set image deployment/worker-agents worker-agents=ghcr.io/albinojimy/neural-hive-mind/worker-agents:7837bfa
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: worker-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-03T14:53:56Z"
    generation: 75
    labels:
      app.kubernetes.io/component: worker-agents
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: worker-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: worker-agents-1.0.0
      neural-hive.io/component: worker-agents
      neural-hive.io/domain: task-execution
      neural-hive.io/layer: execution
    name: worker-agents
    namespace: neural-hive
    resourceVersion: "29939521"
    uid: ebd45574-ca4d-40e2-9351-4db6f6cfeb13
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: worker-agents
        app.kubernetes.io/name: worker-agents
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 8ac2bd0d9fdec2265af9bdd10b674d703f35d020860160fa4577d18160f593e8
          kubectl.kubernetes.io/restartedAt: "2026-02-13T22:06:51+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: worker-agents
          app.kubernetes.io/name: worker-agents
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - worker-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          envFrom:
          - configMapRef:
              name: worker-agents
          image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:3a3e661
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: worker-agents
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 400m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        - name: ghcr-secret-fixed
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: worker-agents
        serviceAccountName: worker-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: worker-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2026-02-13T21:06:19Z"
      lastUpdateTime: "2026-02-13T21:07:50Z"
      message: ReplicaSet "worker-agents-cbfcc57b9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T23:06:29Z"
      lastUpdateTime: "2026-02-13T23:06:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 75
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: neural-hive-jaeger
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:21:49Z"
    generation: 3
    labels:
      app.kubernetes.io/component: all-in-one
      app.kubernetes.io/instance: neural-hive-jaeger
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/version: 2.13.0
      helm.sh/chart: jaeger-4.2.2
      prometheus.io/port: "8888"
      prometheus.io/scrape: "true"
    name: neural-hive-jaeger
    namespace: observability
    resourceVersion: "29939401"
    uid: 5260453c-9c04-4df0-ba7e-d224ec8ac409
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: all-in-one
        app.kubernetes.io/instance: neural-hive-jaeger
        app.kubernetes.io/name: jaeger
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: all-in-one
          app.kubernetes.io/instance: neural-hive-jaeger
          app.kubernetes.io/name: jaeger
      spec:
        containers:
        - env:
          - name: SPAN_STORAGE_TYPE
            value: memory
          - name: COLLECTOR_ZIPKIN_HOST_PORT
            value: :9411
          - name: JAEGER_DISABLED
            value: "false"
          - name: COLLECTOR_OTLP_ENABLED
            value: "true"
          image: jaegertracing/jaeger:2.13.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /status
              port: 13133
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: jaeger
          ports:
          - containerPort: 5775
            protocol: UDP
          - containerPort: 6831
            protocol: UDP
          - containerPort: 6832
            protocol: UDP
          - containerPort: 5778
            protocol: TCP
          - containerPort: 16686
            protocol: TCP
          - containerPort: 16685
            protocol: TCP
          - containerPort: 9411
            protocol: TCP
          - containerPort: 4317
            protocol: TCP
          - containerPort: 4318
            protocol: TCP
          - containerPort: 13133
            protocol: TCP
          - containerPort: 8888
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /status
              port: 13133
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsUser: 10001
        serviceAccount: neural-hive-jaeger
        serviceAccountName: neural-hive-jaeger
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-12-29T10:21:49Z"
      lastUpdateTime: "2025-12-29T10:21:58Z"
      message: ReplicaSet "neural-hive-jaeger-5fbd6fffcc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-13T23:06:04Z"
      lastUpdateTime: "2026-02-13T23:06:04Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 3
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:46Z"
    generation: 5
    labels:
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.3.0
      helm.sh/chart: grafana-10.4.0
    name: neural-hive-prometheus-grafana
    namespace: observability
    resourceVersion: "28922800"
    uid: eb6a21ca-08d8-4c84-adcd-7c9558c6439e
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: neural-hive-prometheus
        app.kubernetes.io/name: grafana
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 6e3b9dc1c22f81738aaabb2d96f820cc2a2860509fafcbceb0a5bdacd7ff792c
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2026-01-15T22:13:56+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: neural-hive-prometheus
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.3.0
          helm.sh/chart: grafana-10.4.0
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: neural-hive-prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: neural-hive-prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          - name: REQ_SKIP_INIT
            value: "true"
          image: quay.io/kiwigrid/k8s-sidecar:2.1.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: neural-hive-prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: neural-hive-prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          - name: REQ_SKIP_INIT
            value: "true"
          image: quay.io/kiwigrid/k8s-sidecar:2.1.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: neural-hive-prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: neural-hive-prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          image: docker.io/grafana/grafana:12.3.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: neural-hive-prometheus-grafana
        serviceAccountName: neural-hive-prometheus-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: neural-hive-prometheus-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: neural-hive-prometheus-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: neural-hive-prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    conditions:
    - lastTransitionTime: "2025-12-29T10:18:46Z"
      lastUpdateTime: "2026-01-15T21:14:28Z"
      message: ReplicaSet "neural-hive-prometheus-grafana-84f8d9f494" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-01-27T16:30:15Z"
      lastUpdateTime: "2026-01-27T16:30:15Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 5
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:46Z"
    generation: 2
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-operator
    namespace: observability
    resourceVersion: "25557746"
    uid: f36c4f6c-7dc9-42b8-8293-9f6e70d30024
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        release: neural-hive-prometheus
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-10T01:45:30+01:00"
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: neural-hive-prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 80.8.0
          chart: kube-prometheus-stack-80.8.0
          heritage: Helm
          release: neural-hive-prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/neural-hive-prometheus-kub-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.87.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: neural-hive-prometheus-kub-operator
        serviceAccountName: neural-hive-prometheus-kub-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: neural-hive-prometheus-kub-admission
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-29T10:18:46Z"
      lastUpdateTime: "2026-01-10T00:45:40Z"
      message: ReplicaSet "neural-hive-prometheus-kub-operator-5fc4d75679" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:01Z"
      lastUpdateTime: "2026-02-01T13:47:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:46Z"
    generation: 2
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-7.0.0
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kube-state-metrics
    namespace: observability
    resourceVersion: "28922917"
    uid: 3f40f467-16bf-41bc-af5f-118b6e12d89c
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: neural-hive-prometheus
        app.kubernetes.io/name: kube-state-metrics
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: neural-hive-prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.17.0
          helm.sh/chart: kube-state-metrics-7.0.0
          release: neural-hive-prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpointslices,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: neural-hive-prometheus-kube-state-metrics
        serviceAccountName: neural-hive-prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2025-12-29T10:18:46Z"
      lastUpdateTime: "2025-12-29T10:18:57Z"
      message: ReplicaSet "neural-hive-prometheus-kube-state-metrics-76c55f9647" has
        successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:01Z"
      lastUpdateTime: "2026-02-01T13:47:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "11"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-01-01T03:36:57Z"
    generation: 18
    labels:
      app.kubernetes.io/component: telemetry-collector
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: neural-hive-otel-collector
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 0.89.0
      helm.sh/chart: neural-hive-otel-collector-1.0.0
      neural-hive.io/component: telemetry-collector
      neural-hive.io/layer: observabilidade
      neural.hive/component: telemetry-collector
      neural.hive/layer: observabilidade
    name: otel-collector-neural-hive-otel-collector
    namespace: observability
    resourceVersion: "29402609"
    uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-12T14:55:23+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/otelcol.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 2GiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-02-12T13:55:46Z"
      lastUpdateTime: "2026-02-12T13:55:46Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2026-01-01T03:36:58Z"
      lastUpdateTime: "2026-02-12T13:55:46Z"
      message: ReplicaSet "otel-collector-neural-hive-otel-collector-64c9989c49" has
        successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 18
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-01-01T03:36:57Z"
    generation: 8
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/version: 0.83.0
      helm.sh/chart: opentelemetry-collector-0.66.0
    name: otel-collector-opentelemetry-collector
    namespace: observability
    resourceVersion: "28922990"
    uid: 53148a65-924a-4a9f-a475-194db748845b
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: opentelemetry-collector
        component: standalone-collector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: e62840abbd07d6d53f26a4056330ef678819c1e217217fd45e350cd335b538fa
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: opentelemetry-collector
          component: standalone-collector
      spec:
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/relay.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: otel/opentelemetry-collector-contrib:0.83.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 13133
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: opentelemetry-collector
          ports:
          - containerPort: 6831
            name: jaeger-compact
            protocol: UDP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-thrift
            protocol: TCP
          - containerPort: 4317
            name: otlp
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 13133
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: opentelemetry-collector-configmap
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: otel-collector-opentelemetry-collector
        serviceAccountName: otel-collector-opentelemetry-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: relay
              path: relay.yaml
            name: otel-collector-opentelemetry-collector
          name: opentelemetry-collector-configmap
  status:
    conditions:
    - lastTransitionTime: "2026-01-01T03:36:58Z"
      lastUpdateTime: "2026-01-01T03:37:07Z"
      message: ReplicaSet "otel-collector-opentelemetry-collector-6b67578c68" has
        successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-11T10:17:17Z"
      lastUpdateTime: "2026-02-11T10:17:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 8
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "5"
      meta.helm.sh/release-name: portainer
      meta.helm.sh/release-namespace: portainer
    creationTimestamp: "2025-11-08T11:51:26Z"
    generation: 7
    labels:
      app.kubernetes.io/instance: portainer
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: portainer
      app.kubernetes.io/version: ce-latest-ee-2.33.6
      helm.sh/chart: portainer-2.33.6
      io.portainer.kubernetes.application.stack: portainer
    name: portainer
    namespace: portainer
    resourceVersion: "26775181"
    uid: 99fcad91-f06b-4e33-b2d0-493fb0f51465
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: portainer
        app.kubernetes.io/name: portainer
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-11T14:15:29+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: portainer
          app.kubernetes.io/name: portainer
      spec:
        containers:
        - args:
          - --tunnel-port=30776
          image: portainer/portainer-ce:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: portainer
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          - containerPort: 9443
            name: https
            protocol: TCP
          - containerPort: 8000
            name: tcp-edge
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: portainer-sa-clusteradmin
        serviceAccountName: portainer-sa-clusteradmin
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: portainer
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-08T11:51:26Z"
      lastUpdateTime: "2026-01-11T13:25:22Z"
      message: ReplicaSet "portainer-d6fcbdd48" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-05T05:50:40Z"
      lastUpdateTime: "2026-02-05T05:50:40Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"redis","namespace":"redis-cluster"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"redis"}},"template":{"metadata":{"annotations":{"sidecar.istio.io/inject":"false"},"labels":{"app":"redis","version":"v1"}},"spec":{"containers":[{"command":["redis-server","/usr/local/etc/redis/redis.conf"],"image":"redis:7-alpine","livenessProbe":{"initialDelaySeconds":30,"periodSeconds":10,"tcpSocket":{"port":6379}},"name":"redis","ports":[{"containerPort":6379,"name":"redis"}],"readinessProbe":{"exec":{"command":["redis-cli","ping"]},"initialDelaySeconds":5,"periodSeconds":5},"resources":{"limits":{"cpu":"500m","memory":"1Gi"},"requests":{"cpu":"250m","memory":"512Mi"}},"volumeMounts":[{"mountPath":"/usr/local/etc/redis","name":"config"},{"mountPath":"/data","name":"data"}]}],"volumes":[{"configMap":{"name":"redis-config"},"name":"config"},{"emptyDir":{},"name":"data"}]}}}}
    creationTimestamp: "2025-10-29T10:57:57Z"
    generation: 6
    name: redis
    namespace: redis-cluster
    resourceVersion: "28939235"
    uid: 599c2b95-82d2-49ba-8bd9-0a99e473df16
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: redis
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:33+01:00"
          sidecar.istio.io/inject: "false"
        creationTimestamp: null
        labels:
          app: redis
          version: v1
      spec:
        containers:
        - command:
          - redis-server
          - /usr/local/etc/redis/redis.conf
          image: redis:7-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - redis-cli
              - ping
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/etc/redis
            name: config
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: redis-config
          name: config
        - emptyDir: {}
          name: data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-10-29T10:57:57Z"
      lastUpdateTime: "2025-12-27T23:32:00Z"
      message: ReplicaSet "redis-66b84474ff" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-11T11:10:18Z"
      lastUpdateTime: "2026-02-11T11:10:18Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 6
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: redis-operator
      meta.helm.sh/release-namespace: redis-operator
    creationTimestamp: "2025-11-19T08:00:27Z"
    generation: 2
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: redis-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: redis-operator
      app.kubernetes.io/part-of: redis-operator
      app.kubernetes.io/version: 0.22.2
      helm.sh/chart: redis-operator-0.22.2
    name: redis-operator
    namespace: redis-operator
    resourceVersion: "26438745"
    uid: 3e5d5c43-169b-4cdc-a79c-1e3dba1d68de
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: redis-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          cert-manager.io/inject-ca-from: redis-operator/serving-cert
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:37+01:00"
        creationTimestamp: null
        labels:
          name: redis-operator
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --leader-elect
          - --metrics-bind-address=:8080
          - --kube-client-timeout=60s
          command:
          - /operator
          - manager
          env:
          - name: OPERATOR_IMAGE
            value: quay.io/opstree/redis-operator:v0.22.2
          - name: ENABLE_WEBHOOKS
            value: "false"
          - name: FEATURE_GATES
            value: GenerateConfigInInitContainer=false
          image: quay.io/opstree/redis-operator:v0.22.2
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: probe
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: redis-operator
          ports:
          - containerPort: 8081
            name: probe
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: probe
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: redis-operator
        serviceAccountName: redis-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-19T08:00:27Z"
      lastUpdateTime: "2025-12-27T23:31:56Z"
      message: ReplicaSet "redis-operator-5b7d784c5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-04T05:15:23Z"
      lastUpdateTime: "2026-02-04T05:15:23Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: registry
      meta.helm.sh/release-namespace: registry
    creationTimestamp: "2025-12-04T14:17:34Z"
    generation: 3
    labels:
      app.kubernetes.io/component: registry
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: docker-registry
    name: docker-registry
    namespace: registry
    resourceVersion: "28851342"
    uid: 9e496a6d-251e-413a-86bd-107d97f85ce7
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/name: docker-registry
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-13T14:48:10+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: docker-registry
      spec:
        containers:
        - env:
          - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
            value: /var/lib/registry
          - name: REGISTRY_HTTP_ADDR
            value: :5000
          - name: REGISTRY_STORAGE_DELETE_ENABLED
            value: "true"
          image: registry:2.8.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: registry
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/registry
            name: registry-data
          - mountPath: /etc/docker/registry
            name: registry-config
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/control-plane: ""
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: registry-data
          persistentVolumeClaim:
            claimName: docker-registry-data
        - configMap:
            defaultMode: 420
            name: docker-registry-config
          name: registry-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2026-01-13T14:31:49Z"
      lastUpdateTime: "2026-01-13T14:31:49Z"
      message: ReplicaSet "docker-registry-6b5c9fffcb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-11T05:42:46Z"
      lastUpdateTime: "2026-02-11T05:42:46Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: admintools
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-admintools
    namespace: temporal
    resourceVersion: "25557813"
    uid: 155da70c-af07-4af7-934f-df9853011803
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: admintools
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: admintools
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
      spec:
        containers:
        - env:
          - name: TEMPORAL_CLI_ADDRESS
            value: temporal-frontend:7233
          - name: TEMPORAL_ADDRESS
            value: temporal-frontend:7233
          image: temporalio/admin-tools:1.29.1-tctl-1.18.4-cli-1.5.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - ls
              - /
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: admin-tools
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-27T08:16:51Z"
      lastUpdateTime: "2025-12-27T23:31:41Z"
      message: ReplicaSet "temporal-admintools-78c898f5f9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:03Z"
      lastUpdateTime: "2026-02-01T13:47:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-frontend
    namespace: temporal
    resourceVersion: "25557781"
    uid: 68892dc8-41a8-4c0f-85ab-cd336bc5f0cc
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: frontend
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
          prometheus.io/job: temporal-frontend
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: frontend
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: frontend
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-frontend
          ports:
          - containerPort: 7233
            name: rpc
            protocol: TCP
          - containerPort: 6933
            name: membership
            protocol: TCP
          - containerPort: 7243
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-27T08:16:51Z"
      lastUpdateTime: "2025-12-27T23:31:26Z"
      message: ReplicaSet "temporal-frontend-bc8b49c8d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:02Z"
      lastUpdateTime: "2026-02-01T13:47:02Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: history
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-history
    namespace: temporal
    resourceVersion: "25557675"
    uid: d759a039-e0cf-4126-87e4-c268183e0a25
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: history
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
          prometheus.io/job: temporal-history
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: history
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: history
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-history
          ports:
          - containerPort: 7234
            name: rpc
            protocol: TCP
          - containerPort: 6934
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-27T08:16:51Z"
      lastUpdateTime: "2025-12-27T23:31:42Z"
      message: ReplicaSet "temporal-history-86c95ccf9b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:00Z"
      lastUpdateTime: "2026-02-01T13:47:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: matching
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-matching
    namespace: temporal
    resourceVersion: "25557853"
    uid: 3db8dbf6-ef67-4dc3-afec-55042873ed16
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: matching
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: matching
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: matching
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-matching
          ports:
          - containerPort: 7235
            name: rpc
            protocol: TCP
          - containerPort: 6935
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-27T08:16:51Z"
      lastUpdateTime: "2025-12-27T23:31:41Z"
      message: ReplicaSet "temporal-matching-5fb946b9c5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:03Z"
      lastUpdateTime: "2026-02-01T13:47:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-web
    namespace: temporal
    resourceVersion: "25557858"
    uid: 213d64b8-c7ba-4ddc-b6de-fbb25ba15ae8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: web
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: web
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
      spec:
        containers:
        - env:
          - name: TEMPORAL_ADDRESS
            value: temporal-frontend.temporal.svc:7233
          image: temporalio/ui:2.42.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: temporal-web
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-27T08:16:51Z"
      lastUpdateTime: "2025-12-27T23:31:48Z"
      message: ReplicaSet "temporal-web-bfbd4b558" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-01T13:47:03Z"
      lastUpdateTime: "2026-02-01T13:47:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
    name: temporal-worker
    namespace: temporal
    resourceVersion: "22401885"
    uid: 00756dd2-dee7-4f7b-931a-3d630e4c0419
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
          prometheus.io/job: temporal-worker
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: worker
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: worker
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          name: temporal-worker
          ports:
          - containerPort: 6939
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-11-27T08:16:51Z"
      lastUpdateTime: "2025-12-27T23:31:49Z"
      message: ReplicaSet "temporal-worker-77df5b85f6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-01-22T16:59:49Z"
      lastUpdateTime: "2026-01-22T16:59:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: minio
      meta.helm.sh/release-namespace: velero
    creationTimestamp: "2025-12-09T20:59:11Z"
    generation: 1
    labels:
      app: minio
      app.kubernetes.io/managed-by: Helm
      chart: minio-5.4.0
      heritage: Helm
      release: minio
    name: minio
    namespace: velero
    resourceVersion: "22400339"
    uid: 6683945c-5ff0-4d16-8c23-7baaf9073125
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: minio
        release: minio
    strategy:
      rollingUpdate:
        maxSurge: 100%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 80e7cc85a7dc2cac261932d8c7c6611e6dbfdb0879e63cce3bd67d7c6b340946
          checksum/secrets: 184cec76b907d0ce15c3e1d11f7c5e20c066057a49968de890f5bba4de52f63a
        creationTimestamp: null
        labels:
          app: minio
          release: minio
        name: minio
      spec:
        containers:
        - command:
          - /bin/sh
          - -ce
          - /usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/
            --address :9000 --console-address :9001
          env:
          - name: MINIO_ROOT_USER
            valueFrom:
              secretKeyRef:
                key: rootUser
                name: minio
          - name: MINIO_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: rootPassword
                name: minio
          - name: MINIO_PROMETHEUS_AUTH_TYPE
            value: public
          image: quay.io/minio/minio:RELEASE.2024-12-18T13-15-44Z
          imagePullPolicy: IfNotPresent
          name: minio
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          - containerPort: 9001
            name: http-console
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/credentials
            name: minio-user
            readOnly: true
          - mountPath: /export
            name: export
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: minio-sa
        serviceAccountName: minio-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - name: export
          persistentVolumeClaim:
            claimName: minio
        - name: minio-user
          secret:
            defaultMode: 420
            secretName: minio
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T20:59:12Z"
      lastUpdateTime: "2025-12-09T20:59:26Z"
      message: ReplicaSet "minio-5c5464c875" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-01-22T16:58:01Z"
      lastUpdateTime: "2026-01-22T16:58:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: velero
      meta.helm.sh/release-namespace: velero
    creationTimestamp: "2025-12-09T21:02:38Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      app.kubernetes.io/version: 1.17.1
      component: velero
      helm.sh/chart: velero-11.2.0
    name: velero
    namespace: velero
    resourceVersion: "26775060"
    uid: 16f65e81-e87c-4bf4-b617-818619233d56
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: velero
        app.kubernetes.io/name: velero
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
          prometheus.io/path: /metrics
          prometheus.io/port: "8085"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: velero
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: velero
          app.kubernetes.io/version: 1.17.1
          helm.sh/chart: velero-11.2.0
          name: velero
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - server
          - --uploader-type=kopia
          - --repo-maintenance-job-configmap=velero-repo-maintenance
          command:
          - /velero
          env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_LIBRARY_PATH
            value: /plugins
          - name: AWS_SHARED_CREDENTIALS_FILE
            value: /credentials/cloud
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /credentials/cloud
          - name: AZURE_CREDENTIALS_FILE
            value: /credentials/cloud
          - name: ALIBABA_CLOUD_CREDENTIALS_FILE
            value: /credentials/cloud
          image: velero/velero:v1.17.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: http-monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: velero
          ports:
          - containerPort: 8085
            name: http-monitoring
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: http-monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /credentials
            name: cloud-credentials
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        initContainers:
        - image: velero/velero-plugin-for-aws:v1.10.0
          imagePullPolicy: IfNotPresent
          name: velero-plugin-for-aws
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /target
            name: plugins
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: velero-server
        serviceAccountName: velero-server
        terminationGracePeriodSeconds: 3600
        volumes:
        - name: cloud-credentials
          secret:
            defaultMode: 420
            secretName: velero
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2025-12-09T21:02:38Z"
      lastUpdateTime: "2025-12-09T21:03:09Z"
      message: ReplicaSet "velero-8445566485" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2026-02-05T05:50:31Z"
      lastUpdateTime: "2026-02-05T05:50:31Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2026-02-02T15:29:10Z"
    generation: 3
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 5777bdb764
    name: approval-service-5777bdb764
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "25917139"
    uid: d992fd9e-b18a-4c70-8c4f-96bba4edaa16
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: 5777bdb764
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-19T15:06:40+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: 5777bdb764
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:10d7a42
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
      deployment.kubernetes.io/revision-history: "9"
    creationTimestamp: "2026-02-02T15:38:01Z"
    generation: 5
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 586bb5bd7
    name: approval-service-586bb5bd7
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "29939298"
    uid: 3f081183-2bae-4fc5-b06f-554de948e962
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: 586bb5bd7
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-02T16:38:00+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: 586bb5bd7
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:10d7a42
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 5
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2026-02-02T15:11:49Z"
    generation: 2
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 5cf4b5b46
    name: approval-service-5cf4b5b46
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "25917001"
    uid: 34a33a0b-60e7-4754-8674-c239d8b6ddb2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: 5cf4b5b46
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-19T15:06:40+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: 5cf4b5b46
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:1.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2026-01-19T14:03:53Z"
    generation: 5
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 5d9455bdf9
    name: approval-service-5d9455bdf9
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "21402603"
    uid: 3315b744-8262-4841-baf0-6861d1d708d6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: 5d9455bdf9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-19T15:03:51+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: 5d9455bdf9
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: 37.60.241.150:30500/approval-service:1.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2026-01-31T15:43:40Z"
    generation: 7
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 5f86596775
    name: approval-service-5f86596775
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "25916767"
    uid: 65b2a9b2-f3be-492a-b19a-305432702505
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: 5f86596775
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-19T15:06:40+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: 5f86596775
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:150cb8e
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 7
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2026-01-19T13:58:55Z"
    generation: 3
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 6bd4c4cb9d
    name: approval-service-6bd4c4cb9d
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "21402194"
    uid: e2f9105e-726d-48d5-a903-01e236a4f717
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: 6bd4c4cb9d
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: 6bd4c4cb9d
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: 37.60.241.150:30500/approval-service:1.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2026-01-19T14:06:41Z"
    generation: 5
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 779494589d
    name: approval-service-779494589d
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "25252142"
    uid: 90dad949-5fb7-48d4-9f4a-98a112c8a8cc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: 779494589d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-19T15:06:40+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: 779494589d
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: 37.60.241.150:30500/approval-service:1.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2026-01-31T15:36:01Z"
    generation: 2
    labels:
      app: approval-service
      component: governance
      pod-template-hash: 7c678c8d6
    name: approval-service-7c678c8d6
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "25251673"
    uid: 19f22ee9-7003-4c52-812f-c355253cf8ad
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: 7c678c8d6
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-19T15:06:40+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: 7c678c8d6
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:150cb8e
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2026-01-19T14:02:39Z"
    generation: 2
    labels:
      app: approval-service
      component: governance
      pod-template-hash: f54b5f8d5
    name: approval-service-f54b5f8d5
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "21402447"
    uid: 10fd16fb-1d5f-43d4-83c3-13da02228927
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: f54b5f8d5
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: f54b5f8d5
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: 37.60.241.150:30500/approval-service:1.0.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "10"
    creationTimestamp: "2026-02-08T14:35:25Z"
    generation: 2
    labels:
      app: approval-service
      component: governance
      pod-template-hash: f98df94f7
    name: approval-service-f98df94f7
    namespace: approval
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 6946055d-1833-4c58-ba74-e26c4841d3e8
    resourceVersion: "27896327"
    uid: d456a4e7-9c18-4430-a238-c04a3b67285a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: approval-service
        pod-template-hash: f98df94f7
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-02T16:38:00+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: approval-service
          component: governance
          pod-template-hash: f98df94f7
      spec:
        containers:
        - envFrom:
          - configMapRef:
              name: approval-service-config
          - secretRef:
              name: approval-service-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:1.0.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 200m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/schemas
            name: schemas
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: cognitive-plan-schema
          name: schemas
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:49Z"
    generation: 2
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: 559676bcc4
    name: cert-manager-559676bcc4
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: d8af4579-4e68-406b-8534-7101364e6c31
    resourceVersion: "8235775"
    uid: 295626f5-d8b8-46ba-b41d-1b42c62f1f56
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 559676bcc4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
          pod-template-hash: 559676bcc4
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.16.2
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-27T23:31:05Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 69c44b659d
    name: cert-manager-69c44b659d
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: d8af4579-4e68-406b-8534-7101364e6c31
    resourceVersion: "25557397"
    uid: 41c3e826-e69a-46c5-b771-a3cfe33de8f4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 69c44b659d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
          pod-template-hash: 69c44b659d
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.2
          - --dns01-recursive-nameservers-only
          - --dns01-recursive-nameservers=162.159.44.25:53,108.162.194.2:53
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.19.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-10T10:25:52Z"
    generation: 2
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 79575bbc77
    name: cert-manager-79575bbc77
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: d8af4579-4e68-406b-8534-7101364e6c31
    resourceVersion: "8245474"
    uid: 2f5aaa48-184f-4877-98d1-7bb30e708a23
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 79575bbc77
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
          pod-template-hash: 79575bbc77
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.2
          - --dns01-recursive-nameservers-only
          - --dns01-recursive-nameservers=173.245.58.0:53,173.245.59.0:53
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.19.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-10T10:34:08Z"
    generation: 2
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 7bcff687dc
    name: cert-manager-7bcff687dc
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: d8af4579-4e68-406b-8534-7101364e6c31
    resourceVersion: "14004001"
    uid: 77bdbcb3-0130-4f54-815f-8cc9932d98d9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 7bcff687dc
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
          pod-template-hash: 7bcff687dc
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.2
          - --dns01-recursive-nameservers-only
          - --dns01-recursive-nameservers=162.159.44.25:53,108.162.194.2:53
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.19.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-10T09:54:32Z"
    generation: 2
    labels:
      app: cert-manager
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 8664484ccd
    name: cert-manager-8664484ccd
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager
      uid: d8af4579-4e68-406b-8534-7101364e6c31
    resourceVersion: "8243421"
    uid: f1703511-c713-4782-b707-0d7116ca0884
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 8664484ccd
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
          pod-template-hash: 8664484ccd
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=kube-system
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.2
          - --dns01-recursive-nameservers-only
          - --dns01-recursive-nameservers=1.1.1.1:53,1.0.0.1:53
          - --max-concurrent-challenges=60
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v1.19.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 8
            httpGet:
              path: /livez
              port: http-healthz
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 15
          name: cert-manager-controller
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          - containerPort: 9403
            name: http-healthz
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager
        serviceAccountName: cert-manager
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:48:33Z"
    generation: 2
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: 64df8d68b9
    name: cert-manager-cainjector-64df8d68b9
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 1598322e-1468-4a27-a0ff-5f825d563505
    resourceVersion: "8235834"
    uid: d29eb202-7f31-4a3c-b60d-377de6b74acc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 64df8d68b9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-09T21:48:33+01:00"
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
          pod-template-hash: 64df8d68b9
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.16.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-10T09:54:32Z"
    generation: 2
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: "666464969"
    name: cert-manager-cainjector-666464969
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 1598322e-1468-4a27-a0ff-5f825d563505
    resourceVersion: "14003910"
    uid: ec4d7f30-10ce-49ee-8df8-dc8eea0a3e2a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: "666464969"
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-09T21:48:33+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
          pod-template-hash: "666464969"
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.19.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-27T23:31:05Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 6b8d568764
    name: cert-manager-cainjector-6b8d568764
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 1598322e-1468-4a27-a0ff-5f825d563505
    resourceVersion: "22400306"
    uid: 1b6b6b3d-9bb5-493f-b1fb-2e5f1951f7d3
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 6b8d568764
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
          pod-template-hash: 6b8d568764
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.19.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:49Z"
    generation: 2
    labels:
      app: cainjector
      app.kubernetes.io/component: cainjector
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: 78d6b6d764
    name: cert-manager-cainjector-78d6b6d764
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-cainjector
      uid: 1598322e-1468-4a27-a0ff-5f825d563505
    resourceVersion: "8050953"
    uid: babe7150-e6ac-4bda-a064-d7c515b9ae0d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: cainjector
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: cainjector
        pod-template-hash: 78d6b6d764
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/component: cainjector
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
          pod-template-hash: 78d6b6d764
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=kube-system
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v1.16.2
          imagePullPolicy: IfNotPresent
          name: cert-manager-cainjector
          ports:
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-cainjector
        serviceAccountName: cert-manager-cainjector
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-10T09:54:32Z"
    generation: 2
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 5d9c78c586
    name: cert-manager-webhook-5d9c78c586
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: fd995350-7df8-41d9-96cb-64b4efe208fb
    resourceVersion: "14004499"
    uid: d7d654c8-dc1f-4be9-9b37-4a5176f8c5d7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 5d9c78c586
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-09T21:48:33+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
          pod-template-hash: 5d9c78c586
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.19.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:48:33Z"
    generation: 2
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: 686d9c4cf7
    name: cert-manager-webhook-686d9c4cf7
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: fd995350-7df8-41d9-96cb-64b4efe208fb
    resourceVersion: "8236044"
    uid: d52964d5-48cd-4a43-85aa-e199948e05a7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 686d9c4cf7
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-09T21:48:33+01:00"
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
          pod-template-hash: 686d9c4cf7
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 25m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-09T20:42:49Z"
    generation: 2
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.16.2
      helm.sh/chart: cert-manager-v1.16.2
      pod-template-hash: 7b9fcd98f8
    name: cert-manager-webhook-7b9fcd98f8
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: fd995350-7df8-41d9-96cb-64b4efe208fb
    resourceVersion: "8051006"
    uid: caf73a4b-c1a6-436e-a3ba-126b689644df
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 7b9fcd98f8
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.16.2
          helm.sh/chart: cert-manager-v1.16.2
          pod-template-hash: 7b9fcd98f8
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.16.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 6080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 25m
              memory: 32Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: cert-manager
      meta.helm.sh/release-namespace: cert-manager
    creationTimestamp: "2025-12-27T23:31:05Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/component: webhook
      app.kubernetes.io/instance: cert-manager
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      app.kubernetes.io/version: v1.19.2
      helm.sh/chart: cert-manager-v1.19.2
      pod-template-hash: 8546d8cdff
    name: cert-manager-webhook-8546d8cdff
    namespace: cert-manager
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cert-manager-webhook
      uid: fd995350-7df8-41d9-96cb-64b4efe208fb
    resourceVersion: "26774999"
    uid: 7ee70508-a7f6-47e3-a024-890fdd45499c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: webhook
        app.kubernetes.io/instance: cert-manager
        app.kubernetes.io/name: webhook
        pod-template-hash: 8546d8cdff
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/component: webhook
          app.kubernetes.io/instance: cert-manager
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          app.kubernetes.io/version: v1.19.2
          helm.sh/chart: cert-manager-v1.19.2
          pod-template-hash: 8546d8cdff
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=10250
          - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)
          - --dynamic-serving-ca-secret-name=cert-manager-webhook-ca
          - --dynamic-serving-dns-names=cert-manager-webhook
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE)
          - --dynamic-serving-dns-names=cert-manager-webhook.$(POD_NAMESPACE).svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v1.19.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: cert-manager-webhook
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          - containerPort: 6080
            name: healthcheck
            protocol: TCP
          - containerPort: 9402
            name: http-metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        nodeSelector:
          kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: cert-manager-webhook
        serviceAccountName: cert-manager-webhook
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: clickhouse-operator
      meta.helm.sh/release-namespace: clickhouse-operator
    creationTimestamp: "2025-12-27T23:32:47Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: clickhouse-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/version: 0.25.5
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chop: 0.25.5
      clickhouse.altinity.com/chop-commit: 9ab22d8
      clickhouse.altinity.com/chop-date: 2025-10-24T08.40.12
      helm.sh/chart: altinity-clickhouse-operator-0.25.5
      pod-template-hash: 59f587466b
    name: clickhouse-operator-altinity-clickhouse-operator-59f587466b
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: clickhouse-operator-altinity-clickhouse-operator
      uid: a800c3b2-36ad-41ff-b287-c65b7bbc9d52
    resourceVersion: "15622101"
    uid: 2797d09d-c399-49d1-a30a-9fed13355ed1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: clickhouse-operator
        app.kubernetes.io/name: altinity-clickhouse-operator
        pod-template-hash: 59f587466b
    template:
      metadata:
        annotations:
          checksum/confd-files: 5edfe9bdcd34fb98956d853ad29d712abd4894854a8135746b003d2b9c133577
          checksum/configd-files: c17d6658e0c99f87a8d73e095c45603c62ded9d66a0041aa588d83f1d66caeee
          checksum/files: e715c2eb8d54de48fe1a0b1ea2b2a5ee693cc53b6bc089374b49ac1e4e7a9095
          checksum/keeper-confd-files: 21795302d930e9eb1b9cb5b1020199582189134766632e46c162b3ada2649a92
          checksum/keeper-configd-files: d93fa1304ddab91942cfa7de911ada5fb02306b234bf5328938efbd78563f7bd
          checksum/keeper-templatesd-files: 6c0b64e4c96322864592ff1c6eb84ed9ddc4f96880c55878f3d15036e6d6b424
          checksum/keeper-usersd-files: 9b0fb56434072a9301b625c5b5857152d637e519ace77e54e0059f20df789e52
          checksum/templatesd-files: f32867861e9a45819ac6bf9ef21943190fcff1e00ff39f5045e98b98b37fdfb3
          checksum/usersd-files: a6af808e6d3a67c93fa43ab9443a22d35cd71fc1429a41dee8314a656b2577c6
          clickhouse-operator-metrics/port: "9999"
          clickhouse-operator-metrics/scrape: "true"
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:32:05+01:00"
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: clickhouse-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: altinity-clickhouse-operator
          app.kubernetes.io/version: 0.25.5
          helm.sh/chart: altinity-clickhouse-operator-0.25.5
          pod-template-hash: 59f587466b
      spec:
        affinity: {}
        containers:
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/clickhouse-operator:0.25.5
          imagePullPolicy: IfNotPresent
          name: altinity-clickhouse-operator
          ports:
          - containerPort: 9999
            name: op-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/metrics-exporter:0.25.5
          imagePullPolicy: IfNotPresent
          name: metrics-exporter
          ports:
          - containerPort: 8888
            name: ch-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: clickhouse-operator-altinity-clickhouse-operator
        serviceAccountName: clickhouse-operator-altinity-clickhouse-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-files
          name: etc-clickhouse-operator-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-confd-files
          name: etc-clickhouse-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-configd-files
          name: etc-clickhouse-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-templatesd-files
          name: etc-clickhouse-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-usersd-files
          name: etc-clickhouse-operator-usersd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-confd-files
          name: etc-keeper-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-configd-files
          name: etc-keeper-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-templatesd-files
          name: etc-keeper-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-usersd-files
          name: etc-keeper-operator-usersd-folder
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: clickhouse-operator
      meta.helm.sh/release-namespace: clickhouse-operator
    creationTimestamp: "2025-11-20T13:27:41Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: clickhouse-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/version: 0.25.5
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chop: 0.25.5
      clickhouse.altinity.com/chop-commit: 9ab22d8
      clickhouse.altinity.com/chop-date: 2025-10-24T08.40.12
      helm.sh/chart: altinity-clickhouse-operator-0.25.5
      pod-template-hash: 5b4fc76cdd
    name: clickhouse-operator-altinity-clickhouse-operator-5b4fc76cdd
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: clickhouse-operator-altinity-clickhouse-operator
      uid: a800c3b2-36ad-41ff-b287-c65b7bbc9d52
    resourceVersion: "14004540"
    uid: b32fd461-510b-409a-a912-a46ddcd531bb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: clickhouse-operator
        app.kubernetes.io/name: altinity-clickhouse-operator
        pod-template-hash: 5b4fc76cdd
    template:
      metadata:
        annotations:
          checksum/confd-files: 5edfe9bdcd34fb98956d853ad29d712abd4894854a8135746b003d2b9c133577
          checksum/configd-files: c17d6658e0c99f87a8d73e095c45603c62ded9d66a0041aa588d83f1d66caeee
          checksum/files: e715c2eb8d54de48fe1a0b1ea2b2a5ee693cc53b6bc089374b49ac1e4e7a9095
          checksum/keeper-confd-files: 21795302d930e9eb1b9cb5b1020199582189134766632e46c162b3ada2649a92
          checksum/keeper-configd-files: d93fa1304ddab91942cfa7de911ada5fb02306b234bf5328938efbd78563f7bd
          checksum/keeper-templatesd-files: 6c0b64e4c96322864592ff1c6eb84ed9ddc4f96880c55878f3d15036e6d6b424
          checksum/keeper-usersd-files: 9b0fb56434072a9301b625c5b5857152d637e519ace77e54e0059f20df789e52
          checksum/templatesd-files: f32867861e9a45819ac6bf9ef21943190fcff1e00ff39f5045e98b98b37fdfb3
          checksum/usersd-files: a6af808e6d3a67c93fa43ab9443a22d35cd71fc1429a41dee8314a656b2577c6
          clickhouse-operator-metrics/port: "9999"
          clickhouse-operator-metrics/scrape: "true"
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: clickhouse-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: altinity-clickhouse-operator
          app.kubernetes.io/version: 0.25.5
          helm.sh/chart: altinity-clickhouse-operator-0.25.5
          pod-template-hash: 5b4fc76cdd
      spec:
        affinity: {}
        containers:
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/clickhouse-operator:0.25.5
          imagePullPolicy: IfNotPresent
          name: altinity-clickhouse-operator
          ports:
          - containerPort: 9999
            name: op-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/metrics-exporter:0.25.5
          imagePullPolicy: IfNotPresent
          name: metrics-exporter
          ports:
          - containerPort: 8888
            name: ch-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: clickhouse-operator-altinity-clickhouse-operator
        serviceAccountName: clickhouse-operator-altinity-clickhouse-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-files
          name: etc-clickhouse-operator-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-confd-files
          name: etc-clickhouse-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-configd-files
          name: etc-clickhouse-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-templatesd-files
          name: etc-clickhouse-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-usersd-files
          name: etc-clickhouse-operator-usersd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-confd-files
          name: etc-keeper-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-configd-files
          name: etc-keeper-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-templatesd-files
          name: etc-keeper-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-usersd-files
          name: etc-keeper-operator-usersd-folder
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: clickhouse-operator
      meta.helm.sh/release-namespace: clickhouse-operator
    creationTimestamp: "2026-01-14T21:19:55Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: clickhouse-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/version: 0.25.5
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chop: 0.25.5
      clickhouse.altinity.com/chop-commit: 9ab22d8
      clickhouse.altinity.com/chop-date: 2025-10-24T08.40.12
      helm.sh/chart: altinity-clickhouse-operator-0.25.5
      pod-template-hash: 6599ffbd4c
    name: clickhouse-operator-altinity-clickhouse-operator-6599ffbd4c
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: clickhouse-operator-altinity-clickhouse-operator
      uid: a800c3b2-36ad-41ff-b287-c65b7bbc9d52
    resourceVersion: "25557679"
    uid: 979376b2-b516-4dea-9e97-e83a7e946580
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: clickhouse-operator
        app.kubernetes.io/name: altinity-clickhouse-operator
        pod-template-hash: 6599ffbd4c
    template:
      metadata:
        annotations:
          checksum/confd-files: 5edfe9bdcd34fb98956d853ad29d712abd4894854a8135746b003d2b9c133577
          checksum/configd-files: c17d6658e0c99f87a8d73e095c45603c62ded9d66a0041aa588d83f1d66caeee
          checksum/files: e715c2eb8d54de48fe1a0b1ea2b2a5ee693cc53b6bc089374b49ac1e4e7a9095
          checksum/keeper-confd-files: 21795302d930e9eb1b9cb5b1020199582189134766632e46c162b3ada2649a92
          checksum/keeper-configd-files: d93fa1304ddab91942cfa7de911ada5fb02306b234bf5328938efbd78563f7bd
          checksum/keeper-templatesd-files: 6c0b64e4c96322864592ff1c6eb84ed9ddc4f96880c55878f3d15036e6d6b424
          checksum/keeper-usersd-files: 9b0fb56434072a9301b625c5b5857152d637e519ace77e54e0059f20df789e52
          checksum/templatesd-files: f32867861e9a45819ac6bf9ef21943190fcff1e00ff39f5045e98b98b37fdfb3
          checksum/usersd-files: a6af808e6d3a67c93fa43ab9443a22d35cd71fc1429a41dee8314a656b2577c6
          clickhouse-operator-metrics/port: "9999"
          clickhouse-operator-metrics/scrape: "true"
          kubectl.kubernetes.io/restartedAt: "2026-01-14T22:19:21+01:00"
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: clickhouse-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: altinity-clickhouse-operator
          app.kubernetes.io/version: 0.25.5
          helm.sh/chart: altinity-clickhouse-operator-0.25.5
          pod-template-hash: 6599ffbd4c
      spec:
        affinity: {}
        containers:
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/clickhouse-operator:0.25.5
          imagePullPolicy: IfNotPresent
          name: altinity-clickhouse-operator
          ports:
          - containerPort: 9999
            name: op-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/metrics-exporter:0.25.5
          imagePullPolicy: IfNotPresent
          name: metrics-exporter
          ports:
          - containerPort: 8888
            name: ch-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: clickhouse-operator-altinity-clickhouse-operator
        serviceAccountName: clickhouse-operator-altinity-clickhouse-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-files
          name: etc-clickhouse-operator-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-confd-files
          name: etc-clickhouse-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-configd-files
          name: etc-clickhouse-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-templatesd-files
          name: etc-clickhouse-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-usersd-files
          name: etc-clickhouse-operator-usersd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-confd-files
          name: etc-keeper-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-configd-files
          name: etc-keeper-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-templatesd-files
          name: etc-keeper-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-usersd-files
          name: etc-keeper-operator-usersd-folder
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: clickhouse-operator
      meta.helm.sh/release-namespace: clickhouse-operator
    creationTimestamp: "2026-01-01T21:40:13Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: clickhouse-operator
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/version: 0.25.5
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chop: 0.25.5
      clickhouse.altinity.com/chop-commit: 9ab22d8
      clickhouse.altinity.com/chop-date: 2025-10-24T08.40.12
      helm.sh/chart: altinity-clickhouse-operator-0.25.5
      pod-template-hash: 686d568cfc
    name: clickhouse-operator-altinity-clickhouse-operator-686d568cfc
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: clickhouse-operator-altinity-clickhouse-operator
      uid: a800c3b2-36ad-41ff-b287-c65b7bbc9d52
    resourceVersion: "19887550"
    uid: 14df8d15-816e-4723-9bb6-aeef3409fdd9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: clickhouse-operator
        app.kubernetes.io/name: altinity-clickhouse-operator
        pod-template-hash: 686d568cfc
    template:
      metadata:
        annotations:
          checksum/confd-files: 5edfe9bdcd34fb98956d853ad29d712abd4894854a8135746b003d2b9c133577
          checksum/configd-files: c17d6658e0c99f87a8d73e095c45603c62ded9d66a0041aa588d83f1d66caeee
          checksum/files: e715c2eb8d54de48fe1a0b1ea2b2a5ee693cc53b6bc089374b49ac1e4e7a9095
          checksum/keeper-confd-files: 21795302d930e9eb1b9cb5b1020199582189134766632e46c162b3ada2649a92
          checksum/keeper-configd-files: d93fa1304ddab91942cfa7de911ada5fb02306b234bf5328938efbd78563f7bd
          checksum/keeper-templatesd-files: 6c0b64e4c96322864592ff1c6eb84ed9ddc4f96880c55878f3d15036e6d6b424
          checksum/keeper-usersd-files: 9b0fb56434072a9301b625c5b5857152d637e519ace77e54e0059f20df789e52
          checksum/templatesd-files: f32867861e9a45819ac6bf9ef21943190fcff1e00ff39f5045e98b98b37fdfb3
          checksum/usersd-files: a6af808e6d3a67c93fa43ab9443a22d35cd71fc1429a41dee8314a656b2577c6
          clickhouse-operator-metrics/port: "9999"
          clickhouse-operator-metrics/scrape: "true"
          kubectl.kubernetes.io/restartedAt: "2026-01-01T22:39:41+01:00"
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: clickhouse-operator
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: altinity-clickhouse-operator
          app.kubernetes.io/version: 0.25.5
          helm.sh/chart: altinity-clickhouse-operator-0.25.5
          pod-template-hash: 686d568cfc
      spec:
        affinity: {}
        containers:
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/clickhouse-operator:0.25.5
          imagePullPolicy: IfNotPresent
          name: altinity-clickhouse-operator
          ports:
          - containerPort: 9999
            name: op-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        - env:
          - name: OPERATOR_POD_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: OPERATOR_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: OPERATOR_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: OPERATOR_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: OPERATOR_CONTAINER_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.cpu
          - name: OPERATOR_CONTAINER_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.cpu
          - name: OPERATOR_CONTAINER_MEM_REQUEST
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: requests.memory
          - name: OPERATOR_CONTAINER_MEM_LIMIT
            valueFrom:
              resourceFieldRef:
                containerName: altinity-clickhouse-operator
                divisor: "0"
                resource: limits.memory
          image: altinity/metrics-exporter:0.25.5
          imagePullPolicy: IfNotPresent
          name: metrics-exporter
          ports:
          - containerPort: 8888
            name: ch-metrics
            protocol: TCP
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-operator
            name: etc-clickhouse-operator-folder
          - mountPath: /etc/clickhouse-operator/chi/conf.d
            name: etc-clickhouse-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chi/config.d
            name: etc-clickhouse-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chi/templates.d
            name: etc-clickhouse-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chi/users.d
            name: etc-clickhouse-operator-usersd-folder
          - mountPath: /etc/clickhouse-operator/chk/conf.d
            name: etc-keeper-operator-confd-folder
          - mountPath: /etc/clickhouse-operator/chk/keeper_config.d
            name: etc-keeper-operator-configd-folder
          - mountPath: /etc/clickhouse-operator/chk/templates.d
            name: etc-keeper-operator-templatesd-folder
          - mountPath: /etc/clickhouse-operator/chk/users.d
            name: etc-keeper-operator-usersd-folder
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: clickhouse-operator-altinity-clickhouse-operator
        serviceAccountName: clickhouse-operator-altinity-clickhouse-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-files
          name: etc-clickhouse-operator-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-confd-files
          name: etc-clickhouse-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-configd-files
          name: etc-clickhouse-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-templatesd-files
          name: etc-clickhouse-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-usersd-files
          name: etc-clickhouse-operator-usersd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-confd-files
          name: etc-keeper-operator-confd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-configd-files
          name: etc-keeper-operator-configd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-templatesd-files
          name: etc-keeper-operator-templatesd-folder
        - configMap:
            defaultMode: 420
            name: clickhouse-operator-altinity-clickhouse-operator-keeper-usersd-files
          name: etc-keeper-operator-usersd-folder
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2026-01-31T21:50:22Z"
    generation: 2
    labels:
      app: github-runner
      pod-template-hash: 6764b8894f
    name: github-runner-6764b8894f
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "25996567"
    uid: c6a24d4c-0070-4c12-96c2-3c9752b08c72
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: 6764b8894f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-31T22:50:19+01:00"
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: 6764b8894f
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: ACCESS_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2026-01-27T00:38:27Z"
    generation: 5
    labels:
      app: github-runner
      pod-template-hash: 6b95d78644
    name: github-runner-6b95d78644
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "24008043"
    uid: e4b382b1-b234-427b-8b56-3b9b8ce7bb87
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: 6b95d78644
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-27T01:38:23+01:00"
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: 6b95d78644
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: RUNNER_TOKEN
            valueFrom:
              secretKeyRef:
                key: RUNNER_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2026-01-27T00:36:19Z"
    generation: 2
    labels:
      app: github-runner
      pod-template-hash: 6d77d7cb5
    name: github-runner-6d77d7cb5
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "23761276"
    uid: d41c5382-dd97-4302-8a23-7400e60db406
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: 6d77d7cb5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: 6d77d7cb5
      spec:
        containers:
        - args:
          - --insecure-registry=37.60.241.150:30500
          env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: RUNNER_TOKEN
            valueFrom:
              secretKeyRef:
                key: RUNNER_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2026-01-27T19:10:26Z"
    generation: 2
    labels:
      app: github-runner
      pod-template-hash: 6d7b99678d
    name: github-runner-6d7b99678d
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "24292700"
    uid: 46b807f6-e948-4036-a60b-5c5d5f538fbf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: 6d7b99678d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-27T20:10:26+01:00"
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: 6d7b99678d
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: RUNNER_TOKEN
            valueFrom:
              secretKeyRef:
                key: RUNNER_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "9"
    creationTimestamp: "2026-02-02T21:37:25Z"
    generation: 4
    labels:
      app: github-runner
      pod-template-hash: 6f6bd8f49d
    name: github-runner-6f6bd8f49d
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "28896595"
    uid: 04e35dc4-b3d2-494b-9b9b-3e09813aa6d0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: 6f6bd8f49d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-02T22:37:23+01:00"
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: 6f6bd8f49d
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: ACCESS_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2026-01-28T16:15:02Z"
    generation: 2
    labels:
      app: github-runner
      pod-template-hash: 7d569cd488
    name: github-runner-7d569cd488
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "25021115"
    uid: c1b76715-1232-4752-93b8-60ef2c652402
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: 7d569cd488
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-27T20:10:26+01:00"
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: 7d569cd488
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: ACCESS_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2026-01-30T22:58:51Z"
    generation: 2
    labels:
      app: github-runner
      pod-template-hash: 859bbbf7bd
    name: github-runner-859bbbf7bd
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "25334914"
    uid: c02523a0-94de-48b1-9bc7-809c07eca746
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: 859bbbf7bd
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-30T23:58:50+01:00"
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: 859bbbf7bd
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: ACCESS_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2026-01-27T00:38:26Z"
    generation: 2
    labels:
      app: github-runner
      pod-template-hash: d7b8dd446
    name: github-runner-d7b8dd446
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "23761323"
    uid: 943de98e-11e3-40a1-b563-bf17c2cd9099
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: d7b8dd446
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: d7b8dd446
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: RUNNER_TOKEN
            valueFrom:
              secretKeyRef:
                key: RUNNER_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "4"
      deployment.kubernetes.io/max-replicas: "5"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2026-02-02T20:47:17Z"
    generation: 4
    labels:
      app: github-runner
      pod-template-hash: ff5859b8b
    name: github-runner-ff5859b8b
    namespace: github-runner
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: github-runner
      uid: b7c737cd-ef72-48d1-92cd-1bed6908005b
    resourceVersion: "26190108"
    uid: 368e610d-78ef-44b0-b67e-b9b70e2942fe
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: github-runner
        pod-template-hash: ff5859b8b
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-02T21:47:16+01:00"
        creationTimestamp: null
        labels:
          app: github-runner
          pod-template-hash: ff5859b8b
      spec:
        containers:
        - env:
          - name: DOCKER_TLS_CERTDIR
          image: docker:24-dind
          imagePullPolicy: IfNotPresent
          name: dind
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/docker
            name: docker-storage
          - mountPath: /runner/_work
            name: runner-work
          - mountPath: /etc/docker
            name: docker-config
        - env:
          - name: REPO_URL
            valueFrom:
              configMapKeyRef:
                key: RUNNER_REPOSITORY_URL
                name: github-runner-config
          - name: RUNNER_NAME
            valueFrom:
              configMapKeyRef:
                key: RUNNER_NAME
                name: github-runner-config
          - name: ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                key: ACCESS_TOKEN
                name: github-runner-secret
          - name: LABELS
            valueFrom:
              configMapKeyRef:
                key: RUNNER_LABELS
                name: github-runner-config
          - name: RUNNER_WORKDIR
            valueFrom:
              configMapKeyRef:
                key: RUNNER_WORKDIR
                name: github-runner-config
          - name: DISABLE_AUTO_UPDATE
            value: "true"
          - name: DOCKER_HOST
            value: tcp://localhost:2375
          image: myoung34/github-runner:latest
          imagePullPolicy: Always
          name: runner
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /runner/_work
            name: runner-work
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - |
            mkdir -p /etc/docker
            cat > /etc/docker/daemon.json << 'EOF'
            {
              "insecure-registries": ["37.60.241.150:30500"]
            }
            EOF
          image: busybox:1.36
          imagePullPolicy: IfNotPresent
          name: init-docker-config
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker
            name: docker-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: github-runner
        serviceAccountName: github-runner
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            sizeLimit: 30Gi
          name: docker-storage
        - emptyDir:
            sizeLimit: 20Gi
          name: runner-work
        - emptyDir: {}
          name: docker-config
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2026-01-30T22:54:23Z"
    generation: 2
    labels:
      app: apicurio-registry
      component: kafka
      pod-template-hash: 544485f898
    name: apicurio-registry-544485f898
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: apicurio-registry
      uid: 03048f81-3343-47a1-b9da-86bc5e0615c2
    resourceVersion: "25283615"
    uid: ac3311d7-5112-439d-9f96-a6f99213bccb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: apicurio-registry
        pod-template-hash: 544485f898
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-30T23:42:13+01:00"
        creationTimestamp: null
        labels:
          app: apicurio-registry
          component: kafka
          pod-template-hash: 544485f898
      spec:
        containers:
        - env:
          - name: REGISTRY_KAFKASQL_BOOTSTRAP_SERVERS
            value: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
          - name: ENABLE_CCOMPAT_LEGACY_ID_MODE
            value: "true"
          - name: QUARKUS_HTTP_SSL_CERTIFICATE_FILES
            value: /etc/tls/tls.crt
          - name: QUARKUS_HTTP_SSL_CERTIFICATE_KEY_FILES
            value: /etc/tls/tls.key
          - name: QUARKUS_HTTP_SSL_PORT
            value: "8443"
          - name: QUARKUS_HTTP_INSECURE_REQUESTS
            value: disabled
          - name: QUARKUS_OTEL_ENABLED
            value: "true"
          - name: QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://otel-collector.observability.svc.cluster.local:4317
          - name: QUARKUS_MICROMETER_EXPORT_PROMETHEUS_ENABLED
            value: "true"
          - name: QUARKUS_MICROMETER_BINDER_JVM_ENABLED
            value: "true"
          - name: QUARKUS_MICROMETER_BINDER_HTTP_SERVER_ENABLED
            value: "true"
          image: apicurio/apicurio-registry-kafkasql:2.5.11.Final
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: apicurio-registry
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls
            name: tls-certs
            readOnly: true
        - command:
          - /bin/sh
          - -c
          - |
            # Instalar dependencias minimas
            pip install --quiet --no-cache-dir requests urllib3

            # Criar servidor HTTP de health check
            cat > /tmp/health_server.py << 'PYTHON_EOF'
            import http.server
            import socketserver
            import json
            import requests
            import threading
            import time
            import urllib3

            # Disable SSL verification for localhost (self-signed cert)
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

            # Estado global do health check
            health_state = {
                "schemas_ok": False,
                "registry_ready": False,
                "missing_schemas": [],
                "last_check": None
            }

            CRITICAL_SCHEMAS = ["plans.ready-value", "execution.tickets-value"]
            REGISTRY_URL = "https://localhost:8443"

            def check_schemas():
                """Verifica schemas criticos periodicamente"""
                global health_state
                while True:
                    try:
                        # Verificar se registry esta pronto
                        try:
                            resp = requests.get(f"{REGISTRY_URL}/health/ready", timeout=5, verify=False)
                            health_state["registry_ready"] = resp.status_code == 200
                        except:
                            health_state["registry_ready"] = False
                            health_state["schemas_ok"] = False
                            time.sleep(30)
                            continue

                        # Verificar schemas criticos
                        missing = []
                        for subject in CRITICAL_SCHEMAS:
                            try:
                                resp = requests.get(
                                    f"{REGISTRY_URL}/apis/ccompat/v6/subjects/{subject}/versions/latest",
                                    timeout=5,
                                    verify=False
                                )
                                if resp.status_code != 200:
                                    missing.append(subject)
                            except:
                                missing.append(subject)

                        health_state["missing_schemas"] = missing
                        health_state["schemas_ok"] = len(missing) == 0
                        health_state["last_check"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

                        status = "OK" if health_state["schemas_ok"] else "DEGRADED"
                        print(f"[{health_state['last_check']}] Schema check: {status}, missing: {missing}")

                    except Exception as e:
                        print(f"Error checking schemas: {e}")

                    time.sleep(60)

            class HealthHandler(http.server.BaseHTTPRequestHandler):
                def log_message(self, format, *args):
                    pass  # Silenciar logs de requisicao

                def do_GET(self):
                    if self.path == "/health/schemas":
                        if health_state["schemas_ok"]:
                            self.send_response(200)
                            status = "healthy"
                        else:
                            self.send_response(503)
                            status = "unhealthy"

                        self.send_header("Content-Type", "application/json")
                        self.end_headers()
                        response = {
                            "status": status,
                            "registry_ready": health_state["registry_ready"],
                            "schemas_ok": health_state["schemas_ok"],
                            "missing_schemas": health_state["missing_schemas"],
                            "critical_schemas": CRITICAL_SCHEMAS,
                            "last_check": health_state["last_check"]
                        }
                        self.wfile.write(json.dumps(response).encode())

                    elif self.path == "/health/live":
                        self.send_response(200)
                        self.send_header("Content-Type", "text/plain")
                        self.end_headers()
                        self.wfile.write(b"OK")

                    else:
                        self.send_response(404)
                        self.end_headers()

            if __name__ == "__main__":
                # Iniciar thread de verificacao de schemas
                checker_thread = threading.Thread(target=check_schemas, daemon=True)
                checker_thread.start()

                # Iniciar servidor HTTP
                PORT = 8090
                with socketserver.TCPServer(("", PORT), HealthHandler) as httpd:
                    print(f"Schema health server running on port {PORT}")
                    httpd.serve_forever()
            PYTHON_EOF

            python /tmp/health_server.py
          image: python:3.11-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: 8090
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: schema-health-checker
          ports:
          - containerPort: 8090
            name: health
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 20m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-certs
          secret:
            defaultMode: 420
            secretName: schema-registry-tls-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2026-01-31T18:06:20Z"
    generation: 2
    labels:
      app: apicurio-registry
      component: kafka
      pod-template-hash: 64746bc795
    name: apicurio-registry-64746bc795
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: apicurio-registry
      uid: 03048f81-3343-47a1-b9da-86bc5e0615c2
    resourceVersion: "27243900"
    uid: 3895a91b-4f3e-4b85-996f-0efdbfd1cc1b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: apicurio-registry
        pod-template-hash: 64746bc795
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-30T23:42:13+01:00"
        creationTimestamp: null
        labels:
          app: apicurio-registry
          component: kafka
          pod-template-hash: 64746bc795
      spec:
        containers:
        - env:
          - name: REGISTRY_KAFKASQL_BOOTSTRAP_SERVERS
            value: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
          - name: ENABLE_CCOMPAT_LEGACY_ID_MODE
            value: "true"
          - name: QUARKUS_HTTP_SSL_CERTIFICATE_FILES
            value: /etc/tls/tls.crt
          - name: QUARKUS_HTTP_SSL_CERTIFICATE_KEY_FILES
            value: /etc/tls/tls.key
          - name: QUARKUS_HTTP_SSL_PORT
            value: "8443"
          - name: QUARKUS_HTTP_INSECURE_REQUESTS
            value: enabled
          - name: QUARKUS_OTEL_ENABLED
            value: "true"
          - name: QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://otel-collector.observability.svc.cluster.local:4317
          - name: QUARKUS_MICROMETER_EXPORT_PROMETHEUS_ENABLED
            value: "true"
          - name: QUARKUS_MICROMETER_BINDER_JVM_ENABLED
            value: "true"
          - name: QUARKUS_MICROMETER_BINDER_HTTP_SERVER_ENABLED
            value: "true"
          - name: QUARKUS_HTTP_PORT
            value: "8080"
          image: apicurio/apicurio-registry-kafkasql:2.5.11.Final
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: apicurio-registry
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls
            name: tls-certs
            readOnly: true
        - command:
          - /bin/sh
          - -c
          - |
            # Instalar dependencias minimas
            pip install --quiet --no-cache-dir requests urllib3

            # Criar servidor HTTP de health check
            cat > /tmp/health_server.py << 'PYTHON_EOF'
            import http.server
            import socketserver
            import json
            import requests
            import threading
            import time
            import urllib3

            # Disable SSL verification for localhost (self-signed cert)
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

            # Estado global do health check
            health_state = {
                "schemas_ok": False,
                "registry_ready": False,
                "missing_schemas": [],
                "last_check": None
            }

            CRITICAL_SCHEMAS = ["plans.ready-value", "execution.tickets-value"]
            REGISTRY_URL = "https://localhost:8443"

            def check_schemas():
                """Verifica schemas criticos periodicamente"""
                global health_state
                while True:
                    try:
                        # Verificar se registry esta pronto
                        try:
                            resp = requests.get(f"{REGISTRY_URL}/health/ready", timeout=5, verify=False)
                            health_state["registry_ready"] = resp.status_code == 200
                        except:
                            health_state["registry_ready"] = False
                            health_state["schemas_ok"] = False
                            time.sleep(30)
                            continue

                        # Verificar schemas criticos
                        missing = []
                        for subject in CRITICAL_SCHEMAS:
                            try:
                                resp = requests.get(
                                    f"{REGISTRY_URL}/apis/ccompat/v6/subjects/{subject}/versions/latest",
                                    timeout=5,
                                    verify=False
                                )
                                if resp.status_code != 200:
                                    missing.append(subject)
                            except:
                                missing.append(subject)

                        health_state["missing_schemas"] = missing
                        health_state["schemas_ok"] = len(missing) == 0
                        health_state["last_check"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

                        status = "OK" if health_state["schemas_ok"] else "DEGRADED"
                        print(f"[{health_state['last_check']}] Schema check: {status}, missing: {missing}")

                    except Exception as e:
                        print(f"Error checking schemas: {e}")

                    time.sleep(60)

            class HealthHandler(http.server.BaseHTTPRequestHandler):
                def log_message(self, format, *args):
                    pass  # Silenciar logs de requisicao

                def do_GET(self):
                    if self.path == "/health/schemas":
                        if health_state["schemas_ok"]:
                            self.send_response(200)
                            status = "healthy"
                        else:
                            self.send_response(503)
                            status = "unhealthy"

                        self.send_header("Content-Type", "application/json")
                        self.end_headers()
                        response = {
                            "status": status,
                            "registry_ready": health_state["registry_ready"],
                            "schemas_ok": health_state["schemas_ok"],
                            "missing_schemas": health_state["missing_schemas"],
                            "critical_schemas": CRITICAL_SCHEMAS,
                            "last_check": health_state["last_check"]
                        }
                        self.wfile.write(json.dumps(response).encode())

                    elif self.path == "/health/live":
                        self.send_response(200)
                        self.send_header("Content-Type", "text/plain")
                        self.end_headers()
                        self.wfile.write(b"OK")

                    else:
                        self.send_response(404)
                        self.end_headers()

            if __name__ == "__main__":
                # Iniciar thread de verificacao de schemas
                checker_thread = threading.Thread(target=check_schemas, daemon=True)
                checker_thread.start()

                # Iniciar servidor HTTP
                PORT = 8090
                with socketserver.TCPServer(("", PORT), HealthHandler) as httpd:
                    print(f"Schema health server running on port {PORT}")
                    httpd.serve_forever()
            PYTHON_EOF

            python /tmp/health_server.py
          env:
          - name: QUARKUS_HTTP_INSECURE_REQUESTS
            value: enabled
          - name: QUARKUS_HTTP_PORT
            value: "8080"
          image: python:3.11-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: 8090
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: schema-health-checker
          ports:
          - containerPort: 8090
            name: health
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 20m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-certs
          secret:
            defaultMode: 420
            secretName: schema-registry-tls-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2026-02-06T15:57:10Z"
    generation: 1
    labels:
      app: apicurio-registry
      component: kafka
      pod-template-hash: 69fbd98587
    name: apicurio-registry-69fbd98587
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: apicurio-registry
      uid: 03048f81-3343-47a1-b9da-86bc5e0615c2
    resourceVersion: "27243889"
    uid: fb4370e6-77db-4065-a8cf-5a91b26ef094
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: apicurio-registry
        pod-template-hash: 69fbd98587
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-30T23:42:13+01:00"
        creationTimestamp: null
        labels:
          app: apicurio-registry
          component: kafka
          pod-template-hash: 69fbd98587
      spec:
        containers:
        - env:
          - name: REGISTRY_KAFKASQL_BOOTSTRAP_SERVERS
            value: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
          - name: ENABLE_CCOMPAT_LEGACY_ID_MODE
            value: "true"
          - name: QUARKUS_HTTP_SSL_CERTIFICATE_FILES
            value: /etc/tls/tls.crt
          - name: QUARKUS_HTTP_SSL_CERTIFICATE_KEY_FILES
            value: /etc/tls/tls.key
          - name: QUARKUS_HTTP_SSL_PORT
            value: "8443"
          - name: QUARKUS_HTTP_INSECURE_REQUESTS
            value: enabled
          - name: QUARKUS_OTEL_ENABLED
            value: "true"
          - name: QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://otel-collector.observability.svc.cluster.local:4317
          - name: QUARKUS_MICROMETER_EXPORT_PROMETHEUS_ENABLED
            value: "true"
          - name: QUARKUS_MICROMETER_BINDER_JVM_ENABLED
            value: "true"
          - name: QUARKUS_MICROMETER_BINDER_HTTP_SERVER_ENABLED
            value: "true"
          - name: QUARKUS_HTTP_PORT
            value: "8080"
          - name: REGISTRY_CCOMPAT_ENABLED
            value: "true"
          - name: REGISTRY_CCOMPAT-storage-enabled
            value: "true"
          image: apicurio/apicurio-registry-kafkasql:2.5.11.Final
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: apicurio-registry
          ports:
          - containerPort: 8443
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: 8443
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/tls
            name: tls-certs
            readOnly: true
        - command:
          - /bin/sh
          - -c
          - |
            # Instalar dependencias minimas
            pip install --quiet --no-cache-dir requests urllib3

            # Criar servidor HTTP de health check
            cat > /tmp/health_server.py << 'PYTHON_EOF'
            import http.server
            import socketserver
            import json
            import requests
            import threading
            import time
            import urllib3

            # Disable SSL verification for localhost (self-signed cert)
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

            # Estado global do health check
            health_state = {
                "schemas_ok": False,
                "registry_ready": False,
                "missing_schemas": [],
                "last_check": None
            }

            CRITICAL_SCHEMAS = ["plans.ready-value", "execution.tickets-value"]
            REGISTRY_URL = "https://localhost:8443"

            def check_schemas():
                """Verifica schemas criticos periodicamente"""
                global health_state
                while True:
                    try:
                        # Verificar se registry esta pronto
                        try:
                            resp = requests.get(f"{REGISTRY_URL}/health/ready", timeout=5, verify=False)
                            health_state["registry_ready"] = resp.status_code == 200
                        except:
                            health_state["registry_ready"] = False
                            health_state["schemas_ok"] = False
                            time.sleep(30)
                            continue

                        # Verificar schemas criticos
                        missing = []
                        for subject in CRITICAL_SCHEMAS:
                            try:
                                resp = requests.get(
                                    f"{REGISTRY_URL}/apis/ccompat/v6/subjects/{subject}/versions/latest",
                                    timeout=5,
                                    verify=False
                                )
                                if resp.status_code != 200:
                                    missing.append(subject)
                            except:
                                missing.append(subject)

                        health_state["missing_schemas"] = missing
                        health_state["schemas_ok"] = len(missing) == 0
                        health_state["last_check"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

                        status = "OK" if health_state["schemas_ok"] else "DEGRADED"
                        print(f"[{health_state['last_check']}] Schema check: {status}, missing: {missing}")

                    except Exception as e:
                        print(f"Error checking schemas: {e}")

                    time.sleep(60)

            class HealthHandler(http.server.BaseHTTPRequestHandler):
                def log_message(self, format, *args):
                    pass  # Silenciar logs de requisicao

                def do_GET(self):
                    if self.path == "/health/schemas":
                        if health_state["schemas_ok"]:
                            self.send_response(200)
                            status = "healthy"
                        else:
                            self.send_response(503)
                            status = "unhealthy"

                        self.send_header("Content-Type", "application/json")
                        self.end_headers()
                        response = {
                            "status": status,
                            "registry_ready": health_state["registry_ready"],
                            "schemas_ok": health_state["schemas_ok"],
                            "missing_schemas": health_state["missing_schemas"],
                            "critical_schemas": CRITICAL_SCHEMAS,
                            "last_check": health_state["last_check"]
                        }
                        self.wfile.write(json.dumps(response).encode())

                    elif self.path == "/health/live":
                        self.send_response(200)
                        self.send_header("Content-Type", "text/plain")
                        self.end_headers()
                        self.wfile.write(b"OK")

                    else:
                        self.send_response(404)
                        self.end_headers()

            if __name__ == "__main__":
                # Iniciar thread de verificacao de schemas
                checker_thread = threading.Thread(target=check_schemas, daemon=True)
                checker_thread.start()

                # Iniciar servidor HTTP
                PORT = 8090
                with socketserver.TCPServer(("", PORT), HealthHandler) as httpd:
                    print(f"Schema health server running on port {PORT}")
                    httpd.serve_forever()
            PYTHON_EOF

            python /tmp/health_server.py
          env:
          - name: QUARKUS_HTTP_INSECURE_REQUESTS
            value: enabled
          - name: QUARKUS_HTTP_PORT
            value: "8080"
          - name: REGISTRY_CCOMPAT_ENABLED
            value: "true"
          - name: REGISTRY_CCOMPAT-storage-enabled
            value: "true"
          image: python:3.11-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: 8090
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: schema-health-checker
          ports:
          - containerPort: 8090
            name: health
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 20m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-certs
          secret:
            defaultMode: 420
            secretName: schema-registry-tls-secret
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-26T09:46:10Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: neural-hive-kafka
      app.kubernetes.io/managed-by: strimzi-cluster-operator
      app.kubernetes.io/name: entity-operator
      app.kubernetes.io/part-of: strimzi-neural-hive-kafka
      pod-template-hash: 67df88987b
      strimzi.io/cluster: neural-hive-kafka
      strimzi.io/component-type: entity-operator
      strimzi.io/kind: Kafka
      strimzi.io/name: neural-hive-kafka-entity-operator
    name: neural-hive-kafka-entity-operator-67df88987b
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: neural-hive-kafka-entity-operator
      uid: bad3b5b4-a881-44ab-9416-3d5bec7567a3
    resourceVersion: "26775199"
    uid: c8f9eef1-d6ff-4406-adf6-5c8388aa487d
  spec:
    replicas: 1
    selector:
      matchLabels:
        pod-template-hash: 67df88987b
        strimzi.io/cluster: neural-hive-kafka
        strimzi.io/kind: Kafka
        strimzi.io/name: neural-hive-kafka-entity-operator
    template:
      metadata:
        annotations:
          strimzi.io/cluster-ca-cert-generation: "0"
          strimzi.io/cluster-ca-key-generation: "0"
          strimzi.io/server-cert-hash: 37244d10
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: neural-hive-kafka
          app.kubernetes.io/managed-by: strimzi-cluster-operator
          app.kubernetes.io/name: entity-operator
          app.kubernetes.io/part-of: strimzi-neural-hive-kafka
          pod-template-hash: 67df88987b
          strimzi.io/cluster: neural-hive-kafka
          strimzi.io/component-type: entity-operator
          strimzi.io/kind: Kafka
          strimzi.io/name: neural-hive-kafka-entity-operator
      spec:
        containers:
        - args:
          - /opt/strimzi/bin/topic_operator_run.sh
          env:
          - name: STRIMZI_RESOURCE_LABELS
            value: strimzi.io/cluster=neural-hive-kafka
          - name: STRIMZI_KAFKA_BOOTSTRAP_SERVERS
            value: neural-hive-kafka-kafka-bootstrap:9091
          - name: STRIMZI_NAMESPACE
            value: kafka
          - name: STRIMZI_SECURITY_PROTOCOL
            value: SSL
          - name: STRIMZI_TLS_ENABLED
            value: "true"
          - name: STRIMZI_GC_LOG_ENABLED
            value: "false"
          image: quay.io/strimzi/operator:0.48.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthy
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: topic-operator
          ports:
          - containerPort: 8080
            name: healthcheck
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          startupProbe:
            failureThreshold: 12
            httpGet:
              path: /healthy
              port: healthcheck
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: strimzi-to-tmp
          - mountPath: /opt/topic-operator/custom-config/
            name: entity-topic-operator-metrics-and-logging
          - mountPath: /etc/eto-certs/
            name: eto-certs
          - mountPath: /etc/tls-sidecar/cluster-ca-certs/
            name: cluster-ca-certs
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: neural-hive-kafka-entity-operator
        serviceAccountName: neural-hive-kafka-entity-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: neural-hive-kafka-entity-topic-operator-config
          name: entity-topic-operator-metrics-and-logging
        - emptyDir:
            medium: Memory
            sizeLimit: 5Mi
          name: strimzi-to-tmp
        - name: eto-certs
          secret:
            defaultMode: 292
            secretName: neural-hive-kafka-entity-topic-operator-certs
        - name: cluster-ca-certs
          secret:
            defaultMode: 292
            secretName: neural-hive-kafka-cluster-ca-cert
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      deployment.kubernetes.io/revision-history: "1"
    creationTimestamp: "2025-10-29T11:22:05Z"
    generation: 1
    labels:
      name: strimzi-cluster-operator
      pod-template-hash: fd565f467
      strimzi.io/kind: cluster-operator
    name: strimzi-cluster-operator-fd565f467
    namespace: kafka
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: strimzi-cluster-operator
      uid: fa0cc085-65a7-46ee-99e9-e285ab9e5f4f
    resourceVersion: "26775258"
    uid: 52b787ef-f045-43ae-a16c-d3224d156421
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: strimzi-cluster-operator
        pod-template-hash: fd565f467
        strimzi.io/kind: cluster-operator
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: strimzi-cluster-operator
          pod-template-hash: fd565f467
          strimzi.io/kind: cluster-operator
      spec:
        containers:
        - args:
          - /opt/strimzi/bin/cluster_operator_run.sh
          env:
          - name: STRIMZI_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS
            value: "120000"
          - name: STRIMZI_OPERATION_TIMEOUT_MS
            value: "300000"
          - name: STRIMZI_DEFAULT_KAFKA_EXPORTER_IMAGE
            value: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_DEFAULT_CRUISE_CONTROL_IMAGE
            value: quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_KAFKA_IMAGES
            value: |
              4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
              4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_KAFKA_CONNECT_IMAGES
            value: |
              4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
              4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_KAFKA_MIRROR_MAKER_2_IMAGES
            value: |
              4.0.0=quay.io/strimzi/kafka:0.48.0-kafka-4.0.0
              4.1.0=quay.io/strimzi/kafka:0.48.0-kafka-4.1.0
          - name: STRIMZI_DEFAULT_TOPIC_OPERATOR_IMAGE
            value: quay.io/strimzi/operator:0.48.0
          - name: STRIMZI_DEFAULT_USER_OPERATOR_IMAGE
            value: quay.io/strimzi/operator:0.48.0
          - name: STRIMZI_DEFAULT_KAFKA_INIT_IMAGE
            value: quay.io/strimzi/operator:0.48.0
          - name: STRIMZI_DEFAULT_KAFKA_BRIDGE_IMAGE
            value: quay.io/strimzi/kafka-bridge:0.33.1
          - name: STRIMZI_DEFAULT_KANIKO_EXECUTOR_IMAGE
            value: quay.io/strimzi/kaniko-executor:0.48.0
          - name: STRIMZI_DEFAULT_MAVEN_BUILDER
            value: quay.io/strimzi/maven-builder:0.48.0
          - name: STRIMZI_OPERATOR_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: STRIMZI_FEATURE_GATES
          - name: STRIMZI_LEADER_ELECTION_ENABLED
            value: "true"
          - name: STRIMZI_LEADER_ELECTION_LEASE_NAME
            value: strimzi-cluster-operator
          - name: STRIMZI_LEADER_ELECTION_LEASE_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: STRIMZI_LEADER_ELECTION_IDENTITY
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: quay.io/strimzi/operator:0.48.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthy
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: strimzi-cluster-operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 384Mi
            requests:
              cpu: 200m
              memory: 384Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: strimzi-tmp
          - mountPath: /opt/strimzi/custom-config/
            name: co-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: strimzi-cluster-operator
        serviceAccountName: strimzi-cluster-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 1Mi
          name: strimzi-tmp
        - configMap:
            defaultMode: 420
            name: strimzi-cluster-operator
          name: co-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "6"
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T21:14:47Z"
    generation: 5
    labels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
      neural-hive.io/component: auth
      neural-hive.io/layer: security
      pod-template-hash: 584c5cd787
    name: keycloak-584c5cd787
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keycloak
      uid: 149d590b-3743-4f01-8e90-f52003e4766c
    resourceVersion: "27310633"
    uid: ebb166bb-2650-4640-a455-c25482020c79
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: keycloak
        pod-template-hash: 584c5cd787
    template:
      metadata:
        annotations:
          checksum/config: 7206792e292638a83360af2cccaf0cf0710804f5815079b6f460602bfcdb47f6
          checksum/secret: 3859a7c073bbb858b1d0db6e1eb821fefc09594aadca3657309caed214199e81
          neural-hive.io/data-classification: confidential
          neural-hive.io/monitoring: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "9000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: keycloak
          app.kubernetes.io/name: keycloak
          neural-hive.io/component: auth
          neural-hive.io/layer: security
          pod-template-hash: 584c5cd787
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: keycloak
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - start
          - --import-realm
          env:
          - name: KEYCLOAK_ADMIN
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: keycloak-secret
          - name: KEYCLOAK_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: keycloak-secret
          - name: KC_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: keycloak-postgresql
          - name: KC_HEALTH_ENABLED
            value: "true"
          - name: KC_METRICS_ENABLED
            value: "true"
          - name: KC_LOG_LEVEL
            value: INFO
          - name: JAVA_OPTS_APPEND
            value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
          image: quay.io/keycloak/keycloak:22.0.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: keycloak
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 9000
            name: management
            protocol: TCP
          - containerPort: 9000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/keycloak/conf/keycloak.conf
            name: keycloak-config
            readOnly: true
            subPath: keycloak.conf
          - mountPath: /opt/keycloak/data/import/realm.json
            name: realm-import
            readOnly: true
            subPath: realm.json
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...';
            sleep 2; done; echo 'Database is ready!'
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          name: wait-for-db
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: keycloak
        serviceAccountName: keycloak
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: keycloak-config
          name: keycloak-config
        - configMap:
            defaultMode: 420
            name: keycloak-realm
          name: realm-import
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "7"
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-02-06T15:41:31Z"
    generation: 4
    labels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
      neural-hive.io/component: auth
      neural-hive.io/layer: security
      pod-template-hash: 5cdb4cbfd
    name: keycloak-5cdb4cbfd
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keycloak
      uid: 149d590b-3743-4f01-8e90-f52003e4766c
    resourceVersion: "29939598"
    uid: 840e3d53-fc2b-47d7-870e-564fa09c90dd
  spec:
    replicas: 2
    selector:
      matchLabels:
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: keycloak
        pod-template-hash: 5cdb4cbfd
    template:
      metadata:
        annotations:
          checksum/config: 7206792e292638a83360af2cccaf0cf0710804f5815079b6f460602bfcdb47f6
          checksum/secret: 3859a7c073bbb858b1d0db6e1eb821fefc09594aadca3657309caed214199e81
          neural-hive.io/data-classification: confidential
          neural-hive.io/monitoring: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "9000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: keycloak
          app.kubernetes.io/name: keycloak
          neural-hive.io/component: auth
          neural-hive.io/layer: security
          pod-template-hash: 5cdb4cbfd
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: keycloak
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - start
          - --import-realm
          env:
          - name: KEYCLOAK_ADMIN
            value: admin
          - name: KEYCLOAK_ADMIN_PASSWORD
            value: Admin123\!
          - name: KC_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: keycloak-postgresql
          - name: KC_HEALTH_ENABLED
            value: "true"
          - name: KC_METRICS_ENABLED
            value: "true"
          - name: KC_LOG_LEVEL
            value: INFO
          - name: JAVA_OPTS_APPEND
            value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
          image: quay.io/keycloak/keycloak:22.0.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: keycloak
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 9000
            name: management
            protocol: TCP
          - containerPort: 9000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/keycloak/conf/keycloak.conf
            name: keycloak-config
            readOnly: true
            subPath: keycloak.conf
          - mountPath: /opt/keycloak/data/import/realm.json
            name: realm-import
            readOnly: true
            subPath: realm.json
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...';
            sleep 2; done; echo 'Database is ready!'
          env:
          - name: KEYCLOAK_ADMIN
            value: admin
          - name: KEYCLOAK_ADMIN_PASSWORD
            value: Admin123\!
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          name: wait-for-db
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: keycloak
        serviceAccountName: keycloak
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: keycloak-config
          name: keycloak-config
        - configMap:
            defaultMode: 420
            name: keycloak-realm
          name: realm-import
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 2
    observedGeneration: 4
    readyReplicas: 1
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T10:46:37Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
      neural-hive.io/component: auth
      neural-hive.io/layer: security
      pod-template-hash: 7644f86cc8
    name: keycloak-7644f86cc8
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keycloak
      uid: 149d590b-3743-4f01-8e90-f52003e4766c
    resourceVersion: "20716193"
    uid: 5783900f-8f4f-4901-b092-65d6aabefa34
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: keycloak
        pod-template-hash: 7644f86cc8
    template:
      metadata:
        annotations:
          checksum/config: 33da136a9300d229335651a23198620dde9701c330eb98b32f0689949d8c7083
          checksum/secret: 9ff014297eb8b7d68eb71140a4d9fc755220d9b34d16a234edfe2e9820959b57
          neural-hive.io/data-classification: confidential
          neural-hive.io/monitoring: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "9000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: keycloak
          app.kubernetes.io/name: keycloak
          neural-hive.io/component: auth
          neural-hive.io/layer: security
          pod-template-hash: 7644f86cc8
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: keycloak
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - start
          - --import-realm
          env:
          - name: KEYCLOAK_ADMIN
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: keycloak-secret
          - name: KEYCLOAK_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: keycloak-secret
          - name: KC_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: keycloak-postgresql
          - name: KC_HEALTH_ENABLED
            value: "true"
          - name: KC_METRICS_ENABLED
            value: "true"
          - name: KC_LOG_LEVEL
            value: INFO
          - name: JAVA_OPTS_APPEND
            value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
          image: 37.60.241.150:30500/keycloak/keycloak:22.0.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: management
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: keycloak
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 9000
            name: management
            protocol: TCP
          - containerPort: 9000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: management
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: management
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/keycloak/conf/keycloak.conf
            name: keycloak-config
            readOnly: true
            subPath: keycloak.conf
          - mountPath: /opt/keycloak/data/import/realm.json
            name: realm-import
            readOnly: true
            subPath: realm.json
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...';
            sleep 2; done; echo 'Database is ready!'
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          name: wait-for-db
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: keycloak
        serviceAccountName: keycloak
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: keycloak-config
          name: keycloak-config
        - configMap:
            defaultMode: 420
            name: keycloak-realm
          name: realm-import
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T21:07:39Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
      neural-hive.io/component: auth
      neural-hive.io/layer: security
      pod-template-hash: 76bd894b78
    name: keycloak-76bd894b78
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keycloak
      uid: 149d590b-3743-4f01-8e90-f52003e4766c
    resourceVersion: "20856084"
    uid: 9fd02851-3582-48df-8afe-3b12218cb2cb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: keycloak
        pod-template-hash: 76bd894b78
    template:
      metadata:
        annotations:
          checksum/config: de1ba6e7d1e426ca33f919cd3d43138b742ba872413860a0b75666e0aebab3ac
          checksum/secret: 17599402e07a60f22c00b26ae7e0a4253216072b27628e02e515f3004a4e9a7f
          neural-hive.io/data-classification: confidential
          neural-hive.io/monitoring: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "9000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: keycloak
          app.kubernetes.io/name: keycloak
          neural-hive.io/component: auth
          neural-hive.io/layer: security
          pod-template-hash: 76bd894b78
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: keycloak
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - start
          - --import-realm
          env:
          - name: KEYCLOAK_ADMIN
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: keycloak-secret
          - name: KEYCLOAK_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: keycloak-secret
          - name: KC_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: keycloak-postgresql
          - name: KC_HEALTH_ENABLED
            value: "true"
          - name: KC_METRICS_ENABLED
            value: "true"
          - name: KC_LOG_LEVEL
            value: INFO
          - name: JAVA_OPTS_APPEND
            value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
          image: quay.io/keycloak/keycloak:22.0.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: keycloak
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 9000
            name: management
            protocol: TCP
          - containerPort: 9000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/keycloak/conf/keycloak.conf
            name: keycloak-config
            readOnly: true
            subPath: keycloak.conf
          - mountPath: /opt/keycloak/data/import/realm.json
            name: realm-import
            readOnly: true
            subPath: realm.json
          - mountPath: /opt/keycloak/certs
            name: tls-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...';
            sleep 2; done; echo 'Database is ready!'
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          name: wait-for-db
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: keycloak
        serviceAccountName: keycloak
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: keycloak-config
          name: keycloak-config
        - configMap:
            defaultMode: 420
            name: keycloak-realm
          name: realm-import
        - name: tls-certs
          secret:
            defaultMode: 420
            secretName: keycloak-tls
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T10:50:20Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
      neural-hive.io/component: auth
      neural-hive.io/layer: security
      pod-template-hash: 7c59cc5d4c
    name: keycloak-7c59cc5d4c
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keycloak
      uid: 149d590b-3743-4f01-8e90-f52003e4766c
    resourceVersion: "20724968"
    uid: ac02af2d-bd4e-48bc-8b95-ed156dc5d3da
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: keycloak
        pod-template-hash: 7c59cc5d4c
    template:
      metadata:
        annotations:
          checksum/config: 33da136a9300d229335651a23198620dde9701c330eb98b32f0689949d8c7083
          checksum/secret: 9ff014297eb8b7d68eb71140a4d9fc755220d9b34d16a234edfe2e9820959b57
          neural-hive.io/data-classification: confidential
          neural-hive.io/monitoring: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "9000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: keycloak
          app.kubernetes.io/name: keycloak
          neural-hive.io/component: auth
          neural-hive.io/layer: security
          pod-template-hash: 7c59cc5d4c
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: keycloak
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - start
          - --import-realm
          env:
          - name: KEYCLOAK_ADMIN
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: keycloak-secret
          - name: KEYCLOAK_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: keycloak-secret
          - name: KC_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: keycloak-postgresql
          - name: KC_HEALTH_ENABLED
            value: "true"
          - name: KC_METRICS_ENABLED
            value: "true"
          - name: KC_LOG_LEVEL
            value: INFO
          - name: JAVA_OPTS_APPEND
            value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
          image: 37.60.241.150:30500/keycloak/keycloak:22.0.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: keycloak
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 9000
            name: management
            protocol: TCP
          - containerPort: 9000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/keycloak/conf/keycloak.conf
            name: keycloak-config
            readOnly: true
            subPath: keycloak.conf
          - mountPath: /opt/keycloak/data/import/realm.json
            name: realm-import
            readOnly: true
            subPath: realm.json
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...';
            sleep 2; done; echo 'Database is ready!'
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          name: wait-for-db
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: keycloak
        serviceAccountName: keycloak
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: keycloak-config
          name: keycloak-config
        - configMap:
            defaultMode: 420
            name: keycloak-realm
          name: realm-import
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "5"
      deployment.kubernetes.io/revision-history: "3"
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T11:26:54Z"
    generation: 4
    labels:
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/name: keycloak
      neural-hive.io/component: auth
      neural-hive.io/layer: security
      pod-template-hash: 99875b596
    name: keycloak-99875b596
    namespace: keycloak
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: keycloak
      uid: 149d590b-3743-4f01-8e90-f52003e4766c
    resourceVersion: "20856292"
    uid: 967a1048-58bf-47af-a616-bfefaefc05dc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: keycloak
        pod-template-hash: 99875b596
    template:
      metadata:
        annotations:
          checksum/config: 250bd37c6b4217a74a5c72f4f6fa6070d2382518bd72fc1701943ff29e109dbc
          checksum/secret: 9ff014297eb8b7d68eb71140a4d9fc755220d9b34d16a234edfe2e9820959b57
          neural-hive.io/data-classification: confidential
          neural-hive.io/monitoring: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "9000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: keycloak
          app.kubernetes.io/name: keycloak
          neural-hive.io/component: auth
          neural-hive.io/layer: security
          pod-template-hash: 99875b596
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: keycloak
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - args:
          - start
          - --import-realm
          env:
          - name: KEYCLOAK_ADMIN
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: keycloak-secret
          - name: KEYCLOAK_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: keycloak-secret
          - name: KC_DB_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: keycloak-postgresql
          - name: KC_HEALTH_ENABLED
            value: "true"
          - name: KC_METRICS_ENABLED
            value: "true"
          - name: KC_LOG_LEVEL
            value: INFO
          - name: JAVA_OPTS_APPEND
            value: -Xms1g -Xmx2g -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local
          image: 37.60.241.150:30500/keycloak/keycloak:22.0.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: keycloak
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 8443
            name: https
            protocol: TCP
          - containerPort: 9000
            name: management
            protocol: TCP
          - containerPort: 9000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/keycloak/conf/keycloak.conf
            name: keycloak-config
            readOnly: true
            subPath: keycloak.conf
          - mountPath: /opt/keycloak/data/import/realm.json
            name: realm-import
            readOnly: true
            subPath: realm.json
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - until nc -z keycloak-postgresql 5432; do echo 'Waiting for database...';
            sleep 2; done; echo 'Database is ready!'
          image: busybox:1.35
          imagePullPolicy: IfNotPresent
          name: wait-for-db
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: keycloak
        serviceAccountName: keycloak
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: keycloak-config
          name: keycloak-config
        - configMap:
            defaultMode: 420
            name: keycloak-realm
          name: realm-import
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-29T10:54:50Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 76f75df574
    name: coredns-76f75df574
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 8964e6ea-902c-498b-91ff-683f73cc79be
    resourceVersion: "29800168"
    uid: 41b0f393-d087-4af1-9083-db4575f72821
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 76f75df574
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 76f75df574
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: k8s-app
                    operator: In
                    values:
                    - kube-dns
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: registry.k8s.io/coredns/coredns:v1.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: external-dns
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-12-10T14:00:09Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: external-dns
      app.kubernetes.io/name: external-dns
      pod-template-hash: 55c5bfbf97
    name: external-dns-55c5bfbf97
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: external-dns
      uid: 740e3866-6e4d-4d6f-a4b5-235bbff9ca8c
    resourceVersion: "29903090"
    uid: 24ecf122-7247-49f1-90f3-dd4efc122daa
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: external-dns
        app.kubernetes.io/name: external-dns
        pod-template-hash: 55c5bfbf97
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: external-dns
          app.kubernetes.io/name: external-dns
          pod-template-hash: 55c5bfbf97
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --log-level=info
          - --log-format=text
          - --interval=1m
          - --source=ingress
          - --source=service
          - --policy=sync
          - --registry=txt
          - --txt-owner-id=neural-hive-k8s
          - --domain-filter=elysiumii.site
          - --provider=cloudflare
          env:
          - name: CF_API_TOKEN
            valueFrom:
              secretKeyRef:
                key: api-token
                name: cloudflare-api-token
          image: registry.k8s.io/external-dns/external-dns:v0.19.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 2
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: external-dns
          ports:
          - containerPort: 7979
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: external-dns
        serviceAccountName: external-dns
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: external-dns
      meta.helm.sh/release-namespace: kube-system
    creationTimestamp: "2025-12-10T08:53:21Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: external-dns
      app.kubernetes.io/name: external-dns
      pod-template-hash: 59547fcd77
    name: external-dns-59547fcd77
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: external-dns
      uid: 740e3866-6e4d-4d6f-a4b5-235bbff9ca8c
    resourceVersion: "8295058"
    uid: 80234280-4071-4bc6-bd89-dfe3d8b4c46b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: external-dns
        app.kubernetes.io/name: external-dns
        pod-template-hash: 59547fcd77
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: external-dns
          app.kubernetes.io/name: external-dns
          pod-template-hash: 59547fcd77
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --log-level=info
          - --log-format=text
          - --interval=1m
          - --source=ingress
          - --source=service
          - --policy=sync
          - --registry=txt
          - --txt-owner-id=neural-hive-k8s
          - --domain-filter=elysiumii.com
          - --provider=cloudflare
          env:
          - name: CF_API_TOKEN
            valueFrom:
              secretKeyRef:
                key: api-token
                name: cloudflare-api-token
          image: registry.k8s.io/external-dns/external-dns:v0.19.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 2
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: external-dns
          ports:
          - containerPort: 7979
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 65532
            runAsNonRoot: true
            runAsUser: 65532
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: external-dns
        serviceAccountName: external-dns
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-26T09:48:20Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 59d465df9f
    name: metrics-server-59d465df9f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 50027e81-9fd9-46bc-9b07-9bf36bf95ceb
    resourceVersion: "25557958"
    uid: edd0abce-57fe-4f7a-bd83-a5c298b077c3
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 59d465df9f
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 59d465df9f
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          - --kubelet-insecure-tls
          image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-11-26T09:48:15Z"
    generation: 2
    labels:
      k8s-app: metrics-server
      pod-template-hash: 856f767b
    name: metrics-server-856f767b
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server
      uid: 50027e81-9fd9-46bc-9b07-9bf36bf95ceb
    resourceVersion: "5049042"
    uid: 2d87aa65-8914-4c7d-a20c-c2ee432f5a84
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 856f767b
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 856f767b
      spec:
        containers:
        - args:
          - --cert-dir=/tmp
          - --secure-port=10250
          - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
          - --kubelet-use-node-status-port
          - --metric-resolution=15s
          image: registry.k8s.io/metrics-server/metrics-server:v0.8.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: metrics-server
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: https
              scheme: HTTPS
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-dir
        dnsPolicy: ClusterFirst
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp-dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-11-21T08:29:57Z"
    generation: 1
    labels:
      app: local-path-provisioner
      pod-template-hash: 844bd8758f
    name: local-path-provisioner-844bd8758f
    namespace: local-path-storage
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: local-path-provisioner
      uid: a4326b79-278f-4d45-a631-27cb706c9e4f
    resourceVersion: "25557845"
    uid: 6e7d7a0a-5858-4b45-afb5-91c7dc96869d
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: local-path-provisioner
        pod-template-hash: 844bd8758f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: local-path-provisioner
          pod-template-hash: 844bd8758f
      spec:
        containers:
        - command:
          - local-path-provisioner
          - --debug
          - start
          - --config
          - /etc/config/config.json
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rancher/local-path-provisioner:v0.0.24
          imagePullPolicy: IfNotPresent
          name: local-path-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config/
            name: config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: local-path-provisioner-service-account
        serviceAccountName: local-path-provisioner-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: local-path-config
          name: config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-27T23:29:14Z"
    generation: 1
    labels:
      app: csi-attacher
      pod-template-hash: 57f8656cc6
    name: csi-attacher-57f8656cc6
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-attacher
      uid: b79461e0-0dd5-44e6-95b9-5a2a4da7b5ba
    resourceVersion: "25557799"
    uid: 5f3ee051-73a6-47e7-992d-52401f719ffa
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: csi-attacher
        pod-template-hash: 57f8656cc6
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: csi-attacher
          pod-template-hash: 57f8656cc6
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-attacher
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-attacher:v4.10.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          ports:
          - containerPort: 8000
            name: csi-attacher
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:23Z"
    generation: 2
    labels:
      app: csi-attacher
      pod-template-hash: 8b9895957
    name: csi-attacher-8b9895957
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-attacher
      uid: b79461e0-0dd5-44e6-95b9-5a2a4da7b5ba
    resourceVersion: "14003156"
    uid: bdb9467f-4504-4ad5-8824-fb254980001f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: csi-attacher
        pod-template-hash: 8b9895957
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-attacher
          pod-template-hash: 8b9895957
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-attacher
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-attacher:v4.10.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          ports:
          - containerPort: 8000
            name: csi-attacher
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:23Z"
    generation: 2
    labels:
      app: csi-provisioner
      pod-template-hash: 6696d6475d
    name: csi-provisioner-6696d6475d
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-provisioner
      uid: 1b9e324c-9989-45b5-98be-6b5900f7e877
    resourceVersion: "14003290"
    uid: 7a0fe951-ecae-43ac-a766-69d877f64341
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: csi-provisioner
        pod-template-hash: 6696d6475d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-provisioner
          pod-template-hash: 6696d6475d
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-provisioner
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --default-fstype=ext4
          - --enable-capacity
          - --capacity-ownerref-level=2
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-provisioner:v5.3.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          ports:
          - containerPort: 8000
            name: csi-provisioner
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-27T23:29:14Z"
    generation: 1
    labels:
      app: csi-provisioner
      pod-template-hash: 6f58d758d9
    name: csi-provisioner-6f58d758d9
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-provisioner
      uid: 1b9e324c-9989-45b5-98be-6b5900f7e877
    resourceVersion: "25557791"
    uid: 21da8bbf-0c13-4888-93b3-6415faff5f90
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: csi-provisioner
        pod-template-hash: 6f58d758d9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: csi-provisioner
          pod-template-hash: 6f58d758d9
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-provisioner
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --default-fstype=ext4
          - --enable-capacity
          - --capacity-ownerref-level=2
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-provisioner:v5.3.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          ports:
          - containerPort: 8000
            name: csi-provisioner
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:24Z"
    generation: 2
    labels:
      app: csi-resizer
      pod-template-hash: 68f78bf796
    name: csi-resizer-68f78bf796
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-resizer
      uid: bead408c-7be4-483d-bcca-7af0e4b535d6
    resourceVersion: "14003254"
    uid: 8f9575ea-f9cb-4a1d-8791-0e6947ec27d3
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: csi-resizer
        pod-template-hash: 68f78bf796
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-resizer
          pod-template-hash: 68f78bf796
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-resizer
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=false
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-resizer:v1.14.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          ports:
          - containerPort: 8000
            name: csi-resizer
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-27T23:29:14Z"
    generation: 1
    labels:
      app: csi-resizer
      pod-template-hash: 755f48c7c9
    name: csi-resizer-755f48c7c9
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-resizer
      uid: bead408c-7be4-483d-bcca-7af0e4b535d6
    resourceVersion: "25557661"
    uid: fd835bd0-3b46-4c5c-a0af-b2e4a76bd2db
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: csi-resizer
        pod-template-hash: 755f48c7c9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: csi-resizer
          pod-template-hash: 755f48c7c9
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-resizer
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          - --handle-volume-inuse-error=false
          - --feature-gates=RecoverVolumeExpansionFailure=false
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-resizer:v1.14.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          ports:
          - containerPort: 8000
            name: csi-resizer
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-27T23:29:15Z"
    generation: 1
    labels:
      app: csi-snapshotter
      pod-template-hash: 7959bd58bb
    name: csi-snapshotter-7959bd58bb
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-snapshotter
      uid: 3feadb0f-876c-4ac3-ad47-23e95fea5d25
    resourceVersion: "25557682"
    uid: 56c67273-a2c5-4b15-b59f-70cf172dd5f1
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: csi-snapshotter
        pod-template-hash: 7959bd58bb
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: csi-snapshotter
          pod-template-hash: 7959bd58bb
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-snapshotter
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-snapshotter:v8.4.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          ports:
          - containerPort: 8000
            name: csi-snapshotter
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      driver.longhorn.io/git-commit: 29b7fda2692c8df5b1aa18c477f77863c7acc1e7
      driver.longhorn.io/version: v1.10.1
      longhorn.io/last-applied-tolerations: '[]'
    creationTimestamp: "2025-12-09T21:23:24Z"
    generation: 2
    labels:
      app: csi-snapshotter
      pod-template-hash: fb648c759
    name: csi-snapshotter-fb648c759
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-snapshotter
      uid: 3feadb0f-876c-4ac3-ad47-23e95fea5d25
    resourceVersion: "14003324"
    uid: 216885e7-494f-4d34-9beb-e69704be912d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: csi-snapshotter
        pod-template-hash: fb648c759
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-snapshotter
          pod-template-hash: fb648c759
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - csi-snapshotter
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - --v=2
          - --csi-address=$(ADDRESS)
          - --timeout=1m50s
          - --leader-election
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --kube-api-qps=50
          - --kube-api-burst=100
          - --http-endpoint=:8000
          env:
          - name: ADDRESS
            value: /csi/csi.sock
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: longhornio/csi-snapshotter:v8.4.0-20251030
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          ports:
          - containerPort: 8000
            name: csi-snapshotter
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi/
            name: socket-dir
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/driver.longhorn.io
            type: DirectoryOrCreate
          name: socket-dir
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:06Z"
    generation: 2
    labels:
      app: longhorn-driver-deployer
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
      pod-template-hash: 69fc49fc89
    name: longhorn-driver-deployer-69fc49fc89
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: longhorn-driver-deployer
      uid: 05740e92-28a4-46a3-af32-61972d9760ad
    resourceVersion: "14003128"
    uid: 1e31421b-f752-496e-8018-df1c9c7b1b6a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: longhorn-driver-deployer
        pod-template-hash: 69fc49fc89
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: longhorn-driver-deployer
          app.kubernetes.io/instance: longhorn
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: longhorn
          app.kubernetes.io/version: v1.10.1
          helm.sh/chart: longhorn-1.10.1
          pod-template-hash: 69fc49fc89
      spec:
        containers:
        - command:
          - longhorn-manager
          - -d
          - deploy-driver
          - --manager-image
          - longhornio/longhorn-manager:v1.10.1
          - --manager-url
          - http://longhorn-backend:9500/v1
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: CSI_ATTACHER_IMAGE
            value: longhornio/csi-attacher:v4.10.0-20251030
          - name: CSI_PROVISIONER_IMAGE
            value: longhornio/csi-provisioner:v5.3.0-20251030
          - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE
            value: longhornio/csi-node-driver-registrar:v2.15.0-20251030
          - name: CSI_RESIZER_IMAGE
            value: longhornio/csi-resizer:v1.14.0-20251030
          - name: CSI_SNAPSHOTTER_IMAGE
            value: longhornio/csi-snapshotter:v8.4.0-20251030
          - name: CSI_LIVENESS_PROBE_IMAGE
            value: longhornio/livenessprobe:v2.17.0-20251030
          - name: CSI_ATTACHER_REPLICA_COUNT
            value: "1"
          - name: CSI_PROVISIONER_REPLICA_COUNT
            value: "1"
          - name: CSI_RESIZER_REPLICA_COUNT
            value: "1"
          - name: CSI_SNAPSHOTTER_REPLICA_COUNT
            value: "1"
          image: longhornio/longhorn-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          name: longhorn-driver-deployer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - while [ $(curl -m 1 -s -o /dev/null -w "%{http_code}" http://longhorn-backend:9500/v1)
            != "200" ]; do echo waiting; sleep 2; done
          image: longhornio/longhorn-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          name: wait-longhorn-manager
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-27T23:29:15Z"
    generation: 1
    labels:
      app: longhorn-driver-deployer
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
      pod-template-hash: 6bcdb9864b
    name: longhorn-driver-deployer-6bcdb9864b
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: longhorn-driver-deployer
      uid: 05740e92-28a4-46a3-af32-61972d9760ad
    resourceVersion: "25557822"
    uid: 9f10720e-61c5-491d-a5c6-5ce42e6f4131
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: longhorn-driver-deployer
        pod-template-hash: 6bcdb9864b
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: longhorn-driver-deployer
          app.kubernetes.io/instance: longhorn
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: longhorn
          app.kubernetes.io/version: v1.10.1
          helm.sh/chart: longhorn-1.10.1
          pod-template-hash: 6bcdb9864b
      spec:
        containers:
        - command:
          - longhorn-manager
          - -d
          - deploy-driver
          - --manager-image
          - longhornio/longhorn-manager:v1.10.1
          - --manager-url
          - http://longhorn-backend:9500/v1
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.serviceAccountName
          - name: CSI_ATTACHER_IMAGE
            value: longhornio/csi-attacher:v4.10.0-20251030
          - name: CSI_PROVISIONER_IMAGE
            value: longhornio/csi-provisioner:v5.3.0-20251030
          - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE
            value: longhornio/csi-node-driver-registrar:v2.15.0-20251030
          - name: CSI_RESIZER_IMAGE
            value: longhornio/csi-resizer:v1.14.0-20251030
          - name: CSI_SNAPSHOTTER_IMAGE
            value: longhornio/csi-snapshotter:v8.4.0-20251030
          - name: CSI_LIVENESS_PROBE_IMAGE
            value: longhornio/livenessprobe:v2.17.0-20251030
          - name: CSI_ATTACHER_REPLICA_COUNT
            value: "1"
          - name: CSI_PROVISIONER_REPLICA_COUNT
            value: "1"
          - name: CSI_RESIZER_REPLICA_COUNT
            value: "1"
          - name: CSI_SNAPSHOTTER_REPLICA_COUNT
            value: "1"
          image: longhornio/longhorn-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          name: longhorn-driver-deployer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - -c
          - while [ $(curl -m 1 -s -o /dev/null -w "%{http_code}" http://longhorn-backend:9500/v1)
            != "200" ]; do echo waiting; sleep 2; done
          image: longhornio/longhorn-manager:v1.10.1
          imagePullPolicy: IfNotPresent
          name: wait-longhorn-manager
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 0
        serviceAccount: longhorn-service-account
        serviceAccountName: longhorn-service-account
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-27T23:29:15Z"
    generation: 3
    labels:
      app: longhorn-ui
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
      pod-template-hash: 56bc6dd9cb
    name: longhorn-ui-56bc6dd9cb
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: longhorn-ui
      uid: 77765e7f-80ec-4430-bde4-f4033b178234
    resourceVersion: "25571928"
    uid: d9a245ba-6c67-4f5b-850f-e2d94c5061d3
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: longhorn-ui
        pod-template-hash: 56bc6dd9cb
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:29:14+01:00"
        creationTimestamp: null
        labels:
          app: longhorn-ui
          app.kubernetes.io/instance: longhorn
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: longhorn
          app.kubernetes.io/version: v1.10.1
          helm.sh/chart: longhorn-1.10.1
          pod-template-hash: 56bc6dd9cb
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - longhorn-ui
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: LONGHORN_MANAGER_IP
            value: http://longhorn-backend:9500
          - name: LONGHORN_UI_PORT
            value: "8000"
          image: longhornio/longhorn-ui:v1.10.1
          imagePullPolicy: IfNotPresent
          name: longhorn-ui
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/cache/nginx/
            name: nginx-cache
          - mountPath: /var/config/nginx/
            name: nginx-config
          - mountPath: /var/run/
            name: var-run
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-ui-service-account
        serviceAccountName: longhorn-ui-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: nginx-cache
        - emptyDir: {}
          name: nginx-config
        - emptyDir: {}
          name: var-run
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: longhorn
      meta.helm.sh/release-namespace: longhorn-system
    creationTimestamp: "2025-12-09T21:22:06Z"
    generation: 3
    labels:
      app: longhorn-ui
      app.kubernetes.io/instance: longhorn
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: longhorn
      app.kubernetes.io/version: v1.10.1
      helm.sh/chart: longhorn-1.10.1
      pod-template-hash: 6dbd77484f
    name: longhorn-ui-6dbd77484f
    namespace: longhorn-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: longhorn-ui
      uid: 77765e7f-80ec-4430-bde4-f4033b178234
    resourceVersion: "14003429"
    uid: 2382f4a2-56d8-49c5-9bc7-4c88991449d6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: longhorn-ui
        pod-template-hash: 6dbd77484f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: longhorn-ui
          app.kubernetes.io/instance: longhorn
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: longhorn
          app.kubernetes.io/version: v1.10.1
          helm.sh/chart: longhorn-1.10.1
          pod-template-hash: 6dbd77484f
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - longhorn-ui
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: LONGHORN_MANAGER_IP
            value: http://longhorn-backend:9500
          - name: LONGHORN_UI_PORT
            value: "8000"
          image: longhornio/longhorn-ui:v1.10.1
          imagePullPolicy: IfNotPresent
          name: longhorn-ui
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/cache/nginx/
            name: nginx-cache
          - mountPath: /var/config/nginx/
            name: nginx-config
          - mountPath: /var/run/
            name: var-run
        dnsPolicy: ClusterFirst
        priorityClassName: longhorn-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: longhorn-ui-service-account
        serviceAccountName: longhorn-ui-service-account
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: nginx-cache
        - emptyDir: {}
          name: nginx-config
        - emptyDir: {}
          name: var-run
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
      deployment.kubernetes.io/revision-history: "5"
    creationTimestamp: "2025-11-15T00:50:05Z"
    generation: 4
    labels:
      app: mlflow
      pod-template-hash: 65dbd874bd
    name: mlflow-65dbd874bd
    namespace: mlflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mlflow
      uid: 49476224-6d57-4990-86de-c482954529d1
    resourceVersion: "5297008"
    uid: 5b3cfd4b-d0ff-45ca-ad88-1967e6d41f85
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: mlflow
        pod-template-hash: 65dbd874bd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mlflow
          pod-template-hash: 65dbd874bd
      spec:
        containers:
        - command:
          - mlflow
          - server
          - --host
          - 0.0.0.0
          - --port
          - "5000"
          - --backend-store-uri
          - sqlite:////mlflow/mlflow.db
          - --default-artifact-root
          - /mlflow/artifacts
          image: ghcr.io/mlflow/mlflow:v2.9.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mlflow
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mlflow
            name: mlflow-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: mlflow-data
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
    creationTimestamp: "2025-11-27T12:19:45Z"
    generation: 2
    labels:
      app: mlflow
      pod-template-hash: 68fc5d6567
    name: mlflow-68fc5d6567
    namespace: mlflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mlflow
      uid: 49476224-6d57-4990-86de-c482954529d1
    resourceVersion: "5299786"
    uid: 69e04c60-98e8-4f03-9115-60a85a3d15c2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: mlflow
        pod-template-hash: 68fc5d6567
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mlflow
          pod-template-hash: 68fc5d6567
      spec:
        containers:
        - command:
          - mlflow
          - server
          - --host
          - 0.0.0.0
          - --port
          - "5000"
          - --backend-store-uri
          - sqlite:////mlflow/mlflow.db
          - --default-artifact-root
          - /mlflow/artifacts
          - --serve-artifacts
          image: ghcr.io/mlflow/mlflow:v2.9.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mlflow
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mlflow
            name: mlflow-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - name: mlflow-data
          persistentVolumeClaim:
            claimName: mlflow-artifacts-pvc
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "14"
      deployment.kubernetes.io/revision-history: "12"
    creationTimestamp: "2025-12-10T21:39:04Z"
    generation: 3
    labels:
      app: mlflow
      pod-template-hash: 6f55659b89
    name: mlflow-6f55659b89
    namespace: mlflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mlflow
      uid: 49476224-6d57-4990-86de-c482954529d1
    resourceVersion: "29742338"
    uid: 61107e71-57dc-4a5c-b23f-c17bc61a5dc5
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: mlflow
        pod-template-hash: 6f55659b89
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-10T22:39:04+01:00"
        creationTimestamp: null
        labels:
          app: mlflow
          pod-template-hash: 6f55659b89
      spec:
        containers:
        - command:
          - mlflow
          - server
          - --host
          - 0.0.0.0
          - --port
          - "5000"
          - --backend-store-uri
          - sqlite:////mlflow/mlflow.db
          - --default-artifact-root
          - mlflow-artifacts:/
          - --artifacts-destination
          - /mlflow/artifacts
          - --serve-artifacts
          image: ghcr.io/mlflow/mlflow:v2.9.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mlflow
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mlflow
            name: mlflow-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - name: mlflow-data
          persistentVolumeClaim:
            claimName: mlflow-artifacts-pvc
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
    creationTimestamp: "2025-11-27T12:37:56Z"
    generation: 4
    labels:
      app: mlflow
      pod-template-hash: 849cb94ddf
    name: mlflow-849cb94ddf
    namespace: mlflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mlflow
      uid: 49476224-6d57-4990-86de-c482954529d1
    resourceVersion: "8419889"
    uid: 3d32345b-eb93-49e1-9793-9310b2f9a1b1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: mlflow
        pod-template-hash: 849cb94ddf
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: mlflow
          pod-template-hash: 849cb94ddf
      spec:
        containers:
        - command:
          - mlflow
          - server
          - --host
          - 0.0.0.0
          - --port
          - "5000"
          - --backend-store-uri
          - sqlite:////mlflow/mlflow.db
          - --default-artifact-root
          - mlflow-artifacts:/
          - --artifacts-destination
          - /mlflow/artifacts
          - --serve-artifacts
          image: ghcr.io/mlflow/mlflow:v2.9.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mlflow
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mlflow
            name: mlflow-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - name: mlflow-data
          persistentVolumeClaim:
            claimName: mlflow-artifacts-pvc
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
    creationTimestamp: "2026-01-03T19:29:57Z"
    generation: 2
    labels:
      app: mlflow
      pod-template-hash: 85845ff94d
    name: mlflow-85845ff94d
    namespace: mlflow
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mlflow
      uid: 49476224-6d57-4990-86de-c482954529d1
    resourceVersion: "18245185"
    uid: 070ee427-ebfa-4f20-b3c9-6c444fd7066f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: mlflow
        pod-template-hash: 85845ff94d
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-10T22:39:04+01:00"
        creationTimestamp: null
        labels:
          app: mlflow
          pod-template-hash: 85845ff94d
      spec:
        containers:
        - command:
          - mlflow
          - server
          - --host
          - 0.0.0.0
          - --port
          - "5000"
          - --backend-store-uri
          - postgresql://mlflow:$(POSTGRES_PASSWORD)@mlflow-postgresql:5432/mlflow
          - --default-artifact-root
          - /mlflow/artifacts
          env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: mlflow-postgres-secret
          image: ghcr.io/mlflow/mlflow:v2.9.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: mlflow
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mlflow
            name: mlflow-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - name: mlflow-data
          persistentVolumeClaim:
            claimName: mlflow-artifacts-pvc
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: mongodb
      meta.helm.sh/release-namespace: mongodb-cluster
    creationTimestamp: "2025-11-21T08:26:32Z"
    generation: 2
    labels:
      app.kubernetes.io/component: mongodb
      app.kubernetes.io/instance: mongodb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: mongodb-18.1.10
      pod-template-hash: 67495fffff
    name: mongodb-67495fffff
    namespace: mongodb-cluster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mongodb
      uid: b704676e-047e-47de-908b-64c64e2d7441
    resourceVersion: "14005573"
    uid: 0722a628-adf8-4e33-8d90-afef6d0adabf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: mongodb
        app.kubernetes.io/instance: mongodb
        app.kubernetes.io/name: mongodb
        pod-template-hash: 67495fffff
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: mongodb
          app.kubernetes.io/instance: mongodb
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: mongodb
          app.kubernetes.io/version: 8.2.2
          helm.sh/chart: mongodb-18.1.10
          pod-template-hash: 67495fffff
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: mongodb
                    app.kubernetes.io/instance: mongodb
                    app.kubernetes.io/name: mongodb
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: MONGODB_ROOT_USER
            value: root
          - name: MONGODB_ROOT_PASSWORD_FILE
            value: /opt/bitnami/mongodb/secrets/mongodb-root-password
          - name: OPENSSL_FIPS
            value: "yes"
          - name: ALLOW_EMPTY_PASSWORD
            value: "no"
          - name: MONGODB_SYSTEM_LOG_VERBOSITY
            value: "0"
          - name: MONGODB_DISABLE_SYSTEM_LOG
            value: "no"
          - name: MONGODB_DISABLE_JAVASCRIPT
            value: "no"
          - name: MONGODB_ENABLE_JOURNAL
            value: "yes"
          - name: MONGODB_PORT_NUMBER
            value: "27017"
          - name: MONGODB_ENABLE_IPV6
            value: "no"
          - name: MONGODB_ENABLE_DIRECTORY_PER_DB
            value: "no"
          image: registry-1.docker.io/bitnami/mongodb:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bitnami/scripts/ping-mongodb.sh
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10
          name: mongodb
          ports:
          - containerPort: 27017
            name: mongodb
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bitnami/scripts/readiness-probe.sh
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/mongodb/conf
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/mongodb/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /opt/bitnami/mongodb/logs
            name: empty-dir
            subPath: app-logs-dir
          - mountPath: /.mongodb
            name: empty-dir
            subPath: mongosh-home
          - mountPath: /bitnami/mongodb
            name: datadir
          - mountPath: /bitnami/scripts
            name: common-scripts
          - mountPath: /opt/bitnami/mongodb/secrets
            name: mongodb-secrets
        - args:
          - "export MONGODB_ROOT_PASSWORD=\"$(< $MONGODB_ROOT_PASSWORD_FILE)\"\n/bin/mongodb_exporter
            \ --collector.diagnosticdata --collector.replicasetstatus --compatible-mode
            --mongodb.direct-connect --mongodb.global-conn-pool --web.listen-address
            \":9216\" --mongodb.uri \"mongodb://$MONGODB_ROOT_USER:$(echo $MONGODB_ROOT_PASSWORD
            | sed -r \"s/@/%40/g;s/:/%3A/g\")@$(hostname -s):27017/admin?\" \n"
          command:
          - /bin/bash
          - -ec
          env:
          - name: MONGODB_ROOT_USER
            value: root
          - name: MONGODB_ROOT_PASSWORD_FILE
            value: /opt/bitnami/mongodb/secrets/mongodb-root-password
          - name: OPENSSL_FIPS
            value: "yes"
          - name: GODEBUG
            value: fips140=on
          image: registry-1.docker.io/bitnami/mongodb-exporter:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: metrics
            timeoutSeconds: 10
          name: metrics
          ports:
          - containerPort: 9216
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/mongodb/secrets
            name: mongodb-secrets
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - args:
          - -ec
          - |
            ln -sf /dev/stdout "/opt/bitnami/mongodb/logs/mongodb.log"
          command:
          - /bin/bash
          env:
          - name: OPENSSL_FIPS
            value: "yes"
          image: registry-1.docker.io/bitnami/mongodb:latest
          imagePullPolicy: IfNotPresent
          name: log-dir
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/mongodb/logs
            name: empty-dir
            subPath: app-logs-dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: mongodb
        serviceAccountName: mongodb
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            defaultMode: 360
            name: mongodb-common-scripts
          name: common-scripts
        - name: mongodb-secrets
          secret:
            defaultMode: 420
            secretName: mongodb
        - name: datadir
          persistentVolumeClaim:
            claimName: mongodb
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: mongodb
      meta.helm.sh/release-namespace: mongodb-cluster
    creationTimestamp: "2025-12-27T23:28:42Z"
    generation: 3
    labels:
      app.kubernetes.io/component: mongodb
      app.kubernetes.io/instance: mongodb
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/version: 8.2.2
      helm.sh/chart: mongodb-18.1.10
      pod-template-hash: 677c7746c4
    name: mongodb-677c7746c4
    namespace: mongodb-cluster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mongodb
      uid: b704676e-047e-47de-908b-64c64e2d7441
    resourceVersion: "26435411"
    uid: 1c7ed1e0-db07-4fac-a3f5-2e5631e3b7fd
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: mongodb
        app.kubernetes.io/instance: mongodb
        app.kubernetes.io/name: mongodb
        pod-template-hash: 677c7746c4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:28:42+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: mongodb
          app.kubernetes.io/instance: mongodb
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: mongodb
          app.kubernetes.io/version: 8.2.2
          helm.sh/chart: mongodb-18.1.10
          pod-template-hash: 677c7746c4
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: mongodb
                    app.kubernetes.io/instance: mongodb
                    app.kubernetes.io/name: mongodb
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: MONGODB_ROOT_USER
            value: root
          - name: MONGODB_ROOT_PASSWORD_FILE
            value: /opt/bitnami/mongodb/secrets/mongodb-root-password
          - name: OPENSSL_FIPS
            value: "yes"
          - name: ALLOW_EMPTY_PASSWORD
            value: "no"
          - name: MONGODB_SYSTEM_LOG_VERBOSITY
            value: "0"
          - name: MONGODB_DISABLE_SYSTEM_LOG
            value: "no"
          - name: MONGODB_DISABLE_JAVASCRIPT
            value: "no"
          - name: MONGODB_ENABLE_JOURNAL
            value: "yes"
          - name: MONGODB_PORT_NUMBER
            value: "27017"
          - name: MONGODB_ENABLE_IPV6
            value: "no"
          - name: MONGODB_ENABLE_DIRECTORY_PER_DB
            value: "no"
          image: registry-1.docker.io/bitnami/mongodb:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bitnami/scripts/ping-mongodb.sh
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 10
          name: mongodb
          ports:
          - containerPort: 27017
            name: mongodb
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bitnami/scripts/readiness-probe.sh
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/mongodb/conf
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/mongodb/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /opt/bitnami/mongodb/logs
            name: empty-dir
            subPath: app-logs-dir
          - mountPath: /.mongodb
            name: empty-dir
            subPath: mongosh-home
          - mountPath: /bitnami/mongodb
            name: datadir
          - mountPath: /bitnami/scripts
            name: common-scripts
          - mountPath: /opt/bitnami/mongodb/secrets
            name: mongodb-secrets
        - args:
          - "export MONGODB_ROOT_PASSWORD=\"$(< $MONGODB_ROOT_PASSWORD_FILE)\"\n/bin/mongodb_exporter
            \ --collector.diagnosticdata --collector.replicasetstatus --compatible-mode
            --mongodb.direct-connect --mongodb.global-conn-pool --web.listen-address
            \":9216\" --mongodb.uri \"mongodb://$MONGODB_ROOT_USER:$(echo $MONGODB_ROOT_PASSWORD
            | sed -r \"s/@/%40/g;s/:/%3A/g\")@$(hostname -s):27017/admin?\" \n"
          command:
          - /bin/bash
          - -ec
          env:
          - name: MONGODB_ROOT_USER
            value: root
          - name: MONGODB_ROOT_PASSWORD_FILE
            value: /opt/bitnami/mongodb/secrets/mongodb-root-password
          - name: OPENSSL_FIPS
            value: "yes"
          - name: GODEBUG
            value: fips140=on
          image: registry-1.docker.io/bitnami/mongodb-exporter:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: metrics
            timeoutSeconds: 10
          name: metrics
          ports:
          - containerPort: 9216
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 150m
              ephemeral-storage: 2Gi
              memory: 192Mi
            requests:
              cpu: 100m
              ephemeral-storage: 50Mi
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/mongodb/secrets
            name: mongodb-secrets
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - args:
          - -ec
          - |
            ln -sf /dev/stdout "/opt/bitnami/mongodb/logs/mongodb.log"
          command:
          - /bin/bash
          env:
          - name: OPENSSL_FIPS
            value: "yes"
          image: registry-1.docker.io/bitnami/mongodb:latest
          imagePullPolicy: IfNotPresent
          name: log-dir
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/bitnami/mongodb/logs
            name: empty-dir
            subPath: app-logs-dir
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: mongodb
        serviceAccountName: mongodb
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
        - configMap:
            defaultMode: 360
            name: mongodb-common-scripts
          name: common-scripts
        - name: mongodb-secrets
          secret:
            defaultMode: 420
            secretName: mongodb
        - name: datadir
          persistentVolumeClaim:
            claimName: mongodb
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      email: support@mongodb.com
      meta.helm.sh/release-name: mongodb-operator
      meta.helm.sh/release-namespace: mongodb-operator
    creationTimestamp: "2025-11-19T08:11:08Z"
    generation: 1
    labels:
      name: mongodb-kubernetes-operator
      pod-template-hash: 7669bd584
    name: mongodb-kubernetes-operator-7669bd584
    namespace: mongodb-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: mongodb-kubernetes-operator
      uid: 0c462e29-9327-492b-9b6d-27a7dd9eab1e
    resourceVersion: "23970747"
    uid: 8de9d88f-376c-4dc0-94b2-df7eafef526c
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: mongodb-kubernetes-operator
        pod-template-hash: 7669bd584
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: mongodb-kubernetes-operator
          pod-template-hash: 7669bd584
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: name
                  operator: In
                  values:
                  - mongodb-kubernetes-operator
              topologyKey: kubernetes.io/hostname
        containers:
        - command:
          - /usr/local/bin/entrypoint
          env:
          - name: WATCH_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: OPERATOR_NAME
            value: mongodb-kubernetes-operator
          - name: AGENT_IMAGE
            value: quay.io/mongodb/mongodb-agent-ubi:108.0.6.8796-1
          - name: VERSION_UPGRADE_HOOK_IMAGE
            value: quay.io/mongodb/mongodb-kubernetes-operator-version-upgrade-post-start-hook:1.0.10
          - name: READINESS_PROBE_IMAGE
            value: quay.io/mongodb/mongodb-kubernetes-readinessprobe:1.0.23
          - name: MONGODB_IMAGE
            value: mongodb-community-server
          - name: MONGODB_REPO_URL
            value: docker.io/mongodb
          - name: MDB_IMAGE_TYPE
            value: ubi8
          image: quay.io/mongodb/mongodb-kubernetes-operator:0.13.0
          imagePullPolicy: Always
          name: mongodb-kubernetes-operator
          resources:
            limits:
              cpu: 1100m
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 2000
        serviceAccount: mongodb-kubernetes-operator
        serviceAccountName: mongodb-kubernetes-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2026-01-01T13:02:31Z"
    generation: 3
    labels:
      app: postgres-sla
      pod-template-hash: 6d998b77dd
    name: postgres-sla-6d998b77dd
    namespace: neural-hive-data
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: postgres-sla
      uid: d4d2f739-4592-4152-8cdc-006654a9206e
    resourceVersion: "28924645"
    uid: 3394b565-8cbb-4eab-99b2-961dc08e971e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: postgres-sla
        pod-template-hash: 6d998b77dd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgres-sla
          pod-template-hash: 6d998b77dd
      spec:
        containers:
        - envFrom:
          - secretRef:
              name: postgres-sla-credentials
          image: postgres:15-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - pg_isready
              - -U
              - sla_user
              - -d
              - sla_management
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: postgres
          ports:
          - containerPort: 5432
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - pg_isready
              - -U
              - sla_user
              - -d
              - sla_management
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data
            name: postgres-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: postgres-sla-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "80"
      deployment.kubernetes.io/revision-history: 69,71,73,75
      kubernetes.io/change-cause: kubectl set image deployment/analyst-agents analyst-agents=ghcr.io/albinojimy/neural-hive-mind/analyst-agents:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: analyst-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T11:11:45Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: analyst-agents
      app.kubernetes.io/name: analyst-agents
      pod-template-hash: 5fb8dfd548
    name: analyst-agents-5fb8dfd548
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: analyst-agents
      uid: 547f60a6-d8cf-4938-bcef-7c8f66273e39
    resourceVersion: "29550625"
    uid: f82ab932-a967-4562-8016-f0582644864a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: analyst-agents
        app.kubernetes.io/name: analyst-agents
        pod-template-hash: 5fb8dfd548
    template:
      metadata:
        annotations:
          checksum/config: 2fcd3a724c65b72bb9230ccc74c77ede86a18c8082703be8cd7739eaabaef9ed
          kubectl.kubernetes.io/restartedAt: "2026-02-11T12:11:44+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: analyst-agents
          app.kubernetes.io/name: analyst-agents
          pod-template-hash: 5fb8dfd548
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - analyst-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: analyst-agents
          image: ghcr.io/albinojimy/neural-hive-mind/analyst-agents:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: analyst-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: analyst-agents
        serviceAccountName: analyst-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "78"
      kubernetes.io/change-cause: kubectl set image deployment/analyst-agents analyst-agents=ghcr.io/albinojimy/neural-hive-mind/analyst-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: analyst-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:43:13Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: analyst-agents
      app.kubernetes.io/name: analyst-agents
      pod-template-hash: 66cb788b76
    name: analyst-agents-66cb788b76
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: analyst-agents
      uid: 547f60a6-d8cf-4938-bcef-7c8f66273e39
    resourceVersion: "29550124"
    uid: 6d708414-0ceb-45cc-a7a1-fe3beecc6603
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: analyst-agents
        app.kubernetes.io/name: analyst-agents
        pod-template-hash: 66cb788b76
    template:
      metadata:
        annotations:
          checksum/config: 2fcd3a724c65b72bb9230ccc74c77ede86a18c8082703be8cd7739eaabaef9ed
          kubectl.kubernetes.io/restartedAt: "2026-02-11T12:11:44+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: analyst-agents
          app.kubernetes.io/name: analyst-agents
          pod-template-hash: 66cb788b76
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - analyst-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: analyst-agents
          image: ghcr.io/albinojimy/neural-hive-mind/analyst-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: analyst-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: analyst-agents
        serviceAccountName: analyst-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "76"
      kubernetes.io/change-cause: kubectl set image deployment/analyst-agents analyst-agents=ghcr.io/albinojimy/neural-hive-mind/analyst-agents:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: analyst-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:50:28Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: analyst-agents
      app.kubernetes.io/name: analyst-agents
      pod-template-hash: 797756bb77
    name: analyst-agents-797756bb77
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: analyst-agents
      uid: 547f60a6-d8cf-4938-bcef-7c8f66273e39
    resourceVersion: "29533724"
    uid: 0723b4a0-2159-4228-9abf-f47bb32b7942
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: analyst-agents
        app.kubernetes.io/name: analyst-agents
        pod-template-hash: 797756bb77
    template:
      metadata:
        annotations:
          checksum/config: 2fcd3a724c65b72bb9230ccc74c77ede86a18c8082703be8cd7739eaabaef9ed
          kubectl.kubernetes.io/restartedAt: "2026-02-11T12:11:44+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: analyst-agents
          app.kubernetes.io/name: analyst-agents
          pod-template-hash: 797756bb77
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - analyst-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: analyst-agents
          image: ghcr.io/albinojimy/neural-hive-mind/analyst-agents:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: analyst-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: analyst-agents
        serviceAccountName: analyst-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "79"
      deployment.kubernetes.io/revision-history: "77"
      kubernetes.io/change-cause: kubectl set image deployment/analyst-agents analyst-agents=ghcr.io/albinojimy/neural-hive-mind/analyst-agents:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: analyst-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:07:54Z"
    generation: 5
    labels:
      app.kubernetes.io/instance: analyst-agents
      app.kubernetes.io/name: analyst-agents
      pod-template-hash: 7cffbfcbd4
    name: analyst-agents-7cffbfcbd4
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: analyst-agents
      uid: 547f60a6-d8cf-4938-bcef-7c8f66273e39
    resourceVersion: "29550639"
    uid: 0cf45b73-d325-4bad-91ae-c87dd565b5b9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: analyst-agents
        app.kubernetes.io/name: analyst-agents
        pod-template-hash: 7cffbfcbd4
    template:
      metadata:
        annotations:
          checksum/config: 2fcd3a724c65b72bb9230ccc74c77ede86a18c8082703be8cd7739eaabaef9ed
          kubectl.kubernetes.io/restartedAt: "2026-02-11T12:11:44+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: analyst-agents
          app.kubernetes.io/name: analyst-agents
          pod-template-hash: 7cffbfcbd4
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - analyst-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: analyst-agents
          image: ghcr.io/albinojimy/neural-hive-mind/analyst-agents:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: analyst-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: analyst-agents
        serviceAccountName: analyst-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2026-02-08T20:18:16Z"
    generation: 3
    labels:
      app.kubernetes.io/name: approval-service
      pod-template-hash: 77d49c4bd8
    name: approval-service-77d49c4bd8
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: approval-service
      uid: 80dc1fc5-763b-4368-ac9b-bc4a892f80f5
    resourceVersion: "28728477"
    uid: 52d76c1c-e286-4dad-8c36-9371541604eb
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/name: approval-service
        pod-template-hash: 77d49c4bd8
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-02T13:35:28+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: approval-service
          pod-template-hash: 77d49c4bd8
      spec:
        containers:
        - env:
          - name: KAFKA_BOOTSTRAP_SERVERS
            value: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
          - name: KAFKA_SECURITY_PROTOCOL
            value: PLAINTEXT
          - name: MONGODB_URI
            value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017
          - name: ENVIRONMENT
            value: development
          - name: LOG_LEVEL
            value: INFO
          - name: APPROVAL_SERVICE_REQUIRE_AUTH
            value: "false"
          image: ghcr.io/albinojimy/neural-hive-mind/approval-service:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 40
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: approval-service
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "20"
      kubernetes.io/change-cause: kubectl set image deployment/code-forge code-forge=ghcr.io/albinojimy/neural-hive-mind/code-forge:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: code-forge
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:50:29Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: code-forge
      app.kubernetes.io/name: code-forge
      pod-template-hash: 55b4b5c777
    name: code-forge-55b4b5c777
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: code-forge
      uid: 421a6526-c232-4ad5-8b56-0434aad00e85
    resourceVersion: "29533708"
    uid: 47faa6c2-610e-43aa-95d7-824d2bcd49a9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: code-forge
        app.kubernetes.io/name: code-forge
        pod-template-hash: 55b4b5c777
    template:
      metadata:
        annotations:
          checksum/config: 04a5ce5560e16ceba234402ba0d53ac1085437c75b520af6e7f9fad989923a22
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:57:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: code-forge
          app.kubernetes.io/name: code-forge
          pod-template-hash: 55b4b5c777
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - code-forge
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: code-forge-config
          - secretRef:
              name: code-forge-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/code-forge:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: code-forge
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 2200m
              memory: 3584Mi
            requests:
              cpu: 800m
              memory: 1536Mi
          startupProbe:
            failureThreshold: 25
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: code-forge
        serviceAccountName: code-forge
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: code-forge
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "23"
      kubernetes.io/change-cause: kubectl set image deployment/code-forge code-forge=ghcr.io/albinojimy/neural-hive-mind/code-forge:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: code-forge
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:34Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: code-forge
      app.kubernetes.io/name: code-forge
      pod-template-hash: 5b5cb4f857
    name: code-forge-5b5cb4f857
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: code-forge
      uid: 421a6526-c232-4ad5-8b56-0434aad00e85
    resourceVersion: "29938795"
    uid: e565f2ae-ea73-4ffc-ab79-b73817c70bfd
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: code-forge
        app.kubernetes.io/name: code-forge
        pod-template-hash: 5b5cb4f857
    template:
      metadata:
        annotations:
          checksum/config: 04a5ce5560e16ceba234402ba0d53ac1085437c75b520af6e7f9fad989923a22
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:57:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: code-forge
          app.kubernetes.io/name: code-forge
          pod-template-hash: 5b5cb4f857
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - code-forge
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: code-forge-config
          - secretRef:
              name: code-forge-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/code-forge:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: code-forge
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 2200m
              memory: 3584Mi
            requests:
              cpu: 800m
              memory: 1536Mi
          startupProbe:
            failureThreshold: 25
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: code-forge
        serviceAccountName: code-forge
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: code-forge
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 2
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "21"
      kubernetes.io/change-cause: kubectl set image deployment/code-forge code-forge=ghcr.io/albinojimy/neural-hive-mind/code-forge:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: code-forge
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:07:54Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: code-forge
      app.kubernetes.io/name: code-forge
      pod-template-hash: 75ccb6ffc6
    name: code-forge-75ccb6ffc6
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: code-forge
      uid: 421a6526-c232-4ad5-8b56-0434aad00e85
    resourceVersion: "29543978"
    uid: 35702b62-a075-4007-aea8-13d73d28c8cb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: code-forge
        app.kubernetes.io/name: code-forge
        pod-template-hash: 75ccb6ffc6
    template:
      metadata:
        annotations:
          checksum/config: 04a5ce5560e16ceba234402ba0d53ac1085437c75b520af6e7f9fad989923a22
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:57:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: code-forge
          app.kubernetes.io/name: code-forge
          pod-template-hash: 75ccb6ffc6
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - code-forge
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: code-forge-config
          - secretRef:
              name: code-forge-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/code-forge:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: code-forge
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 2200m
              memory: 3584Mi
            requests:
              cpu: 800m
              memory: 1536Mi
          startupProbe:
            failureThreshold: 25
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: code-forge
        serviceAccountName: code-forge
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: code-forge
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "22"
      kubernetes.io/change-cause: kubectl set image deployment/code-forge code-forge=ghcr.io/albinojimy/neural-hive-mind/code-forge:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: code-forge
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:43:12Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: code-forge
      app.kubernetes.io/name: code-forge
      pod-template-hash: 94bd8b94d
    name: code-forge-94bd8b94d
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: code-forge
      uid: 421a6526-c232-4ad5-8b56-0434aad00e85
    resourceVersion: "29904745"
    uid: e45a4a6b-f61e-4b48-977e-535f5a2aae2c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: code-forge
        app.kubernetes.io/name: code-forge
        pod-template-hash: 94bd8b94d
    template:
      metadata:
        annotations:
          checksum/config: 04a5ce5560e16ceba234402ba0d53ac1085437c75b520af6e7f9fad989923a22
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:57:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: code-forge
          app.kubernetes.io/name: code-forge
          pod-template-hash: 94bd8b94d
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - code-forge
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: code-forge-config
          - secretRef:
              name: code-forge-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/code-forge:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: code-forge
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 2200m
              memory: 3584Mi
            requests:
              cpu: 800m
              memory: 1536Mi
          startupProbe:
            failureThreshold: 25
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: code-forge
        serviceAccountName: code-forge
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: code-forge
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "83"
      deployment.kubernetes.io/revision-history: "81"
      meta.helm.sh/release-name: consensus-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T11:22:09Z"
    generation: 10
    labels:
      app.kubernetes.io/component: consensus-aggregator
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/name: consensus-engine
      neural-hive.io/domain: consensus
      neural-hive.io/layer: cognitiva
      pod-template-hash: 6bbb4f6d55
    name: consensus-engine-6bbb4f6d55
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: consensus-engine
      uid: bf70ceed-d792-46a9-81bd-6fe61640872b
    resourceVersion: "28155213"
    uid: 806bc29a-13b1-46f7-a7e7-bf8a47a2d9c0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: consensus-engine
        app.kubernetes.io/name: consensus-engine
        pod-template-hash: 6bbb4f6d55
    template:
      metadata:
        annotations:
          checksum/config: ac2b81a4a6b9013bfc630ab07ea38dedf7dab374bf25c85afb35c19760cc8b22
          checksum/secret: c53e1323967446c56b88efd346e3f29681d2227762fc8bfcd3c4e2f8efc1074f
          kubectl.kubernetes.io/restartedAt: "2026-02-08T12:22:06+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: consensus-aggregator
          app.kubernetes.io/instance: consensus-engine
          app.kubernetes.io/name: consensus-engine
          neural-hive.io/domain: consensus
          neural-hive.io/layer: cognitiva
          pod-template-hash: 6bbb4f6d55
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - consensus-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: PYTHONUNBUFFERED
            value: "1"
          envFrom:
          - configMapRef:
              name: consensus-engine-config
          - secretRef:
              name: consensus-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/consensus-engine:3c1994a
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: consensus-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 1500m
              memory: 1280Mi
            requests:
              cpu: 400m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/models/consolidated_decision.py
            name: decision-hotfix
            subPath: consolidated_decision.py
          - mountPath: /app/src/services/consensus_orchestrator.py
            name: consensus-orchestrator-hotfix
            subPath: consensus_orchestrator.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: consensus-engine
        serviceAccountName: consensus-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: consensus-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: consensus-engine-decision-hotfix
          name: decision-hotfix
        - configMap:
            defaultMode: 420
            name: consensus-orchestrator-hotfix
          name: consensus-orchestrator-hotfix
  status:
    observedGeneration: 10
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "86"
      meta.helm.sh/release-name: consensus-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T07:24:00Z"
    generation: 2
    labels:
      app.kubernetes.io/component: consensus-aggregator
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/name: consensus-engine
      neural-hive.io/domain: consensus
      neural-hive.io/layer: cognitiva
      pod-template-hash: 6fbd8d768f
    name: consensus-engine-6fbd8d768f
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: consensus-engine
      uid: bf70ceed-d792-46a9-81bd-6fe61640872b
    resourceVersion: "29319754"
    uid: dd2956e9-1a8b-4a7d-a4fc-c522e882b9f7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: consensus-engine
        app.kubernetes.io/name: consensus-engine
        pod-template-hash: 6fbd8d768f
    template:
      metadata:
        annotations:
          checksum/config: ac2b81a4a6b9013bfc630ab07ea38dedf7dab374bf25c85afb35c19760cc8b22
          checksum/secret: c53e1323967446c56b88efd346e3f29681d2227762fc8bfcd3c4e2f8efc1074f
          kubectl.kubernetes.io/restartedAt: "2026-02-08T12:22:06+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: consensus-aggregator
          app.kubernetes.io/instance: consensus-engine
          app.kubernetes.io/name: consensus-engine
          neural-hive.io/domain: consensus
          neural-hive.io/layer: cognitiva
          pod-template-hash: 6fbd8d768f
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - consensus-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: PYTHONUNBUFFERED
            value: "1"
          envFrom:
          - configMapRef:
              name: consensus-engine-config
          - secretRef:
              name: consensus-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/consensus-engine:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: consensus-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 1500m
              memory: 1280Mi
            requests:
              cpu: 400m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/models/consolidated_decision.py
            name: decision-hotfix
            subPath: consolidated_decision.py
          - mountPath: /app/src/services/consensus_orchestrator.py
            name: consensus-orchestrator-hotfix
            subPath: consensus_orchestrator.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: consensus-engine
        serviceAccountName: consensus-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: consensus-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: consensus-engine-decision-hotfix
          name: decision-hotfix
        - configMap:
            defaultMode: 420
            name: consensus-orchestrator-hotfix
          name: consensus-orchestrator-hotfix
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "85"
      meta.helm.sh/release-name: consensus-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-09T08:52:27Z"
    generation: 9
    labels:
      app.kubernetes.io/component: consensus-aggregator
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/name: consensus-engine
      neural-hive.io/domain: consensus
      neural-hive.io/layer: cognitiva
      pod-template-hash: 7d9f9f9fcb
    name: consensus-engine-7d9f9f9fcb
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: consensus-engine
      uid: bf70ceed-d792-46a9-81bd-6fe61640872b
    resourceVersion: "28940934"
    uid: cbd5962c-f570-4e11-aafe-f2307af240c7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: consensus-engine
        app.kubernetes.io/name: consensus-engine
        pod-template-hash: 7d9f9f9fcb
    template:
      metadata:
        annotations:
          checksum/config: ac2b81a4a6b9013bfc630ab07ea38dedf7dab374bf25c85afb35c19760cc8b22
          checksum/secret: c53e1323967446c56b88efd346e3f29681d2227762fc8bfcd3c4e2f8efc1074f
          kubectl.kubernetes.io/restartedAt: "2026-02-08T12:22:06+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: consensus-aggregator
          app.kubernetes.io/instance: consensus-engine
          app.kubernetes.io/name: consensus-engine
          neural-hive.io/domain: consensus
          neural-hive.io/layer: cognitiva
          pod-template-hash: 7d9f9f9fcb
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - consensus-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: PYTHONUNBUFFERED
            value: "1"
          envFrom:
          - configMapRef:
              name: consensus-engine-config
          - secretRef:
              name: consensus-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/consensus-engine:6f1ee62
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: consensus-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 1500m
              memory: 1280Mi
            requests:
              cpu: 400m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/models/consolidated_decision.py
            name: decision-hotfix
            subPath: consolidated_decision.py
          - mountPath: /app/src/services/consensus_orchestrator.py
            name: consensus-orchestrator-hotfix
            subPath: consensus_orchestrator.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: consensus-engine
        serviceAccountName: consensus-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: consensus-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: consensus-engine-decision-hotfix
          name: decision-hotfix
        - configMap:
            defaultMode: 420
            name: consensus-orchestrator-hotfix
          name: consensus-orchestrator-hotfix
  status:
    observedGeneration: 9
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "84"
      meta.helm.sh/release-name: consensus-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-09T07:44:25Z"
    generation: 4
    labels:
      app.kubernetes.io/component: consensus-aggregator
      app.kubernetes.io/instance: consensus-engine
      app.kubernetes.io/name: consensus-engine
      neural-hive.io/domain: consensus
      neural-hive.io/layer: cognitiva
      pod-template-hash: 7fc994645c
    name: consensus-engine-7fc994645c
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: consensus-engine
      uid: bf70ceed-d792-46a9-81bd-6fe61640872b
    resourceVersion: "28310876"
    uid: 03e99722-7fda-4a73-b910-393c06d4cc0b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: consensus-engine
        app.kubernetes.io/name: consensus-engine
        pod-template-hash: 7fc994645c
    template:
      metadata:
        annotations:
          checksum/config: ac2b81a4a6b9013bfc630ab07ea38dedf7dab374bf25c85afb35c19760cc8b22
          checksum/secret: c53e1323967446c56b88efd346e3f29681d2227762fc8bfcd3c4e2f8efc1074f
          kubectl.kubernetes.io/restartedAt: "2026-02-08T12:22:06+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: consensus-aggregator
          app.kubernetes.io/instance: consensus-engine
          app.kubernetes.io/name: consensus-engine
          neural-hive.io/domain: consensus
          neural-hive.io/layer: cognitiva
          pod-template-hash: 7fc994645c
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - consensus-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: PYTHONUNBUFFERED
            value: "1"
          envFrom:
          - configMapRef:
              name: consensus-engine-config
          - secretRef:
              name: consensus-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/consensus-engine:dac045e
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: consensus-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            limits:
              cpu: 1500m
              memory: 1280Mi
            requests:
              cpu: 400m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/models/consolidated_decision.py
            name: decision-hotfix
            subPath: consolidated_decision.py
          - mountPath: /app/src/services/consensus_orchestrator.py
            name: consensus-orchestrator-hotfix
            subPath: consensus_orchestrator.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: consensus-engine
        serviceAccountName: consensus-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: consensus-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: consensus-engine-decision-hotfix
          name: decision-hotfix
        - configMap:
            defaultMode: 420
            name: consensus-orchestrator-hotfix
          name: consensus-orchestrator-hotfix
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "60"
      kubernetes.io/change-cause: kubectl set image deployment/execution-ticket-service
        execution-ticket-service=ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: execution-ticket-service
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:28Z"
    generation: 2
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: execution-ticket-service
      app.kubernetes.io/name: execution-ticket-service
      neural-hive.io/layer: orchestration
      pod-template-hash: 54988fd44f
    name: execution-ticket-service-54988fd44f
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: execution-ticket-service
      uid: aa96a2cf-93b3-4c80-aef5-cf9bf104dc52
    resourceVersion: "29905073"
    uid: 1a005206-af25-43c1-8541-13288f2c7088
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: execution-ticket-service
        app.kubernetes.io/name: execution-ticket-service
        pod-template-hash: 54988fd44f
    template:
      metadata:
        annotations:
          checksum/config: e723785aafd84b52629c9ca2364d1dc4f4a8026ebfdf1e80f2a4949c98d47679
          kubectl.kubernetes.io/restartedAt: "2026-02-06T09:07:47+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: execution-ticket-service
          app.kubernetes.io/name: execution-ticket-service
          neural-hive.io/layer: orchestration
          pod-template-hash: 54988fd44f
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - execution-ticket-service
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRES_PASSWORD
                name: execution-ticket-service-secrets
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: MONGODB_URI
                name: execution-ticket-service-secrets
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: JWT_SECRET_KEY
                name: execution-ticket-service-secrets
          - name: MAX_CONNECTION_RETRIES
            value: "5"
          - name: INITIAL_RETRY_DELAY_SECONDS
            value: "1"
          envFrom:
          - configMapRef:
              name: execution-ticket-service-config
          image: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: execution-ticket-service
          ports:
          - containerPort: 50052
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: execution-ticket-service
        serviceAccountName: execution-ticket-service
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: execution-ticket-service
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "59"
      kubernetes.io/change-cause: kubectl set image deployment/execution-ticket-service
        execution-ticket-service=ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: execution-ticket-service
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:43:12Z"
    generation: 3
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: execution-ticket-service
      app.kubernetes.io/name: execution-ticket-service
      neural-hive.io/layer: orchestration
      pod-template-hash: 555f5dc456
    name: execution-ticket-service-555f5dc456
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: execution-ticket-service
      uid: aa96a2cf-93b3-4c80-aef5-cf9bf104dc52
    resourceVersion: "29904695"
    uid: 8163c540-dc5e-4691-8751-3805eab5c7f6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: execution-ticket-service
        app.kubernetes.io/name: execution-ticket-service
        pod-template-hash: 555f5dc456
    template:
      metadata:
        annotations:
          checksum/config: e723785aafd84b52629c9ca2364d1dc4f4a8026ebfdf1e80f2a4949c98d47679
          kubectl.kubernetes.io/restartedAt: "2026-02-06T09:07:47+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: execution-ticket-service
          app.kubernetes.io/name: execution-ticket-service
          neural-hive.io/layer: orchestration
          pod-template-hash: 555f5dc456
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - execution-ticket-service
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRES_PASSWORD
                name: execution-ticket-service-secrets
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: MONGODB_URI
                name: execution-ticket-service-secrets
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: JWT_SECRET_KEY
                name: execution-ticket-service-secrets
          - name: MAX_CONNECTION_RETRIES
            value: "5"
          - name: INITIAL_RETRY_DELAY_SECONDS
            value: "1"
          envFrom:
          - configMapRef:
              name: execution-ticket-service-config
          image: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: execution-ticket-service
          ports:
          - containerPort: 50052
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: execution-ticket-service
        serviceAccountName: execution-ticket-service
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: execution-ticket-service
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "58"
      kubernetes.io/change-cause: kubectl set image deployment/execution-ticket-service
        execution-ticket-service=ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: execution-ticket-service
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:07:50Z"
    generation: 3
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: execution-ticket-service
      app.kubernetes.io/name: execution-ticket-service
      neural-hive.io/layer: orchestration
      pod-template-hash: 5756db6f84
    name: execution-ticket-service-5756db6f84
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: execution-ticket-service
      uid: aa96a2cf-93b3-4c80-aef5-cf9bf104dc52
    resourceVersion: "29543998"
    uid: bda67434-a3b2-4b0e-884f-241894f33fe0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: execution-ticket-service
        app.kubernetes.io/name: execution-ticket-service
        pod-template-hash: 5756db6f84
    template:
      metadata:
        annotations:
          checksum/config: e723785aafd84b52629c9ca2364d1dc4f4a8026ebfdf1e80f2a4949c98d47679
          kubectl.kubernetes.io/restartedAt: "2026-02-06T09:07:47+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: execution-ticket-service
          app.kubernetes.io/name: execution-ticket-service
          neural-hive.io/layer: orchestration
          pod-template-hash: 5756db6f84
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - execution-ticket-service
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRES_PASSWORD
                name: execution-ticket-service-secrets
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: MONGODB_URI
                name: execution-ticket-service-secrets
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: JWT_SECRET_KEY
                name: execution-ticket-service-secrets
          - name: MAX_CONNECTION_RETRIES
            value: "5"
          - name: INITIAL_RETRY_DELAY_SECONDS
            value: "1"
          envFrom:
          - configMapRef:
              name: execution-ticket-service-config
          image: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: execution-ticket-service
          ports:
          - containerPort: 50052
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: execution-ticket-service
        serviceAccountName: execution-ticket-service
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: execution-ticket-service
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "57"
      kubernetes.io/change-cause: kubectl set image deployment/execution-ticket-service
        execution-ticket-service=ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: execution-ticket-service
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:50:26Z"
    generation: 2
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: execution-ticket-service
      app.kubernetes.io/name: execution-ticket-service
      neural-hive.io/layer: orchestration
      pod-template-hash: 695794dc85
    name: execution-ticket-service-695794dc85
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: execution-ticket-service
      uid: aa96a2cf-93b3-4c80-aef5-cf9bf104dc52
    resourceVersion: "29533639"
    uid: 02463aec-5254-4725-b7b2-31ae1fe81bdb
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: execution-ticket-service
        app.kubernetes.io/name: execution-ticket-service
        pod-template-hash: 695794dc85
    template:
      metadata:
        annotations:
          checksum/config: e723785aafd84b52629c9ca2364d1dc4f4a8026ebfdf1e80f2a4949c98d47679
          kubectl.kubernetes.io/restartedAt: "2026-02-06T09:07:47+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: execution-ticket-service
          app.kubernetes.io/name: execution-ticket-service
          neural-hive.io/layer: orchestration
          pod-template-hash: 695794dc85
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - execution-ticket-service
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRES_PASSWORD
                name: execution-ticket-service-secrets
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: MONGODB_URI
                name: execution-ticket-service-secrets
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: JWT_SECRET_KEY
                name: execution-ticket-service-secrets
          - name: MAX_CONNECTION_RETRIES
            value: "5"
          - name: INITIAL_RETRY_DELAY_SECONDS
            value: "1"
          envFrom:
          - configMapRef:
              name: execution-ticket-service-config
          image: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: execution-ticket-service
          ports:
          - containerPort: 50052
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: execution-ticket-service
        serviceAccountName: execution-ticket-service
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: execution-ticket-service
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "30"
      kubernetes.io/change-cause: kubectl set image deployment/explainability-api
        explainability-api=ghcr.io/albinojimy/neural-hive-mind/explainability-api:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: explainability-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:38Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: explainability-api
      app.kubernetes.io/name: explainability-api
      pod-template-hash: 55bf67c995
    name: explainability-api-55bf67c995
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: explainability-api
      uid: 045477db-d366-4841-9d55-7728e40ff35f
    resourceVersion: "29904786"
    uid: 916e0a6e-8024-4fab-9447-92fb6cab0f8a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: explainability-api
        app.kubernetes.io/name: explainability-api
        pod-template-hash: 55bf67c995
    template:
      metadata:
        annotations:
          checksum/config: e3837d2b6b9dc131551798941b991c7d75898ac1636b0a9fd69e738edec79648
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:30+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: explainability-api
          app.kubernetes.io/name: explainability-api
          pod-template-hash: 55bf67c995
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - explainability-api
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: explainability-api
          image: ghcr.io/albinojimy/neural-hive-mind/explainability-api:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: explainability-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: explainability-api
        serviceAccountName: explainability-api
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "29"
      kubernetes.io/change-cause: kubectl set image deployment/explainability-api
        explainability-api=ghcr.io/albinojimy/neural-hive-mind/explainability-api:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: explainability-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:43:16Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: explainability-api
      app.kubernetes.io/name: explainability-api
      pod-template-hash: 6df7849d4
    name: explainability-api-6df7849d4
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: explainability-api
      uid: 045477db-d366-4841-9d55-7728e40ff35f
    resourceVersion: "29544087"
    uid: 3b57b573-e7ff-4fa1-952a-c07384da1a97
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: explainability-api
        app.kubernetes.io/name: explainability-api
        pod-template-hash: 6df7849d4
    template:
      metadata:
        annotations:
          checksum/config: e3837d2b6b9dc131551798941b991c7d75898ac1636b0a9fd69e738edec79648
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:30+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: explainability-api
          app.kubernetes.io/name: explainability-api
          pod-template-hash: 6df7849d4
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - explainability-api
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: explainability-api
          image: ghcr.io/albinojimy/neural-hive-mind/explainability-api:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: explainability-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: explainability-api
        serviceAccountName: explainability-api
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "27"
      kubernetes.io/change-cause: kubectl set image deployment/explainability-api
        explainability-api=ghcr.io/albinojimy/neural-hive-mind/explainability-api:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: explainability-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:50:25Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: explainability-api
      app.kubernetes.io/name: explainability-api
      pod-template-hash: f7d676df9
    name: explainability-api-f7d676df9
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: explainability-api
      uid: 045477db-d366-4841-9d55-7728e40ff35f
    resourceVersion: "29528092"
    uid: b99315d2-710b-4e01-ad89-9c53a0da98d7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: explainability-api
        app.kubernetes.io/name: explainability-api
        pod-template-hash: f7d676df9
    template:
      metadata:
        annotations:
          checksum/config: e3837d2b6b9dc131551798941b991c7d75898ac1636b0a9fd69e738edec79648
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:30+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: explainability-api
          app.kubernetes.io/name: explainability-api
          pod-template-hash: f7d676df9
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - explainability-api
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: explainability-api
          image: ghcr.io/albinojimy/neural-hive-mind/explainability-api:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: explainability-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: explainability-api
        serviceAccountName: explainability-api
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "28"
      kubernetes.io/change-cause: kubectl set image deployment/explainability-api
        explainability-api=ghcr.io/albinojimy/neural-hive-mind/explainability-api:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: explainability-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:07:50Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: explainability-api
      app.kubernetes.io/name: explainability-api
      pod-template-hash: fd8f99dd8
    name: explainability-api-fd8f99dd8
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: explainability-api
      uid: 045477db-d366-4841-9d55-7728e40ff35f
    resourceVersion: "29533656"
    uid: 9c2c2bd1-2d87-4802-b2ea-a9ba4110b2dd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: explainability-api
        app.kubernetes.io/name: explainability-api
        pod-template-hash: fd8f99dd8
    template:
      metadata:
        annotations:
          checksum/config: e3837d2b6b9dc131551798941b991c7d75898ac1636b0a9fd69e738edec79648
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:30+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: explainability-api
          app.kubernetes.io/name: explainability-api
          pod-template-hash: fd8f99dd8
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - explainability-api
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: explainability-api
          image: ghcr.io/albinojimy/neural-hive-mind/explainability-api:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: explainability-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: explainability-api
        serviceAccountName: explainability-api
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2026-02-08T14:45:20Z"
    generation: 1
    labels:
      app: feedback-collection
      pod-template-hash: db8c9f98b
    name: feedback-collection-service-db8c9f98b
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: feedback-collection-service
      uid: 5dafa6f1-e370-4852-a6c6-f3147c03cb91
    resourceVersion: "27898414"
    uid: 6ad48897-62f2-453a-8651-7ef5db3f22b6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: feedback-collection
        pod-template-hash: db8c9f98b
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-08T15:44:50+01:00"
        creationTimestamp: null
        labels:
          app: feedback-collection
          pod-template-hash: db8c9f98b
      spec:
        containers:
        - args:
          - /scripts/feedback_service.py
          command:
          - python3
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: feedback-service
          ports:
          - containerPort: 8080
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /scripts
            name: scripts
          - mountPath: /app/ui
            name: ui
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: feedback-script
          name: scripts
        - configMap:
            defaultMode: 420
            name: feedback-ui
          name: ui
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T15:07:10Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: 594bc78dcb
    name: gateway-intencoes-594bc78dcb
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway-intencoes
      uid: cebf7b2b-588d-4ba8-919f-2ef3e85ac583
    resourceVersion: "29906994"
    uid: 25d3335f-79d6-462d-b544-0dd4087c856c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: gateway-intencoes
        app.kubernetes.io/name: gateway-intencoes
        pod-template-hash: 594bc78dcb
    template:
      metadata:
        annotations:
          checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
          checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
          kubectl.kubernetes.io/restartedAt: "2026-02-13T16:07:07+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: gateway-intencoes
          app.kubernetes.io/name: gateway-intencoes
          pod-template-hash: 594bc78dcb
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - gateway-intencoes
                topologyKey: kubernetes.io/hostname
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - gateway-intencoes
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: gateway-intencoes-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: gateway-intencoes-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_URL
            valueFrom:
              configMapKeyRef:
                key: schema_registry_url
                name: gateway-intencoes-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: gateway-intencoes-config
          - name: KAFKA_BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                key: kafka_batch_size
                name: gateway-intencoes-config
          - name: KAFKA_LINGER_MS
            valueFrom:
              configMapKeyRef:
                key: kafka_linger_ms
                name: gateway-intencoes-config
          - name: KAFKA_COMPRESSION_TYPE
            valueFrom:
              configMapKeyRef:
                key: kafka_compression_type
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CERTIFICATE_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_certificate_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_KEY_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_key_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_VERIFY_MODE
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_verify_mode
                name: gateway-intencoes-config
          - name: KAFKA_SSL_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_protocol
                name: gateway-intencoes-config
          - name: KAFKA_SASL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_MECHANISM
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_mechanism
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_token_endpoint
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_SCOPE
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_scope
                name: gateway-intencoes-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: gateway-intencoes-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: gateway-intencoes-config
          - name: REDIS_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_max_connections
                name: gateway-intencoes-config
          - name: REDIS_POOL_SIZE
            valueFrom:
              configMapKeyRef:
                key: redis_pool_size
                name: gateway-intencoes-config
          - name: REDIS_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_timeout
                name: gateway-intencoes-config
          - name: REDIS_RETRY_ON_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_retry_on_timeout
                name: gateway-intencoes-config
          - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_connection_pool_max_connections
                name: gateway-intencoes-config
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERT_REQS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_cert_reqs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CA_CERTS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_ca_certs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERTFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_certfile
                name: gateway-intencoes-config
          - name: REDIS_SSL_KEYFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_keyfile
                name: gateway-intencoes-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: gateway-intencoes-secret
          - name: KEYCLOAK_URL
            valueFrom:
              configMapKeyRef:
                key: keycloak_url
                name: gateway-intencoes-config
          - name: KEYCLOAK_REALM
            valueFrom:
              configMapKeyRef:
                key: keycloak_realm
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                key: keycloak_client_id
                name: gateway-intencoes-config
          - name: JWKS_URI
            valueFrom:
              configMapKeyRef:
                key: jwks_uri
                name: gateway-intencoes-config
          - name: TOKEN_VALIDATION_ENABLED
            valueFrom:
              configMapKeyRef:
                key: token_validation_enabled
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: keycloak-client-secret
                name: gateway-intencoes-secret
          - name: SCHEMA_REGISTRY_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_enabled
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_TLS_VERIFY
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_verify
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: schema_registry_ssl_ca_location
                name: gateway-intencoes-config
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt-secret-key
                name: gateway-intencoes-secret
          - name: ASR_MODEL_NAME
            value: base
          - name: ASR_DEVICE
            value: cpu
          - name: ASR_LAZY_LOADING
            value: "true"
          - name: ASR_MODEL_CACHE_DIR
            value: /app/models/whisper
          - name: NLU_LANGUAGE_MODEL
            value: pt_core_news_sm
          - name: NLU_MODEL_CACHE_DIR
            value: /app/models/spacy
          - name: NLU_CONFIDENCE_THRESHOLD
            value: "0.6"
          - name: NLU_CONFIDENCE_THRESHOLD_STRICT
            value: "0.75"
          - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
            value: "false"
          - name: NLU_RULES_CONFIG_PATH
            value: /app/config/nlu_rules.yaml
          - name: NLU_ROUTING_THRESHOLD_HIGH
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_high
                name: gateway-intencoes-config
          - name: NLU_ROUTING_THRESHOLD_LOW
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_low
                name: gateway-intencoes-config
          - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_use_adaptive_for_decisions
                name: gateway-intencoes-config
          - name: JAEGER_ENDPOINT
            value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
          - name: ENABLE_TRACING
            value: "true"
          - name: OTEL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: otel_enabled
                name: gateway-intencoes-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: gateway-intencoes-config
          - name: RATE_LIMIT_ENABLED
            valueFrom:
              configMapKeyRef:
                key: rate_limit_enabled
                name: gateway-intencoes-config
          - name: RATE_LIMIT_REQUESTS_PER_MINUTE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_requests_per_minute
                name: gateway-intencoes-config
          - name: RATE_LIMIT_BURST_SIZE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_burst_size
                name: gateway-intencoes-config
          - name: RATE_LIMIT_FAIL_OPEN
            valueFrom:
              configMapKeyRef:
                key: rate_limit_fail_open
                name: gateway-intencoes-config
          - name: RATE_LIMIT_TENANT_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_tenant_overrides
                name: gateway-intencoes-config
          - name: RATE_LIMIT_USER_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_user_overrides
                name: gateway-intencoes-config
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: allow_insecure_http_endpoints
                name: gateway-intencoes-config
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 25
            successThreshold: 1
            timeoutSeconds: 8
          name: gateway-intencoes
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 40
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/models
            name: model-cache
          - mountPath: /etc/ssl/certs/schema-registry-ca.crt
            name: schema-registry-ca-cert
            readOnly: true
            subPath: ca.crt
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: gateway-intencoes
        serviceAccountName: gateway-intencoes
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gateway-intencoes-models-pvc
        - name: schema-registry-ca-cert
          secret:
            defaultMode: 420
            items:
            - key: ca.crt
              path: ca.crt
            secretName: schema-registry-tls-secret
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:12:08Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: 644bd4fd7d
    name: gateway-intencoes-644bd4fd7d
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway-intencoes
      uid: cebf7b2b-588d-4ba8-919f-2ef3e85ac583
    resourceVersion: "29907242"
    uid: 191f4d55-db88-4753-b512-f26319bc57f9
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: gateway-intencoes
        app.kubernetes.io/name: gateway-intencoes
        pod-template-hash: 644bd4fd7d
    template:
      metadata:
        annotations:
          checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
          checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
          kubectl.kubernetes.io/restartedAt: "2026-02-13T22:12:06+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: gateway-intencoes
          app.kubernetes.io/name: gateway-intencoes
          pod-template-hash: 644bd4fd7d
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - gateway-intencoes
                topologyKey: kubernetes.io/hostname
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - gateway-intencoes
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: gateway-intencoes-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: gateway-intencoes-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_URL
            valueFrom:
              configMapKeyRef:
                key: schema_registry_url
                name: gateway-intencoes-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: gateway-intencoes-config
          - name: KAFKA_BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                key: kafka_batch_size
                name: gateway-intencoes-config
          - name: KAFKA_LINGER_MS
            valueFrom:
              configMapKeyRef:
                key: kafka_linger_ms
                name: gateway-intencoes-config
          - name: KAFKA_COMPRESSION_TYPE
            valueFrom:
              configMapKeyRef:
                key: kafka_compression_type
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CERTIFICATE_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_certificate_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_KEY_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_key_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_VERIFY_MODE
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_verify_mode
                name: gateway-intencoes-config
          - name: KAFKA_SSL_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_protocol
                name: gateway-intencoes-config
          - name: KAFKA_SASL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_MECHANISM
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_mechanism
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_token_endpoint
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_SCOPE
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_scope
                name: gateway-intencoes-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: gateway-intencoes-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: gateway-intencoes-config
          - name: REDIS_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_max_connections
                name: gateway-intencoes-config
          - name: REDIS_POOL_SIZE
            valueFrom:
              configMapKeyRef:
                key: redis_pool_size
                name: gateway-intencoes-config
          - name: REDIS_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_timeout
                name: gateway-intencoes-config
          - name: REDIS_RETRY_ON_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_retry_on_timeout
                name: gateway-intencoes-config
          - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_connection_pool_max_connections
                name: gateway-intencoes-config
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERT_REQS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_cert_reqs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CA_CERTS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_ca_certs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERTFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_certfile
                name: gateway-intencoes-config
          - name: REDIS_SSL_KEYFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_keyfile
                name: gateway-intencoes-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: gateway-intencoes-secret
          - name: KEYCLOAK_URL
            valueFrom:
              configMapKeyRef:
                key: keycloak_url
                name: gateway-intencoes-config
          - name: KEYCLOAK_REALM
            valueFrom:
              configMapKeyRef:
                key: keycloak_realm
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                key: keycloak_client_id
                name: gateway-intencoes-config
          - name: JWKS_URI
            valueFrom:
              configMapKeyRef:
                key: jwks_uri
                name: gateway-intencoes-config
          - name: TOKEN_VALIDATION_ENABLED
            valueFrom:
              configMapKeyRef:
                key: token_validation_enabled
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: keycloak-client-secret
                name: gateway-intencoes-secret
          - name: SCHEMA_REGISTRY_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_enabled
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_TLS_VERIFY
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_verify
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: schema_registry_ssl_ca_location
                name: gateway-intencoes-config
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt-secret-key
                name: gateway-intencoes-secret
          - name: ASR_MODEL_NAME
            value: base
          - name: ASR_DEVICE
            value: cpu
          - name: ASR_LAZY_LOADING
            value: "true"
          - name: ASR_MODEL_CACHE_DIR
            value: /app/models/whisper
          - name: NLU_LANGUAGE_MODEL
            value: pt_core_news_sm
          - name: NLU_MODEL_CACHE_DIR
            value: /app/models/spacy
          - name: NLU_CONFIDENCE_THRESHOLD
            value: "0.6"
          - name: NLU_CONFIDENCE_THRESHOLD_STRICT
            value: "0.75"
          - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
            value: "false"
          - name: NLU_RULES_CONFIG_PATH
            value: /app/config/nlu_rules.yaml
          - name: NLU_ROUTING_THRESHOLD_HIGH
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_high
                name: gateway-intencoes-config
          - name: NLU_ROUTING_THRESHOLD_LOW
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_low
                name: gateway-intencoes-config
          - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_use_adaptive_for_decisions
                name: gateway-intencoes-config
          - name: JAEGER_ENDPOINT
            value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
          - name: ENABLE_TRACING
            value: "true"
          - name: OTEL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: otel_enabled
                name: gateway-intencoes-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: gateway-intencoes-config
          - name: RATE_LIMIT_ENABLED
            valueFrom:
              configMapKeyRef:
                key: rate_limit_enabled
                name: gateway-intencoes-config
          - name: RATE_LIMIT_REQUESTS_PER_MINUTE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_requests_per_minute
                name: gateway-intencoes-config
          - name: RATE_LIMIT_BURST_SIZE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_burst_size
                name: gateway-intencoes-config
          - name: RATE_LIMIT_FAIL_OPEN
            valueFrom:
              configMapKeyRef:
                key: rate_limit_fail_open
                name: gateway-intencoes-config
          - name: RATE_LIMIT_TENANT_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_tenant_overrides
                name: gateway-intencoes-config
          - name: RATE_LIMIT_USER_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_user_overrides
                name: gateway-intencoes-config
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: allow_insecure_http_endpoints
                name: gateway-intencoes-config
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 25
            successThreshold: 1
            timeoutSeconds: 8
          name: gateway-intencoes
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 40
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/models
            name: model-cache
          - mountPath: /etc/ssl/certs/schema-registry-ca.crt
            name: schema-registry-ca-cert
            readOnly: true
            subPath: ca.crt
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: gateway-intencoes
        serviceAccountName: gateway-intencoes
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gateway-intencoes-models-pvc
        - name: schema-registry-ca-cert
          secret:
            defaultMode: 420
            items:
            - key: ca.crt
              path: ca.crt
            secretName: schema-registry-tls-secret
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 2
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T14:28:29Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: 666f664d8
    name: gateway-intencoes-666f664d8
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway-intencoes
      uid: cebf7b2b-588d-4ba8-919f-2ef3e85ac583
    resourceVersion: "29799354"
    uid: f7779f5e-cefa-4def-bd59-b8bd35a02d6f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: gateway-intencoes
        app.kubernetes.io/name: gateway-intencoes
        pod-template-hash: 666f664d8
    template:
      metadata:
        annotations:
          checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
          checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
          kubectl.kubernetes.io/restartedAt: "2026-02-13T15:27:02+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: gateway-intencoes
          app.kubernetes.io/name: gateway-intencoes
          pod-template-hash: 666f664d8
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - gateway-intencoes
                topologyKey: kubernetes.io/hostname
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - gateway-intencoes
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: gateway-intencoes-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: gateway-intencoes-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_URL
            valueFrom:
              configMapKeyRef:
                key: schema_registry_url
                name: gateway-intencoes-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: gateway-intencoes-config
          - name: KAFKA_BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                key: kafka_batch_size
                name: gateway-intencoes-config
          - name: KAFKA_LINGER_MS
            valueFrom:
              configMapKeyRef:
                key: kafka_linger_ms
                name: gateway-intencoes-config
          - name: KAFKA_COMPRESSION_TYPE
            valueFrom:
              configMapKeyRef:
                key: kafka_compression_type
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CERTIFICATE_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_certificate_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_KEY_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_key_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_VERIFY_MODE
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_verify_mode
                name: gateway-intencoes-config
          - name: KAFKA_SSL_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_protocol
                name: gateway-intencoes-config
          - name: KAFKA_SASL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_MECHANISM
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_mechanism
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_token_endpoint
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_SCOPE
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_scope
                name: gateway-intencoes-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: gateway-intencoes-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: gateway-intencoes-config
          - name: REDIS_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_max_connections
                name: gateway-intencoes-config
          - name: REDIS_POOL_SIZE
            valueFrom:
              configMapKeyRef:
                key: redis_pool_size
                name: gateway-intencoes-config
          - name: REDIS_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_timeout
                name: gateway-intencoes-config
          - name: REDIS_RETRY_ON_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_retry_on_timeout
                name: gateway-intencoes-config
          - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_connection_pool_max_connections
                name: gateway-intencoes-config
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERT_REQS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_cert_reqs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CA_CERTS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_ca_certs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERTFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_certfile
                name: gateway-intencoes-config
          - name: REDIS_SSL_KEYFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_keyfile
                name: gateway-intencoes-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: gateway-intencoes-secret
          - name: KEYCLOAK_URL
            valueFrom:
              configMapKeyRef:
                key: keycloak_url
                name: gateway-intencoes-config
          - name: KEYCLOAK_REALM
            valueFrom:
              configMapKeyRef:
                key: keycloak_realm
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                key: keycloak_client_id
                name: gateway-intencoes-config
          - name: JWKS_URI
            valueFrom:
              configMapKeyRef:
                key: jwks_uri
                name: gateway-intencoes-config
          - name: TOKEN_VALIDATION_ENABLED
            valueFrom:
              configMapKeyRef:
                key: token_validation_enabled
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: keycloak-client-secret
                name: gateway-intencoes-secret
          - name: SCHEMA_REGISTRY_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_enabled
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_TLS_VERIFY
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_verify
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: schema_registry_ssl_ca_location
                name: gateway-intencoes-config
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt-secret-key
                name: gateway-intencoes-secret
          - name: ASR_MODEL_NAME
            value: base
          - name: ASR_DEVICE
            value: cpu
          - name: ASR_LAZY_LOADING
            value: "true"
          - name: ASR_MODEL_CACHE_DIR
            value: /app/models/whisper
          - name: NLU_LANGUAGE_MODEL
            value: pt_core_news_sm
          - name: NLU_MODEL_CACHE_DIR
            value: /app/models/spacy
          - name: NLU_CONFIDENCE_THRESHOLD
            value: "0.6"
          - name: NLU_CONFIDENCE_THRESHOLD_STRICT
            value: "0.75"
          - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
            value: "false"
          - name: NLU_RULES_CONFIG_PATH
            value: /app/config/nlu_rules.yaml
          - name: NLU_ROUTING_THRESHOLD_HIGH
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_high
                name: gateway-intencoes-config
          - name: NLU_ROUTING_THRESHOLD_LOW
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_low
                name: gateway-intencoes-config
          - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_use_adaptive_for_decisions
                name: gateway-intencoes-config
          - name: JAEGER_ENDPOINT
            value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
          - name: ENABLE_TRACING
            value: "true"
          - name: OTEL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: otel_enabled
                name: gateway-intencoes-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: gateway-intencoes-config
          - name: RATE_LIMIT_ENABLED
            valueFrom:
              configMapKeyRef:
                key: rate_limit_enabled
                name: gateway-intencoes-config
          - name: RATE_LIMIT_REQUESTS_PER_MINUTE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_requests_per_minute
                name: gateway-intencoes-config
          - name: RATE_LIMIT_BURST_SIZE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_burst_size
                name: gateway-intencoes-config
          - name: RATE_LIMIT_FAIL_OPEN
            valueFrom:
              configMapKeyRef:
                key: rate_limit_fail_open
                name: gateway-intencoes-config
          - name: RATE_LIMIT_TENANT_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_tenant_overrides
                name: gateway-intencoes-config
          - name: RATE_LIMIT_USER_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_user_overrides
                name: gateway-intencoes-config
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: allow_insecure_http_endpoints
                name: gateway-intencoes-config
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 25
            successThreshold: 1
            timeoutSeconds: 8
          name: gateway-intencoes
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 40
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/models
            name: model-cache
          - mountPath: /etc/ssl/certs/schema-registry-ca.crt
            name: schema-registry-ca-cert
            readOnly: true
            subPath: ca.crt
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: gateway-intencoes
        serviceAccountName: gateway-intencoes
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gateway-intencoes-models-pvc
        - name: schema-registry-ca-cert
          secret:
            defaultMode: 420
            items:
            - key: ca.crt
              path: ca.crt
            secretName: schema-registry-tls-secret
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
      deployment.kubernetes.io/revision-history: "5"
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T14:32:26Z"
    generation: 5
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: 6c54fd8658
    name: gateway-intencoes-6c54fd8658
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway-intencoes
      uid: cebf7b2b-588d-4ba8-919f-2ef3e85ac583
    resourceVersion: "29907217"
    uid: a39fc52c-5f61-4333-a278-5e70438f416e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: gateway-intencoes
        app.kubernetes.io/name: gateway-intencoes
        pod-template-hash: 6c54fd8658
    template:
      metadata:
        annotations:
          checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
          checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
          kubectl.kubernetes.io/restartedAt: "2026-02-13T15:32:24+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: gateway-intencoes
          app.kubernetes.io/name: gateway-intencoes
          pod-template-hash: 6c54fd8658
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - gateway-intencoes
                topologyKey: kubernetes.io/hostname
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - gateway-intencoes
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: gateway-intencoes-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: gateway-intencoes-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_URL
            valueFrom:
              configMapKeyRef:
                key: schema_registry_url
                name: gateway-intencoes-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: gateway-intencoes-config
          - name: KAFKA_BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                key: kafka_batch_size
                name: gateway-intencoes-config
          - name: KAFKA_LINGER_MS
            valueFrom:
              configMapKeyRef:
                key: kafka_linger_ms
                name: gateway-intencoes-config
          - name: KAFKA_COMPRESSION_TYPE
            valueFrom:
              configMapKeyRef:
                key: kafka_compression_type
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CERTIFICATE_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_certificate_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_KEY_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_key_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_VERIFY_MODE
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_verify_mode
                name: gateway-intencoes-config
          - name: KAFKA_SSL_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_protocol
                name: gateway-intencoes-config
          - name: KAFKA_SASL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_MECHANISM
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_mechanism
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_token_endpoint
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_SCOPE
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_scope
                name: gateway-intencoes-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: gateway-intencoes-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: gateway-intencoes-config
          - name: REDIS_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_max_connections
                name: gateway-intencoes-config
          - name: REDIS_POOL_SIZE
            valueFrom:
              configMapKeyRef:
                key: redis_pool_size
                name: gateway-intencoes-config
          - name: REDIS_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_timeout
                name: gateway-intencoes-config
          - name: REDIS_RETRY_ON_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_retry_on_timeout
                name: gateway-intencoes-config
          - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_connection_pool_max_connections
                name: gateway-intencoes-config
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERT_REQS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_cert_reqs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CA_CERTS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_ca_certs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERTFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_certfile
                name: gateway-intencoes-config
          - name: REDIS_SSL_KEYFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_keyfile
                name: gateway-intencoes-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: gateway-intencoes-secret
          - name: KEYCLOAK_URL
            valueFrom:
              configMapKeyRef:
                key: keycloak_url
                name: gateway-intencoes-config
          - name: KEYCLOAK_REALM
            valueFrom:
              configMapKeyRef:
                key: keycloak_realm
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                key: keycloak_client_id
                name: gateway-intencoes-config
          - name: JWKS_URI
            valueFrom:
              configMapKeyRef:
                key: jwks_uri
                name: gateway-intencoes-config
          - name: TOKEN_VALIDATION_ENABLED
            valueFrom:
              configMapKeyRef:
                key: token_validation_enabled
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: keycloak-client-secret
                name: gateway-intencoes-secret
          - name: SCHEMA_REGISTRY_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_enabled
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_TLS_VERIFY
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_verify
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: schema_registry_ssl_ca_location
                name: gateway-intencoes-config
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt-secret-key
                name: gateway-intencoes-secret
          - name: ASR_MODEL_NAME
            value: base
          - name: ASR_DEVICE
            value: cpu
          - name: ASR_LAZY_LOADING
            value: "true"
          - name: ASR_MODEL_CACHE_DIR
            value: /app/models/whisper
          - name: NLU_LANGUAGE_MODEL
            value: pt_core_news_sm
          - name: NLU_MODEL_CACHE_DIR
            value: /app/models/spacy
          - name: NLU_CONFIDENCE_THRESHOLD
            value: "0.6"
          - name: NLU_CONFIDENCE_THRESHOLD_STRICT
            value: "0.75"
          - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
            value: "false"
          - name: NLU_RULES_CONFIG_PATH
            value: /app/config/nlu_rules.yaml
          - name: NLU_ROUTING_THRESHOLD_HIGH
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_high
                name: gateway-intencoes-config
          - name: NLU_ROUTING_THRESHOLD_LOW
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_low
                name: gateway-intencoes-config
          - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_use_adaptive_for_decisions
                name: gateway-intencoes-config
          - name: JAEGER_ENDPOINT
            value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
          - name: ENABLE_TRACING
            value: "true"
          - name: OTEL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: otel_enabled
                name: gateway-intencoes-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: gateway-intencoes-config
          - name: RATE_LIMIT_ENABLED
            valueFrom:
              configMapKeyRef:
                key: rate_limit_enabled
                name: gateway-intencoes-config
          - name: RATE_LIMIT_REQUESTS_PER_MINUTE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_requests_per_minute
                name: gateway-intencoes-config
          - name: RATE_LIMIT_BURST_SIZE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_burst_size
                name: gateway-intencoes-config
          - name: RATE_LIMIT_FAIL_OPEN
            valueFrom:
              configMapKeyRef:
                key: rate_limit_fail_open
                name: gateway-intencoes-config
          - name: RATE_LIMIT_TENANT_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_tenant_overrides
                name: gateway-intencoes-config
          - name: RATE_LIMIT_USER_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_user_overrides
                name: gateway-intencoes-config
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: allow_insecure_http_endpoints
                name: gateway-intencoes-config
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 25
            successThreshold: 1
            timeoutSeconds: 8
          name: gateway-intencoes
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 40
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/models
            name: model-cache
          - mountPath: /etc/ssl/certs/schema-registry-ca.crt
            name: schema-registry-ca-cert
            readOnly: true
            subPath: ca.crt
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: gateway-intencoes
        serviceAccountName: gateway-intencoes
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gateway-intencoes-models-pvc
        - name: schema-registry-ca-cert
          secret:
            defaultMode: 420
            items:
            - key: ca.crt
              path: ca.crt
            secretName: schema-registry-tls-secret
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T14:13:19Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: 784cbd9db5
    name: gateway-intencoes-784cbd9db5
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway-intencoes
      uid: cebf7b2b-588d-4ba8-919f-2ef3e85ac583
    resourceVersion: "29797051"
    uid: 3a851d47-f2f6-469e-ad1a-48c0ad46198e
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: gateway-intencoes
        app.kubernetes.io/name: gateway-intencoes
        pod-template-hash: 784cbd9db5
    template:
      metadata:
        annotations:
          checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
          checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: gateway-intencoes
          app.kubernetes.io/name: gateway-intencoes
          pod-template-hash: 784cbd9db5
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - gateway-intencoes
                topologyKey: kubernetes.io/hostname
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - gateway-intencoes
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: gateway-intencoes-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: gateway-intencoes-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_URL
            valueFrom:
              configMapKeyRef:
                key: schema_registry_url
                name: gateway-intencoes-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: gateway-intencoes-config
          - name: KAFKA_BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                key: kafka_batch_size
                name: gateway-intencoes-config
          - name: KAFKA_LINGER_MS
            valueFrom:
              configMapKeyRef:
                key: kafka_linger_ms
                name: gateway-intencoes-config
          - name: KAFKA_COMPRESSION_TYPE
            valueFrom:
              configMapKeyRef:
                key: kafka_compression_type
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CERTIFICATE_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_certificate_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_KEY_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_key_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_VERIFY_MODE
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_verify_mode
                name: gateway-intencoes-config
          - name: KAFKA_SSL_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_protocol
                name: gateway-intencoes-config
          - name: KAFKA_SASL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_MECHANISM
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_mechanism
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_token_endpoint
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_SCOPE
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_scope
                name: gateway-intencoes-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: gateway-intencoes-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: gateway-intencoes-config
          - name: REDIS_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_max_connections
                name: gateway-intencoes-config
          - name: REDIS_POOL_SIZE
            valueFrom:
              configMapKeyRef:
                key: redis_pool_size
                name: gateway-intencoes-config
          - name: REDIS_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_timeout
                name: gateway-intencoes-config
          - name: REDIS_RETRY_ON_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_retry_on_timeout
                name: gateway-intencoes-config
          - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_connection_pool_max_connections
                name: gateway-intencoes-config
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERT_REQS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_cert_reqs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CA_CERTS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_ca_certs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERTFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_certfile
                name: gateway-intencoes-config
          - name: REDIS_SSL_KEYFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_keyfile
                name: gateway-intencoes-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: gateway-intencoes-secret
          - name: KEYCLOAK_URL
            valueFrom:
              configMapKeyRef:
                key: keycloak_url
                name: gateway-intencoes-config
          - name: KEYCLOAK_REALM
            valueFrom:
              configMapKeyRef:
                key: keycloak_realm
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                key: keycloak_client_id
                name: gateway-intencoes-config
          - name: JWKS_URI
            valueFrom:
              configMapKeyRef:
                key: jwks_uri
                name: gateway-intencoes-config
          - name: TOKEN_VALIDATION_ENABLED
            valueFrom:
              configMapKeyRef:
                key: token_validation_enabled
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: keycloak-client-secret
                name: gateway-intencoes-secret
          - name: SCHEMA_REGISTRY_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_enabled
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_TLS_VERIFY
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_verify
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: schema_registry_ssl_ca_location
                name: gateway-intencoes-config
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt-secret-key
                name: gateway-intencoes-secret
          - name: ASR_MODEL_NAME
            value: base
          - name: ASR_DEVICE
            value: cpu
          - name: ASR_LAZY_LOADING
            value: "true"
          - name: ASR_MODEL_CACHE_DIR
            value: /app/models/whisper
          - name: NLU_LANGUAGE_MODEL
            value: pt_core_news_sm
          - name: NLU_MODEL_CACHE_DIR
            value: /app/models/spacy
          - name: NLU_CONFIDENCE_THRESHOLD
            value: "0.6"
          - name: NLU_CONFIDENCE_THRESHOLD_STRICT
            value: "0.75"
          - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
            value: "false"
          - name: NLU_RULES_CONFIG_PATH
            value: /app/config/nlu_rules.yaml
          - name: NLU_ROUTING_THRESHOLD_HIGH
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_high
                name: gateway-intencoes-config
          - name: NLU_ROUTING_THRESHOLD_LOW
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_low
                name: gateway-intencoes-config
          - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_use_adaptive_for_decisions
                name: gateway-intencoes-config
          - name: JAEGER_ENDPOINT
            value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
          - name: ENABLE_TRACING
            value: "true"
          - name: OTEL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: otel_enabled
                name: gateway-intencoes-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: gateway-intencoes-config
          - name: RATE_LIMIT_ENABLED
            valueFrom:
              configMapKeyRef:
                key: rate_limit_enabled
                name: gateway-intencoes-config
          - name: RATE_LIMIT_REQUESTS_PER_MINUTE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_requests_per_minute
                name: gateway-intencoes-config
          - name: RATE_LIMIT_BURST_SIZE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_burst_size
                name: gateway-intencoes-config
          - name: RATE_LIMIT_FAIL_OPEN
            valueFrom:
              configMapKeyRef:
                key: rate_limit_fail_open
                name: gateway-intencoes-config
          - name: RATE_LIMIT_TENANT_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_tenant_overrides
                name: gateway-intencoes-config
          - name: RATE_LIMIT_USER_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_user_overrides
                name: gateway-intencoes-config
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: allow_insecure_http_endpoints
                name: gateway-intencoes-config
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 25
            successThreshold: 1
            timeoutSeconds: 8
          name: gateway-intencoes
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 40
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/models
            name: model-cache
          - mountPath: /etc/ssl/certs/schema-registry-ca.crt
            name: schema-registry-ca-cert
            readOnly: true
            subPath: ca.crt
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: gateway-intencoes
        serviceAccountName: gateway-intencoes
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gateway-intencoes-models-pvc
        - name: schema-registry-ca-cert
          secret:
            defaultMode: 420
            items:
            - key: ca.crt
              path: ca.crt
            secretName: schema-registry-tls-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T14:27:04Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: 7fddcc5cf6
    name: gateway-intencoes-7fddcc5cf6
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway-intencoes
      uid: cebf7b2b-588d-4ba8-919f-2ef3e85ac583
    resourceVersion: "29798175"
    uid: d25cb2ee-634a-4628-9e4c-1f94f1c4219b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: gateway-intencoes
        app.kubernetes.io/name: gateway-intencoes
        pod-template-hash: 7fddcc5cf6
    template:
      metadata:
        annotations:
          checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
          checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
          kubectl.kubernetes.io/restartedAt: "2026-02-13T15:27:02+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: gateway-intencoes
          app.kubernetes.io/name: gateway-intencoes
          pod-template-hash: 7fddcc5cf6
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - gateway-intencoes
                topologyKey: kubernetes.io/hostname
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - gateway-intencoes
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: gateway-intencoes-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: gateway-intencoes-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_URL
            valueFrom:
              configMapKeyRef:
                key: schema_registry_url
                name: gateway-intencoes-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: gateway-intencoes-config
          - name: KAFKA_BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                key: kafka_batch_size
                name: gateway-intencoes-config
          - name: KAFKA_LINGER_MS
            valueFrom:
              configMapKeyRef:
                key: kafka_linger_ms
                name: gateway-intencoes-config
          - name: KAFKA_COMPRESSION_TYPE
            valueFrom:
              configMapKeyRef:
                key: kafka_compression_type
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CERTIFICATE_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_certificate_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_KEY_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_key_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_VERIFY_MODE
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_verify_mode
                name: gateway-intencoes-config
          - name: KAFKA_SSL_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_protocol
                name: gateway-intencoes-config
          - name: KAFKA_SASL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_MECHANISM
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_mechanism
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_token_endpoint
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_SCOPE
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_scope
                name: gateway-intencoes-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: gateway-intencoes-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: gateway-intencoes-config
          - name: REDIS_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_max_connections
                name: gateway-intencoes-config
          - name: REDIS_POOL_SIZE
            valueFrom:
              configMapKeyRef:
                key: redis_pool_size
                name: gateway-intencoes-config
          - name: REDIS_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_timeout
                name: gateway-intencoes-config
          - name: REDIS_RETRY_ON_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_retry_on_timeout
                name: gateway-intencoes-config
          - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_connection_pool_max_connections
                name: gateway-intencoes-config
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERT_REQS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_cert_reqs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CA_CERTS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_ca_certs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERTFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_certfile
                name: gateway-intencoes-config
          - name: REDIS_SSL_KEYFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_keyfile
                name: gateway-intencoes-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: gateway-intencoes-secret
          - name: KEYCLOAK_URL
            valueFrom:
              configMapKeyRef:
                key: keycloak_url
                name: gateway-intencoes-config
          - name: KEYCLOAK_REALM
            valueFrom:
              configMapKeyRef:
                key: keycloak_realm
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                key: keycloak_client_id
                name: gateway-intencoes-config
          - name: JWKS_URI
            valueFrom:
              configMapKeyRef:
                key: jwks_uri
                name: gateway-intencoes-config
          - name: TOKEN_VALIDATION_ENABLED
            valueFrom:
              configMapKeyRef:
                key: token_validation_enabled
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: keycloak-client-secret
                name: gateway-intencoes-secret
          - name: SCHEMA_REGISTRY_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_enabled
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_TLS_VERIFY
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_verify
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: schema_registry_ssl_ca_location
                name: gateway-intencoes-config
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt-secret-key
                name: gateway-intencoes-secret
          - name: ASR_MODEL_NAME
            value: base
          - name: ASR_DEVICE
            value: cpu
          - name: ASR_LAZY_LOADING
            value: "true"
          - name: ASR_MODEL_CACHE_DIR
            value: /app/models/whisper
          - name: NLU_LANGUAGE_MODEL
            value: pt_core_news_sm
          - name: NLU_MODEL_CACHE_DIR
            value: /app/models/spacy
          - name: NLU_CONFIDENCE_THRESHOLD
            value: "0.6"
          - name: NLU_CONFIDENCE_THRESHOLD_STRICT
            value: "0.75"
          - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
            value: "false"
          - name: NLU_RULES_CONFIG_PATH
            value: /app/config/nlu_rules.yaml
          - name: NLU_ROUTING_THRESHOLD_HIGH
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_high
                name: gateway-intencoes-config
          - name: NLU_ROUTING_THRESHOLD_LOW
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_low
                name: gateway-intencoes-config
          - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_use_adaptive_for_decisions
                name: gateway-intencoes-config
          - name: JAEGER_ENDPOINT
            value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
          - name: ENABLE_TRACING
            value: "true"
          - name: OTEL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: otel_enabled
                name: gateway-intencoes-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: gateway-intencoes-config
          - name: RATE_LIMIT_ENABLED
            valueFrom:
              configMapKeyRef:
                key: rate_limit_enabled
                name: gateway-intencoes-config
          - name: RATE_LIMIT_REQUESTS_PER_MINUTE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_requests_per_minute
                name: gateway-intencoes-config
          - name: RATE_LIMIT_BURST_SIZE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_burst_size
                name: gateway-intencoes-config
          - name: RATE_LIMIT_FAIL_OPEN
            valueFrom:
              configMapKeyRef:
                key: rate_limit_fail_open
                name: gateway-intencoes-config
          - name: RATE_LIMIT_TENANT_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_tenant_overrides
                name: gateway-intencoes-config
          - name: RATE_LIMIT_USER_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_user_overrides
                name: gateway-intencoes-config
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: allow_insecure_http_endpoints
                name: gateway-intencoes-config
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 25
            successThreshold: 1
            timeoutSeconds: 8
          name: gateway-intencoes
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 40
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/models
            name: model-cache
          - mountPath: /etc/ssl/certs/schema-registry-ca.crt
            name: schema-registry-ca-cert
            readOnly: true
            subPath: ca.crt
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        nodeSelector:
          kubernetes.io/hostname: vmi2911680
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: gateway-intencoes
        serviceAccountName: gateway-intencoes
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gateway-intencoes-models-pvc
        - name: schema-registry-ca-cert
          secret:
            defaultMode: 420
            items:
            - key: ca.crt
              path: ca.crt
            secretName: schema-registry-tls-secret
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: gateway-intencoes
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T14:16:58Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: gateway-intencoes
      app.kubernetes.io/name: gateway-intencoes
      pod-template-hash: c95f49675
    name: gateway-intencoes-c95f49675
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway-intencoes
      uid: cebf7b2b-588d-4ba8-919f-2ef3e85ac583
    resourceVersion: "29801227"
    uid: b46c079b-8b9e-4705-846c-b71c691ade40
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: gateway-intencoes
        app.kubernetes.io/name: gateway-intencoes
        pod-template-hash: c95f49675
    template:
      metadata:
        annotations:
          checksum/config: c496992a2759a806c6cbe91dbaf56df2ed1ab44c134ae25424cc22481b6be26a
          checksum/secret: 7c7bed457e2861912e8082b4d18eae9424bd52ffd7f9547b50d5707fc2a21775
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: gateway-intencoes
          app.kubernetes.io/name: gateway-intencoes
          pod-template-hash: c95f49675
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - gateway-intencoes
                topologyKey: kubernetes.io/hostname
              weight: 100
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - gateway-intencoes
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: gateway-intencoes-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: gateway-intencoes-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_URL
            valueFrom:
              configMapKeyRef:
                key: schema_registry_url
                name: gateway-intencoes-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: gateway-intencoes-config
          - name: KAFKA_BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                key: kafka_batch_size
                name: gateway-intencoes-config
          - name: KAFKA_LINGER_MS
            valueFrom:
              configMapKeyRef:
                key: kafka_linger_ms
                name: gateway-intencoes-config
          - name: KAFKA_COMPRESSION_TYPE
            valueFrom:
              configMapKeyRef:
                key: kafka_compression_type
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CERTIFICATE_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_certificate_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_KEY_LOCATION
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_key_location
                name: gateway-intencoes-config
          - name: KAFKA_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SSL_CA_VERIFY_MODE
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_ca_verify_mode
                name: gateway-intencoes-config
          - name: KAFKA_SSL_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_ssl_protocol
                name: gateway-intencoes-config
          - name: KAFKA_SASL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_MECHANISM
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_mechanism
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_ENABLED
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_enabled
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_TOKEN_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_token_endpoint
                name: gateway-intencoes-config
          - name: KAFKA_SASL_OAUTH2_SCOPE
            valueFrom:
              configMapKeyRef:
                key: kafka_sasl_oauth2_scope
                name: gateway-intencoes-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: gateway-intencoes-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: gateway-intencoes-config
          - name: REDIS_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_max_connections
                name: gateway-intencoes-config
          - name: REDIS_POOL_SIZE
            valueFrom:
              configMapKeyRef:
                key: redis_pool_size
                name: gateway-intencoes-config
          - name: REDIS_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_timeout
                name: gateway-intencoes-config
          - name: REDIS_RETRY_ON_TIMEOUT
            valueFrom:
              configMapKeyRef:
                key: redis_retry_on_timeout
                name: gateway-intencoes-config
          - name: REDIS_CONNECTION_POOL_MAX_CONNECTIONS
            valueFrom:
              configMapKeyRef:
                key: redis_connection_pool_max_connections
                name: gateway-intencoes-config
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERT_REQS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_cert_reqs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CA_CERTS
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_ca_certs
                name: gateway-intencoes-config
          - name: REDIS_SSL_CERTFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_certfile
                name: gateway-intencoes-config
          - name: REDIS_SSL_KEYFILE
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_keyfile
                name: gateway-intencoes-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: gateway-intencoes-secret
          - name: KEYCLOAK_URL
            valueFrom:
              configMapKeyRef:
                key: keycloak_url
                name: gateway-intencoes-config
          - name: KEYCLOAK_REALM
            valueFrom:
              configMapKeyRef:
                key: keycloak_realm
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                key: keycloak_client_id
                name: gateway-intencoes-config
          - name: JWKS_URI
            valueFrom:
              configMapKeyRef:
                key: jwks_uri
                name: gateway-intencoes-config
          - name: TOKEN_VALIDATION_ENABLED
            valueFrom:
              configMapKeyRef:
                key: token_validation_enabled
                name: gateway-intencoes-config
          - name: KEYCLOAK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                key: keycloak-client-secret
                name: gateway-intencoes-secret
          - name: SCHEMA_REGISTRY_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_enabled
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_TLS_VERIFY
            valueFrom:
              configMapKeyRef:
                key: schema_registry_tls_verify
                name: gateway-intencoes-config
          - name: SCHEMA_REGISTRY_SSL_CA_LOCATION
            valueFrom:
              configMapKeyRef:
                key: schema_registry_ssl_ca_location
                name: gateway-intencoes-config
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt-secret-key
                name: gateway-intencoes-secret
          - name: ASR_MODEL_NAME
            value: base
          - name: ASR_DEVICE
            value: cpu
          - name: ASR_LAZY_LOADING
            value: "true"
          - name: ASR_MODEL_CACHE_DIR
            value: /app/models/whisper
          - name: NLU_LANGUAGE_MODEL
            value: pt_core_news_sm
          - name: NLU_MODEL_CACHE_DIR
            value: /app/models/spacy
          - name: NLU_CONFIDENCE_THRESHOLD
            value: "0.6"
          - name: NLU_CONFIDENCE_THRESHOLD_STRICT
            value: "0.75"
          - name: NLU_ADAPTIVE_THRESHOLD_ENABLED
            value: "false"
          - name: NLU_RULES_CONFIG_PATH
            value: /app/config/nlu_rules.yaml
          - name: NLU_ROUTING_THRESHOLD_HIGH
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_high
                name: gateway-intencoes-config
          - name: NLU_ROUTING_THRESHOLD_LOW
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_threshold_low
                name: gateway-intencoes-config
          - name: NLU_ROUTING_USE_ADAPTIVE_FOR_DECISIONS
            valueFrom:
              configMapKeyRef:
                key: nlu_routing_use_adaptive_for_decisions
                name: gateway-intencoes-config
          - name: JAEGER_ENDPOINT
            value: http://jaeger-collector.monitoring.svc.cluster.local:14268/api/traces
          - name: ENABLE_TRACING
            value: "true"
          - name: OTEL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: otel_enabled
                name: gateway-intencoes-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: gateway-intencoes-config
          - name: RATE_LIMIT_ENABLED
            valueFrom:
              configMapKeyRef:
                key: rate_limit_enabled
                name: gateway-intencoes-config
          - name: RATE_LIMIT_REQUESTS_PER_MINUTE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_requests_per_minute
                name: gateway-intencoes-config
          - name: RATE_LIMIT_BURST_SIZE
            valueFrom:
              configMapKeyRef:
                key: rate_limit_burst_size
                name: gateway-intencoes-config
          - name: RATE_LIMIT_FAIL_OPEN
            valueFrom:
              configMapKeyRef:
                key: rate_limit_fail_open
                name: gateway-intencoes-config
          - name: RATE_LIMIT_TENANT_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_tenant_overrides
                name: gateway-intencoes-config
          - name: RATE_LIMIT_USER_OVERRIDES
            valueFrom:
              configMapKeyRef:
                key: rate_limit_user_overrides
                name: gateway-intencoes-config
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: allow_insecure_http_endpoints
                name: gateway-intencoes-config
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: ghcr.io/albinojimy/neural-hive-mind/gateway-intencoes:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 25
            successThreshold: 1
            timeoutSeconds: 8
          name: gateway-intencoes
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 40
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/models
            name: model-cache
          - mountPath: /etc/ssl/certs/schema-registry-ca.crt
            name: schema-registry-ca-cert
            readOnly: true
            subPath: ca.crt
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        nodeSelector:
          kubernetes.io/hostname: vmi2911680
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: gateway-intencoes
        serviceAccountName: gateway-intencoes
        terminationGracePeriodSeconds: 30
        volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gateway-intencoes-models-pvc
        - name: schema-registry-ca-cert
          secret:
            defaultMode: 420
            items:
            - key: ca.crt
              path: ca.crt
            secretName: schema-registry-tls-secret
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "63"
      kubernetes.io/change-cause: kubectl set image deployment/guard-agents guard-agents=ghcr.io/albinojimy/neural-hive-mind/guard-agents:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: guard-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:13:04Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: guard-agents
      app.kubernetes.io/name: guard-agents
      pod-template-hash: 5699d44cd6
    name: guard-agents-5699d44cd6
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: guard-agents
      uid: 49650100-d08b-42a9-abba-338560021c7f
    resourceVersion: "29904385"
    uid: 24ff57e0-3d9c-4cb3-aedf-8dc697784b19
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: guard-agents
        app.kubernetes.io/name: guard-agents
        pod-template-hash: 5699d44cd6
    template:
      metadata:
        annotations:
          checksum/config: 39da66c3b643acffb0000cabbc3dbe7e2337c017c9fb87f3c40f93e8d1072470
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:20+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: guard-agents
          app.kubernetes.io/name: guard-agents
          pod-template-hash: 5699d44cd6
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - guard-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          envFrom:
          - configMapRef:
              name: guard-agents-config
          image: ghcr.io/albinojimy/neural-hive-mind/guard-agents:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: guard-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/readiness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health/startup
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: guard-agents
        serviceAccountName: guard-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: guard-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "64"
      kubernetes.io/change-cause: kubectl set image deployment/guard-agents guard-agents=ghcr.io/albinojimy/neural-hive-mind/guard-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: guard-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:48:24Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: guard-agents
      app.kubernetes.io/name: guard-agents
      pod-template-hash: 79d869d666
    name: guard-agents-79d869d666
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: guard-agents
      uid: 49650100-d08b-42a9-abba-338560021c7f
    resourceVersion: "29904806"
    uid: 9b894339-3522-40c8-a407-b2db3d905b35
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: guard-agents
        app.kubernetes.io/name: guard-agents
        pod-template-hash: 79d869d666
    template:
      metadata:
        annotations:
          checksum/config: 39da66c3b643acffb0000cabbc3dbe7e2337c017c9fb87f3c40f93e8d1072470
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:20+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: guard-agents
          app.kubernetes.io/name: guard-agents
          pod-template-hash: 79d869d666
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - guard-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          envFrom:
          - configMapRef:
              name: guard-agents-config
          image: ghcr.io/albinojimy/neural-hive-mind/guard-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: guard-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/readiness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health/startup
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: guard-agents
        serviceAccountName: guard-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: guard-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "65"
      kubernetes.io/change-cause: kubectl set image deployment/guard-agents guard-agents=ghcr.io/albinojimy/neural-hive-mind/guard-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: guard-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:07Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: guard-agents
      app.kubernetes.io/name: guard-agents
      pod-template-hash: 7d66b9cfdf
    name: guard-agents-7d66b9cfdf
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: guard-agents
      uid: 49650100-d08b-42a9-abba-338560021c7f
    resourceVersion: "29939563"
    uid: 8f6a54ec-6c44-44ff-9584-72836fd60332
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: guard-agents
        app.kubernetes.io/name: guard-agents
        pod-template-hash: 7d66b9cfdf
    template:
      metadata:
        annotations:
          checksum/config: 39da66c3b643acffb0000cabbc3dbe7e2337c017c9fb87f3c40f93e8d1072470
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:20+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: guard-agents
          app.kubernetes.io/name: guard-agents
          pod-template-hash: 7d66b9cfdf
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - guard-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          envFrom:
          - configMapRef:
              name: guard-agents-config
          image: ghcr.io/albinojimy/neural-hive-mind/guard-agents:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: guard-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/readiness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health/startup
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: guard-agents
        serviceAccountName: guard-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: guard-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "62"
      kubernetes.io/change-cause: kubectl set image deployment/guard-agents guard-agents=ghcr.io/albinojimy/neural-hive-mind/guard-agents:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: guard-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:55:40Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: guard-agents
      app.kubernetes.io/name: guard-agents
      pod-template-hash: 857b56b97
    name: guard-agents-857b56b97
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: guard-agents
      uid: 49650100-d08b-42a9-abba-338560021c7f
    resourceVersion: "29545648"
    uid: 95402b36-b507-4975-a5f8-536270a0e0f6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: guard-agents
        app.kubernetes.io/name: guard-agents
        pod-template-hash: 857b56b97
    template:
      metadata:
        annotations:
          checksum/config: 39da66c3b643acffb0000cabbc3dbe7e2337c017c9fb87f3c40f93e8d1072470
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:20+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: guard-agents
          app.kubernetes.io/name: guard-agents
          pod-template-hash: 857b56b97
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - guard-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          envFrom:
          - configMapRef:
              name: guard-agents-config
          image: ghcr.io/albinojimy/neural-hive-mind/guard-agents:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: guard-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/readiness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health/startup
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: guard-agents
        serviceAccountName: guard-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: guard-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "12"
      kubernetes.io/change-cause: kubectl set image deployment/memory-layer-api memory-layer-api=ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:48:28Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: memory-layer-api
      app.kubernetes.io/name: memory-layer-api
      pod-template-hash: 7565cd66d7
    name: memory-layer-api-7565cd66d7
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: memory-layer-api
      uid: 0dd57997-ddf1-4813-bcb3-8cc17eb101b7
    resourceVersion: "29545696"
    uid: 56514daf-c58b-498d-bf32-b8d359e47c6f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: memory-layer-api
        app.kubernetes.io/name: memory-layer-api
        pod-template-hash: 7565cd66d7
    template:
      metadata:
        annotations:
          checksum/config: 1a18eae495b8c1273517645f9937d85d2194a61a4be3f628fa7c2f92a1b83ea8
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
          kubectl.kubernetes.io/restartedAt: "2026-02-12T08:50:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: memory-layer-api
          app.kubernetes.io/name: memory-layer-api
          pod-template-hash: 7565cd66d7
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - memory-layer-api
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          - name: DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: memory-layer-api-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: memory-layer-api-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis_password
                name: memory-layer-api-secrets
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: memory-layer-api-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: memory-layer-api-config
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: memory-layer-api-secrets
          - name: MONGODB_DATABASE
            valueFrom:
              configMapKeyRef:
                key: mongodb_database
                name: memory-layer-api-config
          - name: MONGODB_RETENTION_DAYS
            valueFrom:
              configMapKeyRef:
                key: mongodb_retention_days
                name: memory-layer-api-config
          - name: NEO4J_URI
            valueFrom:
              configMapKeyRef:
                key: neo4j_uri
                name: memory-layer-api-config
          - name: NEO4J_USER
            valueFrom:
              configMapKeyRef:
                key: neo4j_user
                name: memory-layer-api-config
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: memory-layer-api-secrets
          - name: NEO4J_DATABASE
            valueFrom:
              configMapKeyRef:
                key: neo4j_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_RETENTION_MONTHS
            valueFrom:
              configMapKeyRef:
                key: clickhouse_retention_months
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: PROMETHEUS_PORT
            valueFrom:
              configMapKeyRef:
                key: prometheus_port
                name: memory-layer-api-config
          - name: ENABLE_CACHE
            valueFrom:
              configMapKeyRef:
                key: enable_cache
                name: memory-layer-api-config
          - name: ENABLE_LINEAGE_TRACKING
            valueFrom:
              configMapKeyRef:
                key: enable_lineage_tracking
                name: memory-layer-api-config
          - name: ENABLE_QUALITY_MONITORING
            valueFrom:
              configMapKeyRef:
                key: enable_quality_monitoring
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            valueFrom:
              configMapKeyRef:
                key: enable_realtime_sync
                name: memory-layer-api-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_DLQ_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_dlq_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: memory-layer-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: memory-layer-api
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "11"
      kubernetes.io/change-cause: kubectl set image deployment/memory-layer-api memory-layer-api=ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:13:15Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: memory-layer-api
      app.kubernetes.io/name: memory-layer-api
      pod-template-hash: 757d4df6d7
    name: memory-layer-api-757d4df6d7
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: memory-layer-api
      uid: 0dd57997-ddf1-4813-bcb3-8cc17eb101b7
    resourceVersion: "29535479"
    uid: e8397341-2ea2-4ead-bebc-94213e47c9d5
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: memory-layer-api
        app.kubernetes.io/name: memory-layer-api
        pod-template-hash: 757d4df6d7
    template:
      metadata:
        annotations:
          checksum/config: 1a18eae495b8c1273517645f9937d85d2194a61a4be3f628fa7c2f92a1b83ea8
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
          kubectl.kubernetes.io/restartedAt: "2026-02-12T08:50:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: memory-layer-api
          app.kubernetes.io/name: memory-layer-api
          pod-template-hash: 757d4df6d7
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - memory-layer-api
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          - name: DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: memory-layer-api-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: memory-layer-api-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis_password
                name: memory-layer-api-secrets
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: memory-layer-api-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: memory-layer-api-config
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: memory-layer-api-secrets
          - name: MONGODB_DATABASE
            valueFrom:
              configMapKeyRef:
                key: mongodb_database
                name: memory-layer-api-config
          - name: MONGODB_RETENTION_DAYS
            valueFrom:
              configMapKeyRef:
                key: mongodb_retention_days
                name: memory-layer-api-config
          - name: NEO4J_URI
            valueFrom:
              configMapKeyRef:
                key: neo4j_uri
                name: memory-layer-api-config
          - name: NEO4J_USER
            valueFrom:
              configMapKeyRef:
                key: neo4j_user
                name: memory-layer-api-config
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: memory-layer-api-secrets
          - name: NEO4J_DATABASE
            valueFrom:
              configMapKeyRef:
                key: neo4j_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_RETENTION_MONTHS
            valueFrom:
              configMapKeyRef:
                key: clickhouse_retention_months
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: PROMETHEUS_PORT
            valueFrom:
              configMapKeyRef:
                key: prometheus_port
                name: memory-layer-api-config
          - name: ENABLE_CACHE
            valueFrom:
              configMapKeyRef:
                key: enable_cache
                name: memory-layer-api-config
          - name: ENABLE_LINEAGE_TRACKING
            valueFrom:
              configMapKeyRef:
                key: enable_lineage_tracking
                name: memory-layer-api-config
          - name: ENABLE_QUALITY_MONITORING
            valueFrom:
              configMapKeyRef:
                key: enable_quality_monitoring
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            valueFrom:
              configMapKeyRef:
                key: enable_realtime_sync
                name: memory-layer-api-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_DLQ_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_dlq_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: memory-layer-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: memory-layer-api
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "10"
      kubernetes.io/change-cause: kubectl set image deployment/memory-layer-api memory-layer-api=ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:55:48Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: memory-layer-api
      app.kubernetes.io/name: memory-layer-api
      pod-template-hash: 7974bf979d
    name: memory-layer-api-7974bf979d
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: memory-layer-api
      uid: 0dd57997-ddf1-4813-bcb3-8cc17eb101b7
    resourceVersion: "29529934"
    uid: 868ce2c5-c763-4666-a8f4-8eb09a580a89
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: memory-layer-api
        app.kubernetes.io/name: memory-layer-api
        pod-template-hash: 7974bf979d
    template:
      metadata:
        annotations:
          checksum/config: 1a18eae495b8c1273517645f9937d85d2194a61a4be3f628fa7c2f92a1b83ea8
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
          kubectl.kubernetes.io/restartedAt: "2026-02-12T08:50:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: memory-layer-api
          app.kubernetes.io/name: memory-layer-api
          pod-template-hash: 7974bf979d
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - memory-layer-api
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          - name: DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: memory-layer-api-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: memory-layer-api-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis_password
                name: memory-layer-api-secrets
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: memory-layer-api-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: memory-layer-api-config
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: memory-layer-api-secrets
          - name: MONGODB_DATABASE
            valueFrom:
              configMapKeyRef:
                key: mongodb_database
                name: memory-layer-api-config
          - name: MONGODB_RETENTION_DAYS
            valueFrom:
              configMapKeyRef:
                key: mongodb_retention_days
                name: memory-layer-api-config
          - name: NEO4J_URI
            valueFrom:
              configMapKeyRef:
                key: neo4j_uri
                name: memory-layer-api-config
          - name: NEO4J_USER
            valueFrom:
              configMapKeyRef:
                key: neo4j_user
                name: memory-layer-api-config
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: memory-layer-api-secrets
          - name: NEO4J_DATABASE
            valueFrom:
              configMapKeyRef:
                key: neo4j_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_RETENTION_MONTHS
            valueFrom:
              configMapKeyRef:
                key: clickhouse_retention_months
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: PROMETHEUS_PORT
            valueFrom:
              configMapKeyRef:
                key: prometheus_port
                name: memory-layer-api-config
          - name: ENABLE_CACHE
            valueFrom:
              configMapKeyRef:
                key: enable_cache
                name: memory-layer-api-config
          - name: ENABLE_LINEAGE_TRACKING
            valueFrom:
              configMapKeyRef:
                key: enable_lineage_tracking
                name: memory-layer-api-config
          - name: ENABLE_QUALITY_MONITORING
            valueFrom:
              configMapKeyRef:
                key: enable_quality_monitoring
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            valueFrom:
              configMapKeyRef:
                key: enable_realtime_sync
                name: memory-layer-api-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_DLQ_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_dlq_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: memory-layer-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: memory-layer-api
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "13"
      kubernetes.io/change-cause: kubectl set image deployment/memory-layer-api memory-layer-api=ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:43Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: memory-layer-api
      app.kubernetes.io/name: memory-layer-api
      pod-template-hash: b6cdc88d6
    name: memory-layer-api-b6cdc88d6
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: memory-layer-api
      uid: 0dd57997-ddf1-4813-bcb3-8cc17eb101b7
    resourceVersion: "29904849"
    uid: 6bca5e23-78f5-4836-88d8-44630cb1f3ca
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: memory-layer-api
        app.kubernetes.io/name: memory-layer-api
        pod-template-hash: b6cdc88d6
    template:
      metadata:
        annotations:
          checksum/config: 1a18eae495b8c1273517645f9937d85d2194a61a4be3f628fa7c2f92a1b83ea8
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
          kubectl.kubernetes.io/restartedAt: "2026-02-12T08:50:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: memory-layer-api
          app.kubernetes.io/name: memory-layer-api
          pod-template-hash: b6cdc88d6
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - memory-layer-api
              topologyKey: topology.kubernetes.io/zone
        containers:
        - env:
          - name: ENVIRONMENT
            valueFrom:
              configMapKeyRef:
                key: environment
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          - name: DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: memory-layer-api-config
          - name: REDIS_CLUSTER_NODES
            valueFrom:
              configMapKeyRef:
                key: redis_cluster_nodes
                name: memory-layer-api-config
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis_password
                name: memory-layer-api-secrets
          - name: REDIS_SSL_ENABLED
            valueFrom:
              configMapKeyRef:
                key: redis_ssl_enabled
                name: memory-layer-api-config
          - name: REDIS_DEFAULT_TTL
            valueFrom:
              configMapKeyRef:
                key: redis_default_ttl
                name: memory-layer-api-config
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: memory-layer-api-secrets
          - name: MONGODB_DATABASE
            valueFrom:
              configMapKeyRef:
                key: mongodb_database
                name: memory-layer-api-config
          - name: MONGODB_RETENTION_DAYS
            valueFrom:
              configMapKeyRef:
                key: mongodb_retention_days
                name: memory-layer-api-config
          - name: NEO4J_URI
            valueFrom:
              configMapKeyRef:
                key: neo4j_uri
                name: memory-layer-api-config
          - name: NEO4J_USER
            valueFrom:
              configMapKeyRef:
                key: neo4j_user
                name: memory-layer-api-config
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: memory-layer-api-secrets
          - name: NEO4J_DATABASE
            valueFrom:
              configMapKeyRef:
                key: neo4j_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: CLICKHOUSE_RETENTION_MONTHS
            valueFrom:
              configMapKeyRef:
                key: clickhouse_retention_months
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: PROMETHEUS_PORT
            valueFrom:
              configMapKeyRef:
                key: prometheus_port
                name: memory-layer-api-config
          - name: ENABLE_CACHE
            valueFrom:
              configMapKeyRef:
                key: enable_cache
                name: memory-layer-api-config
          - name: ENABLE_LINEAGE_TRACKING
            valueFrom:
              configMapKeyRef:
                key: enable_lineage_tracking
                name: memory-layer-api-config
          - name: ENABLE_QUALITY_MONITORING
            valueFrom:
              configMapKeyRef:
                key: enable_quality_monitoring
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            valueFrom:
              configMapKeyRef:
                key: enable_realtime_sync
                name: memory-layer-api-config
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_DLQ_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_dlq_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: memory-layer-api
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: memory-layer-api
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T23:18:03Z"
    generation: 3
    labels:
      app.kubernetes.io/component: sync-consumer
      app.kubernetes.io/name: memory-layer-api-sync-consumer
      neural-hive.io/component: memory-sync-consumer
      pod-template-hash: 575ffc9794
    name: memory-layer-api-sync-consumer-575ffc9794
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: memory-layer-api-sync-consumer
      uid: 59aaf4d4-0d1e-4e40-8c1e-f80b82eabc6e
    resourceVersion: "29313290"
    uid: 50f1adbd-1e4c-4d63-8ba5-d3c0d5a8c7c8
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/name: memory-layer-api-sync-consumer
        pod-template-hash: 575ffc9794
    template:
      metadata:
        annotations:
          checksum/config: 799a2fc41ec81f1efe53dbe048a4bb12ab6d60a2190d7074055896feb7088410
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: sync-consumer
          app.kubernetes.io/name: memory-layer-api-sync-consumer
          neural-hive.io/component: memory-sync-consumer
          pod-template-hash: 575ffc9794
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - memory-layer-api-sync-consumer
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - command:
          - python
          - -m
          - src.consumers.sync_event_consumer
          env:
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            value: "true"
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:1.2.8
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - python
              - -c
              - import sys; sys.exit(0)
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sync-consumer
          readinessProbe:
            exec:
              command:
              - python
              - -c
              - import sys; sys.exit(0)
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T23:30:28Z"
    generation: 7
    labels:
      app.kubernetes.io/component: sync-consumer
      app.kubernetes.io/name: memory-layer-api-sync-consumer
      neural-hive.io/component: memory-sync-consumer
      pod-template-hash: 599669c84b
    name: memory-layer-api-sync-consumer-599669c84b
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: memory-layer-api-sync-consumer
      uid: 59aaf4d4-0d1e-4e40-8c1e-f80b82eabc6e
    resourceVersion: "29331309"
    uid: b6fb5307-7f40-40ea-8512-e88609c93c57
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/name: memory-layer-api-sync-consumer
        pod-template-hash: 599669c84b
    template:
      metadata:
        annotations:
          checksum/config: 1a18eae495b8c1273517645f9937d85d2194a61a4be3f628fa7c2f92a1b83ea8
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
          kubectl.kubernetes.io/restartedAt: "2026-02-12T00:19:22+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: sync-consumer
          app.kubernetes.io/name: memory-layer-api-sync-consumer
          neural-hive.io/component: memory-sync-consumer
          pod-template-hash: 599669c84b
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - memory-layer-api-sync-consumer
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - command:
          - python
          - -m
          - src.consumers.sync_event_consumer
          env:
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            value: "true"
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:7f171c7
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - python
              - -c
              - import sys; sys.exit(0)
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sync-consumer
          readinessProbe:
            exec:
              command:
              - python
              - -c
              - import sys; sys.exit(0)
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 7
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: memory-layer-api
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T23:19:23Z"
    generation: 2
    labels:
      app.kubernetes.io/component: sync-consumer
      app.kubernetes.io/name: memory-layer-api-sync-consumer
      neural-hive.io/component: memory-sync-consumer
      pod-template-hash: b8b9b888c
    name: memory-layer-api-sync-consumer-b8b9b888c
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: memory-layer-api-sync-consumer
      uid: 59aaf4d4-0d1e-4e40-8c1e-f80b82eabc6e
    resourceVersion: "29313297"
    uid: 9cc1f5c9-f3c2-4687-8cd8-bb731a9b7c24
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/name: memory-layer-api-sync-consumer
        pod-template-hash: b8b9b888c
    template:
      metadata:
        annotations:
          checksum/config: 799a2fc41ec81f1efe53dbe048a4bb12ab6d60a2190d7074055896feb7088410
          checksum/secret: a6f3dc8d7f28768500f4239c54e452aeebcd183cc30d9ac4390715ddb0b35877
          kubectl.kubernetes.io/restartedAt: "2026-02-12T00:19:22+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: sync-consumer
          app.kubernetes.io/name: memory-layer-api-sync-consumer
          neural-hive.io/component: memory-sync-consumer
          pod-template-hash: b8b9b888c
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - memory-layer-api-sync-consumer
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - command:
          - python
          - -m
          - src.consumers.sync_event_consumer
          env:
          - name: KAFKA_BOOTSTRAP_SERVERS
            valueFrom:
              configMapKeyRef:
                key: kafka_bootstrap_servers
                name: memory-layer-api-config
          - name: KAFKA_SYNC_TOPIC
            valueFrom:
              configMapKeyRef:
                key: kafka_sync_topic
                name: memory-layer-api-config
          - name: KAFKA_CONSUMER_GROUP
            valueFrom:
              configMapKeyRef:
                key: kafka_consumer_group
                name: memory-layer-api-config
          - name: KAFKA_SECURITY_PROTOCOL
            valueFrom:
              configMapKeyRef:
                key: kafka_security_protocol
                name: memory-layer-api-config
          - name: ENABLE_REALTIME_SYNC
            value: "true"
          - name: CLICKHOUSE_HOST
            valueFrom:
              configMapKeyRef:
                key: clickhouse_host
                name: memory-layer-api-config
          - name: CLICKHOUSE_PORT
            valueFrom:
              configMapKeyRef:
                key: clickhouse_port
                name: memory-layer-api-config
          - name: CLICKHOUSE_USER
            valueFrom:
              configMapKeyRef:
                key: clickhouse_user
                name: memory-layer-api-config
          - name: CLICKHOUSE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: clickhouse_password
                name: memory-layer-api-secrets
          - name: CLICKHOUSE_DATABASE
            valueFrom:
              configMapKeyRef:
                key: clickhouse_database
                name: memory-layer-api-config
          - name: OTEL_ENDPOINT
            valueFrom:
              configMapKeyRef:
                key: otel_endpoint
                name: memory-layer-api-config
          - name: LOG_LEVEL
            valueFrom:
              configMapKeyRef:
                key: log_level
                name: memory-layer-api-config
          image: ghcr.io/albinojimy/neural-hive-mind/memory-layer-api:1.2.8
          imagePullPolicy: Always
          livenessProbe:
            exec:
              command:
              - python
              - -c
              - import sys; sys.exit(0)
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sync-consumer
          readinessProbe:
            exec:
              command:
              - python
              - -c
              - import sys; sys.exit(0)
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T20:37:10Z"
    generation: 2
    labels:
      app: opa
      component: policy-engine
      layer: orchestration
      pod-template-hash: 5bd4887759
    name: opa-5bd4887759
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: opa
      uid: 9fbb2c75-8492-4886-b64d-77a4541cce02
    resourceVersion: "29905811"
    uid: 1d8a9222-4f6b-4b91-90b8-f2df4da08b55
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: opa
        component: policy-engine
        pod-template-hash: 5bd4887759
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: opa
          component: policy-engine
          layer: orchestration
          pod-template-hash: 5bd4887759
      spec:
        containers:
        - args:
          - run
          - --server
          - --addr=0.0.0.0:8181
          - --config-file=/config/config.yaml
          - --log-level=info
          - --log-format=json
          - --bundle
          - /policies
          image: openpolicyagent/opa:0.60.0-rootless
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8181
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          name: opa
          ports:
          - containerPort: 8181
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health?bundle=true
              port: 8181
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 1000
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /policies
            name: policies
            readOnly: true
          - mountPath: /config
            name: opa-config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: opa-policies
          name: policies
        - configMap:
            defaultMode: 420
            name: orchestrator-dynamic-opa-config
          name: opa-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: optimizer-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-09T09:27:44Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 57bf5887c7
    name: optimizer-agents-57bf5887c7
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: optimizer-agents
      uid: 47c66b7c-7c5a-4e23-b2d4-c7f24762f29a
    resourceVersion: "28728308"
    uid: 2a8367c1-ebb3-405f-a2ad-9da1aa837547
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: optimizer-agents
        app.kubernetes.io/name: optimizer-agents
        pod-template-hash: 57bf5887c7
    template:
      metadata:
        annotations:
          checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
          kubectl.kubernetes.io/restartedAt: "2026-02-03T11:29:22+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: optimizer-agents
          app.kubernetes.io/name: optimizer-agents
          pod-template-hash: 57bf5887c7
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - optimizer-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: optimizer-agents
          image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:6f1ee62
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: optimizer-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: optimizer-agents
        serviceAccountName: optimizer-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: optimizer-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-09T08:06:35Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 6854bd7ccb
    name: optimizer-agents-6854bd7ccb
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: optimizer-agents
      uid: 47c66b7c-7c5a-4e23-b2d4-c7f24762f29a
    resourceVersion: "28143830"
    uid: a7fba341-9720-4b88-b3de-eefc1d8aada9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: optimizer-agents
        app.kubernetes.io/name: optimizer-agents
        pod-template-hash: 6854bd7ccb
    template:
      metadata:
        annotations:
          checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
          kubectl.kubernetes.io/restartedAt: "2026-02-03T11:29:22+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: optimizer-agents
          app.kubernetes.io/name: optimizer-agents
          pod-template-hash: 6854bd7ccb
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - optimizer-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: optimizer-agents
          image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:dac045e
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: optimizer-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: optimizer-agents
        serviceAccountName: optimizer-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "9"
      meta.helm.sh/release-name: optimizer-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:50Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 698f8d4fbd
    name: optimizer-agents-698f8d4fbd
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: optimizer-agents
      uid: 47c66b7c-7c5a-4e23-b2d4-c7f24762f29a
    resourceVersion: "29904921"
    uid: a3feaeb9-9e61-4412-9ea8-3964939fa02c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: optimizer-agents
        app.kubernetes.io/name: optimizer-agents
        pod-template-hash: 698f8d4fbd
    template:
      metadata:
        annotations:
          checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
          kubectl.kubernetes.io/restartedAt: "2026-02-13T22:06:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: optimizer-agents
          app.kubernetes.io/name: optimizer-agents
          pod-template-hash: 698f8d4fbd
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - optimizer-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: optimizer-agents
          image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: optimizer-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: optimizer-agents
        serviceAccountName: optimizer-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "8"
      deployment.kubernetes.io/revision-history: "6"
      meta.helm.sh/release-name: optimizer-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T21:58:20Z"
    generation: 5
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 7d5d5d7b5c
    name: optimizer-agents-7d5d5d7b5c
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: optimizer-agents
      uid: 47c66b7c-7c5a-4e23-b2d4-c7f24762f29a
    resourceVersion: "29330135"
    uid: 7f357bc2-6d1e-476f-ba16-1db3c853e0dc
  spec:
    replicas: 3
    selector:
      matchLabels:
        app.kubernetes.io/instance: optimizer-agents
        app.kubernetes.io/name: optimizer-agents
        pod-template-hash: 7d5d5d7b5c
    template:
      metadata:
        annotations:
          checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
          kubectl.kubernetes.io/restartedAt: "2026-02-11T22:58:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: optimizer-agents
          app.kubernetes.io/name: optimizer-agents
          pod-template-hash: 7d5d5d7b5c
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - optimizer-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: optimizer-agents
          image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: optimizer-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: optimizer-agents
        serviceAccountName: optimizer-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    fullyLabeledReplicas: 3
    observedGeneration: 5
    replicas: 3
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
      meta.helm.sh/release-name: optimizer-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-10T22:01:55Z"
    generation: 4
    labels:
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/name: optimizer-agents
      pod-template-hash: 7dcdc9b744
    name: optimizer-agents-7dcdc9b744
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: optimizer-agents
      uid: 47c66b7c-7c5a-4e23-b2d4-c7f24762f29a
    resourceVersion: "29127331"
    uid: 9e12a5b6-a4ba-4633-808c-fdc430336adf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: optimizer-agents
        app.kubernetes.io/name: optimizer-agents
        pod-template-hash: 7dcdc9b744
    template:
      metadata:
        annotations:
          checksum/config: 4c82713ee8108654fcc8bc9b6a3cc90ebc903b25c62313da78289876d9ea9e56
          kubectl.kubernetes.io/restartedAt: "2026-02-03T11:29:22+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: optimizer-agents
          app.kubernetes.io/name: optimizer-agents
          pod-template-hash: 7dcdc9b744
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - optimizer-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          envFrom:
          - configMapRef:
              name: optimizer-agents
          image: ghcr.io/albinojimy/neural-hive-mind/optimizer-agents:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: optimizer-agents
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: optimizer-agents
        serviceAccountName: optimizer-agents
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "140"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:48:43Z"
    generation: 3
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 54dcd69748
    name: orchestrator-dynamic-54dcd69748
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29896163"
    uid: 0c325c9b-75fa-424c-bc9a-ddd3a49081d9
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: 54dcd69748
    template:
      metadata:
        annotations:
          checksum/config: 55470a93f8433c149c692760ef45163f223297b83f4d1ac982c01be161e3413c
          checksum/secret: 6057a0138a7af487f36fc3cf0a850586ee5e6ee65ff787027433029e9338eb64
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: 54dcd69748
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "143"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T20:51:50Z"
    generation: 3
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: "5784454769"
    name: orchestrator-dynamic-5784454769
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29906060"
    uid: f0f9793f-cc66-40ea-ab7c-8ed08c23dd8a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: "5784454769"
    template:
      metadata:
        annotations:
          checksum/config: 1e7c444f34d431cf0f1422f54bfd59651b5347fc2838858437c1f282fc8e24a0
          checksum/secret: cec7ee8d589c1c780ec94fa19000d2ebe1c14f5a587ce9608ba95e67293eabfd
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          vault.hashicorp.com/agent-inject: "true"
          vault.hashicorp.com/agent-inject-secret-kafka: secret/data/orchestrator/kafka
          vault.hashicorp.com/agent-inject-secret-mongodb: secret/data/orchestrator/mongodb
          vault.hashicorp.com/agent-inject-secret-postgres: database/creds/temporal-orchestrator
          vault.hashicorp.com/agent-inject-template-kafka: |-
            {{`{{- with secret "secret/data/orchestrator/kafka" -}}
            export KAFKA_SASL_USERNAME="{{ .Data.data.username }}"
            export KAFKA_SASL_PASSWORD="{{ .Data.data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-mongodb: |-
            {{`{{- with secret "secret/data/orchestrator/mongodb" -}}
            export MONGODB_URI="{{ .Data.data.uri }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-postgres: |-
            {{`{{- with secret "database/creds/temporal-orchestrator" -}}
            export POSTGRES_USER="{{ .Data.username }}"
            export POSTGRES_PASSWORD="{{ .Data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/role: orchestrator-dynamic
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: "5784454769"
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          - name: REQUESTS_CA_BUNDLE
            value: /etc/ssl/certs/ca-certificates.crt
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b-20260210-150856
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /run/spire/sockets
            name: spire-agent-socket
            readOnly: true
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
          - mountPath: /etc/ssl/certs
            name: schema-ca-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - hostPath:
            path: /run/spire/sockets
            type: Directory
          name: spire-agent-socket
        - configMap:
            defaultMode: 420
            name: orchestrator-dynamic-spire-agent-config
          name: spire-agent-config
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
        - configMap:
            defaultMode: 420
            name: orchestrator-schema-ca
          name: schema-ca-certs
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "141"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T20:36:19Z"
    generation: 3
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 5db756d78c
    name: orchestrator-dynamic-5db756d78c
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29900369"
    uid: acb498f4-ee7a-419a-b0f3-97713cebcaf3
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: 5db756d78c
    template:
      metadata:
        annotations:
          checksum/config: 1e7c444f34d431cf0f1422f54bfd59651b5347fc2838858437c1f282fc8e24a0
          checksum/secret: cec7ee8d589c1c780ec94fa19000d2ebe1c14f5a587ce9608ba95e67293eabfd
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          vault.hashicorp.com/agent-inject: "true"
          vault.hashicorp.com/agent-inject-secret-kafka: secret/data/orchestrator/kafka
          vault.hashicorp.com/agent-inject-secret-mongodb: secret/data/orchestrator/mongodb
          vault.hashicorp.com/agent-inject-secret-postgres: database/creds/temporal-orchestrator
          vault.hashicorp.com/agent-inject-template-kafka: |-
            {{`{{- with secret "secret/data/orchestrator/kafka" -}}
            export KAFKA_SASL_USERNAME="{{ .Data.data.username }}"
            export KAFKA_SASL_PASSWORD="{{ .Data.data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-mongodb: |-
            {{`{{- with secret "secret/data/orchestrator/mongodb" -}}
            export MONGODB_URI="{{ .Data.data.uri }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-postgres: |-
            {{`{{- with secret "database/creds/temporal-orchestrator" -}}
            export POSTGRES_USER="{{ .Data.username }}"
            export POSTGRES_PASSWORD="{{ .Data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/role: orchestrator-dynamic
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: 5db756d78c
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b-20260210-150856
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /run/spire/sockets
            name: spire-agent-socket
            readOnly: true
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - hostPath:
            path: /run/spire/sockets
            type: Directory
          name: spire-agent-socket
        - configMap:
            defaultMode: 420
            name: orchestrator-dynamic-spire-agent-config
          name: spire-agent-config
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "132"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:7f171c7d580646267c0f81e95759c14df4df97ae
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T23:35:29Z"
    generation: 2
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 654fb4549d
    name: orchestrator-dynamic-654fb4549d
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29279089"
    uid: 48dcb02c-f912-48e5-b429-ed58f4264232
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: 654fb4549d
    template:
      metadata:
        annotations:
          checksum/config: 55470a93f8433c149c692760ef45163f223297b83f4d1ac982c01be161e3413c
          checksum/secret: 6057a0138a7af487f36fc3cf0a850586ee5e6ee65ff787027433029e9338eb64
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: 654fb4549d
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:7f171c7d580646267c0f81e95759c14df4df97ae
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "138"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:55:57Z"
    generation: 2
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 6bd55c4b9
    name: orchestrator-dynamic-6bd55c4b9
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29535649"
    uid: 7e735057-1e4e-46d2-bb6b-b487ce5d3e3c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: 6bd55c4b9
    template:
      metadata:
        annotations:
          checksum/config: 55470a93f8433c149c692760ef45163f223297b83f4d1ac982c01be161e3413c
          checksum/secret: 6057a0138a7af487f36fc3cf0a850586ee5e6ee65ff787027433029e9338eb64
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: 6bd55c4b9
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "134"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b7e799d2d2794d75593c549f3722574a2106854a
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T07:19:01Z"
    generation: 2
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 6cb48b7747
    name: orchestrator-dynamic-6cb48b7747
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29293532"
    uid: 807c797a-e143-4c4c-89ba-0afd9ae225d1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: 6cb48b7747
    template:
      metadata:
        annotations:
          checksum/config: 55470a93f8433c149c692760ef45163f223297b83f4d1ac982c01be161e3413c
          checksum/secret: 6057a0138a7af487f36fc3cf0a850586ee5e6ee65ff787027433029e9338eb64
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: 6cb48b7747
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b7e799d2d2794d75593c549f3722574a2106854a
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "137"
      deployment.kubernetes.io/revision-history: "135"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b7e799d2d2794d75593c549f3722574a2106854a
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T07:23:26Z"
    generation: 4
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 7596f58f7c
    name: orchestrator-dynamic-7596f58f7c
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29304581"
    uid: 0e6a8227-3946-41dc-ae52-aecb632e1c38
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: 7596f58f7c
    template:
      metadata:
        annotations:
          checksum/config: 55470a93f8433c149c692760ef45163f223297b83f4d1ac982c01be161e3413c
          checksum/secret: 6057a0138a7af487f36fc3cf0a850586ee5e6ee65ff787027433029e9338eb64
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: 7596f58f7c
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "139"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:13:45Z"
    generation: 3
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 7db68688d4
    name: orchestrator-dynamic-7db68688d4
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29545790"
    uid: 98c524c5-851f-4d99-aefa-a3cfef06b90f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: 7db68688d4
    template:
      metadata:
        annotations:
          checksum/config: 55470a93f8433c149c692760ef45163f223297b83f4d1ac982c01be161e3413c
          checksum/secret: 6057a0138a7af487f36fc3cf0a850586ee5e6ee65ff787027433029e9338eb64
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: 7db68688d4
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "145"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:09:45Z"
    generation: 2
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: 7fcb47fdc8
    name: orchestrator-dynamic-7fcb47fdc8
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29906424"
    uid: 2319e27d-fa14-464b-bf4e-18c9ab056927
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: 7fcb47fdc8
    template:
      metadata:
        annotations:
          checksum/config: 1e7c444f34d431cf0f1422f54bfd59651b5347fc2838858437c1f282fc8e24a0
          checksum/secret: cec7ee8d589c1c780ec94fa19000d2ebe1c14f5a587ce9608ba95e67293eabfd
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          vault.hashicorp.com/agent-inject: "true"
          vault.hashicorp.com/agent-inject-secret-kafka: secret/data/orchestrator/kafka
          vault.hashicorp.com/agent-inject-secret-mongodb: secret/data/orchestrator/mongodb
          vault.hashicorp.com/agent-inject-secret-postgres: database/creds/temporal-orchestrator
          vault.hashicorp.com/agent-inject-template-kafka: |-
            {{`{{- with secret "secret/data/orchestrator/kafka" -}}
            export KAFKA_SASL_USERNAME="{{ .Data.data.username }}"
            export KAFKA_SASL_PASSWORD="{{ .Data.data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-mongodb: |-
            {{`{{- with secret "secret/data/orchestrator/mongodb" -}}
            export MONGODB_URI="{{ .Data.data.uri }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-postgres: |-
            {{`{{- with secret "database/creds/temporal-orchestrator" -}}
            export POSTGRES_USER="{{ .Data.username }}"
            export POSTGRES_PASSWORD="{{ .Data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/role: orchestrator-dynamic
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: 7fcb47fdc8
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          - name: REQUESTS_CA_BUNDLE
            value: /etc/ssl/certs/ca-certificates.crt
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /run/spire/sockets
            name: spire-agent-socket
            readOnly: true
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - hostPath:
            path: /run/spire/sockets
            type: Directory
          name: spire-agent-socket
        - configMap:
            defaultMode: 420
            name: orchestrator-dynamic-spire-agent-config
          name: spire-agent-config
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 2
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "144"
      deployment.kubernetes.io/revision-history: "142"
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T20:51:37Z"
    generation: 5
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: b97465f49
    name: orchestrator-dynamic-b97465f49
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29906416"
    uid: 48a56771-535a-4b2e-aa09-894a7431f378
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: b97465f49
    template:
      metadata:
        annotations:
          checksum/config: 1e7c444f34d431cf0f1422f54bfd59651b5347fc2838858437c1f282fc8e24a0
          checksum/secret: cec7ee8d589c1c780ec94fa19000d2ebe1c14f5a587ce9608ba95e67293eabfd
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          vault.hashicorp.com/agent-inject: "true"
          vault.hashicorp.com/agent-inject-secret-kafka: secret/data/orchestrator/kafka
          vault.hashicorp.com/agent-inject-secret-mongodb: secret/data/orchestrator/mongodb
          vault.hashicorp.com/agent-inject-secret-postgres: database/creds/temporal-orchestrator
          vault.hashicorp.com/agent-inject-template-kafka: |-
            {{`{{- with secret "secret/data/orchestrator/kafka" -}}
            export KAFKA_SASL_USERNAME="{{ .Data.data.username }}"
            export KAFKA_SASL_PASSWORD="{{ .Data.data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-mongodb: |-
            {{`{{- with secret "secret/data/orchestrator/mongodb" -}}
            export MONGODB_URI="{{ .Data.data.uri }}"
            {{- end }}`}}
          vault.hashicorp.com/agent-inject-template-postgres: |-
            {{`{{- with secret "database/creds/temporal-orchestrator" -}}
            export POSTGRES_USER="{{ .Data.username }}"
            export POSTGRES_PASSWORD="{{ .Data.password }}"
            {{- end }}`}}
          vault.hashicorp.com/role: orchestrator-dynamic
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: b97465f49
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          - name: REQUESTS_CA_BUNDLE
            value: /etc/ssl/certs/ca-certificates.crt
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b-20260210-150856
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          startupProbe:
            failureThreshold: 30
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /run/spire/sockets
            name: spire-agent-socket
            readOnly: true
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - hostPath:
            path: /run/spire/sockets
            type: Directory
          name: spire-agent-socket
        - configMap:
            defaultMode: 420
            name: orchestrator-dynamic-spire-agent-config
          name: spire-agent-config
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "136"
      deployment.kubernetes.io/revision-history: 129,131,133
      kubernetes.io/change-cause: kubectl set image deployment/orchestrator-dynamic
        orchestrator-dynamic=ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b7e799d2d2794d75593c549f3722574a2106854a
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T10:58:02Z"
    generation: 3
    labels:
      app.kubernetes.io/component: orchestration
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/name: orchestrator-dynamic
      neural-hive.io/layer: orchestration
      pod-template-hash: bf57f8c6
    name: orchestrator-dynamic-bf57f8c6
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orchestrator-dynamic
      uid: 33a48f18-67dc-42d0-932b-df4b486e3c0e
    resourceVersion: "29295393"
    uid: 43e57a6b-1fde-4ccf-b588-892924d7545a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: orchestrator-dynamic
        app.kubernetes.io/name: orchestrator-dynamic
        pod-template-hash: bf57f8c6
    template:
      metadata:
        annotations:
          checksum/config: 55470a93f8433c149c692760ef45163f223297b83f4d1ac982c01be161e3413c
          checksum/secret: 6057a0138a7af487f36fc3cf0a850586ee5e6ee65ff787027433029e9338eb64
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:01+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestration
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/name: orchestrator-dynamic
          neural-hive.io/layer: orchestration
          pod-template-hash: bf57f8c6
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - orchestrator-dynamic
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: EXECUTION_TICKET_SERVICE_URL
            value: http://execution-ticket-service.neural-hive.svc.cluster.local:8000
          - name: SLA_MANAGEMENT_HOST
            value: sla-management-system.neural-hive.svc.cluster.local
          - name: OPA_HOST
            value: opa.neural-hive.svc.cluster.local
          - name: KAFKA_SESSION_TIMEOUT_MS
            value: "30000"
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-config
          - secretRef:
              name: orchestrator-dynamic-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: orchestrator-dynamic
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/src/workflows/orchestration_workflow.py
            name: workflow-fix
            subPath: orchestration_workflow.py
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/activities/result_consolidation.py
            name: result-consolidation-fix
            subPath: result_consolidation.py
          - mountPath: /app/src/clients/mongodb_client.py
            name: mongodb-client-fix
            subPath: mongodb_client.py
          - mountPath: /home/orchestrator/.local/lib/python3.11/site-packages/neural_hive_integration/clients/execution_ticket_client.py
            name: hotfix-volume
            subPath: execution_ticket_client.py
          - mountPath: /app/src/activities/ticket_generation.py
            name: ticket-generation-fix
            subPath: ticket_generation.py
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: orchestrator-dynamic
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - configMap:
            defaultMode: 420
            name: orchestration-workflow-fix
          name: workflow-fix
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: result-consolidation-fix
          name: result-consolidation-fix
        - configMap:
            defaultMode: 420
            name: mongodb-client-fix
          name: mongodb-client-fix
        - configMap:
            defaultMode: 420
            name: execution-ticket-client-hotfix
          name: hotfix-volume
        - configMap:
            defaultMode: 420
            name: ticket-generation-hotfix
          name: ticket-generation-fix
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "68"
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:11Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 5b96b4c956
    name: queen-agent-5b96b4c956
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queen-agent
      uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
    resourceVersion: "29904450"
    uid: 9464ccf0-a243-4300-b189-fe9c504277f7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
        pod-template-hash: 5b96b4c956
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
          pod-template-hash: 5b96b4c956
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 2
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "63"
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:b7e799d2d2794d75593c549f3722574a2106854a
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T07:22:28Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 5c47664f46
    name: queen-agent-5c47664f46
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queen-agent
      uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
    resourceVersion: "29293775"
    uid: 9f676c0a-8bf5-44fe-b43a-794a838f3664
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
        pod-template-hash: 5c47664f46
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
          pod-template-hash: 5c47664f46
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:b7e799d2d2794d75593c549f3722574a2106854a
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "67"
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:48:52Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 69f6574fb9
    name: queen-agent-69f6574fb9
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queen-agent
      uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
    resourceVersion: "29904448"
    uid: aa5b9066-be2e-49c2-93b2-bf649ea2b35d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
        pod-template-hash: 69f6574fb9
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
          pod-template-hash: 69f6574fb9
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "66"
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:13:48Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 6dd787545
    name: queen-agent-6dd787545
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queen-agent
      uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
    resourceVersion: "29545867"
    uid: 22906f91-48e2-4d25-8693-9996c3ed361a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
        pod-template-hash: 6dd787545
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
          pod-template-hash: 6dd787545
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "65"
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:56:06Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 7b5976ffbd
    name: queen-agent-7b5976ffbd
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queen-agent
      uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
    resourceVersion: "29535702"
    uid: 054d7d25-711b-4466-90ec-bd50b3ab31cc
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
        pod-template-hash: 7b5976ffbd
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
          pod-template-hash: 7b5976ffbd
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "61"
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:7f171c7d580646267c0f81e95759c14df4df97ae
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T23:40:23Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 7c66b479f
    name: queen-agent-7c66b479f
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queen-agent
      uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
    resourceVersion: "29279130"
    uid: b1f1480a-c9e2-477c-9b3c-52a695e24d9b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
        pod-template-hash: 7c66b479f
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
          pod-template-hash: 7c66b479f
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:7f171c7d580646267c0f81e95759c14df4df97ae
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "64"
      deployment.kubernetes.io/revision-history: 58,60,62
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:b7e799d2d2794d75593c549f3722574a2106854a
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-10T21:08:59Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: 844d56d769
    name: queen-agent-844d56d769
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queen-agent
      uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
    resourceVersion: "29293753"
    uid: e0dff4e4-7475-499a-ac03-1db8ab5446b4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
        pod-template-hash: 844d56d769
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
          pod-template-hash: 844d56d769
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "59"
      kubernetes.io/change-cause: kubectl set image deployment/queen-agent queen-agent=ghcr.io/albinojimy/neural-hive-mind/queen-agent:dbe8b4b1aaa6ebede4fa346e17b701479f5b916e
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: queen-agent
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T14:59:59Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: queen-agent
      app.kubernetes.io/name: queen-agent
      pod-template-hash: fd58b97fb
    name: queen-agent-fd58b97fb
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queen-agent
      uid: b2aa2beb-3f0c-4290-97c6-4deb76975bea
    resourceVersion: "29108490"
    uid: 2a648d33-1fd1-40e9-925a-26c158d27269
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: queen-agent
        app.kubernetes.io/name: queen-agent
        pod-template-hash: fd58b97fb
    template:
      metadata:
        annotations:
          checksum/config: 80cc4d681c047a020c923b16f1ba572aa3b325c8d359fbe7207c2fa3d361fc51
          checksum/secret: 46b3660d6377379811526264ca26c2e893f18904ecfc46ab3a9c44daec454cf0
          kubectl.kubernetes.io/restartedAt: "2026-02-04T18:11:57+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: queen-agent
          app.kubernetes.io/name: queen-agent
          pod-template-hash: fd58b97fb
      spec:
        containers:
        - command:
          - python
          - /app/fix/startup.py
          env:
          - name: ALLOW_INSECURE_ENDPOINTS
            value: "true"
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: queen-agent-config
          - secretRef:
              name: queen-agent-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/queen-agent:dbe8b4b1aaa6ebede4fa346e17b701479f5b916e
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: queen-agent
          ports:
          - containerPort: 50053
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 300m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/fix
            name: grpc-fix
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: queen-agent
        serviceAccountName: queen-agent
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: grpc-context-fix
          name: grpc-fix
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "23"
      deployment.kubernetes.io/revision-history: 13,15,17,19
      kubernetes.io/change-cause: kubectl set image deployment/scout-agents scout-agents=ghcr.io/albinojimy/neural-hive-mind/scout-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: scout-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T10:58:05Z"
    generation: 8
    labels:
      app.kubernetes.io/instance: scout-agents
      app.kubernetes.io/name: scout-agents
      pod-template-hash: 575db6d7b7
    name: scout-agents-575db6d7b7
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: scout-agents
      uid: b2ad9658-b88a-4626-9af2-d7e1102329de
    resourceVersion: "29551091"
    uid: c73cde10-86c1-4921-9591-1e6d635f560a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: scout-agents
        app.kubernetes.io/name: scout-agents
        pod-template-hash: 575db6d7b7
    template:
      metadata:
        annotations:
          checksum/config: dfe9128ad43e25ff61a0ea9ebb43571dd52ee8975ba58a152935f55a60d19d8a
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: scout-agents
          app.kubernetes.io/name: scout-agents
          pod-template-hash: 575db6d7b7
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: scout-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRES_PORT
            value: "5432"
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_SSL_MODE
            value: disable
          envFrom:
          - configMapRef:
              name: scout-agents
          image: ghcr.io/albinojimy/neural-hive-mind/scout-agents:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: scout-agents
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 2Gi
            requests:
              cpu: 400m
              memory: 768Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: scout-agents
        serviceAccountName: scout-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: scout-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 8
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "21"
      kubernetes.io/change-cause: kubectl set image deployment/scout-agents scout-agents=ghcr.io/albinojimy/neural-hive-mind/scout-agents:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: scout-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:13:57Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: scout-agents
      app.kubernetes.io/name: scout-agents
      pod-template-hash: 65dc6598c9
    name: scout-agents-65dc6598c9
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: scout-agents
      uid: b2ad9658-b88a-4626-9af2-d7e1102329de
    resourceVersion: "29545914"
    uid: bbec8aea-0112-4f18-a452-bfd4b5c26066
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: scout-agents
        app.kubernetes.io/name: scout-agents
        pod-template-hash: 65dc6598c9
    template:
      metadata:
        annotations:
          checksum/config: dfe9128ad43e25ff61a0ea9ebb43571dd52ee8975ba58a152935f55a60d19d8a
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: scout-agents
          app.kubernetes.io/name: scout-agents
          pod-template-hash: 65dc6598c9
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: scout-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRES_PORT
            value: "5432"
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_SSL_MODE
            value: disable
          envFrom:
          - configMapRef:
              name: scout-agents
          image: ghcr.io/albinojimy/neural-hive-mind/scout-agents:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: scout-agents
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 2Gi
            requests:
              cpu: 400m
              memory: 768Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: scout-agents
        serviceAccountName: scout-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: scout-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "22"
      kubernetes.io/change-cause: kubectl set image deployment/scout-agents scout-agents=ghcr.io/albinojimy/neural-hive-mind/scout-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: scout-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:48:55Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: scout-agents
      app.kubernetes.io/name: scout-agents
      pod-template-hash: 6c5bf8b97f
    name: scout-agents-6c5bf8b97f
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: scout-agents
      uid: b2ad9658-b88a-4626-9af2-d7e1102329de
    resourceVersion: "29551102"
    uid: 152dd998-edd3-44ff-9537-2505f6958e31
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: scout-agents
        app.kubernetes.io/name: scout-agents
        pod-template-hash: 6c5bf8b97f
    template:
      metadata:
        annotations:
          checksum/config: dfe9128ad43e25ff61a0ea9ebb43571dd52ee8975ba58a152935f55a60d19d8a
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: scout-agents
          app.kubernetes.io/name: scout-agents
          pod-template-hash: 6c5bf8b97f
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: scout-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRES_PORT
            value: "5432"
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_SSL_MODE
            value: disable
          envFrom:
          - configMapRef:
              name: scout-agents
          image: ghcr.io/albinojimy/neural-hive-mind/scout-agents:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: scout-agents
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 2Gi
            requests:
              cpu: 400m
              memory: 768Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: scout-agents
        serviceAccountName: scout-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: scout-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "20"
      kubernetes.io/change-cause: kubectl set image deployment/scout-agents scout-agents=ghcr.io/albinojimy/neural-hive-mind/scout-agents:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: scout-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:56:15Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: scout-agents
      app.kubernetes.io/name: scout-agents
      pod-template-hash: "96876646"
    name: scout-agents-96876646
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: scout-agents
      uid: b2ad9658-b88a-4626-9af2-d7e1102329de
    resourceVersion: "29535800"
    uid: 2f316b94-11a2-4dcd-a1a2-6e48ab523f40
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: scout-agents
        app.kubernetes.io/name: scout-agents
        pod-template-hash: "96876646"
    template:
      metadata:
        annotations:
          checksum/config: dfe9128ad43e25ff61a0ea9ebb43571dd52ee8975ba58a152935f55a60d19d8a
          kubectl.kubernetes.io/restartedAt: "2026-02-11T11:58:05+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: scout-agents
          app.kubernetes.io/name: scout-agents
          pod-template-hash: "96876646"
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: scout-agents
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRES_PORT
            value: "5432"
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_SSL_MODE
            value: disable
          envFrom:
          - configMapRef:
              name: scout-agents
          image: ghcr.io/albinojimy/neural-hive-mind/scout-agents:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: scout-agents
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 2Gi
            requests:
              cpu: 400m
              memory: 768Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        enableServiceLinks: false
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: scout-agents
        serviceAccountName: scout-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: scout-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "20"
      kubernetes.io/change-cause: kubectl set image deployment/self-healing-engine
        self-healing-engine=ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: self-healing-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:00:58Z"
    generation: 2
    labels:
      app.kubernetes.io/component: self-healing
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/name: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
      pod-template-hash: 55d967687b
    name: self-healing-engine-55d967687b
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: self-healing-engine
      uid: eb06ab5b-6b26-4240-8ab6-7b871959d7ce
    resourceVersion: "29537073"
    uid: cd04b361-5f15-4ce9-9729-42b01089276c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: self-healing-engine
        app.kubernetes.io/name: self-healing-engine
        pod-template-hash: 55d967687b
    template:
      metadata:
        annotations:
          checksum/config: 797d649f766885eaa7c8926451997da40a3aeab5dc4d0eb324951047fa246914
          checksum/secret: 247768ac8899c81bf336763f0488784ed9da5327a122af8ff4d9a0640f59b8fc
          kubectl.kubernetes.io/restartedAt: "2026-01-28T00:05:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: self-healing
          app.kubernetes.io/instance: self-healing-engine
          app.kubernetes.io/name: self-healing-engine
          neural-hive.io/domain: remediation
          neural-hive.io/layer: resilience
          pod-template-hash: 55d967687b
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - self-healing-engine
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: self-healing-engine-config
          - secretRef:
              name: self-healing-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: self-healing-engine
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 1536Mi
            requests:
              cpu: 300m
              memory: 512Mi
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/playbooks
            name: playbooks
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: self-healing-engine
        serviceAccountName: self-healing-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: self-healing-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: self-healing-engine-playbooks
            optional: true
          name: playbooks
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "23"
      kubernetes.io/change-cause: kubectl set image deployment/self-healing-engine
        self-healing-engine=ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: self-healing-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:17Z"
    generation: 3
    labels:
      app.kubernetes.io/component: self-healing
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/name: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
      pod-template-hash: 68b6f6f4cb
    name: self-healing-engine-68b6f6f4cb
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: self-healing-engine
      uid: eb06ab5b-6b26-4240-8ab6-7b871959d7ce
    resourceVersion: "29939324"
    uid: 67926721-df7f-4e20-a68b-dbe9bc762d9e
  spec:
    replicas: 2
    selector:
      matchLabels:
        app.kubernetes.io/instance: self-healing-engine
        app.kubernetes.io/name: self-healing-engine
        pod-template-hash: 68b6f6f4cb
    template:
      metadata:
        annotations:
          checksum/config: 797d649f766885eaa7c8926451997da40a3aeab5dc4d0eb324951047fa246914
          checksum/secret: 247768ac8899c81bf336763f0488784ed9da5327a122af8ff4d9a0640f59b8fc
          kubectl.kubernetes.io/restartedAt: "2026-01-28T00:05:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: self-healing
          app.kubernetes.io/instance: self-healing-engine
          app.kubernetes.io/name: self-healing-engine
          neural-hive.io/domain: remediation
          neural-hive.io/layer: resilience
          pod-template-hash: 68b6f6f4cb
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - self-healing-engine
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: self-healing-engine-config
          - secretRef:
              name: self-healing-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: self-healing-engine
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 1536Mi
            requests:
              cpu: 300m
              memory: 512Mi
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/playbooks
            name: playbooks
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: self-healing-engine
        serviceAccountName: self-healing-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: self-healing-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: self-healing-engine-playbooks
            optional: true
          name: playbooks
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 2
    observedGeneration: 3
    readyReplicas: 1
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "22"
      kubernetes.io/change-cause: kubectl set image deployment/self-healing-engine
        self-healing-engine=ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: self-healing-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:53:37Z"
    generation: 3
    labels:
      app.kubernetes.io/component: self-healing
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/name: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
      pod-template-hash: 68f8c556c7
    name: self-healing-engine-68f8c556c7
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: self-healing-engine
      uid: eb06ab5b-6b26-4240-8ab6-7b871959d7ce
    resourceVersion: "29904529"
    uid: 60d55671-407f-48e1-8b99-4832997a48a3
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: self-healing-engine
        app.kubernetes.io/name: self-healing-engine
        pod-template-hash: 68f8c556c7
    template:
      metadata:
        annotations:
          checksum/config: 797d649f766885eaa7c8926451997da40a3aeab5dc4d0eb324951047fa246914
          checksum/secret: 247768ac8899c81bf336763f0488784ed9da5327a122af8ff4d9a0640f59b8fc
          kubectl.kubernetes.io/restartedAt: "2026-01-28T00:05:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: self-healing
          app.kubernetes.io/instance: self-healing-engine
          app.kubernetes.io/name: self-healing-engine
          neural-hive.io/domain: remediation
          neural-hive.io/layer: resilience
          pod-template-hash: 68f8c556c7
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - self-healing-engine
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: self-healing-engine-config
          - secretRef:
              name: self-healing-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: self-healing-engine
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 1536Mi
            requests:
              cpu: 300m
              memory: 512Mi
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/playbooks
            name: playbooks
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: self-healing-engine
        serviceAccountName: self-healing-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: self-healing-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: self-healing-engine-playbooks
            optional: true
          name: playbooks
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "21"
      kubernetes.io/change-cause: kubectl set image deployment/self-healing-engine
        self-healing-engine=ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: self-healing-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:18:18Z"
    generation: 3
    labels:
      app.kubernetes.io/component: self-healing
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/name: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
      pod-template-hash: 7c8c648cb5
    name: self-healing-engine-7c8c648cb5
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: self-healing-engine
      uid: eb06ab5b-6b26-4240-8ab6-7b871959d7ce
    resourceVersion: "29547316"
    uid: 5f17a96c-a1cb-4bc7-b56a-546543341b3f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: self-healing-engine
        app.kubernetes.io/name: self-healing-engine
        pod-template-hash: 7c8c648cb5
    template:
      metadata:
        annotations:
          checksum/config: 797d649f766885eaa7c8926451997da40a3aeab5dc4d0eb324951047fa246914
          checksum/secret: 247768ac8899c81bf336763f0488784ed9da5327a122af8ff4d9a0640f59b8fc
          kubectl.kubernetes.io/restartedAt: "2026-01-28T00:05:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: self-healing
          app.kubernetes.io/instance: self-healing-engine
          app.kubernetes.io/name: self-healing-engine
          neural-hive.io/domain: remediation
          neural-hive.io/layer: resilience
          pod-template-hash: 7c8c648cb5
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - self-healing-engine
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - envFrom:
          - configMapRef:
              name: self-healing-engine-config
          - secretRef:
              name: self-healing-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/self-healing-engine:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: self-healing-engine
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health/ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 1536Mi
            requests:
              cpu: 300m
              memory: 512Mi
          startupProbe:
            failureThreshold: 18
            httpGet:
              path: /health/live
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/playbooks
            name: playbooks
            readOnly: true
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: self-healing-engine
        serviceAccountName: self-healing-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: self-healing-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: self-healing-engine-playbooks
            optional: true
          name: playbooks
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "83"
      meta.helm.sh/release-name: semantic-translation-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T20:04:00Z"
    generation: 4
    labels:
      app.kubernetes.io/component: semantic-translator
      app.kubernetes.io/instance: semantic-translation-engine
      app.kubernetes.io/name: semantic-translation-engine
      neural-hive.io/domain: plan-generation
      neural-hive.io/layer: cognitiva
      pod-template-hash: 75b6ffc84b
    name: semantic-translation-engine-75b6ffc84b
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: semantic-translation-engine
      uid: 69c48d4d-9204-4fcd-b298-9414bdf44afb
    resourceVersion: "28311500"
    uid: 26b30320-f848-4add-bdf8-0d35ce38dee5
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: semantic-translation-engine
        app.kubernetes.io/name: semantic-translation-engine
        pod-template-hash: 75b6ffc84b
    template:
      metadata:
        annotations:
          checksum/config: d784f022b946275705ff626e2ecc3df1484c46a832a48ba6ca8e6c1c463a1690
          checksum/secret: d7c7026787afcb0bf2ac9218b116581a2ba5999c5f97250c405b4c7e9b6dec21
          kubectl.kubernetes.io/restartedAt: "2026-02-01T11:13:20+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: semantic-translator
          app.kubernetes.io/instance: semantic-translation-engine
          app.kubernetes.io/name: semantic-translation-engine
          neural-hive.io/domain: plan-generation
          neural-hive.io/layer: cognitiva
          pod-template-hash: 75b6ffc84b
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: semantic-translation-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: LOG_LEVEL
            value: DEBUG
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: semantic-translation-engine-config
          - secretRef:
              name: semantic-translation-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/semantic-translation-engine:3b419ba
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: semantic-translation-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 3Gi
            requests:
              cpu: 450m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/main.py
            name: ste-hotfix
            subPath: main.py
          - mountPath: /app/src/services/risk_scorer.py
            name: risk-scorer-hotfix
            subPath: risk_scorer.py
          - mountPath: /app/src/services/dag_generator.py
            name: intent-decomposition
            subPath: dag_generator.py
          - mountPath: /app/src/services/intent_classifier.py
            name: intent-decomposition
            subPath: intent_classifier.py
          - mountPath: /app/src/services/decomposition_templates.py
            name: intent-decomposition
            subPath: decomposition_templates.py
          - mountPath: /app/src/services/semantic_parser.py
            name: semantic-parser
            subPath: semantic_parser.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: semantic-translation-engine
        serviceAccountName: semantic-translation-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: semantic-translation-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: ste-hotfix
          name: ste-hotfix
        - configMap:
            defaultMode: 420
            name: ste-risk-scorer-hotfix
          name: risk-scorer-hotfix
        - configMap:
            defaultMode: 420
            name: ste-intent-decomposition
          name: intent-decomposition
        - configMap:
            defaultMode: 420
            name: ste-semantic-parser
          name: semantic-parser
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "86"
      meta.helm.sh/release-name: semantic-translation-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-10T21:46:10Z"
    generation: 1
    labels:
      app.kubernetes.io/component: semantic-translator
      app.kubernetes.io/instance: semantic-translation-engine
      app.kubernetes.io/name: semantic-translation-engine
      neural-hive.io/domain: plan-generation
      neural-hive.io/layer: cognitiva
      pod-template-hash: 775f4c454d
    name: semantic-translation-engine-775f4c454d
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: semantic-translation-engine
      uid: 69c48d4d-9204-4fcd-b298-9414bdf44afb
    resourceVersion: "29106886"
    uid: 5bf4837c-f822-4c01-8d3e-7bb097d27da3
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: semantic-translation-engine
        app.kubernetes.io/name: semantic-translation-engine
        pod-template-hash: 775f4c454d
    template:
      metadata:
        annotations:
          checksum/config: d784f022b946275705ff626e2ecc3df1484c46a832a48ba6ca8e6c1c463a1690
          checksum/secret: d7c7026787afcb0bf2ac9218b116581a2ba5999c5f97250c405b4c7e9b6dec21
          kubectl.kubernetes.io/restartedAt: "2026-02-01T11:13:20+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: semantic-translator
          app.kubernetes.io/instance: semantic-translation-engine
          app.kubernetes.io/name: semantic-translation-engine
          neural-hive.io/domain: plan-generation
          neural-hive.io/layer: cognitiva
          pod-template-hash: 775f4c454d
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: semantic-translation-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: LOG_LEVEL
            value: DEBUG
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: semantic-translation-engine-config
          - secretRef:
              name: semantic-translation-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/semantic-translation-engine:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: semantic-translation-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 3Gi
            requests:
              cpu: 450m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/main.py
            name: ste-hotfix
            subPath: main.py
          - mountPath: /app/src/services/risk_scorer.py
            name: risk-scorer-hotfix
            subPath: risk_scorer.py
          - mountPath: /app/src/services/dag_generator.py
            name: intent-decomposition
            subPath: dag_generator.py
          - mountPath: /app/src/services/intent_classifier.py
            name: intent-decomposition
            subPath: intent_classifier.py
          - mountPath: /app/src/services/decomposition_templates.py
            name: intent-decomposition
            subPath: decomposition_templates.py
          - mountPath: /app/src/services/semantic_parser.py
            name: semantic-parser
            subPath: semantic_parser.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: semantic-translation-engine
        serviceAccountName: semantic-translation-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: semantic-translation-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: ste-hotfix
          name: ste-hotfix
        - configMap:
            defaultMode: 420
            name: ste-risk-scorer-hotfix
          name: risk-scorer-hotfix
        - configMap:
            defaultMode: 420
            name: ste-intent-decomposition
          name: intent-decomposition
        - configMap:
            defaultMode: 420
            name: ste-semantic-parser
          name: semantic-parser
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "85"
      meta.helm.sh/release-name: semantic-translation-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-09T09:09:52Z"
    generation: 9
    labels:
      app.kubernetes.io/component: semantic-translator
      app.kubernetes.io/instance: semantic-translation-engine
      app.kubernetes.io/name: semantic-translation-engine
      neural-hive.io/domain: plan-generation
      neural-hive.io/layer: cognitiva
      pod-template-hash: 79bd789b97
    name: semantic-translation-engine-79bd789b97
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: semantic-translation-engine
      uid: 69c48d4d-9204-4fcd-b298-9414bdf44afb
    resourceVersion: "28941004"
    uid: caa5791b-58f5-4290-89e2-7e1535ac63bd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: semantic-translation-engine
        app.kubernetes.io/name: semantic-translation-engine
        pod-template-hash: 79bd789b97
    template:
      metadata:
        annotations:
          checksum/config: d784f022b946275705ff626e2ecc3df1484c46a832a48ba6ca8e6c1c463a1690
          checksum/secret: d7c7026787afcb0bf2ac9218b116581a2ba5999c5f97250c405b4c7e9b6dec21
          kubectl.kubernetes.io/restartedAt: "2026-02-01T11:13:20+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: semantic-translator
          app.kubernetes.io/instance: semantic-translation-engine
          app.kubernetes.io/name: semantic-translation-engine
          neural-hive.io/domain: plan-generation
          neural-hive.io/layer: cognitiva
          pod-template-hash: 79bd789b97
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: semantic-translation-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: LOG_LEVEL
            value: DEBUG
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: semantic-translation-engine-config
          - secretRef:
              name: semantic-translation-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/semantic-translation-engine:6f1ee62
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: semantic-translation-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 3Gi
            requests:
              cpu: 450m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/main.py
            name: ste-hotfix
            subPath: main.py
          - mountPath: /app/src/services/risk_scorer.py
            name: risk-scorer-hotfix
            subPath: risk_scorer.py
          - mountPath: /app/src/services/dag_generator.py
            name: intent-decomposition
            subPath: dag_generator.py
          - mountPath: /app/src/services/intent_classifier.py
            name: intent-decomposition
            subPath: intent_classifier.py
          - mountPath: /app/src/services/decomposition_templates.py
            name: intent-decomposition
            subPath: decomposition_templates.py
          - mountPath: /app/src/services/semantic_parser.py
            name: semantic-parser
            subPath: semantic_parser.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: semantic-translation-engine
        serviceAccountName: semantic-translation-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: semantic-translation-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: ste-hotfix
          name: ste-hotfix
        - configMap:
            defaultMode: 420
            name: ste-risk-scorer-hotfix
          name: risk-scorer-hotfix
        - configMap:
            defaultMode: 420
            name: ste-intent-decomposition
          name: intent-decomposition
        - configMap:
            defaultMode: 420
            name: ste-semantic-parser
          name: semantic-parser
  status:
    observedGeneration: 9
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "84"
      meta.helm.sh/release-name: semantic-translation-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-09T07:42:58Z"
    generation: 4
    labels:
      app.kubernetes.io/component: semantic-translator
      app.kubernetes.io/instance: semantic-translation-engine
      app.kubernetes.io/name: semantic-translation-engine
      neural-hive.io/domain: plan-generation
      neural-hive.io/layer: cognitiva
      pod-template-hash: 84f9c64fcb
    name: semantic-translation-engine-84f9c64fcb
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: semantic-translation-engine
      uid: 69c48d4d-9204-4fcd-b298-9414bdf44afb
    resourceVersion: "28311972"
    uid: 87580c4b-7340-4b94-bfc9-a1914a3ed583
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: semantic-translation-engine
        app.kubernetes.io/name: semantic-translation-engine
        pod-template-hash: 84f9c64fcb
    template:
      metadata:
        annotations:
          checksum/config: d784f022b946275705ff626e2ecc3df1484c46a832a48ba6ca8e6c1c463a1690
          checksum/secret: d7c7026787afcb0bf2ac9218b116581a2ba5999c5f97250c405b4c7e9b6dec21
          kubectl.kubernetes.io/restartedAt: "2026-02-01T11:13:20+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8000"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: semantic-translator
          app.kubernetes.io/instance: semantic-translation-engine
          app.kubernetes.io/name: semantic-translation-engine
          neural-hive.io/domain: plan-generation
          neural-hive.io/layer: cognitiva
          pod-template-hash: 84f9c64fcb
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: semantic-translation-engine
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: LOG_LEVEL
            value: DEBUG
          - name: SCHEMA_REGISTRY_URL
            value: http://schema-registry.kafka.svc.cluster.local:8080/apis/ccompat/v6
          - name: ENVIRONMENT
            value: development
          envFrom:
          - configMapRef:
              name: semantic-translation-engine-config
          - secretRef:
              name: semantic-translation-engine-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/semantic-translation-engine:dac045e
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: semantic-translation-engine
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1500m
              memory: 3Gi
            requests:
              cpu: 450m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /app/logs
            name: logs
          - mountPath: /app/src/main.py
            name: ste-hotfix
            subPath: main.py
          - mountPath: /app/src/services/risk_scorer.py
            name: risk-scorer-hotfix
            subPath: risk_scorer.py
          - mountPath: /app/src/services/dag_generator.py
            name: intent-decomposition
            subPath: dag_generator.py
          - mountPath: /app/src/services/intent_classifier.py
            name: intent-decomposition
            subPath: intent_classifier.py
          - mountPath: /app/src/services/decomposition_templates.py
            name: intent-decomposition
            subPath: decomposition_templates.py
          - mountPath: /app/src/services/semantic_parser.py
            name: semantic-parser
            subPath: semantic_parser.py
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: semantic-translation-engine
        serviceAccountName: semantic-translation-engine
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: semantic-translation-engine
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
        - emptyDir: {}
          name: logs
        - configMap:
            defaultMode: 420
            name: ste-hotfix
          name: ste-hotfix
        - configMap:
            defaultMode: 420
            name: ste-risk-scorer-hotfix
          name: risk-scorer-hotfix
        - configMap:
            defaultMode: 420
            name: ste-intent-decomposition
          name: intent-decomposition
        - configMap:
            defaultMode: 420
            name: ste-semantic-parser
          name: semantic-parser
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "44"
      kubernetes.io/change-cause: kubectl set image deployment/service-registry service-registry=ghcr.io/albinojimy/neural-hive-mind/service-registry:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: service-registry
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:53:58Z"
    generation: 2
    labels:
      app.kubernetes.io/component: coordination
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/name: service-registry
      app.kubernetes.io/part-of: neural-hive-mind
      pod-template-hash: 5847d47f86
    name: service-registry-5847d47f86
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: service-registry
      uid: a32a2883-5ebb-42d4-950d-32c57bd2c7de
    resourceVersion: "29904336"
    uid: ecab5c3d-f752-4f3a-b7ec-abc33ec81a57
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: service-registry
        app.kubernetes.io/name: service-registry
        pod-template-hash: 5847d47f86
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:44:08+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordination
          app.kubernetes.io/instance: service-registry
          app.kubernetes.io/name: service-registry
          app.kubernetes.io/part-of: neural-hive-mind
          pod-template-hash: 5847d47f86
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - service-registry
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: service-registry
          - name: SERVICE_VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: development
          - name: LOG_LEVEL
            value: INFO
          - name: GRPC_PORT
            value: "50051"
          - name: METRICS_PORT
            value: "9090"
          - name: ETCD_ENDPOINTS
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: ETCD_PREFIX
            value: neural-hive:agents
          - name: ETCD_TIMEOUT_SECONDS
            value: "5"
          - name: HEALTH_CHECK_INTERVAL_SECONDS
            value: "60"
          - name: HEARTBEAT_TIMEOUT_SECONDS
            value: "120"
          - name: REDIS_CLUSTER_NODES
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: service-registry-secret
          - name: OTEL_EXPORTER_ENDPOINT
            value: http://otel-collector:4317
          image: ghcr.io/albinojimy/neural-hive-mind/service-registry:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: service-registry
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          startupProbe:
            failureThreshold: 20
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: service-registry
        serviceAccountName: service-registry
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: service-registry
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "42"
      kubernetes.io/change-cause: kubectl set image deployment/service-registry service-registry=ghcr.io/albinojimy/neural-hive-mind/service-registry:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: service-registry
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:01:13Z"
    generation: 2
    labels:
      app.kubernetes.io/component: coordination
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/name: service-registry
      app.kubernetes.io/part-of: neural-hive-mind
      pod-template-hash: 5c57547b6c
    name: service-registry-5c57547b6c
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: service-registry
      uid: a32a2883-5ebb-42d4-950d-32c57bd2c7de
    resourceVersion: "29537333"
    uid: 36a5f460-4c76-494c-94b3-1b1ebf8da630
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: service-registry
        app.kubernetes.io/name: service-registry
        pod-template-hash: 5c57547b6c
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:44:08+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordination
          app.kubernetes.io/instance: service-registry
          app.kubernetes.io/name: service-registry
          app.kubernetes.io/part-of: neural-hive-mind
          pod-template-hash: 5c57547b6c
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - service-registry
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: service-registry
          - name: SERVICE_VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: development
          - name: LOG_LEVEL
            value: INFO
          - name: GRPC_PORT
            value: "50051"
          - name: METRICS_PORT
            value: "9090"
          - name: ETCD_ENDPOINTS
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: ETCD_PREFIX
            value: neural-hive:agents
          - name: ETCD_TIMEOUT_SECONDS
            value: "5"
          - name: HEALTH_CHECK_INTERVAL_SECONDS
            value: "60"
          - name: HEARTBEAT_TIMEOUT_SECONDS
            value: "120"
          - name: REDIS_CLUSTER_NODES
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: service-registry-secret
          - name: OTEL_EXPORTER_ENDPOINT
            value: http://otel-collector:4317
          image: ghcr.io/albinojimy/neural-hive-mind/service-registry:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: service-registry
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          startupProbe:
            failureThreshold: 20
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: service-registry
        serviceAccountName: service-registry
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: service-registry
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "43"
      kubernetes.io/change-cause: kubectl set image deployment/service-registry service-registry=ghcr.io/albinojimy/neural-hive-mind/service-registry:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: service-registry
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:18:59Z"
    generation: 2
    labels:
      app.kubernetes.io/component: coordination
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/name: service-registry
      app.kubernetes.io/part-of: neural-hive-mind
      pod-template-hash: 64b5fd8ffd
    name: service-registry-64b5fd8ffd
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: service-registry
      uid: a32a2883-5ebb-42d4-950d-32c57bd2c7de
    resourceVersion: "29547471"
    uid: 309a4ce2-821c-44d5-a920-7b25118f25e6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: service-registry
        app.kubernetes.io/name: service-registry
        pod-template-hash: 64b5fd8ffd
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:44:08+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordination
          app.kubernetes.io/instance: service-registry
          app.kubernetes.io/name: service-registry
          app.kubernetes.io/part-of: neural-hive-mind
          pod-template-hash: 64b5fd8ffd
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - service-registry
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: service-registry
          - name: SERVICE_VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: development
          - name: LOG_LEVEL
            value: INFO
          - name: GRPC_PORT
            value: "50051"
          - name: METRICS_PORT
            value: "9090"
          - name: ETCD_ENDPOINTS
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: ETCD_PREFIX
            value: neural-hive:agents
          - name: ETCD_TIMEOUT_SECONDS
            value: "5"
          - name: HEALTH_CHECK_INTERVAL_SECONDS
            value: "60"
          - name: HEARTBEAT_TIMEOUT_SECONDS
            value: "120"
          - name: REDIS_CLUSTER_NODES
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: service-registry-secret
          - name: OTEL_EXPORTER_ENDPOINT
            value: http://otel-collector:4317
          image: ghcr.io/albinojimy/neural-hive-mind/service-registry:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: service-registry
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          startupProbe:
            failureThreshold: 20
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: service-registry
        serviceAccountName: service-registry
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: service-registry
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "45"
      deployment.kubernetes.io/revision-history: 35,37,39
      kubernetes.io/change-cause: kubectl set image deployment/service-registry service-registry=ghcr.io/albinojimy/neural-hive-mind/service-registry:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: service-registry
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-11T15:05:14Z"
    generation: 7
    labels:
      app.kubernetes.io/component: coordination
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/name: service-registry
      app.kubernetes.io/part-of: neural-hive-mind
      pod-template-hash: 867758cb55
    name: service-registry-867758cb55
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: service-registry
      uid: a32a2883-5ebb-42d4-950d-32c57bd2c7de
    resourceVersion: "29904462"
    uid: 26666e77-8085-41a4-b64f-63a36afa7521
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: service-registry
        app.kubernetes.io/name: service-registry
        pod-template-hash: 867758cb55
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:44:08+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: coordination
          app.kubernetes.io/instance: service-registry
          app.kubernetes.io/name: service-registry
          app.kubernetes.io/part-of: neural-hive-mind
          pod-template-hash: 867758cb55
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - service-registry
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: service-registry
          - name: SERVICE_VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: development
          - name: LOG_LEVEL
            value: INFO
          - name: GRPC_PORT
            value: "50051"
          - name: METRICS_PORT
            value: "9090"
          - name: ETCD_ENDPOINTS
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: ETCD_PREFIX
            value: neural-hive:agents
          - name: ETCD_TIMEOUT_SECONDS
            value: "5"
          - name: HEALTH_CHECK_INTERVAL_SECONDS
            value: "60"
          - name: HEARTBEAT_TIMEOUT_SECONDS
            value: "120"
          - name: REDIS_CLUSTER_NODES
            value: '["neural-hive-cache.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS_PASSWORD
            valueFrom:
              secretKeyRef:
                key: redis-password
                name: service-registry-secret
          - name: OTEL_EXPORTER_ENDPOINT
            value: http://otel-collector:4317
          image: ghcr.io/albinojimy/neural-hive-mind/service-registry:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: service-registry
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            grpc:
              port: 50051
              service: ""
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 256Mi
          startupProbe:
            failureThreshold: 20
            grpc:
              port: 50051
              service: ""
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: service-registry
        serviceAccountName: service-registry
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: service-registry
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 7
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "26"
      kubernetes.io/change-cause: kubectl set image deployment/sla-management-system
        sla-management-system=ghcr.io/albinojimy/neural-hive-mind/sla-management-system:664b9b7475942726480710379cb7ee2b3af95a9c
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:19:06Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      neural-hive.io/layer: monitoring
      pod-template-hash: 578766444f
    name: sla-management-system-578766444f
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sla-management-system
      uid: fea9d645-6081-4ddd-8c49-4ed38fd1b907
    resourceVersion: "29547540"
    uid: 84196e9a-c434-41a0-9d55-b6ea442b44ba
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system
        pod-template-hash: 578766444f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:06:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system
          app.kubernetes.io/part-of: neural-hive-mind
          neural-hive.io/layer: monitoring
          pod-template-hash: 578766444f
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - sla-management-system
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: sla-management-system
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: '["redis-cluster.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: KAFKA__BOOTSTRAP_SERVERS
            value: '["neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"]'
          - name: KAFKA__BUDGET_TOPIC
            value: sla.budgets
          - name: KAFKA__FREEZE_TOPIC
            value: sla.freeze.events
          - name: KAFKA__VIOLATIONS_TOPIC
            value: sla.violations
          - name: ALERTMANAGER__URL
            value: http://alertmanager.monitoring.svc.cluster.local:9093
          - name: ALERTMANAGER__WEBHOOK_PATH
            value: /webhooks/alertmanager
          - name: CALCULATOR__CALCULATION_INTERVAL_SECONDS
            value: "30"
          - name: CALCULATOR__ERROR_BUDGET_WINDOW_DAYS
            value: "30"
          - name: CALCULATOR__BURN_RATE_FAST_THRESHOLD
            value: "14.4"
          - name: CALCULATOR__BURN_RATE_SLOW_THRESHOLD
            value: "6"
          - name: POLICY__FREEZE_THRESHOLD_PERCENT
            value: "20"
          - name: POLICY__AUTO_UNFREEZE_ENABLED
            value: "true"
          - name: POLICY__UNFREEZE_THRESHOLD_PERCENT
            value: "50"
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:664b9b7475942726480710379cb7ee2b3af95a9c
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sla-management-system
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 300m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: sla-management-system
        serviceAccountName: sla-management-system
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "25"
      kubernetes.io/change-cause: kubectl set image deployment/sla-management-system
        sla-management-system=ghcr.io/albinojimy/neural-hive-mind/sla-management-system:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:01:20Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      neural-hive.io/layer: monitoring
      pod-template-hash: 67f67dcb5b
    name: sla-management-system-67f67dcb5b
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sla-management-system
      uid: fea9d645-6081-4ddd-8c49-4ed38fd1b907
    resourceVersion: "29537398"
    uid: 0cb0a885-d696-4970-bc1e-f8c1ff35480a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system
        pod-template-hash: 67f67dcb5b
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:06:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system
          app.kubernetes.io/part-of: neural-hive-mind
          neural-hive.io/layer: monitoring
          pod-template-hash: 67f67dcb5b
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - sla-management-system
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: sla-management-system
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: '["redis-cluster.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: KAFKA__BOOTSTRAP_SERVERS
            value: '["neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"]'
          - name: KAFKA__BUDGET_TOPIC
            value: sla.budgets
          - name: KAFKA__FREEZE_TOPIC
            value: sla.freeze.events
          - name: KAFKA__VIOLATIONS_TOPIC
            value: sla.violations
          - name: ALERTMANAGER__URL
            value: http://alertmanager.monitoring.svc.cluster.local:9093
          - name: ALERTMANAGER__WEBHOOK_PATH
            value: /webhooks/alertmanager
          - name: CALCULATOR__CALCULATION_INTERVAL_SECONDS
            value: "30"
          - name: CALCULATOR__ERROR_BUDGET_WINDOW_DAYS
            value: "30"
          - name: CALCULATOR__BURN_RATE_FAST_THRESHOLD
            value: "14.4"
          - name: CALCULATOR__BURN_RATE_SLOW_THRESHOLD
            value: "6"
          - name: POLICY__FREEZE_THRESHOLD_PERCENT
            value: "20"
          - name: POLICY__AUTO_UNFREEZE_ENABLED
            value: "true"
          - name: POLICY__UNFREEZE_THRESHOLD_PERCENT
            value: "50"
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:f532cfa6b1bc73a36902e1159b901db7cb38d1e6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sla-management-system
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 300m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: sla-management-system
        serviceAccountName: sla-management-system
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "27"
      kubernetes.io/change-cause: kubectl set image deployment/sla-management-system
        sla-management-system=ghcr.io/albinojimy/neural-hive-mind/sla-management-system:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:54:09Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      neural-hive.io/layer: monitoring
      pod-template-hash: 7b97f4556f
    name: sla-management-system-7b97f4556f
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sla-management-system
      uid: fea9d645-6081-4ddd-8c49-4ed38fd1b907
    resourceVersion: "29904601"
    uid: da5e1d3b-915e-4394-a6a9-a2c152270bb6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system
        pod-template-hash: 7b97f4556f
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:06:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system
          app.kubernetes.io/part-of: neural-hive-mind
          neural-hive.io/layer: monitoring
          pod-template-hash: 7b97f4556f
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - sla-management-system
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: sla-management-system
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: '["redis-cluster.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: KAFKA__BOOTSTRAP_SERVERS
            value: '["neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"]'
          - name: KAFKA__BUDGET_TOPIC
            value: sla.budgets
          - name: KAFKA__FREEZE_TOPIC
            value: sla.freeze.events
          - name: KAFKA__VIOLATIONS_TOPIC
            value: sla.violations
          - name: ALERTMANAGER__URL
            value: http://alertmanager.monitoring.svc.cluster.local:9093
          - name: ALERTMANAGER__WEBHOOK_PATH
            value: /webhooks/alertmanager
          - name: CALCULATOR__CALCULATION_INTERVAL_SECONDS
            value: "30"
          - name: CALCULATOR__ERROR_BUDGET_WINDOW_DAYS
            value: "30"
          - name: CALCULATOR__BURN_RATE_FAST_THRESHOLD
            value: "14.4"
          - name: CALCULATOR__BURN_RATE_SLOW_THRESHOLD
            value: "6"
          - name: POLICY__FREEZE_THRESHOLD_PERCENT
            value: "20"
          - name: POLICY__AUTO_UNFREEZE_ENABLED
            value: "true"
          - name: POLICY__UNFREEZE_THRESHOLD_PERCENT
            value: "50"
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sla-management-system
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 300m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: sla-management-system
        serviceAccountName: sla-management-system
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "28"
      kubernetes.io/change-cause: kubectl set image deployment/sla-management-system
        sla-management-system=ghcr.io/albinojimy/neural-hive-mind/sla-management-system:ec7aa3525c3fb1ab4afb9c181f822e5d6220a5c4
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:22Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      neural-hive.io/layer: monitoring
      pod-template-hash: 867c876fbd
    name: sla-management-system-867c876fbd
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sla-management-system
      uid: fea9d645-6081-4ddd-8c49-4ed38fd1b907
    resourceVersion: "29939605"
    uid: 5cb866b6-e57c-48f7-9c0d-4310aa88f487
  spec:
    replicas: 2
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system
        pod-template-hash: 867c876fbd
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-10T22:06:19+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system
          app.kubernetes.io/part-of: neural-hive-mind
          neural-hive.io/layer: monitoring
          pod-template-hash: 867c876fbd
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - sla-management-system
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: SERVICE_NAME
            value: sla-management-system
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: '["redis-cluster.redis-cluster.svc.cluster.local:6379"]'
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: KAFKA__BOOTSTRAP_SERVERS
            value: '["neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"]'
          - name: KAFKA__BUDGET_TOPIC
            value: sla.budgets
          - name: KAFKA__FREEZE_TOPIC
            value: sla.freeze.events
          - name: KAFKA__VIOLATIONS_TOPIC
            value: sla.violations
          - name: ALERTMANAGER__URL
            value: http://alertmanager.monitoring.svc.cluster.local:9093
          - name: ALERTMANAGER__WEBHOOK_PATH
            value: /webhooks/alertmanager
          - name: CALCULATOR__CALCULATION_INTERVAL_SECONDS
            value: "30"
          - name: CALCULATOR__ERROR_BUDGET_WINDOW_DAYS
            value: "30"
          - name: CALCULATOR__BURN_RATE_FAST_THRESHOLD
            value: "14.4"
          - name: CALCULATOR__BURN_RATE_SLOW_THRESHOLD
            value: "6"
          - name: POLICY__FREEZE_THRESHOLD_PERCENT
            value: "20"
          - name: POLICY__AUTO_UNFREEZE_ENABLED
            value: "true"
          - name: POLICY__UNFREEZE_THRESHOLD_PERCENT
            value: "50"
          - name: ALLOW_INSECURE_HTTP_ENDPOINTS
            value: "true"
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: sla-management-system
          ports:
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1200m
              memory: 2Gi
            requests:
              cpu: 300m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1001
        serviceAccount: sla-management-system
        serviceAccountName: sla-management-system
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 3
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "31"
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T20:18:50Z"
    generation: 1
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system-operator
      neural-hive.io/layer: monitoring
      pod-template-hash: 69ccc57584
    name: sla-management-system-operator-69ccc57584
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sla-management-system-operator
      uid: fe4e7014-a879-459e-be33-b920ed6c3119
    resourceVersion: "27982798"
    uid: 0e0d906e-cdfd-41bd-a56f-cc0220070af0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system-operator
        pod-template-hash: 69ccc57584
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-06T11:28:03+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system-operator
          neural-hive.io/layer: monitoring
          pod-template-hash: 69ccc57584
      spec:
        containers:
        - command:
          - kopf
          - run
          - /app/src/operator/main.py
          - --verbose
          env:
          - name: PYTHONPATH
            value: /app
          - name: SERVICE_NAME
            value: sla-management-system-operator
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: redis-cluster.redis-cluster.svc.cluster.local:6379
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: RECONCILIATION_INTERVAL
            value: "300"
          - name: REDIS_CLUSTER_NODES
            value: redis-cluster.neural-hive-cache.svc.cluster.local:6379
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:3b419ba
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: sla-management-system-operator
        serviceAccountName: sla-management-system-operator
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 1
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "32"
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-09T08:51:38Z"
    generation: 2
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system-operator
      neural-hive.io/layer: monitoring
      pod-template-hash: 7bb659c6cf
    name: sla-management-system-operator-7bb659c6cf
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sla-management-system-operator
      uid: fe4e7014-a879-459e-be33-b920ed6c3119
    resourceVersion: "28715693"
    uid: 77a97f16-8f39-4512-883b-dea7ff7af262
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system-operator
        pod-template-hash: 7bb659c6cf
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-06T11:28:03+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system-operator
          neural-hive.io/layer: monitoring
          pod-template-hash: 7bb659c6cf
      spec:
        containers:
        - command:
          - kopf
          - run
          - /app/src/operator/main.py
          - --verbose
          env:
          - name: PYTHONPATH
            value: /app
          - name: SERVICE_NAME
            value: sla-management-system-operator
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: redis-cluster.redis-cluster.svc.cluster.local:6379
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: RECONCILIATION_INTERVAL
            value: "300"
          - name: REDIS_CLUSTER_NODES
            value: redis-cluster.neural-hive-cache.svc.cluster.local:6379
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:6f1ee62
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: sla-management-system-operator
        serviceAccountName: sla-management-system-operator
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "33"
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-10T21:30:27Z"
    generation: 2
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/name: sla-management-system-operator
      neural-hive.io/layer: monitoring
      pod-template-hash: df556c858
    name: sla-management-system-operator-df556c858
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sla-management-system-operator
      uid: fe4e7014-a879-459e-be33-b920ed6c3119
    resourceVersion: "28728018"
    uid: 841a4c93-73ed-47d3-9b31-c00191e162b6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: sla-management-system
        app.kubernetes.io/name: sla-management-system-operator
        pod-template-hash: df556c858
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-02-06T11:28:03+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: operator
          app.kubernetes.io/instance: sla-management-system
          app.kubernetes.io/name: sla-management-system-operator
          neural-hive.io/layer: monitoring
          pod-template-hash: df556c858
      spec:
        containers:
        - command:
          - kopf
          - run
          - /app/src/operator/main.py
          - --verbose
          env:
          - name: PYTHONPATH
            value: /app
          - name: SERVICE_NAME
            value: sla-management-system-operator
          - name: VERSION
            value: 1.0.0
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: INFO
          - name: PROMETHEUS__URL
            value: http://prometheus-server.monitoring.svc.cluster.local:9090
          - name: PROMETHEUS__TIMEOUT_SECONDS
            value: "30"
          - name: PROMETHEUS__MAX_RETRIES
            value: "3"
          - name: POSTGRESQL__HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          - name: POSTGRESQL__PORT
            value: "5432"
          - name: POSTGRESQL__DATABASE
            value: sla_management
          - name: POSTGRESQL__USER
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__USER
                name: sla-management-system-secret
          - name: POSTGRESQL__PASSWORD
            valueFrom:
              secretKeyRef:
                key: POSTGRESQL__PASSWORD
                name: sla-management-system-secret
          - name: POSTGRESQL__POOL_MIN_SIZE
            value: "2"
          - name: POSTGRESQL__POOL_MAX_SIZE
            value: "10"
          - name: REDIS__CLUSTER_NODES
            value: redis-cluster.redis-cluster.svc.cluster.local:6379
          - name: REDIS__PASSWORD
            valueFrom:
              secretKeyRef:
                key: REDIS__PASSWORD
                name: sla-management-system-secret
          - name: REDIS__CACHE_TTL_SECONDS
            value: "60"
          - name: RECONCILIATION_INTERVAL
            value: "300"
          - name: REDIS_CLUSTER_NODES
            value: redis-cluster.neural-hive-cache.svc.cluster.local:6379
          image: ghcr.io/albinojimy/neural-hive-mind/sla-management-system:b4cd999
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: operator
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: sla-management-system-operator
        serviceAccountName: sla-management-system-operator
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "26"
      meta.helm.sh/release-name: specialist-architecture
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T08:46:45Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: specialist-architecture
      app.kubernetes.io/name: specialist-architecture
      neural-hive.io/component: specialist
      pod-template-hash: 75f65497dc
    name: specialist-architecture-75f65497dc
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: specialist-architecture
      uid: 45ba4e9a-2490-4fcb-b614-caddaf46fc80
    resourceVersion: "27880832"
    uid: c471f1bf-2304-435d-be8d-1d9adb46352b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-architecture
        app.kubernetes.io/name: specialist-architecture
        pod-template-hash: 75f65497dc
    template:
      metadata:
        annotations:
          checksum/config: d84b2e53cf70d2acea5239b51f3bfaf6347648f28a5e5fc1c898942f75ebff54
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:44+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-architecture
          app.kubernetes.io/name: specialist-architecture
          neural-hive.io/component: specialist
          pod-template-hash: 75f65497dc
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-architecture-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-architecture
          - name: MLFLOW_MODEL_NAME
            value: architecture-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-architecture-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-architecture-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-architecture:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-architecture
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-architecture
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "27"
      meta.helm.sh/release-name: specialist-behavior
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T08:46:47Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: specialist-behavior
      app.kubernetes.io/name: specialist-behavior
      neural-hive.io/component: specialist
      pod-template-hash: 68c57f76bd
    name: specialist-behavior-68c57f76bd
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: specialist-behavior
      uid: 8e6d9351-36c4-41c7-a22b-594b273cb422
    resourceVersion: "27880881"
    uid: 4919cbb9-9214-499b-bba9-6e7b9380361b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-behavior
        app.kubernetes.io/name: specialist-behavior
        pod-template-hash: 68c57f76bd
    template:
      metadata:
        annotations:
          checksum/config: 5a3489da92e79aaf70bd34e56dd2871d8351f15bfafe0d18a026b0e4ab74a418
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:46+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-behavior
          app.kubernetes.io/name: specialist-behavior
          neural-hive.io/component: specialist
          pod-template-hash: 68c57f76bd
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-behavior-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-behavior
          - name: MLFLOW_MODEL_NAME
            value: behavior-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-behavior-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-behavior-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-behavior:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-behavior
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-behavior
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "27"
      meta.helm.sh/release-name: specialist-business
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T08:46:49Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: specialist-business
      app.kubernetes.io/name: specialist-business
      neural-hive.io/component: specialist
      pod-template-hash: 689d656dc4
    name: specialist-business-689d656dc4
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: specialist-business
      uid: 88b587cd-9c77-472e-8e37-28936035e833
    resourceVersion: "27880794"
    uid: 66229b2a-cbc5-43b0-9f64-a09e49c80da9
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-business
        app.kubernetes.io/name: specialist-business
        pod-template-hash: 689d656dc4
    template:
      metadata:
        annotations:
          checksum/config: 8acc665d6304eaa2fcce6e7ddc07cdbee4970465ed81e2b6ce5e456151f195f3
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:48+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-business
          app.kubernetes.io/name: specialist-business
          neural-hive.io/component: specialist
          pod-template-hash: 689d656dc4
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-business-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-business
          - name: MLFLOW_MODEL_NAME
            value: business-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-business-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-business-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          - name: FEATURE_CACHE_ENABLED
            value: "true"
          - name: FEATURE_CACHE_TTL_SECONDS
            value: "3600"
          - name: ENABLE_BATCH_INFERENCE
            value: "true"
          - name: BATCH_INFERENCE_SIZE
            value: "32"
          - name: BATCH_INFERENCE_MAX_WORKERS
            value: "8"
          - name: ENABLE_GPU_ACCELERATION
            value: "false"
          - name: GPU_DEVICE
            value: auto
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-business:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-business
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-business
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "26"
      meta.helm.sh/release-name: specialist-evolution
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T08:46:51Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: specialist-evolution
      app.kubernetes.io/name: specialist-evolution
      neural-hive.io/component: specialist
      pod-template-hash: 5f6b789f48
    name: specialist-evolution-5f6b789f48
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: specialist-evolution
      uid: 34746f1d-77cc-4082-b649-11aa626847d3
    resourceVersion: "27880860"
    uid: b05836bc-6a2f-431f-b694-86c92e16e241
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-evolution
        app.kubernetes.io/name: specialist-evolution
        pod-template-hash: 5f6b789f48
    template:
      metadata:
        annotations:
          checksum/config: c6c47581f315a6fd65b5b6530d5f156139862aff0d72f692b99f9370daf76752
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:50+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-evolution
          app.kubernetes.io/name: specialist-evolution
          neural-hive.io/component: specialist
          pod-template-hash: 5f6b789f48
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-evolution-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-evolution
          - name: MLFLOW_MODEL_NAME
            value: evolution-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-evolution-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-evolution-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-evolution:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-evolution
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-evolution
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "27"
      meta.helm.sh/release-name: specialist-technical
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T08:46:53Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: specialist-technical
      app.kubernetes.io/name: specialist-technical
      neural-hive.io/component: specialist
      pod-template-hash: fcb8bc9f8
    name: specialist-technical-fcb8bc9f8
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: specialist-technical
      uid: 1e25852a-a17d-40fa-8349-1b454c4a908c
    resourceVersion: "27880779"
    uid: aa4c628a-2973-480c-ad44-9905b583c268
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: specialist-technical
        app.kubernetes.io/name: specialist-technical
        pod-template-hash: fcb8bc9f8
    template:
      metadata:
        annotations:
          checksum/config: 96e2e5b50df5f53c3c4cd828b33ca93fdb45f04dd60e799b43c2e0f2c975ac8e
          kubectl.kubernetes.io/restartedAt: "2026-02-08T09:46:51+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: specialist-technical
          app.kubernetes.io/name: specialist-technical
          neural-hive.io/component: specialist
          pod-template-hash: fcb8bc9f8
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    neural-hive.io/component: specialist
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: ENVIRONMENT
            value: production
          - name: LOG_LEVEL
            value: DEBUG
          - name: ENABLE_JWT_AUTH
            value: "false"
          - name: JWT_SECRET_KEY
            valueFrom:
              secretKeyRef:
                key: jwt_secret_key
                name: specialist-technical-secrets
          - name: ENABLE_PII_DETECTION
            value: "false"
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          - name: MLFLOW_EXPERIMENT_NAME
            value: specialist-technical
          - name: MLFLOW_MODEL_NAME
            value: technical-evaluator
          - name: MLFLOW_MODEL_STAGE
            value: Production
          - name: MONGODB_URI
            valueFrom:
              secretKeyRef:
                key: mongodb_uri
                name: specialist-technical-secrets
          - name: MONGODB_DATABASE
            value: neural_hive
          - name: NEO4J_URI
            value: bolt://neo4j.neo4j-cluster.svc.cluster.local:7687
          - name: NEO4J_USER
            value: neo4j
          - name: NEO4J_PASSWORD
            valueFrom:
              secretKeyRef:
                key: neo4j_password
                name: specialist-technical-secrets
          - name: NEO4J_DATABASE
            value: neo4j
          - name: REDIS_CLUSTER_NODES
            value: neural-hive-cache.redis-cluster.svc.cluster.local:6379
          - name: REDIS_SSL_ENABLED
            value: "false"
          - name: OTEL_EXPORTER_OTLP_ENDPOINT
            value: http://opentelemetry-collector.observability.svc.cluster.local:4317
          - name: GRPC_PORT
            value: "50051"
          - name: HTTP_PORT
            value: "8000"
          - name: PROMETHEUS_PORT
            value: "8080"
          - name: ENABLE_LEDGER
            value: "true"
          - name: LEDGER_REQUIRED
            value: "false"
          - name: LEDGER_INIT_RETRY_ATTEMPTS
            value: "5"
          - name: LEDGER_INIT_RETRY_MAX_WAIT_SECONDS
            value: "30"
          - name: MODEL_REQUIRED
            value: "true"
          - name: STARTUP_SKIP_WARMUP_ON_DEPENDENCY_FAILURE
            value: "true"
          - name: STARTUP_DEPENDENCY_CHECK_TIMEOUT_SECONDS
            value: "10"
          - name: ENABLE_TRACING
            value: "true"
          - name: USE_SEMANTIC_FALLBACK
            value: "true"
          - name: ENABLE_FEEDBACK_COLLECTION
            value: "true"
          - name: FEEDBACK_API_ENABLED
            value: "true"
          - name: FEEDBACK_REQUIRE_AUTHENTICATION
            value: "true"
          - name: FEEDBACK_ALLOWED_ROLES
            value: '["admin","specialist_reviewer","human_expert"]'
          - name: FEEDBACK_MONGODB_COLLECTION
            value: specialist_feedback
          image: ghcr.io/albinojimy/neural-hive-mind/specialist-technical:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 10
          name: specialist-technical
          ports:
          - containerPort: 50051
            name: grpc
            protocol: TCP
          - containerPort: 8000
            name: http
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
          startupProbe:
            failureThreshold: 15
            httpGet:
              path: /health
              port: 8000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/mlruns
            name: mlruns
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: specialist-technical
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: mlruns
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "36"
      kubernetes.io/change-cause: kubectl set image deployment/worker-agents worker-agents=ghcr.io/albinojimy/neural-hive-mind/worker-agents:7837bfa
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: worker-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T21:50:56Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/name: worker-agents
      pod-template-hash: 5d5787b477
    name: worker-agents-5d5787b477
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: worker-agents
      uid: ebd45574-ca4d-40e2-9351-4db6f6cfeb13
    resourceVersion: "29534012"
    uid: 31be27be-553e-4bd4-a01a-4e807723b5c1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: worker-agents
        app.kubernetes.io/name: worker-agents
        pod-template-hash: 5d5787b477
    template:
      metadata:
        annotations:
          checksum/config: 8ac2bd0d9fdec2265af9bdd10b674d703f35d020860160fa4577d18160f593e8
          kubectl.kubernetes.io/restartedAt: "2026-02-06T10:36:36+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: worker-agents
          app.kubernetes.io/name: worker-agents
          pod-template-hash: 5d5787b477
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - worker-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          envFrom:
          - configMapRef:
              name: worker-agents
          image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:f532cfa
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: worker-agents
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 400m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        - name: ghcr-secret-fixed
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: worker-agents
        serviceAccountName: worker-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: worker-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "37"
      kubernetes.io/change-cause: kubectl set image deployment/worker-agents worker-agents=ghcr.io/albinojimy/neural-hive-mind/worker-agents:7837bfa
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: worker-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T22:08:28Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/name: worker-agents
      pod-template-hash: 7bc4b579cf
    name: worker-agents-7bc4b579cf
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: worker-agents
      uid: ebd45574-ca4d-40e2-9351-4db6f6cfeb13
    resourceVersion: "29555451"
    uid: 75fc946a-228a-4954-810e-c6cfb3d8fa9a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: worker-agents
        app.kubernetes.io/name: worker-agents
        pod-template-hash: 7bc4b579cf
    template:
      metadata:
        annotations:
          checksum/config: 8ac2bd0d9fdec2265af9bdd10b674d703f35d020860160fa4577d18160f593e8
          kubectl.kubernetes.io/restartedAt: "2026-02-06T10:36:36+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: worker-agents
          app.kubernetes.io/name: worker-agents
          pod-template-hash: 7bc4b579cf
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - worker-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          envFrom:
          - configMapRef:
              name: worker-agents
          image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:664b9b7
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: worker-agents
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 400m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        - name: ghcr-secret-fixed
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: worker-agents
        serviceAccountName: worker-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: worker-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "38"
      kubernetes.io/change-cause: kubectl set image deployment/worker-agents worker-agents=ghcr.io/albinojimy/neural-hive-mind/worker-agents:7837bfa
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: worker-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-12T23:22:15Z"
    generation: 5
    labels:
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/name: worker-agents
      pod-template-hash: 7c5df66bb6
    name: worker-agents-7c5df66bb6
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: worker-agents
      uid: ebd45574-ca4d-40e2-9351-4db6f6cfeb13
    resourceVersion: "29905172"
    uid: 2d488d5a-66e6-4936-9a57-fdd79df936e5
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: worker-agents
        app.kubernetes.io/name: worker-agents
        pod-template-hash: 7c5df66bb6
    template:
      metadata:
        annotations:
          checksum/config: 8ac2bd0d9fdec2265af9bdd10b674d703f35d020860160fa4577d18160f593e8
          kubectl.kubernetes.io/restartedAt: "2026-02-06T10:36:36+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: worker-agents
          app.kubernetes.io/name: worker-agents
          pod-template-hash: 7c5df66bb6
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - worker-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          envFrom:
          - configMapRef:
              name: worker-agents
          image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:3a3e661
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: worker-agents
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 400m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        - name: ghcr-secret-fixed
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: worker-agents
        serviceAccountName: worker-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: worker-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "39"
      kubernetes.io/change-cause: kubectl set image deployment/worker-agents worker-agents=ghcr.io/albinojimy/neural-hive-mind/worker-agents:7837bfa
        --namespace=neural-hive --record=true
      meta.helm.sh/release-name: worker-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-13T21:06:54Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/name: worker-agents
      pod-template-hash: cbfcc57b9
    name: worker-agents-cbfcc57b9
    namespace: neural-hive
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: worker-agents
      uid: ebd45574-ca4d-40e2-9351-4db6f6cfeb13
    resourceVersion: "29939520"
    uid: c0bc85ad-f67a-4d4b-a477-35550a99a96c
  spec:
    replicas: 2
    selector:
      matchLabels:
        app.kubernetes.io/instance: worker-agents
        app.kubernetes.io/name: worker-agents
        pod-template-hash: cbfcc57b9
    template:
      metadata:
        annotations:
          checksum/config: 8ac2bd0d9fdec2265af9bdd10b674d703f35d020860160fa4577d18160f593e8
          kubectl.kubernetes.io/restartedAt: "2026-02-13T22:06:51+01:00"
          prometheus.io/path: /metrics
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: worker-agents
          app.kubernetes.io/name: worker-agents
          pod-template-hash: cbfcc57b9
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - worker-agents
                topologyKey: topology.kubernetes.io/zone
              weight: 100
        containers:
        - env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POSTGRES_USER
            value: sla_user
          - name: POSTGRES_PASSWORD
            value: neural_hive_sla_2024
          - name: POSTGRES_DB
            value: sla_management
          - name: POSTGRES_HOST
            value: postgres-sla.neural-hive-data.svc.cluster.local
          envFrom:
          - configMapRef:
              name: worker-agents
          image: ghcr.io/albinojimy/neural-hive-mind/worker-agents:3a3e661
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 5
          name: worker-agents
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 1500m
              memory: 1536Mi
            requests:
              cpu: 400m
              memory: 640Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
          startupProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
        dnsPolicy: ClusterFirst
        imagePullSecrets:
        - name: ghcr-secret
        - name: ghcr-secret-fixed
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsGroup: 1001
          runAsNonRoot: true
          runAsUser: 1000
        serviceAccount: worker-agents
        serviceAccountName: worker-agents
        terminationGracePeriodSeconds: 30
        topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: worker-agents
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        volumes:
        - emptyDir: {}
          name: tmp
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 3
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: neural-hive-jaeger
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:21:49Z"
    generation: 3
    labels:
      app.kubernetes.io/component: all-in-one
      app.kubernetes.io/instance: neural-hive-jaeger
      app.kubernetes.io/name: jaeger
      pod-template-hash: 5fbd6fffcc
    name: neural-hive-jaeger-5fbd6fffcc
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: neural-hive-jaeger
      uid: 5260453c-9c04-4df0-ba7e-d224ec8ac409
    resourceVersion: "29939400"
    uid: 63680798-b8fe-48f4-b384-ae2390bf9615
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: all-in-one
        app.kubernetes.io/instance: neural-hive-jaeger
        app.kubernetes.io/name: jaeger
        pod-template-hash: 5fbd6fffcc
    template:
      metadata:
        annotations:
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: all-in-one
          app.kubernetes.io/instance: neural-hive-jaeger
          app.kubernetes.io/name: jaeger
          pod-template-hash: 5fbd6fffcc
      spec:
        containers:
        - env:
          - name: SPAN_STORAGE_TYPE
            value: memory
          - name: COLLECTOR_ZIPKIN_HOST_PORT
            value: :9411
          - name: JAEGER_DISABLED
            value: "false"
          - name: COLLECTOR_OTLP_ENABLED
            value: "true"
          image: jaegertracing/jaeger:2.13.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /status
              port: 13133
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: jaeger
          ports:
          - containerPort: 5775
            protocol: UDP
          - containerPort: 6831
            protocol: UDP
          - containerPort: 6832
            protocol: UDP
          - containerPort: 5778
            protocol: TCP
          - containerPort: 16686
            protocol: TCP
          - containerPort: 16685
            protocol: TCP
          - containerPort: 9411
            protocol: TCP
          - containerPort: 4317
            protocol: TCP
          - containerPort: 4318
            protocol: TCP
          - containerPort: 13133
            protocol: TCP
          - containerPort: 8888
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /status
              port: 13133
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsUser: 10001
        serviceAccount: neural-hive-jaeger
        serviceAccountName: neural-hive-jaeger
        terminationGracePeriodSeconds: 30
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 3
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-01-15T21:13:56Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/name: grafana
      app.kubernetes.io/version: 12.3.0
      helm.sh/chart: grafana-10.4.0
      pod-template-hash: 84f8d9f494
    name: neural-hive-prometheus-grafana-84f8d9f494
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: neural-hive-prometheus-grafana
      uid: eb6a21ca-08d8-4c84-adcd-7c9558c6439e
    resourceVersion: "28922796"
    uid: dec88f28-1be6-4561-8a08-fd746cff6e4b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: neural-hive-prometheus
        app.kubernetes.io/name: grafana
        pod-template-hash: 84f8d9f494
    template:
      metadata:
        annotations:
          checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3
          checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
          checksum/secret: 6e3b9dc1c22f81738aaabb2d96f820cc2a2860509fafcbceb0a5bdacd7ff792c
          kubectl.kubernetes.io/default-container: grafana
          kubectl.kubernetes.io/restartedAt: "2026-01-15T22:13:56+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: neural-hive-prometheus
          app.kubernetes.io/name: grafana
          app.kubernetes.io/version: 12.3.0
          helm.sh/chart: grafana-10.4.0
          pod-template-hash: 84f8d9f494
      spec:
        automountServiceAccountToken: true
        containers:
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_dashboard
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /tmp/dashboards
          - name: RESOURCE
            value: both
          - name: NAMESPACE
            value: ALL
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: neural-hive-prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: neural-hive-prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/dashboards/reload
          - name: REQ_METHOD
            value: POST
          - name: REQ_SKIP_INIT
            value: "true"
          image: quay.io/kiwigrid/k8s-sidecar:2.1.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-dashboard
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
        - env:
          - name: METHOD
            value: WATCH
          - name: LABEL
            value: grafana_datasource
          - name: LABEL_VALUE
            value: "1"
          - name: FOLDER
            value: /etc/grafana/provisioning/datasources
          - name: RESOURCE
            value: both
          - name: REQ_USERNAME
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: neural-hive-prometheus-grafana
          - name: REQ_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: neural-hive-prometheus-grafana
          - name: REQ_URL
            value: http://localhost:3000/api/admin/provisioning/datasources/reload
          - name: REQ_METHOD
            value: POST
          - name: REQ_SKIP_INIT
            value: "true"
          image: quay.io/kiwigrid/k8s-sidecar:2.1.2
          imagePullPolicy: IfNotPresent
          name: grafana-sc-datasources
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: GF_SECURITY_ADMIN_USER
            valueFrom:
              secretKeyRef:
                key: admin-user
                name: neural-hive-prometheus-grafana
          - name: GF_SECURITY_ADMIN_PASSWORD
            valueFrom:
              secretKeyRef:
                key: admin-password
                name: neural-hive-prometheus-grafana
          - name: GF_PATHS_DATA
            value: /var/lib/grafana/
          - name: GF_PATHS_LOGS
            value: /var/log/grafana
          - name: GF_PATHS_PLUGINS
            value: /var/lib/grafana/plugins
          - name: GF_PATHS_PROVISIONING
            value: /etc/grafana/provisioning
          - name: GOMEMLIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.memory
          image: docker.io/grafana/grafana:12.3.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: grafana
          ports:
          - containerPort: 3000
            name: grafana
            protocol: TCP
          - containerPort: 9094
            name: gossip-tcp
            protocol: TCP
          - containerPort: 9094
            name: gossip-udp
            protocol: UDP
          - containerPort: 6060
            name: profiling
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/grafana/grafana.ini
            name: config
            subPath: grafana.ini
          - mountPath: /var/lib/grafana
            name: storage
          - mountPath: /tmp/dashboards
            name: sc-dashboard-volume
          - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
            name: sc-dashboard-provider
            subPath: provider.yaml
          - mountPath: /etc/grafana/provisioning/datasources
            name: sc-datasources-volume
        dnsPolicy: ClusterFirst
        enableServiceLinks: true
        initContainers:
        - command:
          - chown
          - -R
          - 472:472
          - /var/lib/grafana
          image: docker.io/library/busybox:1.31.1
          imagePullPolicy: IfNotPresent
          name: init-chown-data
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/grafana
            name: storage
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 472
          runAsGroup: 472
          runAsNonRoot: true
          runAsUser: 472
        serviceAccount: neural-hive-prometheus-grafana
        serviceAccountName: neural-hive-prometheus-grafana
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: neural-hive-prometheus-grafana
          name: config
        - name: storage
          persistentVolumeClaim:
            claimName: neural-hive-prometheus-grafana
        - emptyDir: {}
          name: sc-dashboard-volume
        - configMap:
            defaultMode: 420
            name: neural-hive-prometheus-grafana-config-dashboards
          name: sc-dashboard-provider
        - emptyDir: {}
          name: sc-datasources-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-01-10T00:45:30Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/component: prometheus-operator
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      pod-template-hash: 5fc4d75679
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kub-operator-5fc4d75679
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: neural-hive-prometheus-kub-operator
      uid: f36c4f6c-7dc9-42b8-8293-9f6e70d30024
    resourceVersion: "25557745"
    uid: 7bfdeaab-5acf-4b5c-b000-6cf32ffce4f1
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kube-prometheus-stack-operator
        pod-template-hash: 5fc4d75679
        release: neural-hive-prometheus
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-10T01:45:30+01:00"
        creationTimestamp: null
        labels:
          app: kube-prometheus-stack-operator
          app.kubernetes.io/component: prometheus-operator
          app.kubernetes.io/instance: neural-hive-prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator
          app.kubernetes.io/part-of: kube-prometheus-stack
          app.kubernetes.io/version: 80.8.0
          chart: kube-prometheus-stack-80.8.0
          heritage: Helm
          pod-template-hash: 5fc4d75679
          release: neural-hive-prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --kubelet-service=kube-system/neural-hive-prometheus-kub-kubelet
          - --kubelet-endpoints=true
          - --kubelet-endpointslice=false
          - --localhost=127.0.0.1
          - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          - --config-reloader-cpu-request=0
          - --config-reloader-cpu-limit=0
          - --config-reloader-memory-request=0
          - --config-reloader-memory-limit=0
          - --thanos-default-base-image=quay.io/thanos/thanos:v0.40.1
          - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1
          - --web.enable-tls=true
          - --web.cert-file=/cert/cert
          - --web.key-file=/cert/key
          - --web.listen-address=:10250
          - --web.tls-min-version=VersionTLS13
          env:
          - name: GOGC
            value: "30"
          image: quay.io/prometheus-operator/prometheus-operator:v0.87.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: kube-prometheus-stack
          ports:
          - containerPort: 10250
            name: https
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: https
              scheme: HTTPS
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /cert
            name: tls-secret
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: neural-hive-prometheus-kub-operator
        serviceAccountName: neural-hive-prometheus-kub-operator
        terminationGracePeriodSeconds: 30
        volumes:
        - name: tls-secret
          secret:
            defaultMode: 420
            secretName: neural-hive-prometheus-kub-admission
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:18:46Z"
    generation: 2
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: neural-hive-prometheus
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.17.0
      helm.sh/chart: kube-state-metrics-7.0.0
      pod-template-hash: 76c55f9647
      release: neural-hive-prometheus
    name: neural-hive-prometheus-kube-state-metrics-76c55f9647
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: neural-hive-prometheus-kube-state-metrics
      uid: 3f40f467-16bf-41bc-af5f-118b6e12d89c
    resourceVersion: "28922916"
    uid: b535bb50-def3-4970-8637-858594e4ca1c
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: neural-hive-prometheus
        app.kubernetes.io/name: kube-state-metrics
        pod-template-hash: 76c55f9647
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: metrics
          app.kubernetes.io/instance: neural-hive-prometheus
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: kube-state-metrics
          app.kubernetes.io/part-of: kube-state-metrics
          app.kubernetes.io/version: 2.17.0
          helm.sh/chart: kube-state-metrics-7.0.0
          pod-template-hash: 76c55f9647
          release: neural-hive-prometheus
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --port=8080
          - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpointslices,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /livez
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kube-state-metrics
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: neural-hive-prometheus-kube-state-metrics
        serviceAccountName: neural-hive-prometheus-kube-state-metrics
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "4"
      deployment.kubernetes.io/revision-history: "2"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-02-01T13:19:04Z"
    generation: 4
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 57586d6bd6
    name: otel-collector-neural-hive-otel-collector-57586d6bd6
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "25551572"
    uid: 0176986a-cea0-4f9b-a748-5de48d12bd82
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: 57586d6bd6
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-01T14:19:04+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: 57586d6bd6
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 512MiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "9"
      deployment.kubernetes.io/revision-history: "7"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-02-01T13:28:07Z"
    generation: 5
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 5b45d5d9c4
    name: otel-collector-neural-hive-otel-collector-5b45d5d9c4
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "25553124"
    uid: 48392651-b028-485d-8a5f-f512970397ef
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: 5b45d5d9c4
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-01T14:28:06+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: 5b45d5d9c4
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/otelcol.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 2GiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    observedGeneration: 5
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-01-01T03:36:58Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 644b85fc6b
    name: otel-collector-neural-hive-otel-collector-644b85fc6b
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "25549731"
    uid: 6113dd1a-4c7d-467e-a5ff-96fec32dbcad
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: 644b85fc6b
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: 644b85fc6b
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 512MiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "5"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-02-01T13:25:40Z"
    generation: 4
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 64bbbfc8f8
    name: otel-collector-neural-hive-otel-collector-64bbbfc8f8
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "25553552"
    uid: d281b82f-dc13-43e2-b9fb-6b04f04948a2
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: 64bbbfc8f8
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-01T14:19:04+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: 64bbbfc8f8
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 2GiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-02-12T13:55:25Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 64c9989c49
    name: otel-collector-neural-hive-otel-collector-64c9989c49
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "29402597"
    uid: 0d27455a-43e2-4ce4-a884-14ebd776ef23
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: 64c9989c49
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-12T14:55:23+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: 64c9989c49
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/otelcol.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 2GiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "6"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-02-01T13:27:39Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 677bfb9bbd
    name: otel-collector-neural-hive-otel-collector-677bfb9bbd
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "25552071"
    uid: 15065d05-8cfe-4ad6-8d3b-b423e1d63b36
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: 677bfb9bbd
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-01T14:19:04+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: 677bfb9bbd
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/otelcol.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 2GiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-02-01T13:29:35Z"
    generation: 3
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 69cf9f9545
    name: otel-collector-neural-hive-otel-collector-69cf9f9545
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "25552823"
    uid: dbe4167b-92d9-4381-b937-ba5e886096d0
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: 69cf9f9545
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-01T14:29:33+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: 69cf9f9545
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/otelcol.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 2GiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-02-01T13:22:49Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: 7866796b64
    name: otel-collector-neural-hive-otel-collector-7866796b64
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "25550906"
    uid: 78fb9a79-77b2-4ce5-97ab-5327add80020
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: 7866796b64
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-01T14:19:04+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: 7866796b64
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/otelcol.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 512MiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "10"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-02-01T13:32:01Z"
    generation: 9
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: neural-hive-otel-collector
      neural.hive/component: telemetry-collector
      neural.hive/instrumented: "true"
      neural.hive/layer: observabilidade
      pod-template-hash: dc4fbd5d5
    name: otel-collector-neural-hive-otel-collector-dc4fbd5d5
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-neural-hive-otel-collector
      uid: b2c73797-e1b9-4cf7-9b7f-2fb24ddd45d9
    resourceVersion: "29402607"
    uid: b385be6c-e8bb-4c18-ab9a-5362f71bc4c1
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: neural-hive-otel-collector
        pod-template-hash: dc4fbd5d5
    template:
      metadata:
        annotations:
          checksum/config: c038e28283c68c5729acc1c23d99c216651f8eae6cc775c8ef1568ac635a4f71
          kubectl.kubernetes.io/restartedAt: "2026-02-01T14:31:58+01:00"
          neural.hive/metrics: enabled
          prometheus.io/path: /metrics
          prometheus.io/port: "8888"
          prometheus.io/scrape: "true"
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: neural-hive-otel-collector
          neural.hive/component: telemetry-collector
          neural.hive/instrumented: "true"
          neural.hive/layer: observabilidade
          pod-template-hash: dc4fbd5d5
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - neural-hive-otel-collector
                topologyKey: kubernetes.io/hostname
              weight: 100
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/otelcol.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: GOMEMLIMIT
            value: 2GiB
          - name: GOGC
            value: "80"
          image: otel/opentelemetry-collector-contrib:0.89.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 15
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 5
          name: neural-hive-otel-collector
          ports:
          - containerPort: 13133
            name: health
            protocol: TCP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-http
            protocol: TCP
          - containerPort: 8888
            name: metrics
            protocol: TCP
          - containerPort: 4317
            name: otlp-grpc
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 1777
            name: pprof
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          - containerPort: 55679
            name: zpages
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: health
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: config
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: otel-collector-neural-hive-otel-collector
        serviceAccountName: otel-collector-neural-hive-otel-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: otelcol.yaml
              path: otelcol.yaml
            name: otel-collector-neural-hive-otel-collector-config
          name: config
  status:
    observedGeneration: 9
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: otel-collector
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2026-01-01T03:36:58Z"
    generation: 8
    labels:
      app.kubernetes.io/instance: otel-collector
      app.kubernetes.io/name: opentelemetry-collector
      component: standalone-collector
      pod-template-hash: 6b67578c68
    name: otel-collector-opentelemetry-collector-6b67578c68
    namespace: observability
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: otel-collector-opentelemetry-collector
      uid: 53148a65-924a-4a9f-a475-194db748845b
    resourceVersion: "28922988"
    uid: 2e1f148f-6819-4007-81bc-8cd7c0e0938d
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: otel-collector
        app.kubernetes.io/name: opentelemetry-collector
        component: standalone-collector
        pod-template-hash: 6b67578c68
    template:
      metadata:
        annotations:
          checksum/config: e62840abbd07d6d53f26a4056330ef678819c1e217217fd45e350cd335b538fa
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: otel-collector
          app.kubernetes.io/name: opentelemetry-collector
          component: standalone-collector
          pod-template-hash: 6b67578c68
      spec:
        containers:
        - command:
          - /otelcol-contrib
          - --config=/conf/relay.yaml
          env:
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: otel/opentelemetry-collector-contrib:0.83.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 13133
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: opentelemetry-collector
          ports:
          - containerPort: 6831
            name: jaeger-compact
            protocol: UDP
          - containerPort: 14250
            name: jaeger-grpc
            protocol: TCP
          - containerPort: 14268
            name: jaeger-thrift
            protocol: TCP
          - containerPort: 4317
            name: otlp
            protocol: TCP
          - containerPort: 4318
            name: otlp-http
            protocol: TCP
          - containerPort: 9411
            name: zipkin
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 13133
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /conf
            name: opentelemetry-collector-configmap
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: otel-collector-opentelemetry-collector
        serviceAccountName: otel-collector-opentelemetry-collector
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: relay
              path: relay.yaml
            name: otel-collector-opentelemetry-collector
          name: opentelemetry-collector-configmap
  status:
    observedGeneration: 8
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: portainer
      meta.helm.sh/release-namespace: portainer
    creationTimestamp: "2025-11-08T11:59:04Z"
    generation: 4
    labels:
      app.kubernetes.io/instance: portainer
      app.kubernetes.io/name: portainer
      pod-template-hash: 6c896d7dfd
    name: portainer-6c896d7dfd
    namespace: portainer
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: portainer
      uid: 99fcad91-f06b-4e33-b2d0-493fb0f51465
    resourceVersion: "14004235"
    uid: 4df61e10-dc80-49bb-b6a6-fb80457e1d78
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: portainer
        app.kubernetes.io/name: portainer
        pod-template-hash: 6c896d7dfd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: portainer
          app.kubernetes.io/name: portainer
          pod-template-hash: 6c896d7dfd
      spec:
        containers:
        - args:
          - --tunnel-port=30776
          image: portainer/portainer-ce:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: portainer
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          - containerPort: 9443
            name: https
            protocol: TCP
          - containerPort: 8000
            name: tcp-edge
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: portainer-sa-clusteradmin
        serviceAccountName: portainer-sa-clusteradmin
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "4"
      meta.helm.sh/release-name: portainer
      meta.helm.sh/release-namespace: portainer
    creationTimestamp: "2026-01-11T13:15:34Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: portainer
      app.kubernetes.io/name: portainer
      pod-template-hash: 7974cc94c4
    name: portainer-7974cc94c4
    namespace: portainer
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: portainer
      uid: 99fcad91-f06b-4e33-b2d0-493fb0f51465
    resourceVersion: "18770087"
    uid: 67799345-19d6-4951-ab58-74df24c1ee1b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: portainer
        app.kubernetes.io/name: portainer
        pod-template-hash: 7974cc94c4
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-11T14:15:29+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: portainer
          app.kubernetes.io/name: portainer
          pod-template-hash: 7974cc94c4
      spec:
        containers:
        - args:
          - --tunnel-port=30776
          image: portainer/portainer-ce:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: portainer
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          - containerPort: 9443
            name: https
            protocol: TCP
          - containerPort: 8000
            name: tcp-edge
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: portainer-sa-clusteradmin
        serviceAccountName: portainer-sa-clusteradmin
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "5"
      meta.helm.sh/release-name: portainer
      meta.helm.sh/release-namespace: portainer
    creationTimestamp: "2026-01-11T13:24:13Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: portainer
      app.kubernetes.io/name: portainer
      pod-template-hash: d6fcbdd48
    name: portainer-d6fcbdd48
    namespace: portainer
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: portainer
      uid: 99fcad91-f06b-4e33-b2d0-493fb0f51465
    resourceVersion: "26775179"
    uid: e0fcaab3-650a-45da-b03d-9e3d7e06c09c
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: portainer
        app.kubernetes.io/name: portainer
        pod-template-hash: d6fcbdd48
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-11T14:15:29+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: portainer
          app.kubernetes.io/name: portainer
          pod-template-hash: d6fcbdd48
      spec:
        containers:
        - args:
          - --tunnel-port=30776
          image: portainer/portainer-ce:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: portainer
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          - containerPort: 9443
            name: https
            protocol: TCP
          - containerPort: 8000
            name: tcp-edge
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: portainer-sa-clusteradmin
        serviceAccountName: portainer-sa-clusteradmin
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: portainer
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: portainer
      meta.helm.sh/release-namespace: portainer
    creationTimestamp: "2025-12-27T23:32:04Z"
    generation: 2
    labels:
      app.kubernetes.io/instance: portainer
      app.kubernetes.io/name: portainer
      pod-template-hash: f54c796bf
    name: portainer-f54c796bf
    namespace: portainer
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: portainer
      uid: 99fcad91-f06b-4e33-b2d0-493fb0f51465
    resourceVersion: "18768088"
    uid: fc6aa123-359d-4282-bb98-abac838e655f
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/instance: portainer
        app.kubernetes.io/name: portainer
        pod-template-hash: f54c796bf
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:39+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: portainer
          app.kubernetes.io/name: portainer
          pod-template-hash: f54c796bf
      spec:
        containers:
        - args:
          - --tunnel-port=30776
          image: portainer/portainer-ce:latest
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: portainer
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          - containerPort: 9443
            name: https
            protocol: TCP
          - containerPort: 8000
            name: tcp-edge
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 9443
              scheme: HTTPS
            initialDelaySeconds: 45
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: portainer-sa-clusteradmin
        serviceAccountName: portainer-sa-clusteradmin
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2025-10-29T10:57:57Z"
    generation: 4
    labels:
      app: redis
      pod-template-hash: 59dbc7c5f
      version: v1
    name: redis-59dbc7c5f
    namespace: redis-cluster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis
      uid: 599c2b95-82d2-49ba-8bd9-0a99e473df16
    resourceVersion: "14004451"
    uid: 2dee808b-62c1-42a5-9418-860e3ddfb018
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: redis
        pod-template-hash: 59dbc7c5f
    template:
      metadata:
        annotations:
          sidecar.istio.io/inject: "false"
        creationTimestamp: null
        labels:
          app: redis
          pod-template-hash: 59dbc7c5f
          version: v1
      spec:
        containers:
        - command:
          - redis-server
          - /usr/local/etc/redis/redis.conf
          image: redis:7-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - redis-cli
              - ping
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/etc/redis
            name: config
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: redis-config
          name: config
        - emptyDir: {}
          name: data
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2025-12-27T23:31:34Z"
    generation: 3
    labels:
      app: redis
      pod-template-hash: 66b84474ff
      version: v1
    name: redis-66b84474ff
    namespace: redis-cluster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis
      uid: 599c2b95-82d2-49ba-8bd9-0a99e473df16
    resourceVersion: "28939234"
    uid: 7dbcfb33-43e3-44c6-8b07-6837b82868a9
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: redis
        pod-template-hash: 66b84474ff
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:33+01:00"
          sidecar.istio.io/inject: "false"
        creationTimestamp: null
        labels:
          app: redis
          pod-template-hash: 66b84474ff
          version: v1
      spec:
        containers:
        - command:
          - redis-server
          - /usr/local/etc/redis/redis.conf
          image: redis:7-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          name: redis
          ports:
          - containerPort: 6379
            name: redis
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - redis-cli
              - ping
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/local/etc/redis
            name: config
          - mountPath: /data
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: redis-config
          name: config
        - emptyDir: {}
          name: data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: redis-operator
      meta.helm.sh/release-namespace: redis-operator
    creationTimestamp: "2025-12-27T23:31:37Z"
    generation: 1
    labels:
      name: redis-operator
      pod-template-hash: 5b7d784c5
    name: redis-operator-5b7d784c5
    namespace: redis-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis-operator
      uid: 3e5d5c43-169b-4cdc-a79c-1e3dba1d68de
    resourceVersion: "26438744"
    uid: b8bdeac5-82e1-40ea-b95d-a92ea4b94984
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: redis-operator
        pod-template-hash: 5b7d784c5
    template:
      metadata:
        annotations:
          cert-manager.io/inject-ca-from: redis-operator/serving-cert
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:37+01:00"
        creationTimestamp: null
        labels:
          name: redis-operator
          pod-template-hash: 5b7d784c5
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --leader-elect
          - --metrics-bind-address=:8080
          - --kube-client-timeout=60s
          command:
          - /operator
          - manager
          env:
          - name: OPERATOR_IMAGE
            value: quay.io/opstree/redis-operator:v0.22.2
          - name: ENABLE_WEBHOOKS
            value: "false"
          - name: FEATURE_GATES
            value: GenerateConfigInInitContainer=false
          image: quay.io/opstree/redis-operator:v0.22.2
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: probe
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: redis-operator
          ports:
          - containerPort: 8081
            name: probe
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: probe
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: redis-operator
        serviceAccountName: redis-operator
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: redis-operator
      meta.helm.sh/release-namespace: redis-operator
    creationTimestamp: "2025-11-19T08:00:27Z"
    generation: 2
    labels:
      name: redis-operator
      pod-template-hash: 64b5cff87f
    name: redis-operator-64b5cff87f
    namespace: redis-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis-operator
      uid: 3e5d5c43-169b-4cdc-a79c-1e3dba1d68de
    resourceVersion: "14004402"
    uid: 7fb758aa-2af9-4ae5-9232-c37ab6ab1966
  spec:
    replicas: 0
    selector:
      matchLabels:
        name: redis-operator
        pod-template-hash: 64b5cff87f
    template:
      metadata:
        annotations:
          cert-manager.io/inject-ca-from: redis-operator/serving-cert
        creationTimestamp: null
        labels:
          name: redis-operator
          pod-template-hash: 64b5cff87f
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - --leader-elect
          - --metrics-bind-address=:8080
          - --kube-client-timeout=60s
          command:
          - /operator
          - manager
          env:
          - name: OPERATOR_IMAGE
            value: quay.io/opstree/redis-operator:v0.22.2
          - name: ENABLE_WEBHOOKS
            value: "false"
          - name: FEATURE_GATES
            value: GenerateConfigInInitContainer=false
          image: quay.io/opstree/redis-operator:v0.22.2
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: probe
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: redis-operator
          ports:
          - containerPort: 8081
            name: probe
            protocol: TCP
          - containerPort: 8080
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readyz
              port: probe
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 500Mi
            requests:
              cpu: 500m
              memory: 500Mi
          securityContext: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: redis-operator
        serviceAccountName: redis-operator
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "3"
      meta.helm.sh/release-name: registry
      meta.helm.sh/release-namespace: registry
    creationTimestamp: "2026-01-13T13:48:10Z"
    generation: 1
    labels:
      app.kubernetes.io/name: docker-registry
      pod-template-hash: 6b5c9fffcb
    name: docker-registry-6b5c9fffcb
    namespace: registry
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: docker-registry
      uid: 9e496a6d-251e-413a-86bd-107d97f85ce7
    resourceVersion: "28851341"
    uid: 2a26bc08-8178-4fc2-9bd6-145a4b15f138
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/name: docker-registry
        pod-template-hash: 6b5c9fffcb
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-13T14:48:10+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: docker-registry
          pod-template-hash: 6b5c9fffcb
      spec:
        containers:
        - env:
          - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
            value: /var/lib/registry
          - name: REGISTRY_HTTP_ADDR
            value: :5000
          - name: REGISTRY_STORAGE_DELETE_ENABLED
            value: "true"
          image: registry:2.8.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: registry
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/registry
            name: registry-data
          - mountPath: /etc/docker/registry
            name: registry-config
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/control-plane: ""
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: registry-data
          persistentVolumeClaim:
            claimName: docker-registry-data
        - configMap:
            defaultMode: 420
            name: docker-registry-config
          name: registry-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: registry
      meta.helm.sh/release-namespace: registry
    creationTimestamp: "2025-12-04T14:17:35Z"
    generation: 2
    labels:
      app.kubernetes.io/name: docker-registry
      pod-template-hash: 7b4667b94
    name: docker-registry-7b4667b94
    namespace: registry
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: docker-registry
      uid: 9e496a6d-251e-413a-86bd-107d97f85ce7
    resourceVersion: "14004475"
    uid: 0a1386fc-e817-428f-8cb8-04ffe274420a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/name: docker-registry
        pod-template-hash: 7b4667b94
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: docker-registry
          pod-template-hash: 7b4667b94
      spec:
        containers:
        - env:
          - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
            value: /var/lib/registry
          - name: REGISTRY_HTTP_ADDR
            value: :5000
          - name: REGISTRY_STORAGE_DELETE_ENABLED
            value: "true"
          image: registry:2.8.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: registry
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/registry
            name: registry-data
          - mountPath: /etc/docker/registry
            name: registry-config
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/control-plane: ""
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: registry-data
          persistentVolumeClaim:
            claimName: docker-registry-data
        - configMap:
            defaultMode: 420
            name: docker-registry-config
          name: registry-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: registry
      meta.helm.sh/release-namespace: registry
    creationTimestamp: "2025-12-27T23:32:20Z"
    generation: 2
    labels:
      app.kubernetes.io/name: docker-registry
      pod-template-hash: f7958d84
    name: docker-registry-f7958d84
    namespace: registry
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: docker-registry
      uid: 9e496a6d-251e-413a-86bd-107d97f85ce7
    resourceVersion: "19450926"
    uid: ffd8d53a-6d70-41be-b5c2-9d9416386830
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/name: docker-registry
        pod-template-hash: f7958d84
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:32:02+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/name: docker-registry
          pod-template-hash: f7958d84
      spec:
        containers:
        - env:
          - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
            value: /var/lib/registry
          - name: REGISTRY_HTTP_ADDR
            value: :5000
          - name: REGISTRY_STORAGE_DELETE_ENABLED
            value: "true"
          image: registry:2.8.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: registry
          ports:
          - containerPort: 5000
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 5000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/registry
            name: registry-data
          - mountPath: /etc/docker/registry
            name: registry-config
        dnsPolicy: ClusterFirst
        nodeSelector:
          node-role.kubernetes.io/control-plane: ""
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: registry-data
          persistentVolumeClaim:
            claimName: docker-registry-data
        - configMap:
            defaultMode: 420
            name: docker-registry-config
          name: registry-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: admintools
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 5b9cc67d9b
    name: temporal-admintools-5b9cc67d9b
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-admintools
      uid: 155da70c-af07-4af7-934f-df9853011803
    resourceVersion: "14004256"
    uid: c42ffc08-3b18-4f4a-8d22-fb5c4e27017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: admintools
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 5b9cc67d9b
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: admintools
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 5b9cc67d9b
      spec:
        containers:
        - env:
          - name: TEMPORAL_CLI_ADDRESS
            value: temporal-frontend:7233
          - name: TEMPORAL_ADDRESS
            value: temporal-frontend:7233
          image: temporalio/admin-tools:1.29.1-tctl-1.18.4-cli-1.5.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - ls
              - /
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: admin-tools
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-12-27T23:31:14Z"
    generation: 1
    labels:
      app.kubernetes.io/component: admintools
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 78c898f5f9
    name: temporal-admintools-78c898f5f9
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-admintools
      uid: 155da70c-af07-4af7-934f-df9853011803
    resourceVersion: "25557809"
    uid: 9fff258a-7374-470d-91aa-77c76f47692f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: admintools
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 78c898f5f9
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: admintools
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 78c898f5f9
      spec:
        containers:
        - env:
          - name: TEMPORAL_CLI_ADDRESS
            value: temporal-frontend:7233
          - name: TEMPORAL_ADDRESS
            value: temporal-frontend:7233
          image: temporalio/admin-tools:1.29.1-tctl-1.18.4-cli-1.5.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - ls
              - /
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: admin-tools
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 86658bdb8b
    name: temporal-frontend-86658bdb8b
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-frontend
      uid: 68892dc8-41a8-4c0f-85ab-cd336bc5f0cc
    resourceVersion: "14004108"
    uid: 921e61b8-7939-4a11-8ae5-edd92e2ddd11
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: frontend
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 86658bdb8b
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          prometheus.io/job: temporal-frontend
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: frontend
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 86658bdb8b
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: frontend
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-frontend
          ports:
          - containerPort: 7233
            name: rpc
            protocol: TCP
          - containerPort: 6933
            name: membership
            protocol: TCP
          - containerPort: 7243
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-12-27T23:31:14Z"
    generation: 1
    labels:
      app.kubernetes.io/component: frontend
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: bc8b49c8d
    name: temporal-frontend-bc8b49c8d
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-frontend
      uid: 68892dc8-41a8-4c0f-85ab-cd336bc5f0cc
    resourceVersion: "25557777"
    uid: d9e32e1a-6719-42cc-8a7c-a271584cc0d7
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: frontend
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: bc8b49c8d
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
          prometheus.io/job: temporal-frontend
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: frontend
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: bc8b49c8d
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: frontend
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-frontend
          ports:
          - containerPort: 7233
            name: rpc
            protocol: TCP
          - containerPort: 6933
            name: membership
            protocol: TCP
          - containerPort: 7243
            name: http
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: history
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 68c766d694
    name: temporal-history-68c766d694
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-history
      uid: d759a039-e0cf-4126-87e4-c268183e0a25
    resourceVersion: "14004268"
    uid: 008a6e98-8ff0-4f0c-9655-61151606acc6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: history
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 68c766d694
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          prometheus.io/job: temporal-history
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: history
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 68c766d694
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: history
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-history
          ports:
          - containerPort: 7234
            name: rpc
            protocol: TCP
          - containerPort: 6934
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-12-27T23:31:14Z"
    generation: 1
    labels:
      app.kubernetes.io/component: history
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 86c95ccf9b
    name: temporal-history-86c95ccf9b
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-history
      uid: d759a039-e0cf-4126-87e4-c268183e0a25
    resourceVersion: "25557673"
    uid: 2139c077-7f01-44a6-b209-783178c546c8
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: history
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 86c95ccf9b
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
          prometheus.io/job: temporal-history
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: history
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 86c95ccf9b
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: history
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-history
          ports:
          - containerPort: 7234
            name: rpc
            protocol: TCP
          - containerPort: 6934
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-12-27T23:31:14Z"
    generation: 1
    labels:
      app.kubernetes.io/component: matching
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 5fb946b9c5
    name: temporal-matching-5fb946b9c5
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-matching
      uid: 3db8dbf6-ef67-4dc3-afec-55042873ed16
    resourceVersion: "25557850"
    uid: a3fd5474-597f-45d9-9d98-7b5cacf7aa46
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: matching
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 5fb946b9c5
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: matching
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 5fb946b9c5
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: matching
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-matching
          ports:
          - containerPort: 7235
            name: rpc
            protocol: TCP
          - containerPort: 6935
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: matching
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 86cf6cfd5
    name: temporal-matching-86cf6cfd5
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-matching
      uid: 3db8dbf6-ef67-4dc3-afec-55042873ed16
    resourceVersion: "14004255"
    uid: 4290ff41-82d0-4d66-8f06-a33f3b3c75cd
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: matching
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 86cf6cfd5
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: matching
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 86cf6cfd5
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: matching
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 150
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: rpc
            timeoutSeconds: 1
          name: temporal-matching
          ports:
          - containerPort: 7235
            name: rpc
            protocol: TCP
          - containerPort: 6935
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 7d4f7c6969
    name: temporal-web-7d4f7c6969
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-web
      uid: 213d64b8-c7ba-4ddc-b6de-fbb25ba15ae8
    resourceVersion: "14004326"
    uid: 436740cb-e3cc-45f9-9b37-9a68ca028185
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: web
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 7d4f7c6969
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: web
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 7d4f7c6969
      spec:
        containers:
        - env:
          - name: TEMPORAL_ADDRESS
            value: temporal-frontend.temporal.svc:7233
          image: temporalio/ui:2.42.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: temporal-web
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-12-27T23:31:15Z"
    generation: 1
    labels:
      app.kubernetes.io/component: web
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: bfbd4b558
    name: temporal-web-bfbd4b558
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-web
      uid: 213d64b8-c7ba-4ddc-b6de-fbb25ba15ae8
    resourceVersion: "25557856"
    uid: ddfedcbb-ff8a-487e-965d-d14af99224c0
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: web
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: bfbd4b558
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: web
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: bfbd4b558
      spec:
        containers:
        - env:
          - name: TEMPORAL_ADDRESS
            value: temporal-frontend.temporal.svc:7233
          image: temporalio/ui:2.42.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: http
            timeoutSeconds: 1
          name: temporal-web
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            limits:
              cpu: 200m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-12-27T23:31:15Z"
    generation: 1
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 77df5b85f6
    name: temporal-worker-77df5b85f6
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-worker
      uid: 00756dd2-dee7-4f7b-931a-3d630e4c0419
    resourceVersion: "22401883"
    uid: 70e60f59-ca2e-4c39-8c6e-8645b4023edc
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 77df5b85f6
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          kubectl.kubernetes.io/restartedAt: "2025-12-28T00:31:13+01:00"
          prometheus.io/job: temporal-worker
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: worker
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 77df5b85f6
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: worker
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          name: temporal-worker
          ports:
          - containerPort: 6939
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: temporal
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:16:51Z"
    generation: 2
    labels:
      app.kubernetes.io/component: worker
      app.kubernetes.io/instance: temporal
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: temporal
      app.kubernetes.io/part-of: temporal
      app.kubernetes.io/version: 1.29.1
      helm.sh/chart: temporal-0.72.0
      pod-template-hash: 77f7b698b
    name: temporal-worker-77f7b698b
    namespace: temporal
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: temporal-worker
      uid: 00756dd2-dee7-4f7b-931a-3d630e4c0419
    resourceVersion: "14004345"
    uid: 86abf1e9-e0f0-4054-b7af-1f8648bdc50b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app.kubernetes.io/component: worker
        app.kubernetes.io/instance: temporal
        app.kubernetes.io/name: temporal
        pod-template-hash: 77f7b698b
    template:
      metadata:
        annotations:
          checksum/config: 0df6d2d8abe447d9fff313796c88ae666f0429b047bbe3ce36927c6174dea0ca
          prometheus.io/job: temporal-worker
          prometheus.io/port: "9090"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: worker
          app.kubernetes.io/instance: temporal
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: temporal
          app.kubernetes.io/part-of: temporal
          app.kubernetes.io/version: 1.29.1
          helm.sh/chart: temporal-0.72.0
          pod-template-hash: 77f7b698b
      spec:
        containers:
        - env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: SERVICES
            value: worker
          - name: TEMPORAL_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-default-store
          - name: TEMPORAL_VISIBILITY_STORE_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: temporal-visibility-store
          image: temporalio/server:1.29.1
          imagePullPolicy: IfNotPresent
          name: temporal-worker
          ports:
          - containerPort: 6939
            name: membership
            protocol: TCP
          - containerPort: 9090
            name: metrics
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/temporal/config/config_template.yaml
            name: config
            subPath: config_template.yaml
          - mountPath: /etc/temporal/dynamic_config
            name: dynamic-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: temporal-config
          name: config
        - configMap:
            defaultMode: 420
            items:
            - key: dynamic_config.yaml
              path: dynamic_config.yaml
            name: temporal-dynamic-config
          name: dynamic-config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: minio
      meta.helm.sh/release-namespace: velero
    creationTimestamp: "2025-12-09T20:59:11Z"
    generation: 1
    labels:
      app: minio
      pod-template-hash: 5c5464c875
      release: minio
    name: minio-5c5464c875
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: minio
      uid: 6683945c-5ff0-4d16-8c23-7baaf9073125
    resourceVersion: "22400336"
    uid: 185ced4e-92e7-432f-a848-d4c2a13c726b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: minio
        pod-template-hash: 5c5464c875
        release: minio
    template:
      metadata:
        annotations:
          checksum/config: 80e7cc85a7dc2cac261932d8c7c6611e6dbfdb0879e63cce3bd67d7c6b340946
          checksum/secrets: 184cec76b907d0ce15c3e1d11f7c5e20c066057a49968de890f5bba4de52f63a
        creationTimestamp: null
        labels:
          app: minio
          pod-template-hash: 5c5464c875
          release: minio
        name: minio
      spec:
        containers:
        - command:
          - /bin/sh
          - -ce
          - /usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/
            --address :9000 --console-address :9001
          env:
          - name: MINIO_ROOT_USER
            valueFrom:
              secretKeyRef:
                key: rootUser
                name: minio
          - name: MINIO_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                key: rootPassword
                name: minio
          - name: MINIO_PROMETHEUS_AUTH_TYPE
            value: public
          image: quay.io/minio/minio:RELEASE.2024-12-18T13-15-44Z
          imagePullPolicy: IfNotPresent
          name: minio
          ports:
          - containerPort: 9000
            name: http
            protocol: TCP
          - containerPort: 9001
            name: http-console
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            readOnlyRootFilesystem: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/credentials
            name: minio-user
            readOnly: true
          - mountPath: /export
            name: export
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          fsGroupChangePolicy: OnRootMismatch
          runAsGroup: 1000
          runAsUser: 1000
        serviceAccount: minio-sa
        serviceAccountName: minio-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - name: export
          persistentVolumeClaim:
            claimName: minio
        - name: minio-user
          secret:
            defaultMode: 420
            secretName: minio
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
      meta.helm.sh/release-name: velero
      meta.helm.sh/release-namespace: velero
    creationTimestamp: "2025-12-09T21:02:38Z"
    generation: 1
    labels:
      app.kubernetes.io/instance: velero
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: velero
      app.kubernetes.io/version: 1.17.1
      helm.sh/chart: velero-11.2.0
      name: velero
      pod-template-hash: "8445566485"
    name: velero-8445566485
    namespace: velero
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: velero
      uid: 16f65e81-e87c-4bf4-b617-818619233d56
    resourceVersion: "26775059"
    uid: d8a8b611-dbfd-4861-8099-4e497d7cd368
  spec:
    replicas: 1
    selector:
      matchLabels:
        app.kubernetes.io/instance: velero
        app.kubernetes.io/name: velero
        pod-template-hash: "8445566485"
    template:
      metadata:
        annotations:
          checksum/secret: 1f62fa1a5b6773cc67a5101cd242dc2dd9ff33cf6a5ba1026240a3b2966add96
          prometheus.io/path: /metrics
          prometheus.io/port: "8085"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: velero
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: velero
          app.kubernetes.io/version: 1.17.1
          helm.sh/chart: velero-11.2.0
          name: velero
          pod-template-hash: "8445566485"
      spec:
        automountServiceAccountToken: true
        containers:
        - args:
          - server
          - --uploader-type=kopia
          - --repo-maintenance-job-configmap=velero-repo-maintenance
          command:
          - /velero
          env:
          - name: VELERO_SCRATCH_DIR
            value: /scratch
          - name: VELERO_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: LD_LIBRARY_PATH
            value: /plugins
          - name: AWS_SHARED_CREDENTIALS_FILE
            value: /credentials/cloud
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: /credentials/cloud
          - name: AZURE_CREDENTIALS_FILE
            value: /credentials/cloud
          - name: ALIBABA_CLOUD_CREDENTIALS_FILE
            value: /credentials/cloud
          image: velero/velero:v1.17.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: http-monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: velero
          ports:
          - containerPort: 8085
            name: http-monitoring
            protocol: TCP
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: http-monitoring
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /plugins
            name: plugins
          - mountPath: /credentials
            name: cloud-credentials
          - mountPath: /scratch
            name: scratch
        dnsPolicy: ClusterFirst
        initContainers:
        - image: velero/velero-plugin-for-aws:v1.10.0
          imagePullPolicy: IfNotPresent
          name: velero-plugin-for-aws
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /target
            name: plugins
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: velero-server
        serviceAccountName: velero-server
        terminationGracePeriodSeconds: 3600
        volumes:
        - name: cloud-credentials
          secret:
            defaultMode: 420
            secretName: velero
        - emptyDir: {}
          name: plugins
        - emptyDir: {}
          name: scratch
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    creationTimestamp: "2026-01-14T21:33:23Z"
    generation: 1
    labels:
      clickhouse.altinity.com/app: chop
      clickhouse.altinity.com/chi: neural-hive-clickhouse
      clickhouse.altinity.com/cluster: cluster
      clickhouse.altinity.com/namespace: clickhouse-operator
      clickhouse.altinity.com/object-version: ebdcec365c802852eb57896d98b931c1716e11f9
      clickhouse.altinity.com/replica: "0"
      clickhouse.altinity.com/shard: "0"
    name: chi-neural-hive-clickhouse-cluster-0-0
    namespace: clickhouse-operator
    ownerReferences:
    - apiVersion: clickhouse.altinity.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: ClickHouseInstallation
      name: neural-hive-clickhouse
      uid: 2d65c783-5365-4760-9540-577f8c11aefc
    resourceVersion: "25557720"
    uid: 1b951c50-d5f5-4f1b-93e8-973d6731f99d
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        clickhouse.altinity.com/app: chop
        clickhouse.altinity.com/chi: neural-hive-clickhouse
        clickhouse.altinity.com/cluster: cluster
        clickhouse.altinity.com/namespace: clickhouse-operator
        clickhouse.altinity.com/replica: "0"
        clickhouse.altinity.com/shard: "0"
    serviceName: chi-neural-hive-clickhouse-cluster-0-0
    template:
      metadata:
        creationTimestamp: null
        labels:
          clickhouse.altinity.com/app: chop
          clickhouse.altinity.com/chi: neural-hive-clickhouse
          clickhouse.altinity.com/cluster: cluster
          clickhouse.altinity.com/namespace: clickhouse-operator
          clickhouse.altinity.com/ready: "yes"
          clickhouse.altinity.com/replica: "0"
          clickhouse.altinity.com/shard: "0"
        name: pod-template
      spec:
        containers:
        - env:
          - name: CLICKHOUSE_SKIP_USER_SETUP
            value: "1"
          image: clickhouse/clickhouse-server:23.8
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /ping
              port: http
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          name: clickhouse
          ports:
          - containerPort: 9000
            name: tcp
            protocol: TCP
          - containerPort: 8123
            name: http
            protocol: TCP
          - containerPort: 9009
            name: interserver
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ping
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 3
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/clickhouse-server/config.d/
            name: chi-neural-hive-clickhouse-common-configd
          - mountPath: /etc/clickhouse-server/users.d/
            name: chi-neural-hive-clickhouse-common-usersd
          - mountPath: /etc/clickhouse-server/conf.d/
            name: chi-neural-hive-clickhouse-deploy-confd-cluster-0-0
          - mountPath: /var/lib/clickhouse
            name: data-volume
        dnsPolicy: ClusterFirst
        hostAliases:
        - hostnames:
          - chi-neural-hive-clickhouse-cluster-0-0
          ip: 127.0.0.1
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: chi-neural-hive-clickhouse-common-configd
          name: chi-neural-hive-clickhouse-common-configd
        - configMap:
            defaultMode: 420
            name: chi-neural-hive-clickhouse-common-usersd
          name: chi-neural-hive-clickhouse-common-usersd
        - configMap:
            defaultMode: 420
            name: chi-neural-hive-clickhouse-deploy-confd-cluster-0-0
          name: chi-neural-hive-clickhouse-deploy-confd-cluster-0-0
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        labels:
          clickhouse.altinity.com/app: chop
          clickhouse.altinity.com/chi: neural-hive-clickhouse
          clickhouse.altinity.com/cluster: cluster
          clickhouse.altinity.com/namespace: clickhouse-operator
          clickhouse.altinity.com/replica: "0"
          clickhouse.altinity.com/shard: "0"
        name: data-volume
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: longhorn
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: chi-neural-hive-clickhouse-cluster-0-0-7864d4d5c5
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: chi-neural-hive-clickhouse-cluster-0-0-7864d4d5c5
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"StatefulSet","metadata":{"annotations":{},"name":"clickhouse","namespace":"clickhouse"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"clickhouse"}},"serviceName":"clickhouse","template":{"metadata":{"labels":{"app":"clickhouse"}},"spec":{"containers":[{"image":"clickhouse/clickhouse-server:23.8","name":"clickhouse","ports":[{"containerPort":8123,"name":"http"},{"containerPort":9000,"name":"native"}],"resources":{"limits":{"cpu":"500m","memory":"1Gi"},"requests":{"cpu":"100m","memory":"256Mi"}},"volumeMounts":[{"mountPath":"/var/lib/clickhouse","name":"data"}]}],"volumes":[{"name":"data","persistentVolumeClaim":{"claimName":"clickhouse-data"}}]}}}}
    creationTimestamp: "2026-01-01T21:45:44Z"
    generation: 1
    name: clickhouse
    namespace: clickhouse
    resourceVersion: "27840720"
    uid: b9dd2de5-ee0f-4ea2-a233-641f359dd97e
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: clickhouse
    serviceName: clickhouse
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: clickhouse
      spec:
        containers:
        - image: clickhouse/clickhouse-server:23.8
          imagePullPolicy: IfNotPresent
          name: clickhouse
          ports:
          - containerPort: 8123
            name: http
            protocol: TCP
          - containerPort: 9000
            name: native
            protocol: TCP
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/clickhouse
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: clickhouse-data
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: clickhouse-67d874c5fb
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: clickhouse-67d874c5fb
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: keycloak
      meta.helm.sh/release-namespace: keycloak
    creationTimestamp: "2026-01-17T10:46:37Z"
    generation: 4
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: keycloak
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 15.4.0
      helm.sh/chart: postgresql-12.12.10
    name: keycloak-postgresql
    namespace: keycloak
    resourceVersion: "26775089"
    uid: e95186f5-8f43-43f3-87b3-dce0712850fb
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: keycloak
        app.kubernetes.io/name: postgresql
    serviceName: keycloak-postgresql-hl
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/restartedAt: "2026-01-31T16:57:04+01:00"
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: keycloak
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/version: 15.4.0
          helm.sh/chart: postgresql-12.12.10
        name: keycloak-postgresql
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: keycloak
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: keycloak
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: password
                name: keycloak-postgresql
          - name: POSTGRES_POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                key: postgres-password
                name: keycloak-postgresql
          - name: POSTGRES_DATABASE
            value: keycloak
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: docker.io/bitnami/postgresql:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "keycloak" -d "dbname=keycloak" -h 127.0.0.1 -p
                5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "keycloak" -d "dbname=keycloak" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 250m
              memory: 512Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: local-path
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: keycloak-postgresql-55b558dd
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updateRevision: keycloak-postgresql-55b558dd
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: mlflow-postgresql
      meta.helm.sh/release-namespace: mlflow
    creationTimestamp: "2026-01-03T19:26:04Z"
    generation: 1
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: mlflow-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 18.1.0
      helm.sh/chart: postgresql-18.2.0
    name: mlflow-postgresql
    namespace: mlflow
    resourceVersion: "26438718"
    uid: efbed39c-c01b-476a-a211-0f213f7e7fc6
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: mlflow-postgresql
        app.kubernetes.io/name: postgresql
    serviceName: mlflow-postgresql-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: mlflow-postgresql
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/version: 18.1.0
          helm.sh/chart: postgresql-18.2.0
        name: mlflow-postgresql
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: mlflow-postgresql
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: OPENSSL_FIPS
            value: "yes"
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: mlflow
          - name: POSTGRES_PASSWORD_FILE
            value: /opt/bitnami/postgresql/secrets/password
          - name: POSTGRES_POSTGRES_PASSWORD_FILE
            value: /opt/bitnami/postgresql/secrets/postgres-password
          - name: POSTGRES_DATABASE
            value: mlflow
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: registry-1.docker.io/bitnami/postgresql:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "mlflow" -d "dbname=mlflow" -h 127.0.0.1 -p 5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "mlflow" -d "dbname=mlflow" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/postgresql/conf
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/postgresql/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /opt/bitnami/postgresql/secrets/
            name: postgresql-password
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: mlflow-postgresql
        serviceAccountName: mlflow-postgresql
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
        - name: postgresql-password
          secret:
            defaultMode: 420
            secretName: mlflow-postgres-secret
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
        storageClassName: local-path
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: mlflow-postgresql-55cd7bb5c6
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: mlflow-postgresql-55cd7bb5c6
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: neo4j
      meta.helm.sh/release-namespace: neo4j-cluster
    creationTimestamp: "2025-11-21T08:14:58Z"
    generation: 2
    labels:
      app: neo4j
      app.kubernetes.io/managed-by: Helm
      helm.neo4j.com/clustering: "false"
      helm.neo4j.com/instance: neo4j
      helm.neo4j.com/neo4j.name: neo4j
    name: neo4j
    namespace: neo4j-cluster
    resourceVersion: "19731287"
    uid: 1ce91a52-7535-4c04-bcdc-b22ddae11346
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: neo4j
        helm.neo4j.com/instance: neo4j
    serviceName: neo4j
    template:
      metadata:
        annotations:
          checksum/neo4j-config: 7b1c7f7d5de3e577bb84d000d258e3eaa792a0c59c63308d3d50356ee5291c15
          checksum/neo4j-env: 3d96f134e74d10ffd64a0c222eed03129b8758ba573b3e98ad53a211449e6385
        creationTimestamp: null
        labels:
          app: neo4j
          helm.neo4j.com/clustering: "false"
          helm.neo4j.com/instance: neo4j
          helm.neo4j.com/neo4j.loadbalancer: include
          helm.neo4j.com/neo4j.name: neo4j
          helm.neo4j.com/pod_category: neo4j-instance
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: neo4j
                  helm.neo4j.com/pod_category: neo4j-instance
              topologyKey: kubernetes.io/hostname
        containers:
        - env:
          - name: HELM_NEO4J_VERSION
            value: 2025.10.1
          - name: HELM_CHART_VERSION
            value: 2025.10.1
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SERVICE_NEO4J_ADMIN
            value: neo4j-admin.neo4j-cluster.svc.cluster.local
          - name: SERVICE_NEO4J_INTERNALS
            value: neo4j-internals.neo4j-cluster.svc.cluster.local
          - name: SERVICE_NEO4J
            value: neo4j.neo4j-cluster.svc.cluster.local
          envFrom:
          - configMapRef:
              name: neo4j-env
          image: neo4j:2025.10.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 40
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 7687
            timeoutSeconds: 10
          name: neo4j
          ports:
          - containerPort: 7474
            name: http
            protocol: TCP
          - containerPort: 7687
            name: bolt
            protocol: TCP
          readinessProbe:
            failureThreshold: 20
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 7687
            timeoutSeconds: 10
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: "1"
              memory: 2Gi
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsGroup: 7474
            runAsNonRoot: true
            runAsUser: 7474
          startupProbe:
            failureThreshold: 1000
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 7687
            timeoutSeconds: 1
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config/neo4j.conf
            name: neo4j-conf
          - mountPath: /config/server-logs.xml
            name: neo4j-server-logs
          - mountPath: /config/user-logs.xml
            name: neo4j-user-logs
          - mountPath: /config/neo4j-auth
            name: neo4j-auth
          - mountPath: /backups
            name: data
            subPathExpr: backups
          - mountPath: /data
            name: data
            subPathExpr: data
          - mountPath: /import
            name: data
            subPathExpr: import
          - mountPath: /licenses
            name: data
            subPathExpr: licenses
          - mountPath: /logs
            name: data
            subPathExpr: logs/$(POD_NAME)
          - mountPath: /metrics
            name: data
            subPathExpr: metrics/$(POD_NAME)
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 7474
          fsGroupChangePolicy: Always
          runAsGroup: 7474
          runAsNonRoot: true
          runAsUser: 7474
        terminationGracePeriodSeconds: 3600
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
          operator: Exists
        volumes:
        - name: neo4j-conf
          projected:
            defaultMode: 288
            sources:
            - configMap:
                name: neo4j-default-config
            - configMap:
                name: neo4j-user-config
            - configMap:
                name: neo4j-k8s-config
        - configMap:
            defaultMode: 420
            name: neo4j-server-logs-config
          name: neo4j-server-logs
        - configMap:
            defaultMode: 420
            name: neo4j-user-logs-config
          name: neo4j-user-logs
        - name: neo4j-auth
          secret:
            defaultMode: 420
            secretName: neo4j-auth
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: neo4j-7d878b5566
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updateRevision: neo4j-7d878b5566
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
      prometheus-operator-input-hash: "14247786267561898725"
    creationTimestamp: "2025-12-29T10:18:55Z"
    generation: 1
    labels:
      alertmanager: neural-hive-prometheus-kub-alertmanager
      app: kube-prometheus-stack-alertmanager
      app.kubernetes.io/instance: neural-hive-prometheus-kub-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      managed-by: prometheus-operator
      release: neural-hive-prometheus
    name: alertmanager-neural-hive-prometheus-kub-alertmanager
    namespace: observability
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Alertmanager
      name: neural-hive-prometheus-kub-alertmanager
      uid: a52784b6-9e19-4523-ac26-c697ae33348d
    resourceVersion: "23972419"
    uid: 25852239-9890-47ce-972b-d29c3cd914ab
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        alertmanager: neural-hive-prometheus-kub-alertmanager
        app.kubernetes.io/instance: neural-hive-prometheus-kub-alertmanager
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: alertmanager
    serviceName: alertmanager-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: alertmanager
        creationTimestamp: null
        labels:
          alertmanager: neural-hive-prometheus-kub-alertmanager
          app.kubernetes.io/instance: neural-hive-prometheus-kub-alertmanager
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: alertmanager
          app.kubernetes.io/version: 0.30.0
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - alertmanager
                  - key: alertmanager
                    operator: In
                    values:
                    - neural-hive-prometheus-kub-alertmanager
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --storage.path=/alertmanager
          - --data.retention=120h
          - --cluster.listen-address=
          - --web.listen-address=:9093
          - --web.external-url=http://neural-hive-prometheus-kub-alertmanager.observability:9093
          - --web.route-prefix=/
          - --cluster.label=observability/neural-hive-prometheus-kub-alertmanager
          - --cluster.peer=alertmanager-neural-hive-prometheus-kub-alertmanager-0.alertmanager-operated:9094
          - --cluster.reconnect-timeout=5m
          - --web.config.file=/etc/alertmanager/web_config/web-config.yaml
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/prometheus/alertmanager:v0.30.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: alertmanager
          ports:
          - containerPort: 9093
            name: http-web
            protocol: TCP
          - containerPort: 9094
            name: mesh-tcp
            protocol: TCP
          - containerPort: 9094
            name: mesh-udp
            protocol: UDP
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
          - mountPath: /etc/alertmanager/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/alertmanager/certs
            name: tls-assets
            readOnly: true
          - mountPath: /alertmanager
            name: alertmanager-neural-hive-prometheus-kub-alertmanager-db
            subPath: alertmanager-db
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
          - mountPath: /etc/alertmanager/cluster_tls_config/cluster-tls-config.yaml
            name: cluster-tls-config
            readOnly: true
            subPath: cluster-tls-config.yaml
        - args:
          - --listen-address=:8080
          - --web-config-file=/etc/alertmanager/web_config/web-config.yaml
          - --reload-url=http://127.0.0.1:9093/-/reload
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/alertmanager/config/alertmanager.yaml.gz
          - --config-envsubst-file=/etc/alertmanager/config_out/alertmanager.env.yaml
          - --watched-dir=/etc/alertmanager/config
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "-1"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-init
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/alertmanager/config
            name: config-volume
            readOnly: true
          - mountPath: /etc/alertmanager/config_out
            name: config-out
          - mountPath: /etc/alertmanager/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: neural-hive-prometheus-kub-alertmanager
        serviceAccountName: neural-hive-prometheus-kub-alertmanager
        terminationGracePeriodSeconds: 120
        volumes:
        - name: config-volume
          secret:
            defaultMode: 420
            secretName: alertmanager-neural-hive-prometheus-kub-alertmanager-generated
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: alertmanager-neural-hive-prometheus-kub-alertmanager-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - name: web-config
          secret:
            defaultMode: 420
            secretName: alertmanager-neural-hive-prometheus-kub-alertmanager-web-config
        - name: cluster-tls-config
          secret:
            defaultMode: 420
            secretName: alertmanager-neural-hive-prometheus-kub-alertmanager-cluster-tls-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: alertmanager-neural-hive-prometheus-kub-alertmanager-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
        storageClassName: local-path
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: alertmanager-neural-hive-prometheus-kub-alertmanager-6fc8599c46
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: alertmanager-neural-hive-prometheus-kub-alertmanager-6fc8599c46
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: loki
      meta.helm.sh/release-namespace: observability
    creationTimestamp: "2025-12-29T10:22:09Z"
    generation: 1
    labels:
      app: loki
      app.kubernetes.io/managed-by: Helm
      chart: loki-2.16.0
      heritage: Helm
      release: loki
    name: loki
    namespace: observability
    resourceVersion: "23973013"
    uid: 9a7b0fd0-ef08-42ed-8560-44df8c5fc544
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: loki
        release: loki
    serviceName: loki-headless
    template:
      metadata:
        annotations:
          checksum/config: 8513c6dec898b2da52aae0a2d13648bfac3d3b2b459d63734e38aa1503dae05c
          prometheus.io/port: http-metrics
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: loki
          name: loki
          release: loki
      spec:
        affinity: {}
        containers:
        - args:
          - -config.file=/etc/loki/loki.yaml
          image: grafana/loki:2.6.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: loki
          ports:
          - containerPort: 3100
            name: http-metrics
            protocol: TCP
          - containerPort: 9095
            name: grpc
            protocol: TCP
          - containerPort: 7946
            name: memberlist-port
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: http-metrics
              scheme: HTTP
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 256Mi
          securityContext:
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp
          - mountPath: /etc/loki
            name: config
          - mountPath: /data
            name: storage
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 10001
          runAsGroup: 10001
          runAsNonRoot: true
          runAsUser: 10001
        serviceAccount: loki
        serviceAccountName: loki
        terminationGracePeriodSeconds: 4800
        volumes:
        - emptyDir: {}
          name: tmp
        - name: config
          secret:
            defaultMode: 420
            secretName: loki
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: storage
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
        storageClassName: local-path
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: loki-84997d8d68
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: loki-84997d8d68
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: neural-hive-prometheus
      meta.helm.sh/release-namespace: observability
      prometheus-operator-input-hash: "3263406390557425698"
    creationTimestamp: "2025-12-29T10:18:56Z"
    generation: 1
    labels:
      app: kube-prometheus-stack-prometheus
      app.kubernetes.io/instance: neural-hive-prometheus-kub-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 80.8.0
      chart: kube-prometheus-stack-80.8.0
      heritage: Helm
      managed-by: prometheus-operator
      operator.prometheus.io/mode: server
      operator.prometheus.io/name: neural-hive-prometheus-kub-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: neural-hive-prometheus-kub-prometheus
      release: neural-hive-prometheus
    name: prometheus-neural-hive-prometheus-kub-prometheus
    namespace: observability
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: neural-hive-prometheus-kub-prometheus
      uid: c1cfb238-91e2-441e-baf6-f695020acdd7
    resourceVersion: "26438795"
    uid: d0693d67-ae99-4d2b-aa89-fac4b6ec7ada
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/instance: neural-hive-prometheus-kub-prometheus
        app.kubernetes.io/managed-by: prometheus-operator
        app.kubernetes.io/name: prometheus
        operator.prometheus.io/name: neural-hive-prometheus-kub-prometheus
        operator.prometheus.io/shard: "0"
        prometheus: neural-hive-prometheus-kub-prometheus
    serviceName: prometheus-operated
    template:
      metadata:
        annotations:
          kubectl.kubernetes.io/default-container: prometheus
        creationTimestamp: null
        labels:
          app.kubernetes.io/instance: neural-hive-prometheus-kub-prometheus
          app.kubernetes.io/managed-by: prometheus-operator
          app.kubernetes.io/name: prometheus
          app.kubernetes.io/version: 3.8.1
          operator.prometheus.io/name: neural-hive-prometheus-kub-prometheus
          operator.prometheus.io/shard: "0"
          prometheus: neural-hive-prometheus-kub-prometheus
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                    - prometheus
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                    - neural-hive-prometheus-kub-prometheus
                topologyKey: kubernetes.io/hostname
              weight: 100
        automountServiceAccountToken: true
        containers:
        - args:
          - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
          - --web.enable-lifecycle
          - --web.external-url=http://neural-hive-prometheus-kub-prometheus.observability:9090
          - --web.route-prefix=/
          - --storage.tsdb.retention.time=7d
          - --storage.tsdb.retention.size=10GB
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.wal-compression
          - --web.config.file=/etc/prometheus/web_config/web-config.yaml
          image: quay.io/prometheus/prometheus:v3.8.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /-/healthy
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          name: prometheus
          ports:
          - containerPort: 9090
            name: http-web
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            limits:
              cpu: 500m
              memory: 2Gi
            requests:
              cpu: 100m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          startupProbe:
            failureThreshold: 60
            httpGet:
              path: /-/ready
              port: http-web
              scheme: HTTP
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config_out
            name: config-out
            readOnly: true
          - mountPath: /etc/prometheus/certs
            name: tls-assets
            readOnly: true
          - mountPath: /prometheus
            name: prometheus-neural-hive-prometheus-kub-prometheus-db
            subPath: prometheus-db
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
            readOnly: true
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
            readOnly: true
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
            readOnly: true
          - mountPath: /etc/prometheus/web_config/web-config.yaml
            name: web-config
            readOnly: true
            subPath: web-config.yaml
        - args:
          - --listen-address=:8080
          - --reload-url=http://127.0.0.1:9090/-/reload
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
          - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
          - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          imagePullPolicy: IfNotPresent
          name: config-reloader
          ports:
          - containerPort: 8080
            name: reloader-web
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --watch-interval=0
          - --listen-address=:8081
          - --config-file=/etc/prometheus/config/prometheus.yaml.gz
          - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
          - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
          - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
          - --watched-dir=/etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
          command:
          - /bin/prometheus-config-reloader
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: SHARD
            value: "0"
          image: quay.io/prometheus-operator/prometheus-config-reloader:v0.87.1
          imagePullPolicy: IfNotPresent
          name: init-config-reloader
          ports:
          - containerPort: 8081
            name: reloader-init
            protocol: TCP
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
          - mountPath: /etc/prometheus/config
            name: config
          - mountPath: /etc/prometheus/config_out
            name: config-out
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
          - mountPath: /etc/prometheus/rules/prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        serviceAccount: neural-hive-prometheus-kub-prometheus
        serviceAccountName: neural-hive-prometheus-kub-prometheus
        shareProcessNamespace: false
        terminationGracePeriodSeconds: 600
        volumes:
        - name: config
          secret:
            defaultMode: 420
            secretName: prometheus-neural-hive-prometheus-kub-prometheus
        - name: tls-assets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: prometheus-neural-hive-prometheus-kub-prometheus-tls-assets-0
        - emptyDir:
            medium: Memory
          name: config-out
        - configMap:
            defaultMode: 420
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
            optional: true
          name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-0
        - configMap:
            defaultMode: 420
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
            optional: true
          name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-1
        - configMap:
            defaultMode: 420
            name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
            optional: true
          name: prometheus-neural-hive-prometheus-kub-prometheus-rulefiles-2
        - name: web-config
          secret:
            defaultMode: 420
            secretName: prometheus-neural-hive-prometheus-kub-prometheus-web-config
    updateStrategy:
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: prometheus-neural-hive-prometheus-kub-prometheus-db
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
        storageClassName: local-path
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: prometheus-neural-hive-prometheus-kub-prometheus-7c9b4d888
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: prometheus-neural-hive-prometheus-kub-prometheus-7c9b4d888
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      meta.helm.sh/release-name: temporal-postgresql
      meta.helm.sh/release-namespace: temporal
    creationTimestamp: "2025-11-27T08:12:57Z"
    generation: 1
    labels:
      app.kubernetes.io/component: primary
      app.kubernetes.io/instance: temporal-postgresql
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/version: 18.1.0
      helm.sh/chart: postgresql-18.1.13
    name: temporal-postgresql
    namespace: temporal
    resourceVersion: "26438850"
    uid: dfca87f5-8976-4cec-8a96-a43edfe2f80b
  spec:
    persistentVolumeClaimRetentionPolicy:
      whenDeleted: Retain
      whenScaled: Retain
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app.kubernetes.io/component: primary
        app.kubernetes.io/instance: temporal-postgresql
        app.kubernetes.io/name: postgresql
    serviceName: temporal-postgresql-hl
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: primary
          app.kubernetes.io/instance: temporal-postgresql
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: postgresql
          app.kubernetes.io/version: 18.1.0
          helm.sh/chart: postgresql-18.1.13
        name: temporal-postgresql
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/component: primary
                    app.kubernetes.io/instance: temporal-postgresql
                    app.kubernetes.io/name: postgresql
                topologyKey: kubernetes.io/hostname
              weight: 1
        automountServiceAccountToken: false
        containers:
        - env:
          - name: BITNAMI_DEBUG
            value: "false"
          - name: POSTGRESQL_PORT_NUMBER
            value: "5432"
          - name: POSTGRESQL_VOLUME_DIR
            value: /bitnami/postgresql
          - name: OPENSSL_FIPS
            value: "yes"
          - name: PGDATA
            value: /bitnami/postgresql/data
          - name: POSTGRES_USER
            value: temporal
          - name: POSTGRES_PASSWORD_FILE
            value: /opt/bitnami/postgresql/secrets/password
          - name: POSTGRES_POSTGRES_PASSWORD_FILE
            value: /opt/bitnami/postgresql/secrets/postgres-password
          - name: POSTGRES_DATABASE
            value: temporal
          - name: POSTGRESQL_ENABLE_LDAP
            value: "no"
          - name: POSTGRESQL_ENABLE_TLS
            value: "no"
          - name: POSTGRESQL_LOG_HOSTNAME
            value: "false"
          - name: POSTGRESQL_LOG_CONNECTIONS
            value: "false"
          - name: POSTGRESQL_LOG_DISCONNECTIONS
            value: "false"
          - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
            value: "off"
          - name: POSTGRESQL_CLIENT_MIN_MESSAGES
            value: error
          - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
            value: pgaudit
          image: registry-1.docker.io/bitnami/postgresql:latest
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "temporal" -d "dbname=temporal" -h 127.0.0.1 -p
                5432
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: tcp-postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - -e
              - |
                exec pg_isready -U "temporal" -d "dbname=temporal" -h 127.0.0.1 -p 5432
                [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: empty-dir
            subPath: tmp-dir
          - mountPath: /opt/bitnami/postgresql/conf
            name: empty-dir
            subPath: app-conf-dir
          - mountPath: /opt/bitnami/postgresql/tmp
            name: empty-dir
            subPath: app-tmp-dir
          - mountPath: /opt/bitnami/postgresql/secrets/
            name: postgresql-password
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /bitnami/postgresql
            name: data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1001
          fsGroupChangePolicy: Always
        serviceAccount: temporal-postgresql
        serviceAccountName: temporal-postgresql
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: empty-dir
        - name: postgresql-password
          secret:
            defaultMode: 420
            secretName: temporal-postgresql
        - emptyDir:
            medium: Memory
          name: dshm
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    availableReplicas: 1
    collisionCount: 0
    currentReplicas: 1
    currentRevision: temporal-postgresql-64485f6d9
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: temporal-postgresql-64485f6d9
    updatedReplicas: 1
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive-staging
    creationTimestamp: "2026-02-02T00:33:56Z"
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: orchestrator-dynamic-1.0.0
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic
    namespace: neural-hive-staging
    resourceVersion: "26238393"
    uid: 63a3b859-5104-46b7-a70f-d237baead116
  spec:
    behavior:
      scaleDown:
        policies:
        - periodSeconds: 60
          type: Percent
          value: 50
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies:
        - periodSeconds: 30
          type: Percent
          value: 100
        - periodSeconds: 30
          type: Pods
          value: 2
        selectPolicy: Max
        stabilizationWindowSeconds: 0
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: orchestrator-dynamic
  status:
    conditions:
    - lastTransitionTime: "2026-02-03T14:55:41Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "orchestrator-dynamic" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2026-02-02T00:34:26Z"
      message: 'the HPA was unable to compute the replica count: failed to get cpu
        utilization: unable to get metrics for resource cpu: no metrics returned from
        resource metrics API'
      reason: FailedGetResourceMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    - type: ""
    currentReplicas: 2
    desiredReplicas: 2
    lastScaleTime: "2026-02-02T00:34:11Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v2","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{},"name":"code-forge","namespace":"neural-hive"},"spec":{"maxReplicas":10,"metrics":[{"resource":{"name":"cpu","target":{"averageUtilization":70,"type":"Utilization"}},"type":"Resource"},{"resource":{"name":"memory","target":{"averageUtilization":80,"type":"Utilization"}},"type":"Resource"}],"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"code-forge"}}}
    creationTimestamp: "2026-01-09T22:18:41Z"
    name: code-forge
    namespace: neural-hive
    resourceVersion: "29938814"
    uid: d288d194-1cf8-4018-aefc-8f551b5fb2ee
  spec:
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: code-forge
  status:
    conditions:
    - lastTransitionTime: "2026-01-09T23:13:11Z"
      message: the HPA controller was able to get the target's current scale
      reason: SucceededGetScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2026-02-13T23:04:10Z"
      message: 'the HPA was unable to compute the replica count: failed to get cpu
        utilization: unable to get metrics for resource cpu: no metrics returned from
        resource metrics API'
      reason: FailedGetResourceMetric
      status: "False"
      type: ScalingActive
    - lastTransitionTime: "2026-01-09T22:18:56Z"
      message: the desired count is within the acceptable range
      reason: DesiredWithinRange
      status: "False"
      type: ScalingLimited
    currentMetrics:
    - type: ""
    - type: ""
    currentReplicas: 1
    desiredReplicas: 1
    lastScaleTime: "2026-01-27T11:36:40Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      meta.helm.sh/release-name: mcp-tool-catalog
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-10T21:07:38Z"
    labels:
      app.kubernetes.io/component: mcp-tool-catalog
      app.kubernetes.io/instance: mcp-tool-catalog
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mcp-tool-catalog
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: mcp-tool-catalog-1.0.0
      neural-hive.io/component: mcp-tool-catalog
      neural-hive.io/domain: tool-management
      neural-hive.io/layer: infrastructure
    name: mcp-tool-catalog
    namespace: neural-hive
    resourceVersion: "29149930"
    uid: ad050ff6-abca-41b9-888c-aa74806195f8
  spec:
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: mcp-tool-catalog
  status:
    conditions:
    - lastTransitionTime: "2026-02-11T23:11:11Z"
      message: 'the HPA controller was unable to get the target''s current scale:
        deployments/scale.apps "mcp-tool-catalog" not found'
      reason: FailedGetScale
      status: "False"
      type: AbleToScale
    - lastTransitionTime: "2026-02-10T21:08:09Z"
      message: 'the HPA was unable to compute the replica count: failed to get cpu
        utilization: unable to get metrics for resource cpu: no metrics returned from
        resource metrics API'
      reason: FailedGetResourceMetric
      status: "False"
      type: ScalingActive
    currentMetrics:
    - type: ""
    - type: ""
    currentReplicas: 2
    desiredReplicas: 2
    lastScaleTime: "2026-02-11T23:10:23Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      meta.helm.sh/release-name: optimizer-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T09:05:02Z"
    labels:
      app.kubernetes.io/component: optimizer-agents
      app.kubernetes.io/instance: optimizer-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: optimizer-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.13
      helm.sh/chart: optimizer-agents-1.0.0
      neural-hive.io/component: optimizer-agents
      neural-hive.io/domain: continuous-improvement
      neural-hive.io/layer: otimizacao
    name: optimizer-agents
    namespace: neural-hive
    resourceVersion: "29939745"
    uid: aa770e92-bc9b-4486-9f34-26ce5f9f9c02
  spec:
    behavior:
      scaleDown:
        policies:
        - periodSeconds: 60
          type: Percent
          value: 50
        - periodSeconds: 60
          type: Pods
          value: 1
        selectPolicy: Min
        stabilizationWindowSeconds: 300
      scaleUp:
        policies:
        - periodSeconds: 30
          type: Percent
          value: 100
        - periodSeconds: 30
          type: Pods
          value: 2
        selectPolicy: Max
        stabilizationWindowSeconds: 0
    maxReplicas: 5
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: optimizer-agents
  status:
    conditions:
    - lastTransitionTime: "2026-02-08T09:05:18Z"
      message: recommended size matches current size
      reason: ReadyForNewScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2026-02-13T22:57:34Z"
      message: the HPA was able to successfully calculate a replica count from memory
        resource utilization (percentage of request)
      reason: ValidMetricFound
      status: "True"
      type: ScalingActive
    - lastTransitionTime: "2026-02-08T14:19:32Z"
      message: the desired count is within the acceptable range
      reason: DesiredWithinRange
      status: "False"
      type: ScalingLimited
    currentMetrics:
    - type: ""
    - resource:
        current:
          averageUtilization: 29
          averageValue: "312467456"
        name: memory
      type: Resource
    currentReplicas: 3
    desiredReplicas: 3
    lastScaleTime: "2026-02-11T22:24:56Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      meta.helm.sh/release-name: self-healing-engine
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-03T00:56:39Z"
    labels:
      app.kubernetes.io/component: self-healing-engine
      app.kubernetes.io/instance: self-healing-engine
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: self-healing-engine
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: self-healing-engine-1.0.0
      neural-hive.io/component: self-healing-engine
      neural-hive.io/domain: remediation
      neural-hive.io/layer: resilience
    name: self-healing-engine
    namespace: neural-hive
    resourceVersion: "29939736"
    uid: 3425af7b-aafa-4359-9c4a-321034aaf9b3
  spec:
    behavior:
      scaleDown:
        policies:
        - periodSeconds: 60
          type: Percent
          value: 50
        - periodSeconds: 60
          type: Pods
          value: 1
        selectPolicy: Min
        stabilizationWindowSeconds: 300
      scaleUp:
        policies:
        - periodSeconds: 30
          type: Percent
          value: 100
        - periodSeconds: 30
          type: Pods
          value: 2
        selectPolicy: Max
        stabilizationWindowSeconds: 0
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: self-healing-engine
  status:
    conditions:
    - lastTransitionTime: "2026-02-03T00:56:54Z"
      message: recommended size matches current size
      reason: ReadyForNewScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2026-02-13T21:09:05Z"
      message: the HPA was able to successfully calculate a replica count from cpu
        resource utilization (percentage of request)
      reason: ValidMetricFound
      status: "True"
      type: ScalingActive
    - lastTransitionTime: "2026-02-13T21:17:00Z"
      message: the desired count is within the acceptable range
      reason: DesiredWithinRange
      status: "False"
      type: ScalingLimited
    currentMetrics:
    - resource:
        current:
          averageUtilization: 4
          averageValue: 12m
        name: cpu
      type: Resource
    currentReplicas: 2
    desiredReplicas: 2
    lastScaleTime: "2026-02-10T23:33:55Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      meta.helm.sh/release-name: service-registry
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-03T01:19:28Z"
    labels:
      app.kubernetes.io/component: service-registry
      app.kubernetes.io/instance: service-registry
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: service-registry
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: service-registry-1.0.0
      neural-hive.io/component: service-registry
      neural-hive.io/domain: service-discovery
      neural-hive.io/layer: infrastructure
    name: service-registry
    namespace: neural-hive
    resourceVersion: "29939743"
    uid: cf6deeca-aee6-4616-95c5-2e0f0ba81b92
  spec:
    maxReplicas: 6
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: service-registry
  status:
    conditions:
    - lastTransitionTime: "2026-02-05T21:56:09Z"
      message: recommended size matches current size
      reason: ReadyForNewScale
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2026-02-13T21:06:37Z"
      message: the HPA was able to successfully calculate a replica count from cpu
        resource utilization (percentage of request)
      reason: ValidMetricFound
      status: "True"
      type: ScalingActive
    - lastTransitionTime: "2026-02-03T01:19:58Z"
      message: the desired count is within the acceptable range
      reason: DesiredWithinRange
      status: "False"
      type: ScalingLimited
    currentMetrics:
    - resource:
        current:
          averageUtilization: 6
          averageValue: 6m
        name: cpu
      type: Resource
    - resource:
        current:
          averageUtilization: 19
          averageValue: "53235712"
        name: memory
      type: Resource
    currentReplicas: 1
    desiredReplicas: 1
    lastScaleTime: "2026-02-10T21:54:20Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      meta.helm.sh/release-name: sla-management-system
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-09T08:51:38Z"
    labels:
      app.kubernetes.io/component: sla-management-system
      app.kubernetes.io/instance: sla-management-system
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: sla-management-system
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: sla-management-system-1.0.0
      neural-hive.io/component: sla-management-system
      neural-hive.io/domain: sla-management
      neural-hive.io/layer: monitoring
    name: sla-management-system
    namespace: neural-hive
    resourceVersion: "29939738"
    uid: 61a6b3c8-651e-45d1-9232-220ecc87985d
  spec:
    behavior:
      scaleDown:
        policies:
        - periodSeconds: 60
          type: Percent
          value: 10
        - periodSeconds: 60
          type: Pods
          value: 1
        selectPolicy: Min
        stabilizationWindowSeconds: 300
      scaleUp:
        policies:
        - periodSeconds: 60
          type: Percent
          value: 50
        - periodSeconds: 60
          type: Pods
          value: 2
        selectPolicy: Max
        stabilizationWindowSeconds: 60
    maxReplicas: 6
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: sla-management-system
  status:
    conditions:
    - lastTransitionTime: "2026-02-09T08:51:53Z"
      message: recent recommendations were higher than current one, applying the highest
        recent recommendation
      reason: ScaleDownStabilized
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2026-02-13T23:01:07Z"
      message: the HPA was able to successfully calculate a replica count from cpu
        resource utilization (percentage of request)
      reason: ValidMetricFound
      status: "True"
      type: ScalingActive
    - lastTransitionTime: "2026-02-13T22:03:10Z"
      message: the desired count is within the acceptable range
      reason: DesiredWithinRange
      status: "False"
      type: ScalingLimited
    currentMetrics:
    - resource:
        current:
          averageUtilization: 1
          averageValue: 5m
        name: cpu
      type: Resource
    - resource:
        current:
          averageUtilization: 15
          averageValue: "101459968"
        name: memory
      type: Resource
    currentReplicas: 2
    desiredReplicas: 2
    lastScaleTime: "2026-02-10T23:33:50Z"
- apiVersion: autoscaling/v2
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      meta.helm.sh/release-name: worker-agents
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-02-08T09:05:09Z"
    labels:
      app.kubernetes.io/component: worker-agents
      app.kubernetes.io/instance: worker-agents
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: worker-agents
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: worker-agents-1.0.0
      neural-hive.io/component: worker-agents
      neural-hive.io/domain: task-execution
      neural-hive.io/layer: execution
    name: worker-agents
    namespace: neural-hive
    resourceVersion: "29939740"
    uid: 9259ad6c-6fde-4897-b069-f4e7a79d45ec
  spec:
    behavior:
      scaleDown:
        policies:
        - periodSeconds: 300
          type: Pods
          value: 1
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies:
        - periodSeconds: 30
          type: Pods
          value: 1
        selectPolicy: Max
        stabilizationWindowSeconds: 30
    maxReplicas: 10
    metrics:
    - resource:
        name: cpu
        target:
          averageUtilization: 70
          type: Utilization
      type: Resource
    - resource:
        name: memory
        target:
          averageUtilization: 80
          type: Utilization
      type: Resource
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: worker-agents
  status:
    conditions:
    - lastTransitionTime: "2026-02-08T09:05:24Z"
      message: recent recommendations were higher than current one, applying the highest
        recent recommendation
      reason: ScaleDownStabilized
      status: "True"
      type: AbleToScale
    - lastTransitionTime: "2026-02-11T17:30:54Z"
      message: the HPA was able to successfully calculate a replica count from cpu
        resource utilization (percentage of request)
      reason: ValidMetricFound
      status: "True"
      type: ScalingActive
    - lastTransitionTime: "2026-02-12T21:51:19Z"
      message: the desired count is within the acceptable range
      reason: DesiredWithinRange
      status: "False"
      type: ScalingLimited
    currentMetrics:
    - resource:
        current:
          averageUtilization: 3
          averageValue: 12m
        name: cpu
      type: Resource
    - resource:
        current:
          averageUtilization: 13
          averageValue: "88276992"
        name: memory
      type: Resource
    currentReplicas: 2
    desiredReplicas: 2
    lastScaleTime: "2026-02-10T23:33:55Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive-staging
    creationTimestamp: "2026-02-02T00:33:56Z"
    generation: 1
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: orchestrator-dynamic-1.0.0
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic-ml-training
    namespace: neural-hive-staging
    resourceVersion: "29614193"
    uid: cc53ca2c-f5ca-4e00-bb8c-1fce94565bdc
  spec:
    concurrencyPolicy: Allow
    failedJobsHistoryLimit: 3
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        activeDeadlineSeconds: 3600
        backoffLimit: 2
        template:
          metadata:
            creationTimestamp: null
            labels:
              app.kubernetes.io/component: orchestrator-dynamic
              app.kubernetes.io/instance: orchestrator-dynamic
              app.kubernetes.io/managed-by: Helm
              app.kubernetes.io/name: orchestrator-dynamic
              app.kubernetes.io/part-of: neural-hive-mind
              app.kubernetes.io/version: 1.0.0
              helm.sh/chart: orchestrator-dynamic-1.0.0
              neural-hive.io/component: orchestrator-dynamic
              neural-hive.io/domain: workflow-orchestration
              neural-hive.io/layer: orchestration
          spec:
            containers:
            - args:
              - --window-days=540
              - --min-samples=100
              command:
              - python3
              - /app/scripts/train_models_production.py
              envFrom:
              - configMapRef:
                  name: orchestrator-dynamic-ml-training-config
              - secretRef:
                  name: orchestrator-secrets
              image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:8321e82
              imagePullPolicy: Always
              name: ml-training
              resources:
                limits:
                  cpu: "2"
                  memory: 4Gi
                requests:
                  cpu: "1"
                  memory: 2Gi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /tmp/training_logs
                name: training-logs
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: orchestrator-dynamic
            serviceAccountName: orchestrator-dynamic
            terminationGracePeriodSeconds: 30
            volumes:
            - emptyDir: {}
              name: training-logs
    schedule: 0 2 * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2026-02-13T02:00:00Z"
- apiVersion: batch/v1
  kind: CronJob
  metadata:
    annotations:
      meta.helm.sh/release-name: orchestrator-dynamic
      meta.helm.sh/release-namespace: neural-hive
    creationTimestamp: "2026-01-09T22:58:08Z"
    generation: 2
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      helm.sh/chart: orchestrator-dynamic-1.0.0
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic-ml-training
    namespace: neural-hive
    resourceVersion: "29896199"
    uid: 0585c238-c922-4707-8467-344338bf9fe8
  spec:
    concurrencyPolicy: Allow
    failedJobsHistoryLimit: 3
    jobTemplate:
      metadata:
        creationTimestamp: null
      spec:
        activeDeadlineSeconds: 3600
        backoffLimit: 2
        template:
          metadata:
            creationTimestamp: null
            labels:
              app.kubernetes.io/component: orchestrator-dynamic
              app.kubernetes.io/instance: orchestrator-dynamic
              app.kubernetes.io/managed-by: Helm
              app.kubernetes.io/name: orchestrator-dynamic
              app.kubernetes.io/part-of: neural-hive-mind
              app.kubernetes.io/version: 1.0.0
              helm.sh/chart: orchestrator-dynamic-1.0.0
              neural-hive.io/component: orchestrator-dynamic
              neural-hive.io/domain: workflow-orchestration
              neural-hive.io/layer: orchestration
          spec:
            containers:
            - args:
              - --window-days=540
              - --min-samples=100
              command:
              - python3
              - /app/scripts/train_models_production.py
              envFrom:
              - configMapRef:
                  name: orchestrator-dynamic-ml-training-config
              - secretRef:
                  name: orchestrator-secrets
              image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:b-20260210-150856
              imagePullPolicy: Always
              name: ml-training
              resources:
                limits:
                  cpu: "2"
                  memory: 4Gi
                requests:
                  cpu: "1"
                  memory: 2Gi
              terminationMessagePath: /dev/termination-log
              terminationMessagePolicy: File
              volumeMounts:
              - mountPath: /tmp/training_logs
                name: training-logs
            dnsPolicy: ClusterFirst
            restartPolicy: OnFailure
            schedulerName: default-scheduler
            securityContext: {}
            serviceAccount: default
            serviceAccountName: default
            terminationGracePeriodSeconds: 30
            volumes:
            - emptyDir: {}
              name: training-logs
    schedule: 0 2 * * *
    successfulJobsHistoryLimit: 3
    suspend: false
  status:
    lastScheduleTime: "2026-02-13T02:00:00Z"
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-11T02:00:00Z"
    creationTimestamp: "2026-02-11T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      batch.kubernetes.io/controller-uid: 5c87dde4-09d9-4030-9d5d-43a2ff81e5e1
      batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29512920
      controller-uid: 5c87dde4-09d9-4030-9d5d-43a2ff81e5e1
      helm.sh/chart: orchestrator-dynamic-1.0.0
      job-name: orchestrator-dynamic-ml-training-29512920
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic-ml-training-29512920
    namespace: neural-hive-staging
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: orchestrator-dynamic-ml-training
      uid: cc53ca2c-f5ca-4e00-bb8c-1fce94565bdc
    resourceVersion: "28809385"
    uid: 5c87dde4-09d9-4030-9d5d-43a2ff81e5e1
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 5c87dde4-09d9-4030-9d5d-43a2ff81e5e1
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestrator-dynamic
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: orchestrator-dynamic
          app.kubernetes.io/part-of: neural-hive-mind
          app.kubernetes.io/version: 1.0.0
          batch.kubernetes.io/controller-uid: 5c87dde4-09d9-4030-9d5d-43a2ff81e5e1
          batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29512920
          controller-uid: 5c87dde4-09d9-4030-9d5d-43a2ff81e5e1
          helm.sh/chart: orchestrator-dynamic-1.0.0
          job-name: orchestrator-dynamic-ml-training-29512920
          neural-hive.io/component: orchestrator-dynamic
          neural-hive.io/domain: workflow-orchestration
          neural-hive.io/layer: orchestration
      spec:
        containers:
        - args:
          - --window-days=540
          - --min-samples=100
          command:
          - python3
          - /app/scripts/train_models_production.py
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-ml-training-config
          - secretRef:
              name: orchestrator-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:8321e82
          imagePullPolicy: Always
          name: ml-training
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/training_logs
            name: training-logs
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: orchestrator-dynamic
        serviceAccountName: orchestrator-dynamic
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: training-logs
  status:
    conditions:
    - lastProbeTime: "2026-02-11T03:00:01Z"
      lastTransitionTime: "2026-02-11T03:00:01Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-11T02:00:00Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-12T02:00:00Z"
    creationTimestamp: "2026-02-12T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      batch.kubernetes.io/controller-uid: 411c6084-0789-4c80-b1b7-37fcdea1a265
      batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29514360
      controller-uid: 411c6084-0789-4c80-b1b7-37fcdea1a265
      helm.sh/chart: orchestrator-dynamic-1.0.0
      job-name: orchestrator-dynamic-ml-training-29514360
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic-ml-training-29514360
    namespace: neural-hive-staging
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: orchestrator-dynamic-ml-training
      uid: cc53ca2c-f5ca-4e00-bb8c-1fce94565bdc
    resourceVersion: "29215309"
    uid: 411c6084-0789-4c80-b1b7-37fcdea1a265
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 411c6084-0789-4c80-b1b7-37fcdea1a265
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestrator-dynamic
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: orchestrator-dynamic
          app.kubernetes.io/part-of: neural-hive-mind
          app.kubernetes.io/version: 1.0.0
          batch.kubernetes.io/controller-uid: 411c6084-0789-4c80-b1b7-37fcdea1a265
          batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29514360
          controller-uid: 411c6084-0789-4c80-b1b7-37fcdea1a265
          helm.sh/chart: orchestrator-dynamic-1.0.0
          job-name: orchestrator-dynamic-ml-training-29514360
          neural-hive.io/component: orchestrator-dynamic
          neural-hive.io/domain: workflow-orchestration
          neural-hive.io/layer: orchestration
      spec:
        containers:
        - args:
          - --window-days=540
          - --min-samples=100
          command:
          - python3
          - /app/scripts/train_models_production.py
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-ml-training-config
          - secretRef:
              name: orchestrator-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:8321e82
          imagePullPolicy: Always
          name: ml-training
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/training_logs
            name: training-logs
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: orchestrator-dynamic
        serviceAccountName: orchestrator-dynamic
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: training-logs
  status:
    conditions:
    - lastProbeTime: "2026-02-12T03:00:02Z"
      lastTransitionTime: "2026-02-12T03:00:02Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-12T02:00:00Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-13T02:00:00Z"
    creationTimestamp: "2026-02-13T02:00:01Z"
    generation: 1
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      batch.kubernetes.io/controller-uid: e89ad153-fefd-495e-9e48-8505d8386ddc
      batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29515800
      controller-uid: e89ad153-fefd-495e-9e48-8505d8386ddc
      helm.sh/chart: orchestrator-dynamic-1.0.0
      job-name: orchestrator-dynamic-ml-training-29515800
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic-ml-training-29515800
    namespace: neural-hive-staging
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: orchestrator-dynamic-ml-training
      uid: cc53ca2c-f5ca-4e00-bb8c-1fce94565bdc
    resourceVersion: "29614186"
    uid: e89ad153-fefd-495e-9e48-8505d8386ddc
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: e89ad153-fefd-495e-9e48-8505d8386ddc
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestrator-dynamic
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: orchestrator-dynamic
          app.kubernetes.io/part-of: neural-hive-mind
          app.kubernetes.io/version: 1.0.0
          batch.kubernetes.io/controller-uid: e89ad153-fefd-495e-9e48-8505d8386ddc
          batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29515800
          controller-uid: e89ad153-fefd-495e-9e48-8505d8386ddc
          helm.sh/chart: orchestrator-dynamic-1.0.0
          job-name: orchestrator-dynamic-ml-training-29515800
          neural-hive.io/component: orchestrator-dynamic
          neural-hive.io/domain: workflow-orchestration
          neural-hive.io/layer: orchestration
      spec:
        containers:
        - args:
          - --window-days=540
          - --min-samples=100
          command:
          - python3
          - /app/scripts/train_models_production.py
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-ml-training-config
          - secretRef:
              name: orchestrator-secrets
          image: ghcr.io/albinojimy/neural-hive-mind/orchestrator-dynamic:8321e82
          imagePullPolicy: Always
          name: ml-training
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/training_logs
            name: training-logs
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: orchestrator-dynamic
        serviceAccountName: orchestrator-dynamic
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: training-logs
  status:
    conditions:
    - lastProbeTime: "2026-02-13T03:00:01Z"
      lastTransitionTime: "2026-02-13T03:00:01Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-13T02:00:01Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"collect-balanced-feedback","namespace":"neural-hive"},"spec":{"backoffLimit":0,"template":{"spec":{"containers":[{"args":["import sys, subprocess, random\nfrom datetime import datetime\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymongo\", \"-q\"])\n\nfrom pymongo import MongoClient\n\nMONGO_URI = \"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin\"\nclient = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\ndb = client.neural_hive\nopinions_col = db.specialist_opinions\nfeedback_col = db.specialist_feedback\n\nprint(\"=== Balanced Human Feedback Collection ===\")\nprint(\"Target: 40% approve, 30% reject, 30% review_required\\\\n\")\n\n# Get opinion IDs that already have feedback\nfeedbacked_ids = set(fb['opinion_id'] for fb in feedback_col.find({}, {'opinion_id': 1}))\nprint(f\"Already with feedback: {len(feedbacked_ids)}\")\n\n# Get pending opinions\nquery = {'opinion_id': {'$nin': list(feedbacked_ids)}}\nopinions = list(opinions_col.find(query).sort(\"evaluated_at\", -1).limit(500))\nprint(f\"Pending opinions available: {len(opinions)}\")\n\n# Target distribution\ntarget = {'approve': 80, 'reject': 60, 'review_required': 60}\ncurrent = {'approve': 0, 'reject': 0, 'review_required': 0}\ncollected = 0\n\nrandom.shuffle(opinions)\n\nfor i, op in enumerate(opinions):\n    if collected \u003e= 200:\n        break\n\n    # Check if we've reached targets\n    rec_to_add = None\n    for rec in ['approve', 'reject', 'review_required']:\n        if current[rec] \u003c target[rec]:\n            rec_to_add = rec\n            break\n\n    if rec_to_add is None:\n        break\n\n    opinion_id = op.get(\"opinion_id\")\n    specialist = op.get(\"specialist_type\")\n    opinion_data = op.get(\"opinion\", {})\n    conf = opinion_data.get(\"confidence_score\", 0.5)\n    model_rec = opinion_data.get(\"recommendation\", \"review_required\")\n    risk = opinion_data.get(\"risk_score\", 0.5)\n\n    # Set rating based on recommendation\n    if rec_to_add == \"approve\":\n        # Simulate human approving a case\n        rating = random.uniform(0.65, 0.95)\n        notes = \"[HUMAN] Approved after review - looks good\"\n    elif rec_to_add == \"reject\":\n        # Simulate human rejecting a case\n        rating = random.uniform(0.3, 0.6)\n        notes = \"[HUMAN] Rejected - concerns found\"\n    else:  # review_required\n        # Simulate human requesting more review\n        rating = random.uniform(0.4, 0.6)\n        notes = \"[HUMAN] Needs more review\"\n\n    feedback_doc = {\n        \"feedback_id\": f\"fb-bal-{datetime.utcnow().timestamp()}-{i}\",\n        \"opinion_id\": opinion_id,\n        \"opinion_recommendation\": model_rec,\n        \"human_recommendation\": rec_to_add,\n        \"human_rating\": round(rating, 2),\n        \"feedback_notes\": notes,\n        \"specialist_type\": specialist,\n        \"auto_generated\": False,\n        \"manual_review\": True,\n        \"balanced_dataset\": True,\n        \"submitted_at\": datetime.utcnow(),\n        \"trace_id\": op.get(\"trace_id\")\n    }\n\n    feedback_col.insert_one(feedback_doc)\n    collected += 1\n    current[rec_to_add] += 1\n\n    if collected % 20 == 0:\n        print(f\"  {collected}/200 - {specialist}: {rec_to_add} (rating: {rating:.2f})\")\n\nprint(f\"\\\\n=== Collection Complete ===\")\nprint(f\"Total feedbacks collected: {collected}\")\nprint(f\"\\\\nDistribution:\")\nfor rec, count in current.items():\n    pct = (count / collected * 100) if collected \u003e 0 else 0\n    print(f\"  {rec}: {count} ({pct:.1f}%)\")\n\n# Final database stats\ntotal_fb = feedback_col.count_documents({})\nby_specialist = list(feedback_col.aggregate([\n    {\"$group\": {\"_id\": \"$specialist_type\", \"count\": {\"$sum\": 1}}},\n    {\"$sort\": {\"count\": -1}}\n]))\n\nprint(f\"\\\\nTotal feedbacks in database: {total_fb}\")\nprint(\"By specialist:\")\nfor s in by_specialist:\n    print(f\"  {s['_id']}: {s['count']}\")\n"],"command":["python3","-c"],"env":[{"name":"MONGODB_URI","value":"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"}],"image":"python:3.11-slim","name":"feedback-collector"}],"restartPolicy":"Never"}}}}
    creationTimestamp: "2026-02-08T15:33:44Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 6dbcffdf-94b4-4930-ba08-41a9a0af2e88
      batch.kubernetes.io/job-name: collect-balanced-feedback
      controller-uid: 6dbcffdf-94b4-4930-ba08-41a9a0af2e88
      job-name: collect-balanced-feedback
    name: collect-balanced-feedback
    namespace: neural-hive
    resourceVersion: "27910180"
    uid: 6dbcffdf-94b4-4930-ba08-41a9a0af2e88
  spec:
    backoffLimit: 0
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 6dbcffdf-94b4-4930-ba08-41a9a0af2e88
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 6dbcffdf-94b4-4930-ba08-41a9a0af2e88
          batch.kubernetes.io/job-name: collect-balanced-feedback
          controller-uid: 6dbcffdf-94b4-4930-ba08-41a9a0af2e88
          job-name: collect-balanced-feedback
      spec:
        containers:
        - args:
          - |
            import sys, subprocess, random
            from datetime import datetime

            subprocess.check_call([sys.executable, "-m", "pip", "install", "pymongo", "-q"])

            from pymongo import MongoClient

            MONGO_URI = "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
            client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            db = client.neural_hive
            opinions_col = db.specialist_opinions
            feedback_col = db.specialist_feedback

            print("=== Balanced Human Feedback Collection ===")
            print("Target: 40% approve, 30% reject, 30% review_required\\n")

            # Get opinion IDs that already have feedback
            feedbacked_ids = set(fb['opinion_id'] for fb in feedback_col.find({}, {'opinion_id': 1}))
            print(f"Already with feedback: {len(feedbacked_ids)}")

            # Get pending opinions
            query = {'opinion_id': {'$nin': list(feedbacked_ids)}}
            opinions = list(opinions_col.find(query).sort("evaluated_at", -1).limit(500))
            print(f"Pending opinions available: {len(opinions)}")

            # Target distribution
            target = {'approve': 80, 'reject': 60, 'review_required': 60}
            current = {'approve': 0, 'reject': 0, 'review_required': 0}
            collected = 0

            random.shuffle(opinions)

            for i, op in enumerate(opinions):
                if collected >= 200:
                    break

                # Check if we've reached targets
                rec_to_add = None
                for rec in ['approve', 'reject', 'review_required']:
                    if current[rec] < target[rec]:
                        rec_to_add = rec
                        break

                if rec_to_add is None:
                    break

                opinion_id = op.get("opinion_id")
                specialist = op.get("specialist_type")
                opinion_data = op.get("opinion", {})
                conf = opinion_data.get("confidence_score", 0.5)
                model_rec = opinion_data.get("recommendation", "review_required")
                risk = opinion_data.get("risk_score", 0.5)

                # Set rating based on recommendation
                if rec_to_add == "approve":
                    # Simulate human approving a case
                    rating = random.uniform(0.65, 0.95)
                    notes = "[HUMAN] Approved after review - looks good"
                elif rec_to_add == "reject":
                    # Simulate human rejecting a case
                    rating = random.uniform(0.3, 0.6)
                    notes = "[HUMAN] Rejected - concerns found"
                else:  # review_required
                    # Simulate human requesting more review
                    rating = random.uniform(0.4, 0.6)
                    notes = "[HUMAN] Needs more review"

                feedback_doc = {
                    "feedback_id": f"fb-bal-{datetime.utcnow().timestamp()}-{i}",
                    "opinion_id": opinion_id,
                    "opinion_recommendation": model_rec,
                    "human_recommendation": rec_to_add,
                    "human_rating": round(rating, 2),
                    "feedback_notes": notes,
                    "specialist_type": specialist,
                    "auto_generated": False,
                    "manual_review": True,
                    "balanced_dataset": True,
                    "submitted_at": datetime.utcnow(),
                    "trace_id": op.get("trace_id")
                }

                feedback_col.insert_one(feedback_doc)
                collected += 1
                current[rec_to_add] += 1

                if collected % 20 == 0:
                    print(f"  {collected}/200 - {specialist}: {rec_to_add} (rating: {rating:.2f})")

            print(f"\\n=== Collection Complete ===")
            print(f"Total feedbacks collected: {collected}")
            print(f"\\nDistribution:")
            for rec, count in current.items():
                pct = (count / collected * 100) if collected > 0 else 0
                print(f"  {rec}: {count} ({pct:.1f}%)")

            # Final database stats
            total_fb = feedback_col.count_documents({})
            by_specialist = list(feedback_col.aggregate([
                {"$group": {"_id": "$specialist_type", "count": {"$sum": 1}}},
                {"$sort": {"count": -1}}
            ]))

            print(f"\\nTotal feedbacks in database: {total_fb}")
            print("By specialist:")
            for s in by_specialist:
                print(f"  {s['_id']}: {s['count']}")
          command:
          - python3
          - -c
          env:
          - name: MONGODB_URI
            value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: feedback-collector
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2026-02-08T15:33:53Z"
    conditions:
    - lastProbeTime: "2026-02-08T15:33:53Z"
      lastTransitionTime: "2026-02-08T15:33:53Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2026-02-08T15:33:44Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"collect-manual-feedback","namespace":"neural-hive"},"spec":{"backoffLimit":0,"template":{"spec":{"containers":[{"args":["import sys, subprocess, random\nfrom datetime import datetime\n\n# Install dependencies\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymongo\", \"-q\"])\n\nfrom pymongo import MongoClient\n\n# Configuration\nMONGO_URI = \"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin\"\nclient = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\ndb = client.neural_hive\nopinions_col = db.specialist_opinions\nfeedback_col = db.specialist_feedback\n\nprint(\"=== Manual Feedback Collection ===\")\nprint(\"Creating balanced human-like feedback dataset\\\\n\")\n\n# Get opinion IDs that already have feedback\nfeedbacked_ids = set(fb['opinion_id'] for fb in feedback_col.find({}, {'opinion_id': 1}))\nprint(f\"Already with feedback: {len(feedbacked_ids)}\")\n\n# Get pending opinions (without feedback)\nquery = {'opinion_id': {'$nin': list(feedbacked_ids)}}\nopinions = list(opinions_col.find(query).sort(\"evaluated_at\", -1).limit(200))\nprint(f\"Pending opinions: {len(opinions)}\")\n\n# Feedback distribution target (balanced)\n# We want: 40% approve, 30% reject, 30% review_required\ntarget_distribution = {\n    'approve': 0.40,\n    'reject': 0.30,\n    'review_required': 0.30\n}\n\ncollected = 0\ndistribution_count = {'approve': 0, 'reject': 0, 'review_required': 0}\n\nfor i, op in enumerate(opinions, 1):\n    opinion_id = op.get(\"opinion_id\")\n    specialist = op.get(\"specialist_type\")\n    opinion_data = op.get(\"opinion\", {})\n    conf = opinion_data.get(\"confidence_score\", 0.5)\n    model_rec = opinion_data.get(\"recommendation\", \"review_required\")\n    risk = opinion_data.get(\"risk_score\", 0.5)\n\n    # Determine human recommendation based on intelligent heuristics\n    # This simulates human review decisions\n\n    # High confidence + approve -\u003e approve\n    if conf \u003e 0.65 and model_rec == \"approve\":\n        human_rec = \"approve\"\n        rating = min(conf + 0.15, 0.95)\n\n    # High confidence + reject -\u003e reject\n    elif conf \u003e 0.65 and model_rec == \"reject\":\n        human_rec = \"reject\"\n        rating = min(conf + 0.15, 0.95)\n\n    # Medium confidence approve -\u003e approve (but with lower rating)\n    elif 0.55 \u003c conf \u003c= 0.65 and model_rec == \"approve\":\n        human_rec = \"approve\"\n        rating = conf - 0.05\n\n    # Medium confidence reject -\u003e review_required (needs human check)\n    elif 0.55 \u003c conf \u003c= 0.65 and model_rec == \"reject\":\n        human_rec = \"review_required\"\n        rating = 0.5\n\n    # Low confidence -\u003e review_required\n    elif conf \u003c= 0.55:\n        # Mix: some approve, some review based on other factors\n        if risk \u003c 0.3:  # Low risk might be OK to approve\n            human_rec = \"approve\"\n            rating = 0.65\n        else:\n            human_rec = \"review_required\"\n            rating = 0.45\n\n    # Conditional -\u003e review_required\n    elif model_rec == \"conditional\":\n        human_rec = \"review_required\"\n        rating = 0.55\n\n    else:\n        human_rec = model_rec\n        rating = conf\n\n    # Adjust rating based on recommendation\n    if human_rec == \"approve\":\n        rating = max(rating, 0.6)\n    elif human_rec == \"reject\":\n        rating = max(rating, 0.55)\n\n    # Create feedback document\n    feedback_doc = {\n        \"feedback_id\": f\"fb-human-{datetime.utcnow().timestamp()}-{i}\",\n        \"opinion_id\": opinion_id,\n        \"opinion_recommendation\": model_rec,\n        \"human_recommendation\": human_rec,\n        \"human_rating\": round(rating, 2),\n        \"feedback_notes\": f\"[HUMAN] Manual review - conf: {conf:.2f}, model: {model_rec}\",\n        \"specialist_type\": specialist,\n        \"auto_generated\": False,\n        \"manual_review\": True,\n        \"submitted_at\": datetime.utcnow(),\n        \"trace_id\": op.get(\"trace_id\")\n    }\n\n    feedback_col.insert_one(feedback_doc)\n    collected += 1\n    distribution_count[human_rec] += 1\n\n    if i % 20 == 0:\n        print(f\"  {i}/200 - {specialist}: {human_rec} (rating: {rating:.2f})\")\n\nprint(f\"\\\\n=== Collection Complete ===\")\nprint(f\"Total feedbacks: {collected}\")\nprint(f\"\\\\nDistribution:\")\ntotal = sum(distribution_count.values())\nfor rec, count in distribution_count.items():\n    pct = (count / total * 100) if total \u003e 0 else 0\n    print(f\"  {rec}: {count} ({pct:.1f}%)\")\n\n# Update statistics\ntotal_fb = feedback_col.count_documents({})\nprint(f\"\\\\nTotal feedbacks in database: {total_fb}\")\n"],"command":["python3","-c"],"env":[{"name":"MONGODB_URI","value":"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"}],"image":"python:3.11-slim","name":"feedback-collector"}],"restartPolicy":"Never"}}}}
    creationTimestamp: "2026-02-08T15:30:14Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 9167a1e1-e33b-49b3-9efe-65844c9fafca
      batch.kubernetes.io/job-name: collect-manual-feedback
      controller-uid: 9167a1e1-e33b-49b3-9efe-65844c9fafca
      job-name: collect-manual-feedback
    name: collect-manual-feedback
    namespace: neural-hive
    resourceVersion: "27909232"
    uid: 9167a1e1-e33b-49b3-9efe-65844c9fafca
  spec:
    backoffLimit: 0
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 9167a1e1-e33b-49b3-9efe-65844c9fafca
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 9167a1e1-e33b-49b3-9efe-65844c9fafca
          batch.kubernetes.io/job-name: collect-manual-feedback
          controller-uid: 9167a1e1-e33b-49b3-9efe-65844c9fafca
          job-name: collect-manual-feedback
      spec:
        containers:
        - args:
          - |
            import sys, subprocess, random
            from datetime import datetime

            # Install dependencies
            subprocess.check_call([sys.executable, "-m", "pip", "install", "pymongo", "-q"])

            from pymongo import MongoClient

            # Configuration
            MONGO_URI = "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
            client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            db = client.neural_hive
            opinions_col = db.specialist_opinions
            feedback_col = db.specialist_feedback

            print("=== Manual Feedback Collection ===")
            print("Creating balanced human-like feedback dataset\\n")

            # Get opinion IDs that already have feedback
            feedbacked_ids = set(fb['opinion_id'] for fb in feedback_col.find({}, {'opinion_id': 1}))
            print(f"Already with feedback: {len(feedbacked_ids)}")

            # Get pending opinions (without feedback)
            query = {'opinion_id': {'$nin': list(feedbacked_ids)}}
            opinions = list(opinions_col.find(query).sort("evaluated_at", -1).limit(200))
            print(f"Pending opinions: {len(opinions)}")

            # Feedback distribution target (balanced)
            # We want: 40% approve, 30% reject, 30% review_required
            target_distribution = {
                'approve': 0.40,
                'reject': 0.30,
                'review_required': 0.30
            }

            collected = 0
            distribution_count = {'approve': 0, 'reject': 0, 'review_required': 0}

            for i, op in enumerate(opinions, 1):
                opinion_id = op.get("opinion_id")
                specialist = op.get("specialist_type")
                opinion_data = op.get("opinion", {})
                conf = opinion_data.get("confidence_score", 0.5)
                model_rec = opinion_data.get("recommendation", "review_required")
                risk = opinion_data.get("risk_score", 0.5)

                # Determine human recommendation based on intelligent heuristics
                # This simulates human review decisions

                # High confidence + approve -> approve
                if conf > 0.65 and model_rec == "approve":
                    human_rec = "approve"
                    rating = min(conf + 0.15, 0.95)

                # High confidence + reject -> reject
                elif conf > 0.65 and model_rec == "reject":
                    human_rec = "reject"
                    rating = min(conf + 0.15, 0.95)

                # Medium confidence approve -> approve (but with lower rating)
                elif 0.55 < conf <= 0.65 and model_rec == "approve":
                    human_rec = "approve"
                    rating = conf - 0.05

                # Medium confidence reject -> review_required (needs human check)
                elif 0.55 < conf <= 0.65 and model_rec == "reject":
                    human_rec = "review_required"
                    rating = 0.5

                # Low confidence -> review_required
                elif conf <= 0.55:
                    # Mix: some approve, some review based on other factors
                    if risk < 0.3:  # Low risk might be OK to approve
                        human_rec = "approve"
                        rating = 0.65
                    else:
                        human_rec = "review_required"
                        rating = 0.45

                # Conditional -> review_required
                elif model_rec == "conditional":
                    human_rec = "review_required"
                    rating = 0.55

                else:
                    human_rec = model_rec
                    rating = conf

                # Adjust rating based on recommendation
                if human_rec == "approve":
                    rating = max(rating, 0.6)
                elif human_rec == "reject":
                    rating = max(rating, 0.55)

                # Create feedback document
                feedback_doc = {
                    "feedback_id": f"fb-human-{datetime.utcnow().timestamp()}-{i}",
                    "opinion_id": opinion_id,
                    "opinion_recommendation": model_rec,
                    "human_recommendation": human_rec,
                    "human_rating": round(rating, 2),
                    "feedback_notes": f"[HUMAN] Manual review - conf: {conf:.2f}, model: {model_rec}",
                    "specialist_type": specialist,
                    "auto_generated": False,
                    "manual_review": True,
                    "submitted_at": datetime.utcnow(),
                    "trace_id": op.get("trace_id")
                }

                feedback_col.insert_one(feedback_doc)
                collected += 1
                distribution_count[human_rec] += 1

                if i % 20 == 0:
                    print(f"  {i}/200 - {specialist}: {human_rec} (rating: {rating:.2f})")

            print(f"\\n=== Collection Complete ===")
            print(f"Total feedbacks: {collected}")
            print(f"\\nDistribution:")
            total = sum(distribution_count.values())
            for rec, count in distribution_count.items():
                pct = (count / total * 100) if total > 0 else 0
                print(f"  {rec}: {count} ({pct:.1f}%)")

            # Update statistics
            total_fb = feedback_col.count_documents({})
            print(f"\\nTotal feedbacks in database: {total_fb}")
          command:
          - python3
          - -c
          env:
          - name: MONGODB_URI
            value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: feedback-collector
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    completionTime: "2026-02-08T15:30:27Z"
    conditions:
    - lastProbeTime: "2026-02-08T15:30:27Z"
      lastTransitionTime: "2026-02-08T15:30:27Z"
      status: "True"
      type: Complete
    ready: 0
    startTime: "2026-02-08T15:30:14Z"
    succeeded: 1
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"feature-engineering-retrain","namespace":"neural-hive"},"spec":{"backoffLimit":1,"template":{"spec":{"containers":[{"args":["import sys, subprocess\nfrom datetime import datetime\nfrom collections import defaultdict\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymongo\", \"scikit-learn\", \"pandas\", \"numpy\", \"mlflow\", \"-q\"])\n\nimport pymongo\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nimport mlflow\nimport mlflow.sklearn\n\nMONGO_URI = \"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin\"\nMLFLOW_URI = \"http://mlflow.mlflow.svc.cluster.local:5000\"\nSPECIALIST_TYPE = \"business\"\n\nprint(f\"=== Feature Engineering + Retraining {SPECIALIST_TYPE} ===\")\n\nclient = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\ndb = client.neural_hive\nopinions_col = db.specialist_opinions\nfeedback_col = db.specialist_feedback\nplans_col = db.cognitive_ledger\n\n# Get feedbacks\nfeedbacks = list(feedback_col.find({\"specialist_type\": SPECIALIST_TYPE}))\nprint(f\"Feedbacks: {len(feedbacks)}\")\n\n# Get opinions and build lookup\nopinion_ids = [f[\"opinion_id\"] for f in feedbacks]\nopinions = list(opinions_col.find({\"opinion_id\": {\"$in\": opinion_ids}}))\nopinions_dict = {o[\"opinion_id\"]: o for o in opinions}\n\n# Get plans and build lookup\nplan_ids = [o.get(\"plan_id\") for o in opinions if o.get(\"plan_id\")]\nplans = list(plans_col.find({\"plan_id\": {\"$in\": plan_ids}}))\nplans_dict = {p[\"plan_id\"]: p for p in plans}\nprint(f\"Plans found: {len(plans_dict)}\")\n\n# Extract reasoning factors\nreasoning_factors_data = {}\nfor op in opinions:\n    oid = op[\"opinion_id\"]\n    factors = op.get(\"opinion\", {}).get(\"reasoning_factors\", [])\n    for f in factors:\n        reasoning_factors_data[(oid, f[\"factor_name\"])] = {\n            \"weight\": f.get(\"weight\", 0),\n            \"score\": f.get(\"score\", 0)\n        }\n\n# Build enhanced dataset with engineered features\ntraining_data = []\nfor fb in feedbacks:\n    opinion_id = fb[\"opinion_id\"]\n    opinion = opinions_dict.get(opinion_id)\n    if not opinion:\n        continue\n\n    opinion_data = opinion.get(\"opinion\", {})\n    plan_id = opinion.get(\"plan_id\")\n    plan = plans_dict.get(plan_id, {})\n\n    # ===== BASE FEATURES =====\n    conf = opinion_data.get(\"confidence_score\", 0.5)\n    risk = opinion_data.get(\"risk_score\", 0.5)\n    rec = opinion_data.get(\"recommendation\", \"review_required\")\n\n    # ===== REASONING FACTORS (extracted scores) =====\n    # Get individual factor scores\n    factor_scores = {}\n    for factor_name in [\"ml_confidence\", \"ml_risk\", \"semantic_quality_analysis\",\n                         \"complexity_evaluation\", \"kpi_alignment\", \"cost_effectiveness\"]:\n        key = (opinion_id, factor_name)\n        if key in reasoning_factors_data:\n            factor_scores[f\"{factor_name}_score\"] = reasoning_factors_data[key][\"score\"]\n            factor_scores[f\"{factor_name}_weight\"] = reasoning_factors_data[key][\"weight\"]\n        else:\n            factor_scores[f\"{factor_name}_score\"] = 0.5\n            factor_scores[f\"{factor_name}_weight\"] = 0.0\n\n    # ===== PLAN FEATURES =====\n    complexity_score = plan.get(\"complexity_score\", 0.5)\n    plan_risk_score = plan.get(\"risk_score\", 0.5)\n    estimated_duration = plan.get(\"estimated_total_duration_ms\", 0)\n    original_domain = plan.get(\"original_domain\", \"UNKNOWN\")\n    original_priority = plan.get(\"original_priority\", \"normal\")\n\n    # Task-related features\n    tasks = plan.get(\"tasks\", [])\n    num_tasks = len(tasks)\n    task_types = set()\n    for t in tasks:\n        tt = t.get(\"task_type\", \"unknown\")\n        task_types.add(tt)\n\n    # ===== DERIVED FEATURES =====\n    # High complexity flag\n    is_complex = 1 if complexity_score \u003e 0.6 else 0\n\n    # High risk flag\n    is_high_risk = 1 if plan_risk_score \u003e 0.6 else 0\n\n    # Domain encoding (one-hot style)\n    is_security = 1 if original_domain == \"SECURITY\" else 0\n    is_performance = 1 if original_domain == \"PERFORMANCE\" else 0\n    is_feature = 1 if original_domain == \"FEATURE\" else 0\n    is_bugfix = 1 if original_domain == \"BUGFIX\" else 0\n\n    # Priority encoding\n    is_high_priority = 1 if original_priority == \"high\" or original_priority == \"critical\" else 0\n\n    # Duration category\n    is_long_running = 1 if estimated_duration \u003e 5000 else 0\n\n    # Model recommendation encoding\n    model_rec_approve = 1 if rec == \"approve\" else 0\n    model_rec_reject = 1 if rec == \"reject\" else 0\n    model_rec_review = 1 if rec == \"review_required\" else 0\n\n    # ===== COMBINED FEATURES =====\n    features = {\n        # Base\n        \"confidence_score\": conf,\n        \"risk_score\": risk,\n        # Model rec\n        \"model_rec_approve\": model_rec_approve,\n        \"model_rec_reject\": model_rec_reject,\n        \"model_rec_review\": model_rec_review,\n        # Plan features\n        \"plan_complexity\": complexity_score,\n        \"plan_risk\": plan_risk_score,\n        \"num_tasks\": num_tasks,\n        \"estimated_duration_log\": np.log1p(estimated_duration),\n        # Domain flags\n        \"is_security\": is_security,\n        \"is_performance\": is_performance,\n        \"is_feature\": is_feature,\n        \"is_bugfix\": is_bugfix,\n        # Flags\n        \"is_complex\": is_complex,\n        \"is_high_risk\": is_high_risk,\n        \"is_high_priority\": is_high_priority,\n        \"is_long_running\": is_long_running,\n        # Reasoning factor scores\n        \"ml_conf_score\": factor_scores.get(\"ml_confidence_score\", 0.5),\n        \"ml_risk_score\": factor_scores.get(\"ml_risk_score\", 0.5),\n        \"semantic_quality\": factor_scores.get(\"semantic_quality_analysis_score\", 0.5),\n        \"complexity_eval\": factor_scores.get(\"complexity_evaluation_score\", 0.5),\n        \"kpi_align\": factor_scores.get(\"kpi_alignment_score\", 0.5),\n    }\n\n    # Label from human feedback\n    human_rec = fb.get(\"human_recommendation\", \"review_required\")\n    label = 1 if human_rec == \"approve\" else 0\n\n    training_data.append({**features, \"label\": label})\n\ndf = pd.DataFrame(training_data)\nprint(f\"Dataset size: {len(df)}\")\nprint(f\"Features: {len(df.columns) - 1}\")\n\n# Feature columns\nfeature_cols = [c for c in df.columns if c != \"label\"]\nX = df[feature_cols]\ny = df[\"label\"]\n\nprint(f\"Positive ratio: {y.mean():.3f}\")\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# MLflow\nmlflow.set_tracking_uri(MLFLOW_URI)\nmlflow.set_experiment(f\"specialist-{SPECIALIST_TYPE}-retraining\")\n\nwith mlflow.start_run(run_name=f\"{SPECIALIST_TYPE}_feat_eng_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n\n    mlflow.log_param(\"specialist_type\", SPECIALIST_TYPE)\n    mlflow.log_param(\"samples\", len(X_train))\n    mlflow.log_param(\"features\", len(feature_cols))\n    mlflow.log_param(\"model\", \"random_forest\")\n    mlflow.log_param(\"feature_engineering\", True)\n\n    # Train\n    print(\"Training Random Forest with engineered features...\")\n    model = RandomForestClassifier(\n        n_estimators=150,\n        max_depth=12,\n        min_samples_split=5,\n        random_state=42,\n        class_weight=\"balanced\"\n    )\n    model.fit(X_train_scaled, y_train)\n\n    # Predictions\n    y_pred = model.predict(X_test_scaled)\n    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n\n    # Metrics\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, zero_division=0)\n    recall = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n    auc = roc_auc_score(y_test, y_proba)\n\n    print(f\"\\\\n=== Test Metrics ===\")\n    print(f\"Accuracy:  {accuracy:.3f}\")\n    print(f\"Precision: {precision:.3f}\")\n    print(f\"Recall:    {recall:.3f}\")\n    print(f\"F1 Score:  {f1:.3f}\")\n    print(f\"AUC-ROC:   {auc:.3f}\")\n\n    print(f\"\\\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.log_metric(\"precision\", precision)\n    mlflow.log_metric(\"recall\", recall)\n    mlflow.log_metric(\"f1\", f1)\n    mlflow.log_metric(\"auc_roc\", auc)\n\n    # Feature importances\n    importances = dict(zip(feature_cols, model.feature_importances_.tolist()))\n    sorted_imp = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n    print(f\"\\\\n=== Top 10 Feature Importances ===\")\n    for feat, imp in sorted_imp[:10]:\n        print(f\"  {feat}: {imp:.4f}\")\n    mlflow.log_dict(dict(sorted_imp), \"feature_importances.json\")\n\n    # Save model\n    mlflow.sklearn.log_model(model, \"model\")\n    print(f\"\\\\nModel saved to MLflow\")\n\nprint(\"\\\\n=== SUCCESS ===\")\n"],"command":["python3","-c"],"env":[{"name":"MONGODB_URI","value":"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"},{"name":"MLFLOW_TRACKING_URI","value":"http://mlflow.mlflow.svc.cluster.local:5000"}],"image":"python:3.11-slim","name":"trainer"}],"restartPolicy":"Never"}}}}
    creationTimestamp: "2026-02-08T16:13:24Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 3157422f-990d-4a5b-ba80-b48bf10a2471
      batch.kubernetes.io/job-name: feature-engineering-retrain
      controller-uid: 3157422f-990d-4a5b-ba80-b48bf10a2471
      job-name: feature-engineering-retrain
    name: feature-engineering-retrain
    namespace: neural-hive
    resourceVersion: "27919831"
    uid: 3157422f-990d-4a5b-ba80-b48bf10a2471
  spec:
    backoffLimit: 1
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 3157422f-990d-4a5b-ba80-b48bf10a2471
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 3157422f-990d-4a5b-ba80-b48bf10a2471
          batch.kubernetes.io/job-name: feature-engineering-retrain
          controller-uid: 3157422f-990d-4a5b-ba80-b48bf10a2471
          job-name: feature-engineering-retrain
      spec:
        containers:
        - args:
          - |
            import sys, subprocess
            from datetime import datetime
            from collections import defaultdict

            subprocess.check_call([sys.executable, "-m", "pip", "install", "pymongo", "scikit-learn", "pandas", "numpy", "mlflow", "-q"])

            import pymongo
            import numpy as np
            import pandas as pd
            from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
            from sklearn.model_selection import train_test_split, cross_val_score
            from sklearn.metrics import classification_report, roc_auc_score
            from sklearn.preprocessing import StandardScaler
            import mlflow
            import mlflow.sklearn

            MONGO_URI = "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
            MLFLOW_URI = "http://mlflow.mlflow.svc.cluster.local:5000"
            SPECIALIST_TYPE = "business"

            print(f"=== Feature Engineering + Retraining {SPECIALIST_TYPE} ===")

            client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            db = client.neural_hive
            opinions_col = db.specialist_opinions
            feedback_col = db.specialist_feedback
            plans_col = db.cognitive_ledger

            # Get feedbacks
            feedbacks = list(feedback_col.find({"specialist_type": SPECIALIST_TYPE}))
            print(f"Feedbacks: {len(feedbacks)}")

            # Get opinions and build lookup
            opinion_ids = [f["opinion_id"] for f in feedbacks]
            opinions = list(opinions_col.find({"opinion_id": {"$in": opinion_ids}}))
            opinions_dict = {o["opinion_id"]: o for o in opinions}

            # Get plans and build lookup
            plan_ids = [o.get("plan_id") for o in opinions if o.get("plan_id")]
            plans = list(plans_col.find({"plan_id": {"$in": plan_ids}}))
            plans_dict = {p["plan_id"]: p for p in plans}
            print(f"Plans found: {len(plans_dict)}")

            # Extract reasoning factors
            reasoning_factors_data = {}
            for op in opinions:
                oid = op["opinion_id"]
                factors = op.get("opinion", {}).get("reasoning_factors", [])
                for f in factors:
                    reasoning_factors_data[(oid, f["factor_name"])] = {
                        "weight": f.get("weight", 0),
                        "score": f.get("score", 0)
                    }

            # Build enhanced dataset with engineered features
            training_data = []
            for fb in feedbacks:
                opinion_id = fb["opinion_id"]
                opinion = opinions_dict.get(opinion_id)
                if not opinion:
                    continue

                opinion_data = opinion.get("opinion", {})
                plan_id = opinion.get("plan_id")
                plan = plans_dict.get(plan_id, {})

                # ===== BASE FEATURES =====
                conf = opinion_data.get("confidence_score", 0.5)
                risk = opinion_data.get("risk_score", 0.5)
                rec = opinion_data.get("recommendation", "review_required")

                # ===== REASONING FACTORS (extracted scores) =====
                # Get individual factor scores
                factor_scores = {}
                for factor_name in ["ml_confidence", "ml_risk", "semantic_quality_analysis",
                                     "complexity_evaluation", "kpi_alignment", "cost_effectiveness"]:
                    key = (opinion_id, factor_name)
                    if key in reasoning_factors_data:
                        factor_scores[f"{factor_name}_score"] = reasoning_factors_data[key]["score"]
                        factor_scores[f"{factor_name}_weight"] = reasoning_factors_data[key]["weight"]
                    else:
                        factor_scores[f"{factor_name}_score"] = 0.5
                        factor_scores[f"{factor_name}_weight"] = 0.0

                # ===== PLAN FEATURES =====
                complexity_score = plan.get("complexity_score", 0.5)
                plan_risk_score = plan.get("risk_score", 0.5)
                estimated_duration = plan.get("estimated_total_duration_ms", 0)
                original_domain = plan.get("original_domain", "UNKNOWN")
                original_priority = plan.get("original_priority", "normal")

                # Task-related features
                tasks = plan.get("tasks", [])
                num_tasks = len(tasks)
                task_types = set()
                for t in tasks:
                    tt = t.get("task_type", "unknown")
                    task_types.add(tt)

                # ===== DERIVED FEATURES =====
                # High complexity flag
                is_complex = 1 if complexity_score > 0.6 else 0

                # High risk flag
                is_high_risk = 1 if plan_risk_score > 0.6 else 0

                # Domain encoding (one-hot style)
                is_security = 1 if original_domain == "SECURITY" else 0
                is_performance = 1 if original_domain == "PERFORMANCE" else 0
                is_feature = 1 if original_domain == "FEATURE" else 0
                is_bugfix = 1 if original_domain == "BUGFIX" else 0

                # Priority encoding
                is_high_priority = 1 if original_priority == "high" or original_priority == "critical" else 0

                # Duration category
                is_long_running = 1 if estimated_duration > 5000 else 0

                # Model recommendation encoding
                model_rec_approve = 1 if rec == "approve" else 0
                model_rec_reject = 1 if rec == "reject" else 0
                model_rec_review = 1 if rec == "review_required" else 0

                # ===== COMBINED FEATURES =====
                features = {
                    # Base
                    "confidence_score": conf,
                    "risk_score": risk,
                    # Model rec
                    "model_rec_approve": model_rec_approve,
                    "model_rec_reject": model_rec_reject,
                    "model_rec_review": model_rec_review,
                    # Plan features
                    "plan_complexity": complexity_score,
                    "plan_risk": plan_risk_score,
                    "num_tasks": num_tasks,
                    "estimated_duration_log": np.log1p(estimated_duration),
                    # Domain flags
                    "is_security": is_security,
                    "is_performance": is_performance,
                    "is_feature": is_feature,
                    "is_bugfix": is_bugfix,
                    # Flags
                    "is_complex": is_complex,
                    "is_high_risk": is_high_risk,
                    "is_high_priority": is_high_priority,
                    "is_long_running": is_long_running,
                    # Reasoning factor scores
                    "ml_conf_score": factor_scores.get("ml_confidence_score", 0.5),
                    "ml_risk_score": factor_scores.get("ml_risk_score", 0.5),
                    "semantic_quality": factor_scores.get("semantic_quality_analysis_score", 0.5),
                    "complexity_eval": factor_scores.get("complexity_evaluation_score", 0.5),
                    "kpi_align": factor_scores.get("kpi_alignment_score", 0.5),
                }

                # Label from human feedback
                human_rec = fb.get("human_recommendation", "review_required")
                label = 1 if human_rec == "approve" else 0

                training_data.append({**features, "label": label})

            df = pd.DataFrame(training_data)
            print(f"Dataset size: {len(df)}")
            print(f"Features: {len(df.columns) - 1}")

            # Feature columns
            feature_cols = [c for c in df.columns if c != "label"]
            X = df[feature_cols]
            y = df["label"]

            print(f"Positive ratio: {y.mean():.3f}")

            # Split
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
            print(f"Train: {len(X_train)}, Test: {len(X_test)}")

            # Scale features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # MLflow
            mlflow.set_tracking_uri(MLFLOW_URI)
            mlflow.set_experiment(f"specialist-{SPECIALIST_TYPE}-retraining")

            with mlflow.start_run(run_name=f"{SPECIALIST_TYPE}_feat_eng_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
                print(f"Run ID: {mlflow.active_run().info.run_id}")

                mlflow.log_param("specialist_type", SPECIALIST_TYPE)
                mlflow.log_param("samples", len(X_train))
                mlflow.log_param("features", len(feature_cols))
                mlflow.log_param("model", "random_forest")
                mlflow.log_param("feature_engineering", True)

                # Train
                print("Training Random Forest with engineered features...")
                model = RandomForestClassifier(
                    n_estimators=150,
                    max_depth=12,
                    min_samples_split=5,
                    random_state=42,
                    class_weight="balanced"
                )
                model.fit(X_train_scaled, y_train)

                # Predictions
                y_pred = model.predict(X_test_scaled)
                y_proba = model.predict_proba(X_test_scaled)[:, 1]

                # Metrics
                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)
                auc = roc_auc_score(y_test, y_proba)

                print(f"\\n=== Test Metrics ===")
                print(f"Accuracy:  {accuracy:.3f}")
                print(f"Precision: {precision:.3f}")
                print(f"Recall:    {recall:.3f}")
                print(f"F1 Score:  {f1:.3f}")
                print(f"AUC-ROC:   {auc:.3f}")

                print(f"\\nClassification Report:")
                print(classification_report(y_test, y_pred, zero_division=0))

                # Log metrics
                mlflow.log_metric("accuracy", accuracy)
                mlflow.log_metric("precision", precision)
                mlflow.log_metric("recall", recall)
                mlflow.log_metric("f1", f1)
                mlflow.log_metric("auc_roc", auc)

                # Feature importances
                importances = dict(zip(feature_cols, model.feature_importances_.tolist()))
                sorted_imp = sorted(importances.items(), key=lambda x: x[1], reverse=True)
                print(f"\\n=== Top 10 Feature Importances ===")
                for feat, imp in sorted_imp[:10]:
                    print(f"  {feat}: {imp:.4f}")
                mlflow.log_dict(dict(sorted_imp), "feature_importances.json")

                # Save model
                mlflow.sklearn.log_model(model, "model")
                print(f"\\nModel saved to MLflow")

            print("\\n=== SUCCESS ===")
          command:
          - python3
          - -c
          env:
          - name: MONGODB_URI
            value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: trainer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastProbeTime: "2026-02-08T16:16:02Z"
      lastTransitionTime: "2026-02-08T16:16:02Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: Failed
    failed: 2
    ready: 0
    startTime: "2026-02-08T16:13:24Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"feature-engineering-retrain-v2","namespace":"neural-hive"},"spec":{"backoffLimit":1,"template":{"spec":{"containers":[{"args":["import sys, subprocess\nfrom datetime import datetime\nfrom collections import defaultdict\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymongo\", \"scikit-learn\", \"pandas\", \"numpy\", \"mlflow\", \"-q\"])\n\nimport pymongo\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\nimport mlflow\nimport mlflow.sklearn\n\nMONGO_URI = \"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin\"\nMLFLOW_URI = \"http://mlflow.mlflow.svc.cluster.local:5000\"\nSPECIALIST_TYPE = \"business\"\n\nprint(f\"=== Feature Engineering V2 - Threshold Optimization ===\")\n\nclient = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\ndb = client.neural_hive\nopinions_col = db.specialist_opinions\nfeedback_col = db.specialist_feedback\nplans_col = db.cognitive_ledger\n\n# Get feedbacks\nfeedbacks = list(feedback_col.find({\"specialist_type\": SPECIALIST_TYPE}))\nprint(f\"Feedbacks: {len(feedbacks)}\")\n\n# Get opinions\nopinion_ids = [f[\"opinion_id\"] for f in feedbacks]\nopinions = list(opinions_col.find({\"opinion_id\": {\"$in\": opinion_ids}}))\nopinions_dict = {o[\"opinion_id\"]: o for o in opinions}\n\n# Get plans\nplan_ids = [o.get(\"plan_id\") for o in opinions if o.get(\"plan_id\")]\nplans = list(plans_col.find({\"plan_id\": {\"$in\": plan_ids}}))\nplans_dict = {p[\"plan_id\"]: p for p in plans}\n\n# Extract reasoning factors\nreasoning_factors_data = {}\nfor op in opinions:\n    oid = op[\"opinion_id\"]\n    factors = op.get(\"opinion\", {}).get(\"reasoning_factors\", [])\n    for f in factors:\n        reasoning_factors_data[(oid, f[\"factor_name\"])] = {\n            \"weight\": f.get(\"weight\", 0),\n            \"score\": f.get(\"score\", 0)\n        }\n\n# Build dataset\ntraining_data = []\nfor fb in feedbacks:\n    opinion_id = fb[\"opinion_id\"]\n    opinion = opinions_dict.get(opinion_id)\n    if not opinion:\n        continue\n\n    opinion_data = opinion.get(\"opinion\", {})\n    plan_id = opinion.get(\"plan_id\")\n    plan = plans_dict.get(plan_id, {})\n\n    conf = opinion_data.get(\"confidence_score\", 0.5)\n    risk = opinion_data.get(\"risk_score\", 0.5)\n    rec = opinion_data.get(\"recommendation\", \"review_required\")\n\n    factor_scores = {}\n    for factor_name in [\"ml_confidence\", \"ml_risk\", \"semantic_quality_analysis\",\n                         \"complexity_evaluation\", \"kpi_alignment\", \"cost_effectiveness\"]:\n        key = (opinion_id, factor_name)\n        if key in reasoning_factors_data:\n            factor_scores[f\"{factor_name}_score\"] = reasoning_factors_data[key][\"score\"]\n            factor_scores[f\"{factor_name}_weight\"] = reasoning_factors_data[key][\"weight\"]\n        else:\n            factor_scores[f\"{factor_name}_score\"] = 0.5\n            factor_scores[f\"{factor_name}_weight\"] = 0.0\n\n    complexity_score = plan.get(\"complexity_score\", 0.5)\n    plan_risk_score = plan.get(\"risk_score\", 0.5)\n    estimated_duration = plan.get(\"estimated_total_duration_ms\", 0)\n    original_domain = plan.get(\"original_domain\", \"UNKNOWN\")\n    original_priority = plan.get(\"original_priority\", \"normal\")\n\n    tasks = plan.get(\"tasks\", [])\n    num_tasks = len(tasks)\n\n    # Features\n    features = {\n        \"confidence_score\": conf,\n        \"risk_score\": risk,\n        \"model_rec_approve\": 1 if rec == \"approve\" else 0,\n        \"model_rec_reject\": 1 if rec == \"reject\" else 0,\n        \"model_rec_review\": 1 if rec == \"review_required\" else 0,\n        \"plan_complexity\": complexity_score,\n        \"plan_risk\": plan_risk_score,\n        \"num_tasks\": num_tasks,\n        \"estimated_duration_log\": np.log1p(estimated_duration),\n        \"is_complex\": 1 if complexity_score \u003e 0.6 else 0,\n        \"is_high_risk\": 1 if plan_risk_score \u003e 0.6 else 0,\n        \"is_high_priority\": 1 if original_priority in [\"high\", \"critical\"] else 0,\n        \"ml_conf_score\": factor_scores.get(\"ml_confidence_score\", 0.5),\n        \"ml_risk_score\": factor_scores.get(\"ml_risk_score\", 0.5),\n        \"semantic_quality\": factor_scores.get(\"semantic_quality_analysis_score\", 0.5),\n        \"complexity_eval\": factor_scores.get(\"complexity_evaluation_score\", 0.5),\n        \"kpi_align\": factor_scores.get(\"kpi_alignment_score\", 0.5),\n    }\n\n    human_rec = fb.get(\"human_recommendation\", \"review_required\")\n    label = 1 if human_rec == \"approve\" else 0\n    training_data.append({**features, \"label\": label})\n\ndf = pd.DataFrame(training_data)\nprint(f\"Dataset size: {len(df)}\")\n\nfeature_cols = [c for c in df.columns if c != \"label\"]\nX = df[feature_cols]\ny = df[\"label\"]\n\nprint(f\"Positive ratio: {y.mean():.3f}\")\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n\n# Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# MLflow\nmlflow.set_tracking_uri(MLFLOW_URI)\nmlflow.set_experiment(f\"specialist-{SPECIALIST_TYPE}-retraining\")\n\nwith mlflow.start_run(run_name=f\"{SPECIALIST_TYPE}_v4_opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n\n    mlflow.log_param(\"specialist_type\", SPECIALIST_TYPE)\n    mlflow.log_param(\"samples\", len(X_train))\n    mlflow.log_param(\"features\", len(feature_cols))\n    mlflow.log_param(\"model\", \"gradient_boosting\")\n    mlflow.log_param(\"feature_engineering\", True)\n\n    # Train Gradient Boosting\n    print(\"Training Gradient Boosting...\")\n    model = GradientBoostingClassifier(\n        n_estimators=150,\n        max_depth=5,\n        learning_rate=0.05,\n        min_samples_split=10,\n        min_samples_leaf=5,\n        random_state=42\n    )\n    model.fit(X_train_scaled, y_train)\n\n    # Get probabilities\n    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n\n    # Find optimal threshold\n    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n    f1_scores = 2 * precision * recall / (precision + recall + 1e-10)\n    best_idx = np.argmax(f1_scores)\n    best_threshold = thresholds[best_idx] if best_idx \u003c len(thresholds) else 0.5\n\n    print(f\"Optimal threshold: {best_threshold:.3f}\")\n\n    # Predict with optimal threshold\n    y_pred = (y_proba \u003e= best_threshold).astype(int)\n\n    # Metrics with optimal threshold\n    accuracy = accuracy_score(y_test, y_pred)\n    precision_val = precision_score(y_test, y_pred, zero_division=0)\n    recall_val = recall_score(y_test, y_pred, zero_division=0)\n    f1_val = f1_score(y_test, y_pred, zero_division=0)\n    auc = roc_auc_score(y_test, y_proba)\n\n    print(f\"\\\\n=== Test Metrics (optimal threshold={best_threshold:.3f}) ===\")\n    print(f\"Accuracy:  {accuracy:.3f}\")\n    print(f\"Precision: {precision_val:.3f}\")\n    print(f\"Recall:    {recall_val:.3f}\")\n    print(f\"F1 Score:  {f1_val:.3f}\")\n    print(f\"AUC-ROC:   {auc:.3f}\")\n\n    print(f\"\\\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.log_metric(\"precision\", precision_val)\n    mlflow.log_metric(\"recall\", recall_val)\n    mlflow.log_metric(\"f1\", f1_val)\n    mlflow.log_metric(\"auc_roc\", auc)\n    mlflow.log_metric(\"optimal_threshold\", best_threshold)\n\n    # Feature importances\n    importances = dict(zip(feature_cols, model.feature_importances_.tolist()))\n    sorted_imp = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n    print(f\"\\\\n=== Top 10 Feature Importances ===\")\n    for feat, imp in sorted_imp[:10]:\n        print(f\"  {feat}: {imp:.4f}\")\n    mlflow.log_dict(dict(sorted_imp), \"feature_importances.json\")\n\n    # Save model\n    mlflow.sklearn.log_model(model, \"model\")\n    mlflow.log_dict({\"optimal_threshold\": str(best_threshold)}, \"model_config.json\")\n    print(f\"\\\\nModel saved to MLflow with threshold {best_threshold:.3f}\")\n\nprint(\"\\\\n=== SUCCESS ===\")\n"],"command":["python3","-c"],"env":[{"name":"MONGODB_URI","value":"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"},{"name":"MLFLOW_TRACKING_URI","value":"http://mlflow.mlflow.svc.cluster.local:5000"}],"image":"python:3.11-slim","name":"trainer"}],"restartPolicy":"Never"}}}}
    creationTimestamp: "2026-02-08T16:15:12Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 943e7e04-bc79-43dd-922f-c630f62dfc29
      batch.kubernetes.io/job-name: feature-engineering-retrain-v2
      controller-uid: 943e7e04-bc79-43dd-922f-c630f62dfc29
      job-name: feature-engineering-retrain-v2
    name: feature-engineering-retrain-v2
    namespace: neural-hive
    resourceVersion: "27920336"
    uid: 943e7e04-bc79-43dd-922f-c630f62dfc29
  spec:
    backoffLimit: 1
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 943e7e04-bc79-43dd-922f-c630f62dfc29
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 943e7e04-bc79-43dd-922f-c630f62dfc29
          batch.kubernetes.io/job-name: feature-engineering-retrain-v2
          controller-uid: 943e7e04-bc79-43dd-922f-c630f62dfc29
          job-name: feature-engineering-retrain-v2
      spec:
        containers:
        - args:
          - |
            import sys, subprocess
            from datetime import datetime
            from collections import defaultdict

            subprocess.check_call([sys.executable, "-m", "pip", "install", "pymongo", "scikit-learn", "pandas", "numpy", "mlflow", "-q"])

            import pymongo
            import numpy as np
            import pandas as pd
            from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
            from sklearn.model_selection import train_test_split, cross_val_score
            from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score
            from sklearn.preprocessing import StandardScaler
            import mlflow
            import mlflow.sklearn

            MONGO_URI = "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
            MLFLOW_URI = "http://mlflow.mlflow.svc.cluster.local:5000"
            SPECIALIST_TYPE = "business"

            print(f"=== Feature Engineering V2 - Threshold Optimization ===")

            client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            db = client.neural_hive
            opinions_col = db.specialist_opinions
            feedback_col = db.specialist_feedback
            plans_col = db.cognitive_ledger

            # Get feedbacks
            feedbacks = list(feedback_col.find({"specialist_type": SPECIALIST_TYPE}))
            print(f"Feedbacks: {len(feedbacks)}")

            # Get opinions
            opinion_ids = [f["opinion_id"] for f in feedbacks]
            opinions = list(opinions_col.find({"opinion_id": {"$in": opinion_ids}}))
            opinions_dict = {o["opinion_id"]: o for o in opinions}

            # Get plans
            plan_ids = [o.get("plan_id") for o in opinions if o.get("plan_id")]
            plans = list(plans_col.find({"plan_id": {"$in": plan_ids}}))
            plans_dict = {p["plan_id"]: p for p in plans}

            # Extract reasoning factors
            reasoning_factors_data = {}
            for op in opinions:
                oid = op["opinion_id"]
                factors = op.get("opinion", {}).get("reasoning_factors", [])
                for f in factors:
                    reasoning_factors_data[(oid, f["factor_name"])] = {
                        "weight": f.get("weight", 0),
                        "score": f.get("score", 0)
                    }

            # Build dataset
            training_data = []
            for fb in feedbacks:
                opinion_id = fb["opinion_id"]
                opinion = opinions_dict.get(opinion_id)
                if not opinion:
                    continue

                opinion_data = opinion.get("opinion", {})
                plan_id = opinion.get("plan_id")
                plan = plans_dict.get(plan_id, {})

                conf = opinion_data.get("confidence_score", 0.5)
                risk = opinion_data.get("risk_score", 0.5)
                rec = opinion_data.get("recommendation", "review_required")

                factor_scores = {}
                for factor_name in ["ml_confidence", "ml_risk", "semantic_quality_analysis",
                                     "complexity_evaluation", "kpi_alignment", "cost_effectiveness"]:
                    key = (opinion_id, factor_name)
                    if key in reasoning_factors_data:
                        factor_scores[f"{factor_name}_score"] = reasoning_factors_data[key]["score"]
                        factor_scores[f"{factor_name}_weight"] = reasoning_factors_data[key]["weight"]
                    else:
                        factor_scores[f"{factor_name}_score"] = 0.5
                        factor_scores[f"{factor_name}_weight"] = 0.0

                complexity_score = plan.get("complexity_score", 0.5)
                plan_risk_score = plan.get("risk_score", 0.5)
                estimated_duration = plan.get("estimated_total_duration_ms", 0)
                original_domain = plan.get("original_domain", "UNKNOWN")
                original_priority = plan.get("original_priority", "normal")

                tasks = plan.get("tasks", [])
                num_tasks = len(tasks)

                # Features
                features = {
                    "confidence_score": conf,
                    "risk_score": risk,
                    "model_rec_approve": 1 if rec == "approve" else 0,
                    "model_rec_reject": 1 if rec == "reject" else 0,
                    "model_rec_review": 1 if rec == "review_required" else 0,
                    "plan_complexity": complexity_score,
                    "plan_risk": plan_risk_score,
                    "num_tasks": num_tasks,
                    "estimated_duration_log": np.log1p(estimated_duration),
                    "is_complex": 1 if complexity_score > 0.6 else 0,
                    "is_high_risk": 1 if plan_risk_score > 0.6 else 0,
                    "is_high_priority": 1 if original_priority in ["high", "critical"] else 0,
                    "ml_conf_score": factor_scores.get("ml_confidence_score", 0.5),
                    "ml_risk_score": factor_scores.get("ml_risk_score", 0.5),
                    "semantic_quality": factor_scores.get("semantic_quality_analysis_score", 0.5),
                    "complexity_eval": factor_scores.get("complexity_evaluation_score", 0.5),
                    "kpi_align": factor_scores.get("kpi_alignment_score", 0.5),
                }

                human_rec = fb.get("human_recommendation", "review_required")
                label = 1 if human_rec == "approve" else 0
                training_data.append({**features, "label": label})

            df = pd.DataFrame(training_data)
            print(f"Dataset size: {len(df)}")

            feature_cols = [c for c in df.columns if c != "label"]
            X = df[feature_cols]
            y = df["label"]

            print(f"Positive ratio: {y.mean():.3f}")

            # Split
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
            print(f"Train: {len(X_train)}, Test: {len(X_test)}")

            # Scale
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # MLflow
            mlflow.set_tracking_uri(MLFLOW_URI)
            mlflow.set_experiment(f"specialist-{SPECIALIST_TYPE}-retraining")

            with mlflow.start_run(run_name=f"{SPECIALIST_TYPE}_v4_opt_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
                print(f"Run ID: {mlflow.active_run().info.run_id}")

                mlflow.log_param("specialist_type", SPECIALIST_TYPE)
                mlflow.log_param("samples", len(X_train))
                mlflow.log_param("features", len(feature_cols))
                mlflow.log_param("model", "gradient_boosting")
                mlflow.log_param("feature_engineering", True)

                # Train Gradient Boosting
                print("Training Gradient Boosting...")
                model = GradientBoostingClassifier(
                    n_estimators=150,
                    max_depth=5,
                    learning_rate=0.05,
                    min_samples_split=10,
                    min_samples_leaf=5,
                    random_state=42
                )
                model.fit(X_train_scaled, y_train)

                # Get probabilities
                y_proba = model.predict_proba(X_test_scaled)[:, 1]

                # Find optimal threshold
                precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
                f1_scores = 2 * precision * recall / (precision + recall + 1e-10)
                best_idx = np.argmax(f1_scores)
                best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5

                print(f"Optimal threshold: {best_threshold:.3f}")

                # Predict with optimal threshold
                y_pred = (y_proba >= best_threshold).astype(int)

                # Metrics with optimal threshold
                accuracy = accuracy_score(y_test, y_pred)
                precision_val = precision_score(y_test, y_pred, zero_division=0)
                recall_val = recall_score(y_test, y_pred, zero_division=0)
                f1_val = f1_score(y_test, y_pred, zero_division=0)
                auc = roc_auc_score(y_test, y_proba)

                print(f"\\n=== Test Metrics (optimal threshold={best_threshold:.3f}) ===")
                print(f"Accuracy:  {accuracy:.3f}")
                print(f"Precision: {precision_val:.3f}")
                print(f"Recall:    {recall_val:.3f}")
                print(f"F1 Score:  {f1_val:.3f}")
                print(f"AUC-ROC:   {auc:.3f}")

                print(f"\\nClassification Report:")
                print(classification_report(y_test, y_pred, zero_division=0))

                # Log metrics
                mlflow.log_metric("accuracy", accuracy)
                mlflow.log_metric("precision", precision_val)
                mlflow.log_metric("recall", recall_val)
                mlflow.log_metric("f1", f1_val)
                mlflow.log_metric("auc_roc", auc)
                mlflow.log_metric("optimal_threshold", best_threshold)

                # Feature importances
                importances = dict(zip(feature_cols, model.feature_importances_.tolist()))
                sorted_imp = sorted(importances.items(), key=lambda x: x[1], reverse=True)
                print(f"\\n=== Top 10 Feature Importances ===")
                for feat, imp in sorted_imp[:10]:
                    print(f"  {feat}: {imp:.4f}")
                mlflow.log_dict(dict(sorted_imp), "feature_importances.json")

                # Save model
                mlflow.sklearn.log_model(model, "model")
                mlflow.log_dict({"optimal_threshold": str(best_threshold)}, "model_config.json")
                print(f"\\nModel saved to MLflow with threshold {best_threshold:.3f}")

            print("\\n=== SUCCESS ===")
          command:
          - python3
          - -c
          env:
          - name: MONGODB_URI
            value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: trainer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastProbeTime: "2026-02-08T16:18:07Z"
      lastTransitionTime: "2026-02-08T16:18:07Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: Failed
    failed: 2
    ready: 0
    startTime: "2026-02-08T16:15:12Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-11T02:00:00Z"
    creationTimestamp: "2026-02-11T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      batch.kubernetes.io/controller-uid: 1ec65e58-34a3-49cb-9adb-4bb33fd96e2d
      batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29512920
      controller-uid: 1ec65e58-34a3-49cb-9adb-4bb33fd96e2d
      helm.sh/chart: orchestrator-dynamic-1.0.0
      job-name: orchestrator-dynamic-ml-training-29512920
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic-ml-training-29512920
    namespace: neural-hive
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: orchestrator-dynamic-ml-training
      uid: 0585c238-c922-4707-8467-344338bf9fe8
    resourceVersion: "28809384"
    uid: 1ec65e58-34a3-49cb-9adb-4bb33fd96e2d
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 1ec65e58-34a3-49cb-9adb-4bb33fd96e2d
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestrator-dynamic
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: orchestrator-dynamic
          app.kubernetes.io/part-of: neural-hive-mind
          app.kubernetes.io/version: 1.0.0
          batch.kubernetes.io/controller-uid: 1ec65e58-34a3-49cb-9adb-4bb33fd96e2d
          batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29512920
          controller-uid: 1ec65e58-34a3-49cb-9adb-4bb33fd96e2d
          helm.sh/chart: orchestrator-dynamic-1.0.0
          job-name: orchestrator-dynamic-ml-training-29512920
          neural-hive.io/component: orchestrator-dynamic
          neural-hive.io/domain: workflow-orchestration
          neural-hive.io/layer: orchestration
      spec:
        containers:
        - args:
          - --window-days=540
          - --min-samples=100
          command:
          - python3
          - /app/scripts/train_models_production.py
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-ml-training-config
          - secretRef:
              name: orchestrator-secrets
          image: 37.60.241.150:30500/orchestrator-dynamic:latest
          imagePullPolicy: IfNotPresent
          name: ml-training
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/training_logs
            name: training-logs
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: training-logs
  status:
    conditions:
    - lastProbeTime: "2026-02-11T03:00:01Z"
      lastTransitionTime: "2026-02-11T03:00:01Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-11T02:00:00Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-12T02:00:00Z"
    creationTimestamp: "2026-02-12T02:00:00Z"
    generation: 1
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      batch.kubernetes.io/controller-uid: 61dd3159-bd0c-4953-8ff2-45977040e669
      batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29514360
      controller-uid: 61dd3159-bd0c-4953-8ff2-45977040e669
      helm.sh/chart: orchestrator-dynamic-1.0.0
      job-name: orchestrator-dynamic-ml-training-29514360
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic-ml-training-29514360
    namespace: neural-hive
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: orchestrator-dynamic-ml-training
      uid: 0585c238-c922-4707-8467-344338bf9fe8
    resourceVersion: "29215305"
    uid: 61dd3159-bd0c-4953-8ff2-45977040e669
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 61dd3159-bd0c-4953-8ff2-45977040e669
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestrator-dynamic
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: orchestrator-dynamic
          app.kubernetes.io/part-of: neural-hive-mind
          app.kubernetes.io/version: 1.0.0
          batch.kubernetes.io/controller-uid: 61dd3159-bd0c-4953-8ff2-45977040e669
          batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29514360
          controller-uid: 61dd3159-bd0c-4953-8ff2-45977040e669
          helm.sh/chart: orchestrator-dynamic-1.0.0
          job-name: orchestrator-dynamic-ml-training-29514360
          neural-hive.io/component: orchestrator-dynamic
          neural-hive.io/domain: workflow-orchestration
          neural-hive.io/layer: orchestration
      spec:
        containers:
        - args:
          - --window-days=540
          - --min-samples=100
          command:
          - python3
          - /app/scripts/train_models_production.py
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-ml-training-config
          - secretRef:
              name: orchestrator-secrets
          image: 37.60.241.150:30500/orchestrator-dynamic:latest
          imagePullPolicy: IfNotPresent
          name: ml-training
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/training_logs
            name: training-logs
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: training-logs
  status:
    conditions:
    - lastProbeTime: "2026-02-12T03:00:02Z"
      lastTransitionTime: "2026-02-12T03:00:02Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-12T02:00:00Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      batch.kubernetes.io/cronjob-scheduled-timestamp: "2026-02-13T02:00:00Z"
    creationTimestamp: "2026-02-13T02:00:01Z"
    generation: 1
    labels:
      app.kubernetes.io/component: orchestrator-dynamic
      app.kubernetes.io/instance: orchestrator-dynamic
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: orchestrator-dynamic
      app.kubernetes.io/part-of: neural-hive-mind
      app.kubernetes.io/version: 1.0.0
      batch.kubernetes.io/controller-uid: d28b694d-ee08-42c2-9109-09f37cfe129c
      batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29515800
      controller-uid: d28b694d-ee08-42c2-9109-09f37cfe129c
      helm.sh/chart: orchestrator-dynamic-1.0.0
      job-name: orchestrator-dynamic-ml-training-29515800
      neural-hive.io/component: orchestrator-dynamic
      neural-hive.io/domain: workflow-orchestration
      neural-hive.io/layer: orchestration
    name: orchestrator-dynamic-ml-training-29515800
    namespace: neural-hive
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: CronJob
      name: orchestrator-dynamic-ml-training
      uid: 0585c238-c922-4707-8467-344338bf9fe8
    resourceVersion: "29614183"
    uid: d28b694d-ee08-42c2-9109-09f37cfe129c
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 2
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: d28b694d-ee08-42c2-9109-09f37cfe129c
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          app.kubernetes.io/component: orchestrator-dynamic
          app.kubernetes.io/instance: orchestrator-dynamic
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: orchestrator-dynamic
          app.kubernetes.io/part-of: neural-hive-mind
          app.kubernetes.io/version: 1.0.0
          batch.kubernetes.io/controller-uid: d28b694d-ee08-42c2-9109-09f37cfe129c
          batch.kubernetes.io/job-name: orchestrator-dynamic-ml-training-29515800
          controller-uid: d28b694d-ee08-42c2-9109-09f37cfe129c
          helm.sh/chart: orchestrator-dynamic-1.0.0
          job-name: orchestrator-dynamic-ml-training-29515800
          neural-hive.io/component: orchestrator-dynamic
          neural-hive.io/domain: workflow-orchestration
          neural-hive.io/layer: orchestration
      spec:
        containers:
        - args:
          - --window-days=540
          - --min-samples=100
          command:
          - python3
          - /app/scripts/train_models_production.py
          envFrom:
          - configMapRef:
              name: orchestrator-dynamic-ml-training-config
          - secretRef:
              name: orchestrator-secrets
          image: 37.60.241.150:30500/orchestrator-dynamic:latest
          imagePullPolicy: IfNotPresent
          name: ml-training
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/training_logs
            name: training-logs
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: default
        serviceAccountName: default
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: training-logs
  status:
    conditions:
    - lastProbeTime: "2026-02-13T03:00:01Z"
      lastTransitionTime: "2026-02-13T03:00:01Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 1
    ready: 0
    startTime: "2026-02-13T02:00:01Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"retrain-business-model","namespace":"neural-hive"},"spec":{"backoffLimit":1,"template":{"spec":{"containers":[{"args":["import sys, subprocess, os\nfrom datetime import datetime\n\n# Install dependencies\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymongo\", \"scikit-learn\", \"pandas\", \"numpy\", \"mlflow\", \"-q\"])\n\nimport pymongo\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\nimport mlflow\nimport mlflow.sklearn\n\n# Configuration\nMONGO_URI = \"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin\"\nMLFLOW_URI = \"http://mlflow.mlflow.svc.cluster.local:5000\"\nSPECIALIST_TYPE = \"business\"\n\nprint(f\"=== Retraining {SPECIALIST_TYPE} model with balanced data ===\")\n\n# Connect to MongoDB\nclient = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\ndb = client.neural_hive\nopinions_col = db.specialist_opinions\nfeedback_col = db.specialist_feedback\n\n# Get feedbacks for this specialist\nfeedbacks = list(feedback_col.find({\"specialist_type\": SPECIALIST_TYPE}))\nprint(f\"Found {len(feedbacks)} feedbacks for {SPECIALIST_TYPE}\")\n\n# Get opinions with feedback\nopinion_ids = [f[\"opinion_id\"] for f in feedbacks]\nopinions = list(opinions_col.find({\"opinion_id\": {\"$in\": opinion_ids}}))\nopinions_dict = {o[\"opinion_id\"]: o for o in opinions}\nprint(f\"Found {len(opinions_dict)} matching opinions\")\n\n# Build training dataset from feedbacks\ntraining_data = []\nfor fb in feedbacks:\n    opinion_id = fb[\"opinion_id\"]\n    opinion = opinions_dict.get(opinion_id)\n    if not opinion:\n        continue\n\n    opinion_data = opinion.get(\"opinion\", {})\n    conf = opinion_data.get(\"confidence_score\", 0.5)\n    risk = opinion_data.get(\"risk_score\", 0.5)\n    rec = opinion_data.get(\"recommendation\", \"review_required\")\n\n    # Features\n    features = {\n        \"confidence_score\": conf,\n        \"risk_score\": risk,\n        \"opinion_rec_approve\": 1 if rec == \"approve\" else 0,\n        \"opinion_rec_reject\": 1 if rec == \"reject\" else 0,\n        \"opinion_rec_review\": 1 if rec == \"review_required\" else 0,\n        \"opinion_rec_conditional\": 1 if rec == \"conditional\" else 0,\n        \"human_rating\": fb.get(\"human_rating\", 0.5),\n    }\n\n    # Label from human feedback (1=approve, 0=other)\n    human_rec = fb.get(\"human_recommendation\", \"review_required\")\n    label = 1 if human_rec == \"approve\" else (2 if human_rec == \"reject\" else 0)\n\n    training_data.append({**features, \"label\": label})\n\nprint(f\"Training dataset size: {len(training_data)}\")\n\n# Check class distribution\ndf = pd.DataFrame(training_data)\nlabel_dist = df[\"label\"].value_counts().sort_index()\nprint(f\"Label distribution: {dict(label_dist)}\")\n\n# Map labels: 0=review, 1=approve, 2=reject\nlabel_names = {0: \"review\", 1: \"approve\", 2: \"reject\"}\n\n# For binary classification: approve vs not-approve\ndf_y_binary = (df[\"label\"] == 1).astype(int)\nprint(f\"Binary distribution: approve={df_y_binary.sum()}, not_approve={len(df_y_binary) - df_y_binary.sum()}\")\n\nif len(df) \u003c 100:\n    print(\"ERROR: Not enough training data!\")\n    sys.exit(1)\n\n# Features\nX = df[[\"confidence_score\", \"risk_score\", \"opinion_rec_approve\", \"opinion_rec_reject\", \"opinion_rec_review\", \"opinion_rec_conditional\", \"human_rating\"]]\ny = df_y_binary\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\nprint(f\"Positive ratio in train: {y_train.mean():.2f}\")\n\n# Configure MLflow\nmlflow.set_tracking_uri(MLFLOW_URI)\nmlflow.set_experiment(f\"specialist-{SPECIALIST_TYPE}-retraining\")\n\nwith mlflow.start_run(run_name=f\"{SPECIALIST_TYPE}_retrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n    print(f\"MLflow Run ID: {mlflow.active_run().info.run_id}\")\n\n    # Log parameters\n    mlflow.log_param(\"specialist_type\", SPECIALIST_TYPE)\n    mlflow.log_param(\"training_samples\", len(X_train))\n    mlflow.log_param(\"test_samples\", len(X_test))\n    mlflow.log_param(\"feedback_count\", len(feedbacks))\n    mlflow.log_param(\"model_type\", \"random_forest\")\n    mlflow.log_param(\"positive_ratio\", float(y_train.mean()))\n\n    # Train model\n    print(\"Training Random Forest...\")\n    model = RandomForestClassifier(\n        n_estimators=100,\n        max_depth=10,\n        min_samples_split=5,\n        random_state=42,\n        class_weight=\"balanced\"\n    )\n    model.fit(X_train, y_train)\n\n    # Predictions\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)[:, 1]\n\n    # Metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, zero_division=0)\n    recall = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    print(f\"\\\\n=== Metrics ===\")\n    print(f\"Accuracy:  {accuracy:.3f}\")\n    print(f\"Precision: {precision:.3f}\")\n    print(f\"Recall:    {recall:.3f}\")\n    print(f\"F1 Score:  {f1:.3f}\")\n\n    # Classification report\n    print(f\"\\\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.log_metric(\"precision\", precision)\n    mlflow.log_metric(\"recall\", recall)\n    mlflow.log_metric(\"f1\", f1)\n\n    # Feature importances\n    importances = dict(zip(X.columns, model.feature_importances_.tolist()))\n    mlflow.log_dict(importances, \"feature_importances.json\")\n    print(f\"\\\\nFeature Importances: {importances}\")\n\n    # Log model (without registration to avoid 404)\n    mlflow.sklearn.log_model(model, \"model\")\n\n    print(f\"\\\\n=== Training Complete ===\")\n    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n    print(f\"Model artifacts saved in MLflow\")\n\nprint(\"\\\\n=== SUCCESS ===\")\n"],"command":["python3","-c"],"env":[{"name":"MONGODB_URI","value":"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"},{"name":"MLFLOW_TRACKING_URI","value":"http://mlflow.mlflow.svc.cluster.local:5000"}],"image":"python:3.11-slim","name":"trainer"}],"restartPolicy":"Never"}}}}
    creationTimestamp: "2026-02-08T15:36:35Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: 1bc95fa4-65ed-4475-be99-7b3b6511461b
      batch.kubernetes.io/job-name: retrain-business-model
      controller-uid: 1bc95fa4-65ed-4475-be99-7b3b6511461b
      job-name: retrain-business-model
    name: retrain-business-model
    namespace: neural-hive
    resourceVersion: "27911501"
    uid: 1bc95fa4-65ed-4475-be99-7b3b6511461b
  spec:
    backoffLimit: 1
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: 1bc95fa4-65ed-4475-be99-7b3b6511461b
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: 1bc95fa4-65ed-4475-be99-7b3b6511461b
          batch.kubernetes.io/job-name: retrain-business-model
          controller-uid: 1bc95fa4-65ed-4475-be99-7b3b6511461b
          job-name: retrain-business-model
      spec:
        containers:
        - args:
          - |
            import sys, subprocess, os
            from datetime import datetime

            # Install dependencies
            subprocess.check_call([sys.executable, "-m", "pip", "install", "pymongo", "scikit-learn", "pandas", "numpy", "mlflow", "-q"])

            import pymongo
            import numpy as np
            import pandas as pd
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report
            import mlflow
            import mlflow.sklearn

            # Configuration
            MONGO_URI = "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
            MLFLOW_URI = "http://mlflow.mlflow.svc.cluster.local:5000"
            SPECIALIST_TYPE = "business"

            print(f"=== Retraining {SPECIALIST_TYPE} model with balanced data ===")

            # Connect to MongoDB
            client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            db = client.neural_hive
            opinions_col = db.specialist_opinions
            feedback_col = db.specialist_feedback

            # Get feedbacks for this specialist
            feedbacks = list(feedback_col.find({"specialist_type": SPECIALIST_TYPE}))
            print(f"Found {len(feedbacks)} feedbacks for {SPECIALIST_TYPE}")

            # Get opinions with feedback
            opinion_ids = [f["opinion_id"] for f in feedbacks]
            opinions = list(opinions_col.find({"opinion_id": {"$in": opinion_ids}}))
            opinions_dict = {o["opinion_id"]: o for o in opinions}
            print(f"Found {len(opinions_dict)} matching opinions")

            # Build training dataset from feedbacks
            training_data = []
            for fb in feedbacks:
                opinion_id = fb["opinion_id"]
                opinion = opinions_dict.get(opinion_id)
                if not opinion:
                    continue

                opinion_data = opinion.get("opinion", {})
                conf = opinion_data.get("confidence_score", 0.5)
                risk = opinion_data.get("risk_score", 0.5)
                rec = opinion_data.get("recommendation", "review_required")

                # Features
                features = {
                    "confidence_score": conf,
                    "risk_score": risk,
                    "opinion_rec_approve": 1 if rec == "approve" else 0,
                    "opinion_rec_reject": 1 if rec == "reject" else 0,
                    "opinion_rec_review": 1 if rec == "review_required" else 0,
                    "opinion_rec_conditional": 1 if rec == "conditional" else 0,
                    "human_rating": fb.get("human_rating", 0.5),
                }

                # Label from human feedback (1=approve, 0=other)
                human_rec = fb.get("human_recommendation", "review_required")
                label = 1 if human_rec == "approve" else (2 if human_rec == "reject" else 0)

                training_data.append({**features, "label": label})

            print(f"Training dataset size: {len(training_data)}")

            # Check class distribution
            df = pd.DataFrame(training_data)
            label_dist = df["label"].value_counts().sort_index()
            print(f"Label distribution: {dict(label_dist)}")

            # Map labels: 0=review, 1=approve, 2=reject
            label_names = {0: "review", 1: "approve", 2: "reject"}

            # For binary classification: approve vs not-approve
            df_y_binary = (df["label"] == 1).astype(int)
            print(f"Binary distribution: approve={df_y_binary.sum()}, not_approve={len(df_y_binary) - df_y_binary.sum()}")

            if len(df) < 100:
                print("ERROR: Not enough training data!")
                sys.exit(1)

            # Features
            X = df[["confidence_score", "risk_score", "opinion_rec_approve", "opinion_rec_reject", "opinion_rec_review", "opinion_rec_conditional", "human_rating"]]
            y = df_y_binary

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

            print(f"Train size: {len(X_train)}, Test size: {len(X_test)}")
            print(f"Positive ratio in train: {y_train.mean():.2f}")

            # Configure MLflow
            mlflow.set_tracking_uri(MLFLOW_URI)
            mlflow.set_experiment(f"specialist-{SPECIALIST_TYPE}-retraining")

            with mlflow.start_run(run_name=f"{SPECIALIST_TYPE}_retrain_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
                print(f"MLflow Run ID: {mlflow.active_run().info.run_id}")

                # Log parameters
                mlflow.log_param("specialist_type", SPECIALIST_TYPE)
                mlflow.log_param("training_samples", len(X_train))
                mlflow.log_param("test_samples", len(X_test))
                mlflow.log_param("feedback_count", len(feedbacks))
                mlflow.log_param("model_type", "random_forest")
                mlflow.log_param("positive_ratio", float(y_train.mean()))

                # Train model
                print("Training Random Forest...")
                model = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    random_state=42,
                    class_weight="balanced"
                )
                model.fit(X_train, y_train)

                # Predictions
                y_pred = model.predict(X_test)
                y_proba = model.predict_proba(X_test)[:, 1]

                # Metrics
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)

                print(f"\\n=== Metrics ===")
                print(f"Accuracy:  {accuracy:.3f}")
                print(f"Precision: {precision:.3f}")
                print(f"Recall:    {recall:.3f}")
                print(f"F1 Score:  {f1:.3f}")

                # Classification report
                print(f"\\nClassification Report:")
                print(classification_report(y_test, y_pred, zero_division=0))

                # Log metrics
                mlflow.log_metric("accuracy", accuracy)
                mlflow.log_metric("precision", precision)
                mlflow.log_metric("recall", recall)
                mlflow.log_metric("f1", f1)

                # Feature importances
                importances = dict(zip(X.columns, model.feature_importances_.tolist()))
                mlflow.log_dict(importances, "feature_importances.json")
                print(f"\\nFeature Importances: {importances}")

                # Log model (without registration to avoid 404)
                mlflow.sklearn.log_model(model, "model")

                print(f"\\n=== Training Complete ===")
                print(f"Run ID: {mlflow.active_run().info.run_id}")
                print(f"Model artifacts saved in MLflow")

            print("\\n=== SUCCESS ===")
          command:
          - python3
          - -c
          env:
          - name: MONGODB_URI
            value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: trainer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastProbeTime: "2026-02-08T15:39:33Z"
      lastTransitionTime: "2026-02-08T15:39:33Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: Failed
    failed: 2
    ready: 0
    startTime: "2026-02-08T15:36:35Z"
    terminating: 0
    uncountedTerminatedPods: {}
- apiVersion: batch/v1
  kind: Job
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"retrain-business-model-v2","namespace":"neural-hive"},"spec":{"backoffLimit":1,"template":{"spec":{"containers":[{"args":["import sys, subprocess, os\nfrom datetime import datetime\n\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymongo\", \"scikit-learn\", \"pandas\", \"numpy\", \"mlflow\", \"-q\"])\n\nimport pymongo\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, roc_auc_score\nimport mlflow\nimport mlflow.sklearn\n\nMONGO_URI = \"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin\"\nMLFLOW_URI = \"http://mlflow.mlflow.svc.cluster.local:5000\"\nSPECIALIST_TYPE = \"business\"\n\nprint(f\"=== Retraining {SPECIALIST_TYPE} model (V2 - no data leakage) ===\")\n\nclient = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\ndb = client.neural_hive\nopinions_col = db.specialist_opinions\nfeedback_col = db.specialist_feedback\n\n# Get feedbacks\nfeedbacks = list(feedback_col.find({\"specialist_type\": SPECIALIST_TYPE}))\nprint(f\"Feedbacks: {len(feedbacks)}\")\n\n# Get opinions\nopinion_ids = [f[\"opinion_id\"] for f in feedbacks]\nopinions = list(opinions_col.find({\"opinion_id\": {\"$in\": opinion_ids}}))\nopinions_dict = {o[\"opinion_id\"]: o for o in opinions}\n\n# Build training dataset WITHOUT human_rating (data leakage)\ntraining_data = []\nfor fb in feedbacks:\n    opinion_id = fb[\"opinion_id\"]\n    opinion = opinions_dict.get(opinion_id)\n    if not opinion:\n        continue\n\n    opinion_data = opinion.get(\"opinion\", {})\n    conf = opinion_data.get(\"confidence_score\", 0.5)\n    risk = opinion_data.get(\"risk_score\", 0.5)\n    rec = opinion_data.get(\"recommendation\", \"review_required\")\n\n    # Features from the original model (no human feedback)\n    features = {\n        \"confidence_score\": conf,\n        \"risk_score\": risk,\n        \"opinion_rec_approve\": 1 if rec == \"approve\" else 0,\n        \"opinion_rec_reject\": 1 if rec == \"reject\" else 0,\n        \"opinion_rec_review\": 1 if rec == \"review_required\" else 0,\n        \"opinion_rec_conditional\": 1 if rec == \"conditional\" else 0,\n    }\n\n    # Label from human feedback\n    human_rec = fb.get(\"human_recommendation\", \"review_required\")\n    label = 1 if human_rec == \"approve\" else 0\n\n    training_data.append({**features, \"label\": label})\n\ndf = pd.DataFrame(training_data)\nprint(f\"Dataset size: {len(df)}\")\n\n# Features (NO human_rating)\nfeature_cols = [\"confidence_score\", \"risk_score\", \"opinion_rec_approve\",\n                \"opinion_rec_reject\", \"opinion_rec_review\", \"opinion_rec_conditional\"]\nX = df[feature_cols]\ny = df[\"label\"]\n\nprint(f\"Positive ratio: {y.mean():.3f}\")\n\nif len(df) \u003c 100:\n    print(\"ERROR: Not enough data!\")\n    sys.exit(1)\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n\n# MLflow\nmlflow.set_tracking_uri(MLFLOW_URI)\nmlflow.set_experiment(f\"specialist-{SPECIALIST_TYPE}-retraining\")\n\nwith mlflow.start_run(run_name=f\"{SPECIALIST_TYPE}_v2_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n    print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n\n    mlflow.log_param(\"specialist_type\", SPECIALIST_TYPE)\n    mlflow.log_param(\"samples\", len(X_train))\n    mlflow.log_param(\"model\", \"gradient_boosting\")\n    mlflow.log_param(\"positive_ratio\", float(y.mean()))\n\n    # Try Gradient Boosting for better performance\n    print(\"Training Gradient Boosting...\")\n    model = GradientBoostingClassifier(\n        n_estimators=100,\n        max_depth=5,\n        learning_rate=0.1,\n        random_state=42\n    )\n    model.fit(X_train, y_train)\n\n    # Predictions\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)[:, 1]\n\n    # Cross-validation\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n    print(f\"CV F1 scores: {cv_scores}\")\n    print(f\"Mean CV F1: {cv_scores.mean():.3f}\")\n\n    # Metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, zero_division=0)\n    recall = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n    auc = roc_auc_score(y_test, y_proba)\n\n    print(f\"\\\\n=== Test Metrics ===\")\n    print(f\"Accuracy:  {accuracy:.3f}\")\n    print(f\"Precision: {precision:.3f}\")\n    print(f\"Recall:    {recall:.3f}\")\n    print(f\"F1 Score:  {f1:.3f}\")\n    print(f\"AUC-ROC:   {auc:.3f}\")\n\n    print(f\"\\\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.log_metric(\"precision\", precision)\n    mlflow.log_metric(\"recall\", recall)\n    mlflow.log_metric(\"f1\", f1)\n    mlflow.log_metric(\"auc_roc\", auc)\n    mlflow.log_metric(\"cv_f1_mean\", cv_scores.mean())\n\n    # Feature importances\n    importances = dict(zip(X.columns, model.feature_importances_.tolist()))\n    mlflow.log_dict(importances, \"feature_importances.json\")\n    print(f\"\\\\nFeature Importances: {importances}\")\n\n    # Save model\n    mlflow.sklearn.log_model(model, \"model\")\n    print(f\"\\\\nModel saved to MLflow\")\n\nprint(\"\\\\n=== SUCCESS ===\")\n"],"command":["python3","-c"],"env":[{"name":"MONGODB_URI","value":"mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"},{"name":"MLFLOW_TRACKING_URI","value":"http://mlflow.mlflow.svc.cluster.local:5000"}],"image":"python:3.11-slim","name":"trainer"}],"restartPolicy":"Never"}}}}
    creationTimestamp: "2026-02-08T16:05:28Z"
    generation: 1
    labels:
      batch.kubernetes.io/controller-uid: bbccca3b-3d9f-430a-9d04-b8b4e18c97cf
      batch.kubernetes.io/job-name: retrain-business-model-v2
      controller-uid: bbccca3b-3d9f-430a-9d04-b8b4e18c97cf
      job-name: retrain-business-model-v2
    name: retrain-business-model-v2
    namespace: neural-hive
    resourceVersion: "27918023"
    uid: bbccca3b-3d9f-430a-9d04-b8b4e18c97cf
  spec:
    backoffLimit: 1
    completionMode: NonIndexed
    completions: 1
    manualSelector: false
    parallelism: 1
    podReplacementPolicy: TerminatingOrFailed
    selector:
      matchLabels:
        batch.kubernetes.io/controller-uid: bbccca3b-3d9f-430a-9d04-b8b4e18c97cf
    suspend: false
    template:
      metadata:
        creationTimestamp: null
        labels:
          batch.kubernetes.io/controller-uid: bbccca3b-3d9f-430a-9d04-b8b4e18c97cf
          batch.kubernetes.io/job-name: retrain-business-model-v2
          controller-uid: bbccca3b-3d9f-430a-9d04-b8b4e18c97cf
          job-name: retrain-business-model-v2
      spec:
        containers:
        - args:
          - |
            import sys, subprocess, os
            from datetime import datetime

            subprocess.check_call([sys.executable, "-m", "pip", "install", "pymongo", "scikit-learn", "pandas", "numpy", "mlflow", "-q"])

            import pymongo
            import numpy as np
            import pandas as pd
            from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
            from sklearn.model_selection import train_test_split, cross_val_score
            from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, roc_auc_score
            import mlflow
            import mlflow.sklearn

            MONGO_URI = "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
            MLFLOW_URI = "http://mlflow.mlflow.svc.cluster.local:5000"
            SPECIALIST_TYPE = "business"

            print(f"=== Retraining {SPECIALIST_TYPE} model (V2 - no data leakage) ===")

            client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            db = client.neural_hive
            opinions_col = db.specialist_opinions
            feedback_col = db.specialist_feedback

            # Get feedbacks
            feedbacks = list(feedback_col.find({"specialist_type": SPECIALIST_TYPE}))
            print(f"Feedbacks: {len(feedbacks)}")

            # Get opinions
            opinion_ids = [f["opinion_id"] for f in feedbacks]
            opinions = list(opinions_col.find({"opinion_id": {"$in": opinion_ids}}))
            opinions_dict = {o["opinion_id"]: o for o in opinions}

            # Build training dataset WITHOUT human_rating (data leakage)
            training_data = []
            for fb in feedbacks:
                opinion_id = fb["opinion_id"]
                opinion = opinions_dict.get(opinion_id)
                if not opinion:
                    continue

                opinion_data = opinion.get("opinion", {})
                conf = opinion_data.get("confidence_score", 0.5)
                risk = opinion_data.get("risk_score", 0.5)
                rec = opinion_data.get("recommendation", "review_required")

                # Features from the original model (no human feedback)
                features = {
                    "confidence_score": conf,
                    "risk_score": risk,
                    "opinion_rec_approve": 1 if rec == "approve" else 0,
                    "opinion_rec_reject": 1 if rec == "reject" else 0,
                    "opinion_rec_review": 1 if rec == "review_required" else 0,
                    "opinion_rec_conditional": 1 if rec == "conditional" else 0,
                }

                # Label from human feedback
                human_rec = fb.get("human_recommendation", "review_required")
                label = 1 if human_rec == "approve" else 0

                training_data.append({**features, "label": label})

            df = pd.DataFrame(training_data)
            print(f"Dataset size: {len(df)}")

            # Features (NO human_rating)
            feature_cols = ["confidence_score", "risk_score", "opinion_rec_approve",
                            "opinion_rec_reject", "opinion_rec_review", "opinion_rec_conditional"]
            X = df[feature_cols]
            y = df["label"]

            print(f"Positive ratio: {y.mean():.3f}")

            if len(df) < 100:
                print("ERROR: Not enough data!")
                sys.exit(1)

            # Split
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
            print(f"Train: {len(X_train)}, Test: {len(X_test)}")

            # MLflow
            mlflow.set_tracking_uri(MLFLOW_URI)
            mlflow.set_experiment(f"specialist-{SPECIALIST_TYPE}-retraining")

            with mlflow.start_run(run_name=f"{SPECIALIST_TYPE}_v2_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
                print(f"Run ID: {mlflow.active_run().info.run_id}")

                mlflow.log_param("specialist_type", SPECIALIST_TYPE)
                mlflow.log_param("samples", len(X_train))
                mlflow.log_param("model", "gradient_boosting")
                mlflow.log_param("positive_ratio", float(y.mean()))

                # Try Gradient Boosting for better performance
                print("Training Gradient Boosting...")
                model = GradientBoostingClassifier(
                    n_estimators=100,
                    max_depth=5,
                    learning_rate=0.1,
                    random_state=42
                )
                model.fit(X_train, y_train)

                # Predictions
                y_pred = model.predict(X_test)
                y_proba = model.predict_proba(X_test)[:, 1]

                # Cross-validation
                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
                print(f"CV F1 scores: {cv_scores}")
                print(f"Mean CV F1: {cv_scores.mean():.3f}")

                # Metrics
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)
                auc = roc_auc_score(y_test, y_proba)

                print(f"\\n=== Test Metrics ===")
                print(f"Accuracy:  {accuracy:.3f}")
                print(f"Precision: {precision:.3f}")
                print(f"Recall:    {recall:.3f}")
                print(f"F1 Score:  {f1:.3f}")
                print(f"AUC-ROC:   {auc:.3f}")

                print(f"\\nClassification Report:")
                print(classification_report(y_test, y_pred, zero_division=0))

                # Log metrics
                mlflow.log_metric("accuracy", accuracy)
                mlflow.log_metric("precision", precision)
                mlflow.log_metric("recall", recall)
                mlflow.log_metric("f1", f1)
                mlflow.log_metric("auc_roc", auc)
                mlflow.log_metric("cv_f1_mean", cv_scores.mean())

                # Feature importances
                importances = dict(zip(X.columns, model.feature_importances_.tolist()))
                mlflow.log_dict(importances, "feature_importances.json")
                print(f"\\nFeature Importances: {importances}")

                # Save model
                mlflow.sklearn.log_model(model, "model")
                print(f"\\nModel saved to MLflow")

            print("\\n=== SUCCESS ===")
          command:
          - python3
          - -c
          env:
          - name: MONGODB_URI
            value: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin
          - name: MLFLOW_TRACKING_URI
            value: http://mlflow.mlflow.svc.cluster.local:5000
          image: python:3.11-slim
          imagePullPolicy: IfNotPresent
          name: trainer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Never
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastProbeTime: "2026-02-08T16:08:27Z"
      lastTransitionTime: "2026-02-08T16:08:27Z"
      message: Job has reached the specified backoff limit
      reason: BackoffLimitExceeded
      status: "True"
      type: Failed
    failed: 2
    ready: 0
    startTime: "2026-02-08T16:05:28Z"
    terminating: 0
    uncountedTerminatedPods: {}
kind: List
metadata:
  resourceVersion: ""
