# Metadados para templates comuns
component: "otel-collector"
layer: "observabilidade"

nameOverride: ""
fullnameOverride: ""

# Service Account configuration
serviceAccount:
  create: true
  name: ""
  annotations: {}

# Configuração global
global:
  domain: neural-hive.local
  environment: production

replicaCount: 2

autoscaling:
  enabled: false
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

image:
  repository: otel/opentelemetry-collector-contrib
  tag: ""  # Usa .Chart.AppVersion
  pullPolicy: IfNotPresent

# Estrutura compatível com neural-hive.service
service:
  type: ClusterIP
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    neural.hive/metrics: "enabled"
    prometheus.io/scrape: "true"
    prometheus.io/port: "8888"
    prometheus.io/path: "/metrics"
  ports:
    otlp-grpc:
      port: 4317
      targetPort: 4317
      protocol: TCP
    otlp-http:
      port: 4318
      targetPort: 4318
      protocol: TCP
    jaeger-grpc:
      port: 14250
      targetPort: 14250
      protocol: TCP
    jaeger-http:
      port: 14268
      targetPort: 14268
      protocol: TCP
    zipkin:
      port: 9411
      targetPort: 9411
      protocol: TCP
    metrics:
      port: 8888
      targetPort: 8888
      protocol: TCP
    health:
      port: 13133
      targetPort: 13133
      protocol: TCP
    pprof:
      port: 1777
      targetPort: 1777
      protocol: TCP
    zpages:
      port: 55679
      targetPort: 55679
      protocol: TCP

# TLS Configuration
tls:
  enabled: false
  # Certificados podem vir de cert-manager ou secrets manuais
  certSecret: "otel-collector-tls"
  mountPath: "/etc/otel-tls"

# Configurações de recursos
resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

# Configurações de segurança
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 65534
  fsGroup: 65534
  seccompProfile:
    type: RuntimeDefault

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true

# Anti-affinity para alta disponibilidade
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - neural-hive-otel-collector
        topologyKey: kubernetes.io/hostname

nodeSelector: {}
tolerations: []

# Deployment strategy
deployment:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

# Istio integration
istio:
  enabled: true
  mtls:
    mode: STRICT

# Observability
observability:
  prometheus:
    enabled: true

podAnnotations:
  sidecar.istio.io/inject: "true"
  neural.hive/metrics: "enabled"
  prometheus.io/scrape: "true"
  prometheus.io/port: "8888"
  prometheus.io/path: "/metrics"

podLabels:
  neural.hive/component: telemetry-collector
  neural.hive/layer: observabilidade
  neural.hive/instrumented: "true"

# Probes
livenessProbe:
  httpGet:
    path: /
    port: health
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: health
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Command override para o container
containerCommand:
  - /otelcol-contrib
  - --config=/conf/otelcol.yaml

# Configuração do OpenTelemetry Collector
collector:
  config:
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
        check_collector_pipeline:
          enabled: true
          interval: "5m"
          exporter_failure_threshold: 5
      pprof:
        endpoint: 0.0.0.0:1777
        block_profile_fraction: 0
        mutex_profile_fraction: 0
      zpages:
        endpoint: 0.0.0.0:55679
      memory_ballast:
        size_mib: 683
      resourcedetection:
        detectors: [env, system, k8snode, kubernetes]
        timeout: 2s
        override: false
        k8snode:
          auth_type: "serviceAccount"
        kubernetes:
          auth_type: "serviceAccount"
          node_from_env_var: "K8S_NODE_NAME"
          extract:
            metadata:
              - k8s.pod.name
              - k8s.pod.uid
              - k8s.deployment.name
              - k8s.namespace.name
              - k8s.node.name
              - k8s.pod.start_time
            annotations:
              - service_name: "neural.hive/service-name"
              - version: "neural.hive/version"
              - domain: "neural.hive/domain"
            labels:
              - service_version: "app.kubernetes.io/version"
              - team: "app.kubernetes.io/managed-by"

    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          global:
            scrape_interval: 15s
            evaluation_interval: 15s
          scrape_configs:
          - job_name: 'neural-hive-services'
            kubernetes_sd_configs:
            - role: pod
              namespaces:
                names:
                - default
                - neural-hive
                - gateway
            relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_neural_hive_metrics]
              action: keep
              regex: enabled
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268
      otlp/logs:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      memory_limiter:
        check_interval: 1s
        limit_mib: 1536
      batch:
        send_batch_size: 1024
        timeout: 10s
        send_batch_max_size: 2048
      resource:
        attributes:
        - key: deployment.environment
          value: production
          action: upsert
        - key: service.namespace
          from_attribute: k8s.namespace.name
          action: insert
        - key: neural.hive.cluster
          value: main
          action: upsert
      attributes/neural_hive:
        actions:
        - key: neural.hive.intent.id
          from_attribute: http.request.header.x-neural-hive-intent-id
          action: upsert
        - key: neural.hive.plan.id
          from_attribute: http.request.header.x-neural-hive-plan-id
          action: upsert
        - key: neural.hive.domain
          from_attribute: http.request.header.x-neural-hive-domain
          action: upsert
        - key: neural.hive.user.id
          from_attribute: http.request.header.x-neural-hive-user-id
          action: upsert
      tail_sampling: {}

    exporters:
      prometheus:
        endpoint: "0.0.0.0:8888"
        const_labels:
          cluster: neural-hive-main
        send_timestamps: true
        metric_expiration: 180m
        enable_open_metrics: true
      otlp/jaeger:
        endpoint: neural-hive-jaeger.observability.svc.cluster.local:4317
        tls:
          insecure: true
      otlp/traces:
        endpoint: neural-hive-jaeger.observability.svc.cluster.local:4317
        tls:
          insecure: true
        headers:
          X-Neural-Hive-Source: "otel-collector"
      debug:
        verbosity: normal
      logging:
        loglevel: info
        sampling_initial: 5
        sampling_thereafter: 200

    service:
      extensions: [health_check, pprof, zpages, memory_ballast, resourcedetection]
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          processors: [memory_limiter, resource, attributes/neural_hive, batch]
          exporters: [otlp/jaeger, debug]
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, resource, attributes/neural_hive, batch]
          exporters: [prometheus]
        logs:
          receivers: [otlp/logs]
          processors: [memory_limiter, resource, attributes/neural_hive, batch]
          exporters: [logging]
      telemetry:
        logs:
          level: "info"
        metrics:
          level: detailed
          address: 0.0.0.0:8889  # Porta diferente do exporter Prometheus (8888) para evitar conflito

# Configurações de monitoramento
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    labels:
      neural.hive/metrics: "enabled"
    interval: 30s
    scrapeTimeout: 10s
    path: /metrics

# Network policies
networkPolicy:
  enabled: true
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: neural-hive
      - namespaceSelector:
          matchLabels:
            name: gateway
      - namespaceSelector:
          matchLabels:
            name: istio-system
      ports:
      - protocol: TCP
        port: 4317
      - protocol: TCP
        port: 4318
      - protocol: TCP
        port: 14250
      - protocol: TCP
        port: 14268
  egress:
    - to:
      - namespaceSelector:
          matchLabels:
            name: observability
      ports:
      - protocol: TCP
        port: 9090
      - protocol: TCP
        port: 14250

# Configuração do subchart opentelemetry-collector
opentelemetry-collector:
  mode: deployment

# Configurações de ambiente específicas
environments:
  development:
    replicaCount: 1
    collector:
      config:
        processors:
          tail_sampling: {}
  staging:
    replicaCount: 1
    collector:
      config:
        processors:
          tail_sampling: {}
  production:
    replicaCount: 2
    collector:
      config:
        processors:
          tail_sampling: {}
