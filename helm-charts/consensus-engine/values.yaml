# Metadados para templates comuns
component: "consensus-engine"
layer: "cognitiva"

replicaCount: 2

autoscaling:
  enabled: false  # Desabilitado por padrão para manter comportamento atual
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

image:
  repository: ghcr.io/albinojimy/neural-hive-mind/consensus-engine
  tag: "1.2.0"
  pullPolicy: Always

service:
  type: ClusterIP
  annotations: {}
  ports:
    http:
      port: 8000
      targetPort: 8000
      protocol: TCP
    metrics:
      port: 8080
      targetPort: 8080
      protocol: TCP

# Pós-deploy (obrigatório após redução de memória e failureThreshold):
# - Monitorar container_memory_usage_bytes e eventos dos pods no consensus-engine e nos specialists.
# - Se houver OOMKill ou uso sustentado >80-85% do limit de 1Gi, aumentar resources.limits.memory em +256Mi neste arquivo (ex: 1.25Gi, 1.5Gi) e redeploy.
# - Se "Startup probe failed" ocorrer de forma recorrente sem erro funcional, elevar startupProbe.failureThreshold para 20 ou 30 mantendo o restante igual.
resources:
  # Optimized after Phase 4 (migrated to python-grpc-base, removed neural_hive_specialists bloat)
  # Image reduced from ~1.5GB to ~480MB (68% reduction)
  # Bayesian aggregation saturates brief CPU bursts when reconciling 5 specialists + pheromone metrics
  # Profiling showed average usage <300m CPU / 400Mi (down from 650Mi), so we right-size requests to 400m/512Mi
  # while still allowing 1.5 CPU / 1.25Gi for reconciliation spikes without starving Kafka consumers
  requests:
    cpu: 400m
    memory: 512Mi
  limits:
    cpu: 1500m
    memory: 1280Mi

config:
  environment: production
  logLevel: DEBUG

  kafka:
    bootstrapServers: "neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"
    consumerGroupId: "consensus-engine"
    # Nomenclatura atual: plans.ready (consumo) e plans.consensus (produção)
    # Nota: Difere do padrão specialists.opinions.* / decisions.* solicitado
    # Para migração futura, atualizar ambos os tópicos e coordenar com STE/Gateway
    plansTopicConsume: "plans.ready"
    consensusTopicProduce: "plans.consensus"
    securityProtocol: "SASL_SSL"
    saslMechanism: "SCRAM-SHA-512"
    schemaRegistryUrl: "https://schema-registry.kafka.svc.cluster.local:8081/apis/ccompat/v6"

  specialists:
    businessEndpoint: "specialist-business.neural-hive.svc.cluster.local:50051"
    technicalEndpoint: "specialist-technical.neural-hive.svc.cluster.local:50051"
    behaviorEndpoint: "specialist-behavior.neural-hive.svc.cluster.local:50051"
    evolutionEndpoint: "specialist-evolution.neural-hive.svc.cluster.local:50051"
    architectureEndpoint: "specialist-architecture.neural-hive.svc.cluster.local:50051"
    # Timeout global (fallback para specialists sem timeout específico)
    # Aumentado para 120000ms (120 segundos) para acomodar tempo de processamento de 49-66 segundos para ML inference
    grpcTimeoutMs: 120000
    grpcMaxRetries: 3
    # Timeouts específicos por specialist (opcional, fallback para grpcTimeoutMs)
    # Configurar apenas para specialists que requerem timeout diferente do padrão
    specialistTimeouts:
      # Business Specialist requer timeout maior devido a processamento ML pesado (49-66s observado)
      business: 120000  # 2 minutos
      # Outros specialists usam timeout padrão (grpcTimeoutMs) se não especificado
      # technical: 30000
      # behavior: 30000
      # evolution: 30000
      # architecture: 30000

  queenAgent:
    grpcHost: "queen-agent.queen-agent.svc.cluster.local"
    grpcPort: 50051

  spiffe:
    enabled: false
    enableX509: false
    socketPath: "unix:///run/spire/sockets/agent.sock"
    trustDomain: "neural-hive.local"
    jwtAudience: "neural-hive.local"
    jwtTtlSeconds: 3600

  mongodb:
    uri: "mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017"
    database: "neural_hive"
    consensusCollection: "consensus_decisions"

  redis:
    clusterNodes: "neural-hive-cache.redis-cluster.svc.cluster.local:6379"
    sslEnabled: true
    pheromoneTtl: 3600
    pheromoneDecayRate: 0.1

  openTelemetry:
    enabled: true  # Habilitar OpenTelemetry (true em prod, false para dev local via Helm values)
    endpoint: "http://opentelemetry-collector.observability.svc.cluster.local:4317"
    samplingRate: 1.0  # 100% em dev, 50% em staging, 10% em prod (via environment values)

  consensus:
    minConfidenceScore: 0.8
    maxDivergenceThreshold: 0.05
    highRiskThreshold: 0.7
    criticalRiskThreshold: 0.9
    bayesianPriorWeight: 0.1
    votingWeightDecay: 0.95
    requireUnanimousForCritical: true
    fallbackToDeterministic: true

  features:
    enableBayesianAveraging: true
    enablePheromones: true
    enableFallback: true
    enableParallelInvocation: true
    enableTracing: true

secrets:
  kafkaSaslPassword: ""
  mongodbPassword: ""
  redisPassword: ""

serviceMonitor:
  enabled: true
  interval: 30s
  scrapeTimeout: 10s

podDisruptionBudget:
  enabled: true
  minAvailable: 1

networkPolicy:
  enabled: true
  ingress:
  - namespace: observability
    ports: [8080]
  - namespace: istio-system
    ports: [8000]
  egress:
  - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
  - namespaceSelector:
      matchLabels: kafka
    ports:
    - protocol: TCP
      port: 9092
  - namespaceSelector:
      matchLabels: mongodb-cluster
    ports:
    - protocol: TCP
      port: 27017
  - namespaceSelector:
      matchLabels: redis-cluster
    ports:
    - protocol: TCP
      port: 6379
  - namespaceSelector:
      matchLabels: neural-hive-specialists
    ports:
    - protocol: TCP
      port: 50051
  - namespaceSelector:
      matchLabels: observability
    ports:
    - protocol: TCP
      port: 4317

istio:
  enabled: true
  mtls:
    mode: STRICT

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

podLabels:
  app.kubernetes.io/component: consensus-aggregator
  neural-hive.io/layer: cognitiva
  neural-hive.io/domain: consensus

serviceAccount:
  create: true
  name: ""

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true

tolerations: []

nodeSelector: {}

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - consensus-engine
          topologyKey: topology.kubernetes.io/zone

# Spread consensus pods evenly per zone to keep gRPC fan-in latency predictable for quorum formation
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: consensus-engine

startupProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 15  # 15 * 10s = 2.5 minutes (reduced from 3.3min after Phase 4 optimizations)
  successThreshold: 1

livenessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8000
  initialDelaySeconds: 0
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

labels:
  app.kubernetes.io/name: consensus-engine
  app.kubernetes.io/component: consensus-aggregator
  neural-hive.io/layer: cognitiva
  neural-hive.io/domain: consensus
