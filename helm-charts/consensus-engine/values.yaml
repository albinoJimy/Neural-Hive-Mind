replicaCount: 2

autoscaling:
  enabled: false  # Desabilitado por padrão para manter comportamento atual
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

image:
  repository: 077878370245.dkr.ecr.us-east-1.amazonaws.com/dev/consensus-engine
  tag: "1.0.7"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8000
  metricsPort: 8080

resources:
  # Bayesian aggregation saturates brief CPU bursts when reconciling 5 specialists + pheromone metrics.
  # Profiling showed average usage <300m CPU / 650Mi, so we right-size requests to 400m/1Gi
  # while still allowing 1.5 CPU / 2.5Gi for reconciliation spikes without starving Kafka consumers.
  requests:
    cpu: 400m
    memory: 1Gi
  limits:
    cpu: 1500m
    memory: 2.5Gi

config:
  environment: production
  logLevel: DEBUG

  kafka:
    bootstrapServers: "neural-hive-kafka-bootstrap.neural-hive-kafka.svc.cluster.local:9092"
    consumerGroupId: "consensus-engine"
    # Nomenclatura atual: plans.ready (consumo) e plans.consensus (produção)
    # Nota: Difere do padrão specialists.opinions.* / decisions.* solicitado
    # Para migração futura, atualizar ambos os tópicos e coordenar com STE/Gateway
    plansTopicConsume: "plans.ready"
    consensusTopicProduce: "plans.consensus"
    securityProtocol: "SASL_SSL"
    saslMechanism: "SCRAM-SHA-512"

  specialists:
    businessEndpoint: "specialist-business.neural-hive.svc.cluster.local:50051"
    technicalEndpoint: "specialist-technical.neural-hive.svc.cluster.local:50051"
    behaviorEndpoint: "specialist-behavior.neural-hive.svc.cluster.local:50051"
    evolutionEndpoint: "specialist-evolution.neural-hive.svc.cluster.local:50051"
    architectureEndpoint: "specialist-architecture.neural-hive.svc.cluster.local:50051"
    # Timeout increased to 120000ms (120 seconds) to accommodate specialist processing time of 49-66 seconds for ML inference
    grpcTimeoutMs: 120000
    grpcMaxRetries: 3

  mongodb:
    uri: "mongodb://mongodb.mongodb-cluster.svc.cluster.local:27017"
    database: "neural_hive"
    consensusCollection: "consensus_decisions"

  redis:
    clusterNodes: "neural-hive-cache.redis-cluster.svc.cluster.local:6379"
    sslEnabled: true
    pheromoneTtl: 3600
    pheromoneDecayRate: 0.1

  openTelemetry:
    endpoint: "http://opentelemetry-collector.observability.svc.cluster.local:4317"
    samplingRate: 1.0

  consensus:
    minConfidenceScore: 0.8
    maxDivergenceThreshold: 0.05
    highRiskThreshold: 0.7
    criticalRiskThreshold: 0.9
    bayesianPriorWeight: 0.1
    votingWeightDecay: 0.95
    requireUnanimousForCritical: true
    fallbackToDeterministic: true

  features:
    enableBayesianAveraging: true
    enablePheromones: true
    enableFallback: true
    enableParallelInvocation: true
    enableTracing: true

secrets:
  kafkaSaslPassword: ""
  mongodbPassword: ""
  redisPassword: ""

serviceMonitor:
  enabled: true
  interval: 30s
  scrapeTimeout: 10s

podDisruptionBudget:
  enabled: true
  minAvailable: 1

networkPolicy:
  enabled: true
  ingress:
  - namespace: observability
    ports: [8080]
  - namespace: istio-system
    ports: [8000]
  egress:
  - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
  - namespaceSelector:
      matchLabels: kafka
    ports:
    - protocol: TCP
      port: 9092
  - namespaceSelector:
      matchLabels: mongodb-cluster
    ports:
    - protocol: TCP
      port: 27017
  - namespaceSelector:
      matchLabels: redis-cluster
    ports:
    - protocol: TCP
      port: 6379
  - namespaceSelector:
      matchLabels: neural-hive-specialists
    ports:
    - protocol: TCP
      port: 50051
  - namespaceSelector:
      matchLabels: observability
    ports:
    - protocol: TCP
      port: 4317

istio:
  enabled: true
  mtls:
    mode: STRICT

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

podLabels:
  app.kubernetes.io/component: consensus-aggregator
  neural-hive.io/layer: cognitiva
  neural-hive.io/domain: consensus

serviceAccount:
  create: true
  name: ""

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true

tolerations: []

nodeSelector: {}

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - consensus-engine
          topologyKey: topology.kubernetes.io/zone

# Spread consensus pods evenly per zone to keep gRPC fan-in latency predictable for quorum formation
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: consensus-engine

startupProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 20  # ~3.3min para carregar caches Bayesianos/Kafka offsets
  successThreshold: 1

livenessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8000
  initialDelaySeconds: 0
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

labels:
  app.kubernetes.io/name: consensus-engine
  app.kubernetes.io/component: consensus-aggregator
  neural-hive.io/layer: cognitiva
  neural-hive.io/domain: consensus
