component: "execution-ticket-service"
layer: "orchestration"

replicaCount: 1

# Disable Kubernetes service links to prevent env var collisions with pydantic settings
enableServiceLinks: false

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  name: ""

image:
  repository: ghcr.io/albinojimy/neural-hive-mind/execution-ticket-service
  tag: "1.2.3"
  pullPolicy: Always

imagePullSecrets:
  - name: ghcr-secret

deployment:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

service:
  type: ClusterIP
  annotations: {}
  ports:
    http:
      port: 8000
      targetPort: 8000
      protocol: TCP
    grpc:
      port: 50052
      targetPort: 50052
      protocol: TCP
    metrics:
      port: 9090
      targetPort: 9090
      protocol: TCP

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"
  prometheus.io/path: "/metrics"

podLabels:
  app.kubernetes.io/name: execution-ticket-service
  app.kubernetes.io/component: orchestration
  neural-hive.io/layer: orchestration

resources:
  requests:
    cpu: 100m      # Reduced for local development
    memory: 256Mi
  limits:
    cpu: 500m
    memory: 512Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

config:
  enabled: true
  serviceName: execution-ticket-service
  serviceVersion: "1.0.0"
  environment: development
  logLevel: INFO

  postgres:
    host: postgres-sla.neural-hive-data.svc.cluster.local
    port: 5432
    database: sla_management
    user: sla_user
    sslMode: disable  # Internal cluster connection

  mongodb:
    uri: mongodb://root:local_dev_password@mongodb.mongodb-cluster.svc.cluster.local:27017
    database: neural_hive_orchestration

  kafka:
    bootstrapServers: neural-hive-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092
    consumerGroupId: execution-ticket-service
    ticketsTopic: execution.tickets
    schemaRegistryUrl: https://schema-registry.kafka.svc.cluster.local:8081
    securityProtocol: PLAINTEXT
    saslMechanism: SCRAM-SHA-512
    saslUsername: ""
    saslPassword: ""
    sslCaLocation: ""
    sslCertificateLocation: ""
    sslKeyLocation: ""
    schemasBasePath: /app/schemas

  jwt:
    algorithm: HS256
    expirationSeconds: 3600

  webhooks:
    enabled: true
    timeoutSeconds: 10
    maxRetries: 3

  grpc:
    port: 50052
    maxWorkers: 10
    # gRPC é o protocolo primário para comunicação com orchestrator-dynamic
    critical: true
    healthCheckService: ""  # Usa serviço padrão de health check gRPC

  observability:
    otelExporterEndpoint: http://otel-collector:4317
    prometheusPort: 9090

secrets:
  # Use existingSecret para referenciar secrets criados pelo create-phase2-secrets.sh
  # Os Helm charts usam secretKeyRef para ler valores diretamente dos secrets K8s
  create: false
  existingSecret: "execution-ticket-service-secrets"
  # Mapeamento de chaves do secret para variáveis de ambiente
  postgresPasswordKey: POSTGRES_PASSWORD
  mongodbUriKey: MONGODB_URI
  jwtSecretKeyKey: JWT_SECRET_KEY

serviceMonitor:
  enabled: true
  interval: 30s
  scrapeTimeout: 10s

observability:
  prometheus:
    enabled: true

istio:
  enabled: true
  mtls:
    mode: STRICT

pdb:
  enabled: true
  minAvailable: 1

networkPolicy:
  enabled: false

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - execution-ticket-service
        topologyKey: topology.kubernetes.io/zone

tolerations: []

nodeSelector: {}

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: execution-ticket-service

# Health Checks Configuration
startupProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 30  # Allow time for container initialization before first probe
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 30  # 5 minutos total (30 * 10s) para conectar PostgreSQL, MongoDB, criar índices
  successThreshold: 1

livenessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 0
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8000
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Arquitetura Dual-Protocol:
# - HTTP (8000): Health checks, métricas, REST API (secundário)
# - gRPC (50052): Protocolo primário para comunicação com orchestrator-dynamic
#
# NOTA: readinessProbe usa HTTP /ready temporariamente porque o servidor gRPC
# tem um bug de import circular que impede sua inicialização.
# TODO: Migrar para gRPC readinessProbe quando o bug for corrigido:
#   grpc:
#     port: 50052
#     service: ""

# Connection retry configuration
connectionRetry:
  maxRetries: 5
  initialDelaySeconds: 1.0
