# Metadados para templates comuns
component: "specialist-behavior"
layer: "cognitiva"

replicaCount: 2

autoscaling:
  enabled: false  # Desabilitado por padrão para compatibilidade com local
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

podDisruptionBudget:
  enabled: false  # Desabilitado por padrão para compatibilidade com local
  minAvailable: 1

serviceAccount:
  create: true
  name: ""
  annotations: {}

secrets:
  # Set create=false when using External Secrets Operator to prevent conflicts
  # When false, Helm will not create the Secret resource (ExternalSecret controller will manage it)
  create: true
  # IMPORTANTE: Em produção, substitua CHANGEME_PRODUCTION_PASSWORD por:
  # 1. Senha real se usando secrets inline (não recomendado)
  # 2. Referência a External Secret (recomendado): usar ExternalSecrets Operator
  # 3. Sealed Secret do Bitnami para GitOps seguro
  # Exemplo External Secret: mongodbUri será sobrescrito por external-secret-mongodb
  mongodbUri: "mongodb://root:CHANGEME_PRODUCTION_PASSWORD@mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive?authSource=admin"
  neo4jPassword: "neo4j-password-changeme"
  redisPassword: ""

image:
  repository: 077878370245.dkr.ecr.us-east-1.amazonaws.com/dev/specialist-behavior
  tag: "1.0.7"
  pullPolicy: IfNotPresent

deployment:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

# Pós-deploy (obrigatório após redução de memória e failureThreshold):
# - Monitorar container_memory_usage_bytes e eventos dos pods nos specialists e no consensus-engine.
# - Se houver OOMKill ou uso sustentado >80-85% do limit de 1Gi, aumentar resources.limits.memory em +256Mi neste arquivo (ex: 1.25Gi, 1.5Gi) e redeploy.
# - Se "Startup probe failed" ocorrer de forma recorrente sem erro funcional, elevar startupProbe.failureThreshold para 20 ou 30 mantendo o restante igual.
resources:
  # Optimized after Phase 1-4 dependency cleanup (removed ~1.75GB unused ML libs: shap, lime, evidently, presidio, sentence-transformers)
  # New baseline: python-specialist-base (~1.2GB) + application code (~100MB) + runtime overhead (~200MB) = ~1.5GB image, ~400-500MB runtime
  # Production defaults based on optimized images: ~300-400MB memory, ~200-300m CPU durante inferências
  # Ratios: 3.3x CPU burst capacity, 2x memory headroom for peak loads
  # Para clusters pequenos/dev, considere reduzir para:
  #   requests: cpu: 200m, memory: 384Mi
  #   limits: cpu: 800m, memory: 768Mi
  requests:
    cpu: 300m
    memory: 512Mi
  limits:
    cpu: 1
    memory: 1Gi

service:
  type: ClusterIP
  ports:
    grpc:
      port: 50051
      targetPort: 50051
      protocol: TCP
    http:
      port: 8000
      targetPort: 8000
      protocol: TCP
    metrics:
      port: 8080
      targetPort: 8080
      protocol: TCP

config:
  enableJwtAuth: false
  environment: production
  logLevel: INFO

  mlflow:
    trackingUri: "http://mlflow.mlflow.svc.cluster.local:5000"
    experimentName: "behavior-specialist"
    modelName: "behavior-evaluator"
    modelStage: "Production"

  mongodb:
    # Note: config.mongodb.uri is used for documentation/NOTES.txt display only.
    # The service uses MONGODB_URI from secrets (with authentication).
    uri: "mongodb://mongodb.mongodb-cluster.svc.cluster.local:27017"
    database: "neural_hive"
    opinionsCollection: "specialist_opinions"

  neo4j:
    uri: "bolt://neo4j-bolt.neo4j-cluster.svc.cluster.local:7687"
    user: "neo4j"
    database: "neo4j"

  redis:
    clusterNodes: "neural-hive-cache.redis-cluster.svc.cluster.local:6379"
    sslEnabled: false
    cacheTtl: 300

  # OpenTelemetry Configuration
  # Custom attributes exported by specialists:
  # - neural.hive.intent.id, neural.hive.plan.id, neural.hive.user.id (baggage)
  # - specialist.type, specialist.version, plan.id, intent.id (span attributes)
  # - opinion.confidence_score, opinion.risk_score, opinion.recommendation
  # - inference.source (ml_model, semantic_pipeline, heuristics)
  # - features.count, extraction.time_ms, processing.time_ms
  otel:
    endpoint: "http://opentelemetry-collector.observability.svc.cluster.local:4317"

  grpc:
    port: 50051
    maxWorkers: 10
    maxMessageLength: 4194304

  http:
    port: 8000

  prometheus:
    port: 8080

  thresholds:
    minConfidenceScore: 0.8
    highRiskThreshold: 0.7

  timeouts:
    evaluationMs: 5000
    modelInferenceMs: 3000

  features:
    enableExplainability: true
    enableCaching: true
    enableModelMonitoring: true
    enableTracing: true
    useSemanticFallback: true

  behavior:
    accessibilityWcagLevel: "AA"
    usabilityThresholdHigh: 0.8
    usabilityThresholdLow: 0.5
    responseTimeThresholdMs: 300
    interactionCostThreshold: 0.7

serviceMonitor:
  enabled: true
  interval: 30s
  scrapeTimeout: 10s
  labels:
    prometheus: neural-hive

networkPolicy:
  enabled: true
  policyTypes:
    - Ingress
    - Egress

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - specialist-behavior
          topologyKey: topology.kubernetes.io/zone

# Soft spread keeps specialists evenly distributed per zone for latency-sensitive gRPC quorum calls
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: specialist-behavior

# Startup probe - allows up to 2.5 minutes for initial container startup before liveness probe takes over
# Reduced from 5 minutes after Phase 1-4 optimizations (removed heavy deps, optimized base images)
# Observed startup time: ~30-45s (vs ~2-3 min before), so 2.5 min provides 3-5x safety margin
# TUNING GUIDE:
# - Se pods crasham com "Startup probe failed" mas logs mostram progresso normal:
#   Aumentar failureThreshold (ex: 20 para 3.3 minutos, 30 para 5 minutos)
# - Se startup é lento em clusters com I/O lento:
#   Aumentar failureThreshold ou periodSeconds
# - Para clusters rápidos com SSD e rede rápida:
#   Reduzir failureThreshold para 10 (1.6 minutos)
startupProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 15  # 15 * 10s = 2.5 minutes max startup time
  successThreshold: 1

# Liveness probe - only runs after startup probe succeeds
livenessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

# Readiness probe - only runs after startup probe succeeds
readinessProbe:
  httpGet:
    path: /ready
    port: 8000
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

labels:
  app.kubernetes.io/name: specialist-behavior
  app.kubernetes.io/component: behavior-specialist
  app.kubernetes.io/part-of: neural-hive-mind
  neural-hive.io/component: behavior-specialist
  neural-hive.io/layer: cognitiva
  neural-hive.io/domain: behavior-analysis

annotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

# Production Secrets Management
# Para ambientes de produção, recomenda-se:
# 1. Usar External Secrets Operator com AWS Secrets Manager, Azure Key Vault, ou HashiCorp Vault
# 2. Criar ExternalSecret resource que popula o Secret specialist-behavior-secret
# 3. Nunca commitar senhas reais no Git
# Exemplo: kubectl apply -f external-secret-mongodb.yaml
