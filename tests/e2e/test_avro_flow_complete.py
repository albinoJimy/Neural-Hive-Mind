"""
Testes E2E para validar serializa√ß√£o/deserializa√ß√£o Avro em todo o pipeline.

Valida o fluxo completo:
- Schema Registry (Apicurio) ‚Üí registro e recupera√ß√£o de schemas
- Semantic Translation Engine ‚Üí produ√ß√£o de mensagens Avro
- Consensus Engine ‚Üí consumo e deserializa√ß√£o Avro
- Specialists ‚Üí invoca√ß√£o via gRPC
- Orchestrator ‚Üí gera√ß√£o de execution tickets

Autor: Neural Hive Mind Team
"""

import asyncio
import json
import time
import uuid
from typing import Dict, List, Optional

import httpx
import pytest
from confluent_kafka.avro import AvroConsumer, AvroProducer

from tests.e2e.utils.assertions import (
    assert_avro_message_valid,
    assert_cognitive_plan_structure,
    assert_consolidated_decision_structure,
    assert_execution_ticket_structure,
    assert_specialist_invoked,
)
from tests.e2e.utils.kafka_helpers import (
    collect_avro_messages,
    wait_for_avro_message,
)
from tests.e2e.utils.k8s_helpers import get_pod_logs

pytestmark = [pytest.mark.e2e, pytest.mark.asyncio, pytest.mark.slow, pytest.mark.avro]


# ============================================
# Teste 1: Schema Registry
# ============================================


@pytest.mark.timeout(30)
class TestSchemaRegistryValidation:
    """Testes de valida√ß√£o do Schema Registry (Apicurio)."""

    async def test_schema_registry_has_cognitive_plan_schema(
        self,
        schema_registry_client: httpx.AsyncClient,
    ):
        """
        Valida que o schema CognitivePlan est√° registrado no Schema Registry.

        Verifica√ß√µes:
        - Subject plans.ready-value existe
        - Schema cont√©m campos obrigat√≥rios
        - Vers√£o >= 1
        """
        # Buscar lista de subjects
        response = await schema_registry_client.get("/apis/ccompat/v6/subjects")
        assert response.status_code == 200, f"Falha ao listar subjects: {response.text}"

        subjects = response.json()
        assert "plans.ready-value" in subjects, "Subject plans.ready-value n√£o encontrado"

        # Buscar vers√µes do subject
        response = await schema_registry_client.get(
            "/apis/ccompat/v6/subjects/plans.ready-value/versions"
        )
        assert response.status_code == 200, f"Falha ao listar vers√µes: {response.text}"

        versions = response.json()
        assert len(versions) >= 1, "Nenhuma vers√£o do schema encontrada"

        # Buscar schema mais recente
        response = await schema_registry_client.get(
            "/apis/ccompat/v6/subjects/plans.ready-value/versions/latest"
        )
        assert response.status_code == 200, f"Falha ao buscar schema: {response.text}"

        schema_data = response.json()
        schema_str = schema_data.get("schema", "{}")
        schema = json.loads(schema_str)

        # Validar campos obrigat√≥rios
        field_names = [f["name"] for f in schema.get("fields", [])]
        required_fields = ["plan_id", "tasks", "execution_order", "risk_score"]

        for field in required_fields:
            assert field in field_names, f"Campo obrigat√≥rio ausente: {field}"

    async def test_schema_registry_has_consolidated_decision_schema(
        self,
        schema_registry_client: httpx.AsyncClient,
    ):
        """Valida que o schema ConsolidatedDecision est√° registrado."""
        response = await schema_registry_client.get("/apis/ccompat/v6/subjects")
        subjects = response.json()

        assert "plans.consensus-value" in subjects, (
            "Subject plans.consensus-value n√£o encontrado"
        )

    async def test_schema_registry_has_execution_ticket_schema(
        self,
        schema_registry_client: httpx.AsyncClient,
    ):
        """Valida que o schema ExecutionTicket est√° registrado."""
        response = await schema_registry_client.get("/apis/ccompat/v6/subjects")
        subjects = response.json()

        assert "execution.tickets-value" in subjects, (
            "Subject execution.tickets-value n√£o encontrado"
        )


# ============================================
# Teste 2: Serializa√ß√£o Avro no Semantic Translation Engine
# ============================================


@pytest.mark.timeout(90)
class TestSemanticTranslationEngineSerialization:
    """Testes de serializa√ß√£o Avro no Semantic Translation Engine."""

    async def test_semantic_translation_engine_produces_avro_messages(
        self,
        gateway_client: httpx.AsyncClient,
        cognitive_plan_avro_consumer: AvroConsumer,
        test_kafka_topics: Dict[str, str],
        sample_intent_for_avro_flow: Dict,
    ):
        """
        Valida que o Semantic Translation Engine produz mensagens Avro corretamente.

        Fluxo:
        1. Publicar intent via Gateway
        2. Aguardar mensagem no t√≥pico plans.ready
        3. Validar estrutura Avro
        """
        intent = sample_intent_for_avro_flow
        correlation_id = intent["correlation_id"]

        # Publicar intent via Gateway
        response = await gateway_client.post("/api/v1/intentions", json=intent)
        assert response.status_code in [200, 201, 202], (
            f"Falha ao publicar intent: {response.text}"
        )

        # Aguardar mensagem no t√≥pico plans.ready
        plan = await wait_for_avro_message(
            consumer=cognitive_plan_avro_consumer,
            topic=test_kafka_topics["plans.ready"],
            filter_fn=lambda msg: msg.get("correlation_id") == correlation_id,
            timeout=60,
        )

        assert plan is not None, "CognitivePlan n√£o recebido no t√≥pico plans.ready"

        # Validar estrutura do plano
        assert_cognitive_plan_structure(plan)

        # Validar campos espec√≠ficos
        assert plan["correlation_id"] == correlation_id
        assert len(plan["tasks"]) > 0, "Plano deve ter pelo menos uma task"
        assert plan["risk_band"] in ["low", "medium", "high", "critical"]
        assert plan["status"] in ["draft", "validated", "approved", "rejected"]


# ============================================
# Teste 3: Deserializa√ß√£o Avro no Consensus Engine
# ============================================


@pytest.mark.timeout(120)
class TestConsensusEngineDeserialization:
    """Testes de deserializa√ß√£o Avro no Consensus Engine."""

    async def test_consensus_engine_deserializes_avro_messages(
        self,
        cognitive_plan_avro_producer: AvroProducer,
        consolidated_decision_avro_consumer: AvroConsumer,
        complete_cognitive_plan_avro_with_order: Dict,
        k8s_client,
        test_kafka_topics: Dict[str, str],
    ):
        """
        Valida que o Consensus Engine deserializa mensagens Avro corretamente.

        Fluxo:
        1. Publicar CognitivePlan Avro no t√≥pico plans.ready
        2. Verificar logs do consensus-engine
        3. Aguardar decis√£o consolidada
        """
        plan = complete_cognitive_plan_avro_with_order
        plan_id = plan["plan_id"]

        # Publicar CognitivePlan
        cognitive_plan_avro_producer.produce(
            topic=test_kafka_topics["plans.ready"],
            value=plan,
        )
        cognitive_plan_avro_producer.flush()

        # Aguardar processamento
        await asyncio.sleep(5)

        # Verificar logs do consensus-engine
        logs = get_pod_logs(
            k8s_client,
            namespace="neural-hive-orchestration",
            label_selector="app=consensus-engine",
            tail_lines=200,
        )

        # Validar que n√£o h√° erros de magic byte
        assert "Invalid magic byte" not in logs, (
            "Erro de magic byte detectado no consensus-engine"
        )

        # Aguardar decis√£o consolidada
        decision = await wait_for_avro_message(
            consumer=consolidated_decision_avro_consumer,
            topic=test_kafka_topics["plans.consensus"],
            filter_fn=lambda msg: msg.get("plan_id") == plan_id,
            timeout=90,
        )

        assert decision is not None, "ConsolidatedDecision n√£o recebida"
        assert decision["plan_id"] == plan_id


# ============================================
# Teste 4: Invoca√ß√£o dos 5 Specialists via gRPC
# ============================================


@pytest.mark.timeout(180)
class TestSpecialistsInvocation:
    """Testes de invoca√ß√£o dos Specialists via gRPC."""

    async def test_consensus_engine_invokes_all_specialists(
        self,
        cognitive_plan_avro_producer: AvroProducer,
        consolidated_decision_avro_consumer: AvroConsumer,
        complete_cognitive_plan_avro_with_order: Dict,
        k8s_client,
        all_specialist_types: List[str],
        test_kafka_topics: Dict[str, str],
    ):
        """
        Valida que o Consensus Engine invoca todos os 5 specialists.

        Verifica√ß√µes:
        - Cada specialist recebeu EvaluatePlan para o plan_id (via logs)
        - Decis√£o consolidada cont√©m 5 specialist_votes
        - Falha se algum specialist n√£o for invocado
        """
        plan = complete_cognitive_plan_avro_with_order
        plan_id = plan["plan_id"]

        # Publicar CognitivePlan
        cognitive_plan_avro_producer.produce(
            topic=test_kafka_topics["plans.ready"],
            value=plan,
        )
        cognitive_plan_avro_producer.flush()

        print(f"\nüì§ CognitivePlan publicado - plan_id: {plan_id}")

        # Verificar invoca√ß√£o de CADA specialist via logs ANTES de validar votos
        # Isso garante que o gRPC foi chamado para cada um
        specialists_invoked = []
        specialists_not_invoked = []

        for specialist_type in all_specialist_types:
            try:
                await assert_specialist_invoked(
                    k8s_client=k8s_client,
                    specialist_type=specialist_type,
                    plan_id=plan_id,
                    timeout=60,
                )
                specialists_invoked.append(specialist_type)
                print(f"   ‚úÖ Specialist {specialist_type} invocado para plan_id: {plan_id}")
            except AssertionError as e:
                specialists_not_invoked.append(specialist_type)
                print(f"   ‚ùå Specialist {specialist_type} N√ÉO foi invocado: {e}")

        # Falhar teste se algum specialist n√£o foi invocado
        assert len(specialists_not_invoked) == 0, (
            f"Os seguintes specialists n√£o foram invocados para plan_id {plan_id}: "
            f"{specialists_not_invoked}. "
            f"Invocados com sucesso: {specialists_invoked}"
        )

        print(f"\n‚úÖ Todos os 5 specialists foram invocados via gRPC")

        # Aguardar decis√£o consolidada com specialist_votes
        decision = await wait_for_avro_message(
            consumer=consolidated_decision_avro_consumer,
            topic=test_kafka_topics["plans.consensus"],
            filter_fn=lambda msg: msg.get("plan_id") == plan_id,
            timeout=150,
        )

        assert decision is not None, "ConsolidatedDecision n√£o recebida"

        # Validar specialist_votes
        specialist_votes = decision.get("specialist_votes", [])
        assert len(specialist_votes) == 5, (
            f"Esperado 5 specialist_votes, recebido {len(specialist_votes)}"
        )

        # Validar que todos os tipos de specialists est√£o presentes nos votos
        vote_types = {vote["specialist_type"] for vote in specialist_votes}
        for specialist_type in all_specialist_types:
            assert specialist_type in vote_types, (
                f"Voto do specialist {specialist_type} n√£o encontrado na decis√£o"
            )

        # Validar estrutura de cada voto
        for vote in specialist_votes:
            assert "opinion_id" in vote
            assert "confidence_score" in vote
            assert 0.0 <= vote["confidence_score"] <= 1.0
            assert "recommendation" in vote
            assert "processing_time_ms" in vote

        print(f"   ‚úÖ ConsolidatedDecision cont√©m votos de todos os 5 specialists")


# ============================================
# Teste 5: Gera√ß√£o de Execution Tickets
# ============================================


@pytest.mark.timeout(240)
class TestExecutionTicketsGeneration:
    """Testes de gera√ß√£o de Execution Tickets."""

    async def test_execution_tickets_generated_from_consensus(
        self,
        gateway_client: httpx.AsyncClient,
        cognitive_plan_avro_consumer: AvroConsumer,
        consolidated_decision_avro_consumer: AvroConsumer,
        execution_ticket_avro_consumer: AvroConsumer,
        sample_intent_for_avro_flow: Dict,
        test_kafka_topics: Dict[str, str],
    ):
        """
        Valida que execution tickets s√£o gerados a partir do consenso.

        Fluxo:
        1. Publicar intent
        2. Aguardar plano cognitivo
        3. Aguardar decis√£o consolidada
        4. Aguardar execution tickets
        """
        intent = sample_intent_for_avro_flow
        correlation_id = intent["correlation_id"]

        # Publicar intent
        response = await gateway_client.post("/api/v1/intentions", json=intent)
        assert response.status_code in [200, 201, 202]

        # Aguardar plano
        plan = await wait_for_avro_message(
            consumer=cognitive_plan_avro_consumer,
            topic=test_kafka_topics["plans.ready"],
            filter_fn=lambda msg: msg.get("correlation_id") == correlation_id,
            timeout=60,
        )
        assert plan is not None, "CognitivePlan n√£o recebido"

        num_tasks = len(plan.get("tasks", []))

        # Aguardar decis√£o
        decision = await wait_for_avro_message(
            consumer=consolidated_decision_avro_consumer,
            topic=test_kafka_topics["plans.consensus"],
            filter_fn=lambda msg: msg.get("plan_id") == plan["plan_id"],
            timeout=90,
        )
        assert decision is not None, "ConsolidatedDecision n√£o recebida"

        # Aguardar tickets (se decis√£o for approve)
        if decision.get("final_decision") == "approve":
            tickets = await collect_avro_messages(
                consumer=execution_ticket_avro_consumer,
                topic=test_kafka_topics["execution.tickets"],
                filter_fn=lambda msg: msg.get("plan_id") == plan["plan_id"],
                timeout=60,
                expected_count=num_tasks,
            )

            assert len(tickets) >= 1, "Nenhum ExecutionTicket recebido"

            # Validar estrutura dos tickets
            for ticket in tickets:
                assert_execution_ticket_structure(ticket)
                assert ticket["status"] == "PENDING"


# ============================================
# Teste 6: Fluxo Completo A ‚Üí B ‚Üí C
# ============================================


@pytest.mark.timeout(360)
class TestCompleteAvroFlow:
    """Testes do fluxo Avro completo de ponta a ponta."""

    async def test_complete_avro_flow_intent_to_execution(
        self,
        gateway_client: httpx.AsyncClient,
        cognitive_plan_avro_consumer: AvroConsumer,
        consolidated_decision_avro_consumer: AvroConsumer,
        execution_ticket_avro_consumer: AvroConsumer,
        sample_intent_for_avro_flow: Dict,
        k8s_client,
        mongodb_client,
        redis_client,
        test_kafka_topics: Dict[str, str],
    ):
        """
        Valida o fluxo completo A ‚Üí B ‚Üí C com serializa√ß√£o Avro.

        Fluxo A: Intent ‚Üí Gateway ‚Üí Kafka
        Fluxo B: Semantic Translation ‚Üí plans.ready ‚Üí Consensus ‚Üí Specialists ‚Üí plans.consensus
        Fluxo C: Orchestrator ‚Üí execution.tickets

        M√©tricas coletadas:
        - Lat√™ncia total e por etapa
        - N√∫mero de mensagens por t√≥pico
        """
        intent = sample_intent_for_avro_flow
        correlation_id = intent["correlation_id"]
        metrics = {"start_time": time.time()}

        # ========== FLUXO A ==========
        # Publicar intent via Gateway
        response = await gateway_client.post("/api/v1/intentions", json=intent)
        assert response.status_code in [200, 201, 202], f"Falha no Fluxo A: {response.text}"

        metrics["flow_a_end"] = time.time()

        # Verificar Redis (cache e dedup)
        intent_key = f"intent:{correlation_id}"
        dedup_key = f"dedup:{correlation_id}"

        # Redis pode n√£o ter as chaves imediatamente
        await asyncio.sleep(2)

        # ========== FLUXO B ==========
        # Aguardar CognitivePlan
        plan = await wait_for_avro_message(
            consumer=cognitive_plan_avro_consumer,
            topic=test_kafka_topics["plans.ready"],
            filter_fn=lambda msg: msg.get("correlation_id") == correlation_id,
            timeout=60,
        )
        assert plan is not None, "Falha no Fluxo B: CognitivePlan n√£o recebido"

        metrics["plan_received"] = time.time()

        # Validar estrutura do plano
        assert_cognitive_plan_structure(plan)

        # Aguardar ConsolidatedDecision
        decision = await wait_for_avro_message(
            consumer=consolidated_decision_avro_consumer,
            topic=test_kafka_topics["plans.consensus"],
            filter_fn=lambda msg: msg.get("plan_id") == plan["plan_id"],
            timeout=90,
        )
        assert decision is not None, "Falha no Fluxo B: ConsolidatedDecision n√£o recebida"

        metrics["decision_received"] = time.time()

        # Validar decis√£o
        assert_consolidated_decision_structure(decision)

        # Validar specialist_votes (5 specialists)
        assert len(decision.get("specialist_votes", [])) == 5

        # Verificar MongoDB (consensus_decisions)
        consensus_doc = mongodb_client["consensus_decisions"].find_one(
            {"decision_id": decision["decision_id"]}
        )
        # MongoDB pode n√£o ter o documento se n√£o estiver configurado
        # assert consensus_doc is not None

        metrics["flow_b_end"] = time.time()

        # ========== FLUXO C ==========
        if decision.get("final_decision") == "approve":
            num_tasks = len(plan.get("tasks", []))

            # Aguardar ExecutionTickets
            tickets = await collect_avro_messages(
                consumer=execution_ticket_avro_consumer,
                topic=test_kafka_topics["execution.tickets"],
                filter_fn=lambda msg: msg.get("plan_id") == plan["plan_id"],
                timeout=60,
                expected_count=num_tasks,
            )

            metrics["flow_c_end"] = time.time()

            # Validar tickets
            for ticket in tickets:
                assert_execution_ticket_structure(ticket)
                assert ticket["correlation_id"] == correlation_id

            # Verificar MongoDB (execution_tickets e workflows)
            # Documentos podem n√£o existir se MongoDB n√£o estiver configurado

        # ========== M√âTRICAS ==========
        metrics["total_latency_ms"] = (metrics.get("flow_c_end", metrics["flow_b_end"]) - metrics["start_time"]) * 1000
        metrics["flow_a_latency_ms"] = (metrics["flow_a_end"] - metrics["start_time"]) * 1000
        metrics["flow_b_latency_ms"] = (metrics["flow_b_end"] - metrics["flow_a_end"]) * 1000

        if "flow_c_end" in metrics:
            metrics["flow_c_latency_ms"] = (metrics["flow_c_end"] - metrics["flow_b_end"]) * 1000

        # Log de m√©tricas para an√°lise
        print(f"\nüìä M√©tricas do Fluxo Avro Completo:")
        print(f"   Lat√™ncia Total: {metrics['total_latency_ms']:.2f}ms")
        print(f"   Fluxo A: {metrics['flow_a_latency_ms']:.2f}ms")
        print(f"   Fluxo B: {metrics['flow_b_latency_ms']:.2f}ms")
        if "flow_c_latency_ms" in metrics:
            print(f"   Fluxo C: {metrics['flow_c_latency_ms']:.2f}ms")


# ============================================
# Teste 7: Schema Evolution
# ============================================


@pytest.mark.timeout(120)
class TestSchemaEvolution:
    """Testes de evolu√ß√£o de schema (backward compatibility)."""

    async def test_schema_evolution_backward_compatible(
        self,
        schema_registry_client: httpx.AsyncClient,
    ):
        """
        Valida que o Schema Registry suporta evolu√ß√£o backward-compatible.

        Verifica√ß√µes:
        - Schema pode ser atualizado com novos campos opcionais
        - Consumers com schema antigo ainda funcionam
        """
        # Buscar compatibilidade configurada
        response = await schema_registry_client.get(
            "/apis/ccompat/v6/config/plans.ready-value"
        )

        if response.status_code == 200:
            config = response.json()
            compatibility = config.get("compatibilityLevel", "BACKWARD")
            assert compatibility in ["BACKWARD", "BACKWARD_TRANSITIVE", "FULL"], (
                f"Compatibilidade n√£o √© backward: {compatibility}"
            )

    async def test_schema_evolution_cross_version_serialization(
        self,
        schema_registry_client: httpx.AsyncClient,
        k8s_service_endpoints: Dict[str, str],
        test_kafka_topics: Dict[str, str],
    ):
        """
        Valida compatibilidade de serializa√ß√£o/deserializa√ß√£o entre vers√µes de schema.

        Fluxo:
        1. Registrar schema v1 (campos base)
        2. Registrar schema v2 (campos base + novo campo opcional)
        3. Publicar mensagem com schema v1
        4. Publicar mensagem com schema v2
        5. Consumir com AvroConsumer v1 e v2 - ambos devem deserializar corretamente
        """
        from confluent_kafka import avro
        from confluent_kafka.avro import AvroConsumer, AvroProducer
        from confluent_kafka.avro.error import ClientError

        test_subject = f"schema-evolution-test-{uuid.uuid4().hex[:8]}-value"
        test_topic = f"schema-evolution-{uuid.uuid4().hex[:8]}"

        # Schema v1: campos base
        schema_v1_str = json.dumps({
            "type": "record",
            "name": "SchemaEvolutionTest",
            "namespace": "com.neuralhive.test",
            "fields": [
                {"name": "id", "type": "string"},
                {"name": "name", "type": "string"},
                {"name": "created_at", "type": "long"},
            ]
        })

        # Schema v2: campos base + campo opcional (backward compatible)
        schema_v2_str = json.dumps({
            "type": "record",
            "name": "SchemaEvolutionTest",
            "namespace": "com.neuralhive.test",
            "fields": [
                {"name": "id", "type": "string"},
                {"name": "name", "type": "string"},
                {"name": "created_at", "type": "long"},
                {"name": "description", "type": ["null", "string"], "default": None},
            ]
        })

        # Registrar schema v1
        response_v1 = await schema_registry_client.post(
            f"/apis/ccompat/v6/subjects/{test_subject}/versions",
            json={"schema": schema_v1_str},
        )
        assert response_v1.status_code in [200, 201], (
            f"Falha ao registrar schema v1: {response_v1.text}"
        )
        schema_v1_id = response_v1.json().get("id")
        assert schema_v1_id is not None, "Schema v1 n√£o retornou ID"

        # Registrar schema v2 (deve ser compat√≠vel)
        response_v2 = await schema_registry_client.post(
            f"/apis/ccompat/v6/subjects/{test_subject}/versions",
            json={"schema": schema_v2_str},
        )
        assert response_v2.status_code in [200, 201], (
            f"Falha ao registrar schema v2 (n√£o compat√≠vel?): {response_v2.text}"
        )
        schema_v2_id = response_v2.json().get("id")
        assert schema_v2_id is not None, "Schema v2 n√£o retornou ID"

        # Carregar schemas para producer
        schema_v1 = avro.loads(schema_v1_str)
        schema_v2 = avro.loads(schema_v2_str)

        kafka_config = {
            "bootstrap.servers": k8s_service_endpoints["kafka"],
            "schema.registry.url": f"http://{k8s_service_endpoints['schema_registry']}",
        }

        # Mensagem v1 (sem campo opcional)
        msg_v1 = {
            "id": f"v1-{uuid.uuid4().hex[:8]}",
            "name": "Mensagem schema v1",
            "created_at": int(time.time() * 1000),
        }

        # Mensagem v2 (com campo opcional)
        msg_v2 = {
            "id": f"v2-{uuid.uuid4().hex[:8]}",
            "name": "Mensagem schema v2",
            "created_at": int(time.time() * 1000),
            "description": "Campo adicional do schema v2",
        }

        # Publicar com producer v1
        producer_v1 = AvroProducer(kafka_config, default_value_schema=schema_v1)
        producer_v1.produce(topic=test_topic, value=msg_v1)
        producer_v1.flush()

        # Publicar com producer v2
        producer_v2 = AvroProducer(kafka_config, default_value_schema=schema_v2)
        producer_v2.produce(topic=test_topic, value=msg_v2)
        producer_v2.flush()

        # Consumir com consumer (deve deserializar ambas as mensagens)
        consumer_config = {
            **kafka_config,
            "group.id": f"e2e-schema-evolution-{uuid.uuid4().hex[:8]}",
            "auto.offset.reset": "earliest",
            "enable.auto.commit": False,
        }
        consumer = AvroConsumer(consumer_config)
        consumer.subscribe([test_topic])

        messages_received = []
        start_time = time.time()
        timeout = 30

        while time.time() - start_time < timeout and len(messages_received) < 2:
            msg = await asyncio.to_thread(consumer.poll, 1.0)
            if msg is None:
                continue
            if msg.error():
                pytest.fail(f"Erro ao consumir mensagem: {msg.error()}")
            messages_received.append(msg.value())

        consumer.close()

        # Validar que ambas as mensagens foram deserializadas
        assert len(messages_received) == 2, (
            f"Esperado 2 mensagens, recebido {len(messages_received)}"
        )

        # Validar mensagem v1
        msg_v1_received = next(
            (m for m in messages_received if m.get("id") == msg_v1["id"]), None
        )
        assert msg_v1_received is not None, "Mensagem v1 n√£o recebida"
        assert msg_v1_received["name"] == msg_v1["name"]

        # Validar mensagem v2
        msg_v2_received = next(
            (m for m in messages_received if m.get("id") == msg_v2["id"]), None
        )
        assert msg_v2_received is not None, "Mensagem v2 n√£o recebida"
        assert msg_v2_received["name"] == msg_v2["name"]
        assert msg_v2_received.get("description") == msg_v2["description"]

        # Cleanup: remover subject de teste
        await schema_registry_client.delete(
            f"/apis/ccompat/v6/subjects/{test_subject}"
        )

        print(f"\n‚úÖ Schema Evolution Test:")
        print(f"   Schema v1 ID: {schema_v1_id}")
        print(f"   Schema v2 ID: {schema_v2_id}")
        print(f"   Mensagens deserializadas: {len(messages_received)}")


# ============================================
# Teste 8: Error Handling - Schema N√£o Registrado
# ============================================


@pytest.mark.timeout(30)
class TestErrorHandlingSchemaNotRegistered:
    """Testes de tratamento de erro para schema n√£o registrado."""

    async def test_error_handling_schema_not_registered(
        self,
        schema_registry_client: httpx.AsyncClient,
        k8s_service_endpoints: Dict[str, str],
    ):
        """
        Valida tratamento de erro quando schema n√£o est√° registrado.

        Fluxo:
        1. Criar producer com schema para subject inexistente
        2. Tentar produzir mensagem
        3. Verificar que ClientError √© lan√ßado
        """
        from confluent_kafka import avro
        from confluent_kafka.avro import AvroProducer
        from confluent_kafka.avro.error import ClientError

        # Subject que definitivamente n√£o existe
        nonexistent_subject = f"nonexistent-subject-{uuid.uuid4().hex}"
        fake_topic = f"fake-topic-{uuid.uuid4().hex[:8]}"

        # Schema de teste
        fake_schema_str = json.dumps({
            "type": "record",
            "name": "FakeSchema",
            "namespace": "com.neuralhive.test.nonexistent",
            "fields": [
                {"name": "id", "type": "string"},
                {"name": "test_field", "type": "string"},
            ],
        })

        fake_schema = avro.loads(fake_schema_str)

        # Criar producer SEM auto-registro de schema
        kafka_config = {
            "bootstrap.servers": k8s_service_endpoints["kafka"],
            "schema.registry.url": f"http://{k8s_service_endpoints['schema_registry']}",
            "auto.register.schemas": False,  # Desabilitar auto-registro
        }

        producer = AvroProducer(kafka_config, default_value_schema=fake_schema)

        fake_message = {
            "id": f"fake-{uuid.uuid4().hex[:8]}",
            "test_field": "valor de teste",
        }

        # Verificar que produzir com subject n√£o registrado gera erro
        error_caught = False
        error_message = ""

        try:
            producer.produce(
                topic=fake_topic,
                value=fake_message,
            )
            producer.flush()
        except ClientError as e:
            error_caught = True
            error_message = str(e)
            print(f"\n‚úÖ ClientError capturado como esperado: {error_message}")
        except Exception as e:
            # Outros erros relacionados a schema tamb√©m s√£o v√°lidos
            if "schema" in str(e).lower() or "subject" in str(e).lower():
                error_caught = True
                error_message = str(e)
                print(f"\n‚úÖ Erro de schema capturado: {error_message}")
            else:
                raise

        assert error_caught, (
            "Esperava-se ClientError ao produzir com schema n√£o registrado, "
            "mas nenhum erro foi lan√ßado"
        )

        # Verificar que o subject realmente n√£o existe
        response = await schema_registry_client.get("/apis/ccompat/v6/subjects")
        subjects = response.json()
        assert f"{fake_topic}-value" not in subjects, (
            f"Subject {fake_topic}-value n√£o deveria existir"
        )

        print(f"   Subject verificado como inexistente: {fake_topic}-value")

    async def test_error_handling_deleted_subject(
        self,
        schema_registry_client: httpx.AsyncClient,
        k8s_service_endpoints: Dict[str, str],
    ):
        """
        Valida tratamento de erro ap√≥s remover subject do Registry.

        Fluxo:
        1. Registrar um subject tempor√°rio
        2. Remover o subject
        3. Tentar produzir mensagem para o subject removido
        4. Verificar que erro √© gerado
        """
        from confluent_kafka import avro
        from confluent_kafka.avro import AvroProducer
        from confluent_kafka.avro.error import ClientError

        temp_subject = f"temp-delete-test-{uuid.uuid4().hex[:8]}-value"
        temp_topic = f"temp-delete-test-{uuid.uuid4().hex[:8]}"

        # Schema tempor√°rio
        temp_schema_str = json.dumps({
            "type": "record",
            "name": "TempDeleteTest",
            "namespace": "com.neuralhive.test.temp",
            "fields": [
                {"name": "id", "type": "string"},
            ],
        })

        # Registrar schema temporariamente
        response = await schema_registry_client.post(
            f"/apis/ccompat/v6/subjects/{temp_subject}/versions",
            json={"schema": temp_schema_str},
        )
        assert response.status_code in [200, 201], (
            f"Falha ao registrar schema tempor√°rio: {response.text}"
        )

        # Remover o subject
        delete_response = await schema_registry_client.delete(
            f"/apis/ccompat/v6/subjects/{temp_subject}"
        )
        assert delete_response.status_code in [200, 204], (
            f"Falha ao remover subject: {delete_response.text}"
        )

        # Aguardar propaga√ß√£o da remo√ß√£o
        await asyncio.sleep(1)

        # Criar producer sem auto-registro
        kafka_config = {
            "bootstrap.servers": k8s_service_endpoints["kafka"],
            "schema.registry.url": f"http://{k8s_service_endpoints['schema_registry']}",
            "auto.register.schemas": False,
        }

        temp_schema = avro.loads(temp_schema_str)
        producer = AvroProducer(kafka_config, default_value_schema=temp_schema)

        # Tentar produzir deve falhar
        error_caught = False

        try:
            producer.produce(
                topic=temp_topic,
                value={"id": "test-after-delete"},
            )
            producer.flush()
        except (ClientError, Exception) as e:
            error_caught = True
            print(f"\n‚úÖ Erro capturado ap√≥s remo√ß√£o de subject: {e}")

        assert error_caught, (
            "Esperava-se erro ao produzir para subject removido"
        )


# ============================================
# Teste 9: Error Handling - Invalid Magic Byte
# ============================================


@pytest.mark.timeout(90)
class TestErrorHandlingInvalidMagicByte:
    """Testes de tratamento de erro para magic byte inv√°lido."""

    async def test_error_handling_invalid_magic_byte(
        self,
        kafka_test_helper,
        k8s_client,
        consolidated_decision_avro_consumer: AvroConsumer,
        test_kafka_topics: Dict[str, str],
    ):
        """
        Valida tratamento de erro quando mensagem n√£o tem magic byte Avro.

        Fluxo:
        1. Publicar mensagem JSON pura (sem magic byte Avro)
        2. Verificar que logs do consensus-engine cont√™m erro esperado
        3. Verificar que nenhuma ConsolidatedDecision √© emitida para esse plan_id
        4. Verificar que consumer continua ativo (n√£o travou)

        Falha o teste se:
        - Erro n√£o for observado nos logs
        - Servi√ßo parar de consumir
        """
        # Gerar plan_id √∫nico para rastrear
        fake_plan_id = f"invalid-magic-{uuid.uuid4().hex[:8]}"

        # Publicar mensagem JSON pura no t√≥pico plans.ready (sem magic byte Avro)
        producer = kafka_test_helper.get_producer()

        fake_message = {
            "plan_id": fake_plan_id,
            "intent_id": f"fake-intent-{uuid.uuid4().hex[:8]}",
            "correlation_id": f"fake-corr-{uuid.uuid4().hex[:8]}",
            "version": "1.0.0",
            "tasks": [],
            "execution_order": [],
            "risk_score": 0.1,
            "risk_band": "low",
            "risk_factors": {},
            "explainability_token": "fake-token",
            "reasoning_summary": "Mensagem de teste sem magic byte",
            "status": "validated",
            "created_at": int(time.time() * 1000),
            "complexity_score": 0.1,
            "original_domain": "TEST",
            "original_priority": "LOW",
            "original_security_level": "PUBLIC",
        }

        producer.produce(
            test_kafka_topics["plans.ready"],
            value=json.dumps(fake_message).encode("utf-8"),
        )
        producer.flush()

        # Aguardar processamento pelo consensus-engine
        await asyncio.sleep(10)

        # Verificar logs do consensus-engine para erro de magic byte
        logs = get_pod_logs(
            k8s_client,
            namespace="neural-hive-orchestration",
            label_selector="app=consensus-engine",
            tail_lines=300,
        )

        # Erros esperados nos logs
        error_indicators = [
            "Invalid magic byte",
            "invalid magic byte",
            "magic byte",
            "deserialization error",
            "SerializationException",
            "AvroDeserializer",
            "failed to deserialize",
            "Falha ao deserializar",
        ]

        error_found = any(indicator in logs for indicator in error_indicators)
        assert error_found, (
            f"Erro de magic byte n√£o encontrado nos logs do consensus-engine. "
            f"Esperado um dos: {error_indicators}. "
            f"Plan ID: {fake_plan_id}"
        )

        print(f"\n‚úÖ Erro de magic byte detectado nos logs para plan_id: {fake_plan_id}")

        # Verificar que NENHUMA ConsolidatedDecision foi emitida para esse plan_id
        consolidated_decision_avro_consumer.subscribe([test_kafka_topics["plans.consensus"]])

        decision_found = False
        start_time = time.time()
        check_timeout = 15  # Tempo suficiente para verificar que n√£o h√° decis√£o

        while time.time() - start_time < check_timeout:
            msg = await asyncio.to_thread(consolidated_decision_avro_consumer.poll, 1.0)
            if msg is None:
                continue
            if msg.error():
                continue

            value = msg.value()
            if isinstance(value, dict) and value.get("plan_id") == fake_plan_id:
                decision_found = True
                break

        assert not decision_found, (
            f"ConsolidatedDecision foi emitida para plan_id inv√°lido: {fake_plan_id}. "
            "Esperava-se que mensagens com magic byte inv√°lido n√£o gerassem decis√µes."
        )

        print(f"   ‚úÖ Nenhuma ConsolidatedDecision emitida para plan_id inv√°lido")

        # Verificar que o consumer do consensus-engine continua ativo
        # Publicar uma mensagem v√°lida ap√≥s o erro e verificar que √© processada
        # (para garantir que o servi√ßo n√£o travou)

        # Obter logs novamente para verificar se n√£o h√° erros fatais
        logs_after = get_pod_logs(
            k8s_client,
            namespace="neural-hive-orchestration",
            label_selector="app=consensus-engine",
            tail_lines=50,
        )

        fatal_indicators = [
            "FATAL",
            "panic",
            "Panic",
            "crashed",
            "terminated",
            "OOMKilled",
        ]

        service_crashed = any(indicator in logs_after for indicator in fatal_indicators)
        assert not service_crashed, (
            f"Consensus-engine aparenta ter falhado fatalmente ap√≥s mensagem inv√°lida. "
            f"Indicadores encontrados nos logs: {logs_after[-500:]}"
        )

        print(f"   ‚úÖ Consensus-engine continua ativo ap√≥s erro de magic byte")


# ============================================
# Teste 10: Performance
# ============================================


@pytest.mark.timeout(120)
@pytest.mark.performance
class TestPerformanceAvroSerialization:
    """Testes de performance de serializa√ß√£o Avro."""

    async def test_performance_avro_serialization_throughput(
        self,
        cognitive_plan_avro_producer: AvroProducer,
        complete_cognitive_plan_avro: Dict,
        test_kafka_topics: Dict[str, str],
    ):
        """
        Valida throughput de serializa√ß√£o Avro.

        M√©tricas:
        - Throughput >= 50 mensagens/segundo
        - Lat√™ncia m√©dia < 20ms por mensagem
        """
        num_messages = 100
        latencies = []

        start_time = time.time()

        for i in range(num_messages):
            plan = complete_cognitive_plan_avro.copy()
            plan["plan_id"] = f"perf-{uuid.uuid4().hex[:8]}"

            msg_start = time.time()
            cognitive_plan_avro_producer.produce(
                topic=test_kafka_topics["plans.ready"],
                value=plan,
            )
            latencies.append((time.time() - msg_start) * 1000)

        cognitive_plan_avro_producer.flush()
        total_time = time.time() - start_time

        # Calcular m√©tricas
        throughput = num_messages / total_time
        avg_latency = sum(latencies) / len(latencies)

        print(f"\nüìä M√©tricas de Performance Avro:")
        print(f"   Mensagens: {num_messages}")
        print(f"   Tempo Total: {total_time:.2f}s")
        print(f"   Throughput: {throughput:.2f} msgs/s")
        print(f"   Lat√™ncia M√©dia: {avg_latency:.2f}ms")

        # Valida√ß√µes
        assert throughput >= 50, f"Throughput baixo: {throughput:.2f} msgs/s"
        assert avg_latency < 20, f"Lat√™ncia alta: {avg_latency:.2f}ms"
