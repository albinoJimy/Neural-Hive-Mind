"""
Cognitive Plan Data Models

Define os modelos Pydantic para Planos Cognitivos (artefato central do Fluxo B).
"""

import uuid
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from enum import Enum
from pydantic import BaseModel, Field, validator


class RiskBand(str, Enum):
    """Risk classification bands"""
    LOW = 'low'
    MEDIUM = 'medium'
    HIGH = 'high'
    CRITICAL = 'critical'


class PlanStatus(str, Enum):
    """Plan status"""
    DRAFT = 'draft'
    VALIDATED = 'validated'
    APPROVED = 'approved'
    REJECTED = 'rejected'


class ApprovalStatus(str, Enum):
    """Approval status for cognitive plans"""
    PENDING = 'pending'
    APPROVED = 'approved'
    REJECTED = 'rejected'


class TaskNode(BaseModel):
    """Individual task node in the DAG"""

    task_id: str = Field(..., description='Unique task ID')
    task_type: str = Field(..., description='Task type (query, transform, validate, etc.)')
    description: str = Field(..., description='Task description')
    dependencies: List[str] = Field(
        default_factory=list,
        description='IDs of predecessor tasks'
    )
    estimated_duration_ms: Optional[int] = Field(
        None,
        description='Estimated duration in milliseconds'
    )
    required_capabilities: List[str] = Field(
        default_factory=list,
        description='Required capabilities'
    )
    parameters: Dict[str, Any] = Field(
        default_factory=dict,
        description='Task parameters'
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description='Additional metadata'
    )

    class Config:
        use_enum_values = True


class CognitivePlan(BaseModel):
    """
    Cognitive Plan generated by Semantic Translation Engine

    Represents the output of Fluxo B (B1-B6), containing:
    - DAG of executable tasks
    - Risk assessment
    - Explainability information
    - Execution metadata
    """

    # Plan identification
    plan_id: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        description='Unique plan ID'
    )
    version: str = Field(default='1.0.0', description='Plan version')
    intent_id: str = Field(..., description='Originating intent ID')
    correlation_id: Optional[str] = Field(None, description='Correlation ID')
    trace_id: Optional[str] = Field(None, description='OpenTelemetry trace ID')
    span_id: Optional[str] = Field(None, description='OpenTelemetry span ID')

    # DAG of tasks
    tasks: List[TaskNode] = Field(..., description='List of tasks in the plan')
    execution_order: List[str] = Field(..., description='Topological execution order')

    # Risk assessment
    risk_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description='Risk score (0-1)'
    )
    risk_band: RiskBand = Field(..., description='Risk classification')
    risk_factors: Dict[str, float] = Field(
        default_factory=dict,
        description='Individual risk factors'
    )

    # Explainability
    explainability_token: str = Field(..., description='Token for detailed explanation')
    reasoning_summary: str = Field(..., description='Reasoning summary')

    # Metadata
    status: PlanStatus = Field(default=PlanStatus.DRAFT, description='Plan status')
    created_at: datetime = Field(
        default_factory=datetime.utcnow,
        description='Creation timestamp'
    )
    valid_until: Optional[datetime] = Field(None, description='Plan validity')
    estimated_total_duration_ms: Optional[int] = Field(
        None,
        description='Total estimated duration'
    )
    complexity_score: float = Field(..., ge=0.0, description='Complexity score')

    # Original context
    original_domain: str = Field(..., description='Original intent domain')
    original_priority: str = Field(..., description='Original priority')
    original_security_level: str = Field(..., description='Original security level')

    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description='Additional metadata'
    )

    # Approval workflow fields
    requires_approval: bool = Field(
        default=False,
        description='Whether this plan requires human approval before execution'
    )
    approval_status: Optional[ApprovalStatus] = Field(
        None,
        description='Current approval status (pending/approved/rejected)'
    )
    approved_by: Optional[str] = Field(
        None,
        description='User ID or system identifier who approved the plan'
    )
    approved_at: Optional[datetime] = Field(
        None,
        description='Timestamp when the plan was approved'
    )

    # Destructive operation analysis fields
    is_destructive: bool = Field(
        default=False,
        description='Whether this plan contains destructive operations'
    )
    destructive_tasks: List[str] = Field(
        default_factory=list,
        description='List of task IDs identified as destructive operations'
    )
    risk_matrix: Optional[Dict[str, float]] = Field(
        None,
        description='Multi-domain risk scores (BUSINESS, SECURITY, OPERATIONAL, etc.)'
    )

    @validator('tasks')
    def validate_dag_acyclic(cls, v):
        """Validate that the DAG is acyclic"""
        import networkx as nx

        if not v:
            return v

        # Build graph
        G = nx.DiGraph()
        for task in v:
            G.add_node(task.task_id)
            for dep in task.dependencies:
                G.add_edge(dep, task.task_id)

        # Check for cycles
        if not nx.is_directed_acyclic_graph(G):
            cycles = list(nx.simple_cycles(G))
            raise ValueError(f'DAG contains cycles: {cycles}')

        return v

    @validator('execution_order')
    def validate_execution_order(cls, v, values):
        """Validate execution order matches tasks"""
        if 'tasks' not in values:
            return v

        task_ids = {task.task_id for task in values['tasks']}
        order_ids = set(v)

        if task_ids != order_ids:
            missing = task_ids - order_ids
            extra = order_ids - task_ids
            raise ValueError(
                f'Execution order mismatch. Missing: {missing}, Extra: {extra}'
            )

        return v

    def to_avro_dict(self) -> Dict[str, Any]:
        """Convert to Avro-compatible dictionary"""
        return {
            'plan_id': self.plan_id,
            'version': self.version,
            'intent_id': self.intent_id,
            'correlation_id': self.correlation_id,
            'trace_id': self.trace_id,
            'span_id': self.span_id,
            'tasks': [
                {
                    'task_id': task.task_id,
                    'task_type': task.task_type,
                    'description': task.description,
                    'dependencies': task.dependencies,
                    'estimated_duration_ms': task.estimated_duration_ms,
                    'required_capabilities': task.required_capabilities,
                    'parameters': {k: str(v) for k, v in task.parameters.items()} if isinstance(task.parameters, dict) else {},
                    'metadata': {k: str(v) for k, v in task.metadata.items()} if isinstance(task.metadata, dict) else {}
                }
                for task in self.tasks
            ],
            'execution_order': self.execution_order,
            'risk_score': self.risk_score,
            'risk_band': self.risk_band.value if hasattr(self.risk_band, 'value') else self.risk_band,
            'risk_factors': self.risk_factors,
            'explainability_token': self.explainability_token,
            'reasoning_summary': self.reasoning_summary,
            'status': self.status.value if hasattr(self.status, 'value') else self.status,
            'created_at': int(self.created_at.timestamp() * 1000),
            'valid_until': int(self.valid_until.timestamp() * 1000) if self.valid_until else None,
            'estimated_total_duration_ms': self.estimated_total_duration_ms,
            'complexity_score': self.complexity_score,
            'original_domain': self.original_domain,
            'original_priority': self.original_priority,
            'original_security_level': self.original_security_level,
            'metadata': {k: str(v) for k, v in self.metadata.items()} if isinstance(self.metadata, dict) else {},
            'requires_approval': self.requires_approval,
            'approval_status': self.approval_status.value if self.approval_status and hasattr(self.approval_status, 'value') else self.approval_status,
            'approved_by': self.approved_by,
            'approved_at': int(self.approved_at.timestamp() * 1000) if self.approved_at else None,
            'is_destructive': self.is_destructive,
            'destructive_tasks': self.destructive_tasks,
            'risk_matrix': self.risk_matrix if self.risk_matrix is not None else None,
            'schema_version': 1
        }

    def get_partition_key(self) -> str:
        """Get partition key for Kafka (by domain)"""
        return self.original_domain

    def to_cache_dict(self) -> Dict[str, Any]:
        """Compact version for Redis cache"""
        return {
            'plan_id': self.plan_id,
            'intent_id': self.intent_id,
            'risk_band': self.risk_band.value if hasattr(self.risk_band, 'value') else self.risk_band,
            'status': self.status.value if hasattr(self.status, 'value') else self.status,
            'num_tasks': len(self.tasks),
            'created_at': self.created_at.isoformat()
        }

    class Config:
        use_enum_values = True
        validate_assignment = True
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }
