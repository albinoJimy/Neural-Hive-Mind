# OpenTelemetry Collector Values Template
# Generated by Terraform for Neural Hive-Mind Observability Stack

mode: deployment

image:
  repository: otel/opentelemetry-collector-contrib
  tag: "0.88.0"

# Configuração de recursos
resources:
  requests:
    memory: "${resources.requests.memory}"
    cpu: "${resources.requests.cpu}"
  limits:
    memory: "${resources.limits.memory}"
    cpu: "${resources.limits.cpu}"

# Configuração de réplicas
replicaCount: 2

# Configuração de service
service:
  type: ClusterIP
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    jaeger-grpc:
      enabled: true
      containerPort: 14250
      servicePort: 14250
      protocol: TCP
    jaeger-thrift:
      enabled: true
      containerPort: 14268
      servicePort: 14268
      protocol: TCP
    zipkin:
      enabled: true
      containerPort: 9411
      servicePort: 9411
      protocol: TCP
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

# Configuração do collector
config:
  exporters:
    # Prometheus exporter para métricas
    prometheus:
      endpoint: "0.0.0.0:8888"
      const_labels:
        cluster: neural-hive-main
        environment: production
      send_timestamps: true
      metric_expiration: 180m
      enable_open_metrics: true

    # Jaeger exporter para traces
    jaeger:
      endpoint: "${jaeger_endpoint}"
      tls:
        insecure: true
      sending_queue:
        enabled: true
        num_consumers: 2
        queue_size: 100
      retry_on_failure:
        enabled: true
        initial_interval: 5s
        max_interval: 30s
        max_elapsed_time: 300s

    # OTLP exporter para traces (backup)
    otlp/traces:
      endpoint: "${jaeger_endpoint}"
      tls:
        insecure: true
      headers:
        X-Neural-Hive-Source: "otel-collector"

    # Logging para debug
    logging:
      loglevel: info

  extensions:
    health_check:
      endpoint: 0.0.0.0:13133
    pprof:
      endpoint: 0.0.0.0:1777
    zpages:
      endpoint: 0.0.0.0:55679

  processors:
    # Memory limiter
    memory_limiter:
      check_interval: 1s
      limit_mib: 512
      spike_limit_mib: 128

    # Batch processor
    batch:
      send_batch_size: 1024
      timeout: 10s
      send_batch_max_size: 2048

    # Resource processor
    resource:
      attributes:
      - key: deployment.environment
        value: production
        action: upsert
      - key: neural.hive.cluster
        value: neural-hive-main
        action: upsert
      - key: k8s.cluster.name
        value: neural-hive-main
        action: upsert

    # Neural Hive attributes processor
    attributes/neural_hive:
      actions:
      # Extrair correlation IDs de headers HTTP
      - key: neural.hive.intent.id
        from_attribute: http.request.header.x-neural-hive-intent-id
        action: upsert
      - key: neural.hive.plan.id
        from_attribute: http.request.header.x-neural-hive-plan-id
        action: upsert
      - key: neural.hive.domain
        from_attribute: http.request.header.x-neural-hive-domain
        action: upsert
      - key: neural.hive.user.id
        from_attribute: http.request.header.x-neural-hive-user-id
        action: upsert

    # Tail sampling
    tail_sampling:
      decision_wait: 30s
      num_traces: 50000
      expected_new_traces_per_sec: 100
      policies:
      # Sempre amostrar errors
      - name: neural_hive_errors
        type: status_code
        status_code:
          status_codes: [ERROR]
      # Sempre amostrar alta latência
      - name: neural_hive_high_latency
        type: latency
        latency:
          threshold_ms: 1000
      # Sampling probabilístico
      - name: neural_hive_probabilistic
        type: probabilistic
        probabilistic:
          sampling_percentage: ${sampling_rate}

    # Filter para reduzir ruído
    filter/drop_metrics:
      metrics:
        exclude:
          match_type: regexp
          metric_names:
          - "go_.*"
          - "process_.*"
          - "promhttp_.*"

  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
          max_recv_msg_size: 4194304
        http:
          endpoint: 0.0.0.0:4318

    # Jaeger receiver
    jaeger:
      protocols:
        grpc:
          endpoint: 0.0.0.0:14250
        thrift_http:
          endpoint: 0.0.0.0:14268

    # Zipkin receiver
    zipkin:
      endpoint: 0.0.0.0:9411

    # Prometheus receiver
    prometheus:
      config:
        global:
          scrape_interval: 15s
          evaluation_interval: 15s
        scrape_configs:
        - job_name: 'neural-hive-services'
          kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
              - default
              - neural-hive
              - gateway
              - observability
          relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_neural_hive_metrics]
            action: keep
            regex: enabled
          - source_labels: [__meta_kubernetes_pod_label_neural_hive_component]
            target_label: neural_hive_component
          - source_labels: [__meta_kubernetes_pod_label_neural_hive_layer]
            target_label: neural_hive_layer

  service:
    extensions: [health_check, pprof, zpages]
    pipelines:
      traces:
        receivers: [otlp, jaeger, zipkin]
        processors: [memory_limiter, resource, attributes/neural_hive, tail_sampling, batch]
        exporters: [jaeger, otlp/traces, logging]

      metrics:
        receivers: [otlp, prometheus]
        processors: [memory_limiter, resource, attributes/neural_hive, filter/drop_metrics, batch]
        exporters: [prometheus]

      logs:
        receivers: [otlp]
        processors: [memory_limiter, resource, attributes/neural_hive, batch]
        exporters: [logging]

    telemetry:
      logs:
        level: "info"
      metrics:
        level: detailed
        address: 0.0.0.0:8888

# Pod Security Context
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 10001
  fsGroup: 10001

# Container Security Context
securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL

# Node affinity para nós de observabilidade
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      preference:
        matchExpressions:
        - key: "node-role.kubernetes.io/observability"
          operator: "In"
          values: ["true"]

# Tolerations para nós dedicados
tolerations:
- key: "observability"
  operator: "Equal"
  value: "true"
  effect: "NoSchedule"

# Labels e annotations
podLabels:
  neural.hive/component: "telemetry-collector"
  neural.hive/layer: "observabilidade"

podAnnotations:
  neural.hive/metrics: "enabled"
  prometheus.io/scrape: "true"
  prometheus.io/port: "8888"
  prometheus.io/path: "/metrics"

# ServiceMonitor para Prometheus
serviceMonitor:
  enabled: true
  labels:
    prometheus: kube-prometheus
  interval: 30s