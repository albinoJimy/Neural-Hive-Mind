# Configura√ß√£o de Exemplo do Alertmanager para Integra√ß√£o Slack/PagerDuty
#
# IMPORTANTE: Este √© um arquivo de EXEMPLO. Para usar em produ√ß√£o:
# 1. Substitua os placeholders YOUR_WEBHOOK_URL, YOUR_INTEGRATION_KEY, etc.
# 2. Armazene credenciais sens√≠veis em Kubernetes Secrets
# 3. Merge com sua configura√ß√£o existente do Alertmanager
# 4. Teste as rotas com: amtool config routes test
#
# Para merge com ConfigMap existente:
# kubectl get configmap alertmanager-config -n monitoring -o yaml > current-config.yaml
# # Merge manualmente as se√ß√µes 'routes' e 'receivers'
# kubectl apply -f merged-config.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-slack-pagerduty-config
  namespace: monitoring
  labels:
    app.kubernetes.io/name: alertmanager
    app.kubernetes.io/component: config
  annotations:
    description: "Exemplo de configura√ß√£o para routing de alertas SLA para Slack e PagerDuty"
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      # Slack API URL - obter em: https://api.slack.com/messaging/webhooks
      slack_api_url: 'https://hooks.slack.com/services/YOUR_WEBHOOK_URL'
      # PagerDuty API URL
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

    # Templates customizados para formata√ß√£o de mensagens
    templates:
      - '/etc/alertmanager/templates/*.tmpl'

    # Configura√ß√£o de Rotas
    route:
      # Receptor padr√£o para alertas n√£o matcheados
      receiver: 'default-receiver'

      # Agrupar alertas por estas labels
      group_by: ['alertname', 'cluster', 'service']

      # Tempo para aguardar antes de enviar notifica√ß√£o ap√≥s primeiro alerta
      group_wait: 10s

      # Tempo para aguardar antes de enviar notifica√ß√£o sobre novos alertas do mesmo grupo
      group_interval: 5m

      # Tempo para aguardar antes de reenviar notifica√ß√£o
      repeat_interval: 4h

      # Rotas espec√≠ficas para alertas de SLA
      routes:
        # Alertas Cr√≠ticos de SLA ‚Üí PagerDuty
        - match:
            severity: critical
            component: orchestrator-dynamic
          receiver: pagerduty-critical
          group_wait: 5s
          repeat_interval: 1h
          continue: true  # Continua processando outras rotas

        # Alertas de Warning de SLA ‚Üí Slack
        - match:
            severity: warning
            component: orchestrator-dynamic
          receiver: slack-sla-warnings
          group_wait: 30s
          repeat_interval: 4h

        # Budget Cr√≠tico ‚Üí Slack + PagerDuty
        - match:
            alertname: OrchestratorBudgetCritical
          receiver: slack-sla-warnings
          continue: true
        - match:
            alertname: OrchestratorBudgetCritical
          receiver: pagerduty-critical

        # Deadline Approaching ‚Üí Slack
        - match:
            alertname: OrchestratorDeadlineApproaching
          receiver: slack-sla-warnings

        # Deadline Exceeded ‚Üí PagerDuty
        - match:
            alertname: OrchestratorDeadlineExceeded
          receiver: pagerduty-critical

        # Burn Rate Alto ‚Üí Slack
        - match:
            alertname: OrchestratorHighBurnRate
          receiver: slack-sla-warnings

        # Schema Registry Alerts ‚Üí Slack
        - match:
            neural_hive_component: schema-registry
            severity: warning
          receiver: slack-schema-registry-warnings
          group_wait: 30s
          repeat_interval: 4h

        # Schema Registry Critical ‚Üí Slack + PagerDuty
        - match:
            neural_hive_component: schema-registry
            severity: critical
          receiver: slack-schema-registry-warnings
          continue: true
        - match:
            neural_hive_component: schema-registry
            severity: critical
          receiver: pagerduty-critical

        # SLA Management System Webhook (regex match para todas SLOs)
        - match_re:
            slo: '.*'
          receiver: sla-management-system-webhook
          continue: true

    # Configura√ß√£o de Receivers
    receivers:
      # Receptor padr√£o
      - name: 'default-receiver'
        slack_configs:
          - channel: '#alertas-gerais'
            title: '[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

      # Slack para avisos de SLA
      - name: 'slack-sla-warnings'
        slack_configs:
          - channel: '#sla-alerts'  # CONFIGURAR: Seu canal Slack
            send_resolved: true
            title_link: '{{ .ExternalURL }}'

            # T√≠tulo com status e nome do alerta
            title: '[{{ .Status | toUpper }}] SLA Alert: {{ .GroupLabels.alertname }}'

            # Cor baseada na severidade
            color: '{{ if eq .Status "firing" }}{{ if eq .CommonLabels.severity "critical" }}danger{{ else }}warning{{ end }}{{ else }}good{{ end }}'

            # Template da mensagem (referencia slack-message-template.tmpl)
            text: '{{ template "slack.sla.text" . }}'

            # Campos estruturados
            fields:
              - title: "Servi√ßo"
                value: "{{ .CommonLabels.service_name | default \"orchestrator-dynamic\" }}"
                short: true
              - title: "Severidade"
                value: "{{ .CommonLabels.severity | toUpper }}"
                short: true
              - title: "Workflow ID"
                value: "{{ .CommonAnnotations.workflow_id | default \"N/A\" }}"
                short: true
              - title: "Budget Restante"
                value: "{{ .CommonAnnotations.budget_remaining | default \"N/A\" }}%"
                short: true
              - title: "Tempo Restante"
                value: "{{ .CommonAnnotations.remaining_seconds | default \"N/A\" }}s"
                short: true
              - title: "Burn Rate"
                value: "{{ .CommonAnnotations.burn_rate | default \"N/A\" }}"
                short: true

            # A√ß√µes (bot√µes)
            actions:
              - type: button
                text: 'üìä View Dashboard'
                url: '{{ .CommonAnnotations.dashboard_url | default "http://grafana/d/fluxo-c-orchestration" }}'
              - type: button
                text: 'üìñ Runbook'
                url: '{{ .CommonAnnotations.runbook_url | default "https://docs.example.com/runbooks/sla" }}'
              - type: button
                text: 'üîá Silence'
                url: '{{ .ExternalURL }}/#/silences/new?filter=%7Balertname%3D%22{{ .GroupLabels.alertname }}%22%7D'

      # PagerDuty para alertas cr√≠ticos
      - name: 'pagerduty-critical'
        pagerduty_configs:
          - service_key: 'YOUR_INTEGRATION_KEY'  # CONFIGURAR: Sua integration key do PagerDuty
            send_resolved: true

            # Routing key baseada no alerta
            routing_key: '{{ .GroupLabels.alertname }}'

            # Descri√ß√£o do incidente
            description: '[{{ .CommonLabels.severity | toUpper }}] {{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'

            # Severidade mapeada
            severity: '{{ if eq .CommonLabels.severity "critical" }}critical{{ else if eq .CommonLabels.severity "warning" }}warning{{ else }}info{{ end }}'

            # Detalhes customizados
            details:
              alert_name: '{{ .GroupLabels.alertname }}'
              service: '{{ .CommonLabels.service_name | default "orchestrator-dynamic" }}'
              component: '{{ .CommonLabels.component }}'
              workflow_id: '{{ .CommonAnnotations.workflow_id | default "N/A" }}'
              ticket_id: '{{ .CommonAnnotations.ticket_id | default "N/A" }}'
              budget_remaining: '{{ .CommonAnnotations.budget_remaining | default "N/A" }}%'
              remaining_seconds: '{{ .CommonAnnotations.remaining_seconds | default "N/A" }}s'
              burn_rate: '{{ .CommonAnnotations.burn_rate | default "N/A" }}'
              violation_type: '{{ .CommonAnnotations.violation_type | default "N/A" }}'
              delay_ms: '{{ .CommonAnnotations.delay_ms | default "N/A" }}'
              firing_alerts: '{{ .Alerts.Firing | len }}'
              resolved_alerts: '{{ .Alerts.Resolved | len }}'

            # Links para dashboard e logs
            links:
              - href: '{{ .CommonAnnotations.dashboard_url | default "http://grafana/d/fluxo-c-orchestration" }}'
                text: 'Grafana Dashboard'
              - href: '{{ .CommonAnnotations.runbook_url | default "https://docs.example.com/runbooks/sla" }}'
                text: 'Runbook'
              - href: '{{ .GeneratorURL }}'
                text: 'Prometheus Alert'

      # Slack para alertas de Schema Registry
      - name: 'slack-schema-registry-warnings'
        slack_configs:
          - channel: '#schema-registry-alerts'
            send_resolved: true
            title_link: '{{ .ExternalURL }}'
            title: '[{{ .Status | toUpper }}] Schema Registry: {{ .GroupLabels.alertname }}'
            color: '{{ if eq .Status "firing" }}{{ if eq .CommonLabels.severity "critical" }}danger{{ else }}warning{{ end }}{{ else }}good{{ end }}'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
            fields:
              - title: "Severidade"
                value: "{{ .CommonLabels.severity | toUpper }}"
                short: true
              - title: "Componente"
                value: "{{ .CommonLabels.neural_hive_component }}"
                short: true
              - title: "Namespace"
                value: "kafka"
                short: true
            actions:
              - type: button
                text: 'üìä View Dashboard'
                url: 'https://grafana.neural-hive.local/d/schema-registry-overview'
              - type: button
                text: 'üìñ Runbook'
                url: '{{ .CommonAnnotations.runbook_url | default "https://docs.neural-hive.local/runbooks/schema-registry" }}'
              - type: button
                text: 'üîß Run Init Job'
                url: 'https://docs.neural-hive.local/runbooks/schema-registry-init-job'

      # SLA Management System Webhook (refer√™ncia da configura√ß√£o existente)
      - name: 'sla-management-system-webhook'
        webhook_configs:
          - url: 'http://sla-management-system.neural-hive-orchestration.svc.cluster.local:8000/webhooks/alertmanager'
            send_resolved: true
            http_config:
              follow_redirects: true
            max_alerts: 0  # Sem limite de alertas por requisi√ß√£o

    # Regras de Inibi√ß√£o
    inhibit_rules:
      # Inibir warnings quando cr√≠ticos est√£o ativos (mesmo alertname e servi√ßo)
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'service_name']

      # Inibir DeadlineApproaching quando DeadlineExceeded est√° ativo
      - source_match:
          alertname: 'OrchestratorDeadlineExceeded'
        target_match:
          alertname: 'OrchestratorDeadlineApproaching'
        equal: ['workflow_id']

      # Inibir BudgetWarning quando BudgetCritical est√° ativo
      - source_match:
          alertname: 'OrchestratorBudgetCritical'
        target_match:
          alertname: 'OrchestratorBudgetWarning'
        equal: ['service_name']

      # Inibir BudgetCritical quando BudgetExhausted est√° ativo
      - source_match:
          alertname: 'OrchestratorBudgetExhausted'
        target_match:
          alertname: 'OrchestratorBudgetCritical'
        equal: ['service_name']

---
# INSTRU√á√ïES DE DEPLOYMENT
#
# 1. OBTER CREDENCIAIS:
#
#    Slack Webhook URL:
#    - Acesse: https://api.slack.com/messaging/webhooks
#    - Crie um Incoming Webhook para seu workspace
#    - Copie a URL: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXX
#    - Substitua YOUR_WEBHOOK_URL na configura√ß√£o global.slack_api_url
#
#    PagerDuty Integration Key:
#    - Acesse: https://your-subdomain.pagerduty.com/services
#    - Selecione o servi√ßo ou crie um novo
#    - V√° em "Integrations" ‚Üí "Add Integration"
#    - Escolha "Events API V2"
#    - Copie a "Integration Key"
#    - Substitua YOUR_INTEGRATION_KEY em pagerduty_configs.service_key
#
# 2. CRIAR KUBERNETES SECRET (recomendado para produ√ß√£o):
#
#    kubectl create secret generic alertmanager-secrets -n monitoring \
#      --from-literal=slack-webhook-url='https://hooks.slack.com/services/...' \
#      --from-literal=pagerduty-key='YOUR_INTEGRATION_KEY'
#
#    Referencie no ConfigMap usando vari√°veis de ambiente ou volume mounts
#
# 3. MERGE COM CONFIGURA√á√ÉO EXISTENTE:
#
#    # Backup da configura√ß√£o atual
#    kubectl get configmap alertmanager-config -n monitoring -o yaml > alertmanager-backup.yaml
#
#    # Edite manualmente para merge das se√ß√µes 'routes' e 'receivers'
#    # N√ÉO substitua toda a configura√ß√£o, apenas adicione as novas rotas/receivers
#
# 4. APLICAR CONFIGURA√á√ÉO:
#
#    kubectl apply -f alertmanager-slack-pagerduty-config.yaml
#
#    # OU merge com existente:
#    kubectl apply -f alertmanager-merged-config.yaml
#
# 5. VALIDAR CONFIGURA√á√ÉO:
#
#    # Verificar se ConfigMap foi aplicado
#    kubectl get configmap alertmanager-slack-pagerduty-config -n monitoring
#
#    # Verificar logs do Alertmanager
#    kubectl logs -n monitoring -l app=alertmanager --tail=50
#
#    # Testar rotas com amtool
#    kubectl exec -n monitoring alertmanager-0 -- amtool config routes test \
#      --config.file=/etc/alertmanager/alertmanager.yml \
#      alertname=OrchestratorBudgetCritical severity=critical
#
# 6. TESTAR ALERTAS:
#
#    # Enviar alerta de teste via curl
#    curl -H "Content-Type: application/json" -d '[{
#      "labels": {
#        "alertname": "OrchestratorBudgetCritical",
#        "severity": "critical",
#        "component": "orchestrator-dynamic",
#        "service_name": "orchestrator-dynamic"
#      },
#      "annotations": {
#        "summary": "Budget cr√≠tico de teste",
#        "budget_remaining": "15",
#        "burn_rate": "8.5"
#      }
#    }]' http://alertmanager:9093/api/v1/alerts
#
# 7. TROUBLESHOOTING:
#
#    # Verificar status do Alertmanager
#    kubectl get pods -n monitoring -l app=alertmanager
#
#    # Verificar configura√ß√£o carregada
#    kubectl exec -n monitoring alertmanager-0 -- amtool config show
#
#    # Verificar alertas ativos
#    kubectl exec -n monitoring alertmanager-0 -- amtool alert
#
#    # Verificar silences
#    kubectl exec -n monitoring alertmanager-0 -- amtool silence query
#
# 8. CONFIGURA√á√ïES ADICIONAIS:
#
#    - Rate limiting: Adicione 'group_interval' e 'repeat_interval' nas rotas
#    - Timeouts: Configure http_config.timeout para webhooks
#    - Retries: Configure max_retries em webhook_configs
#    - TLS: Configure tls_config para HTTPS endpoints
#
# REFER√äNCIAS:
#   - Alertmanager Config: https://prometheus.io/docs/alerting/latest/configuration/
#   - Slack API: https://api.slack.com/messaging/webhooks
#   - PagerDuty Events API: https://developer.pagerduty.com/docs/events-api-v2/overview/
