# üîß Comandos √öteis - Neural Hive-Mind Local

## üìã Monitoramento

### Status Geral
```bash
# Status de todos os pods
kubectl get pods -A | grep neural-hive

# Status dos namespaces
kubectl get namespaces | grep neural-hive

# Status dos servi√ßos
kubectl get svc -A | grep neural-hive

# Verificar recursos por namespace
kubectl top pods -A | grep neural-hive
```

### Logs
```bash
# Logs do servi√ßo de teste
kubectl logs -f deployment/neural-test-service -n neural-hive-system

# Logs do Istio
kubectl logs -f deployment/istiod -n istio-system

# Logs do Gatekeeper
kubectl logs -f deployment/gatekeeper-controller-manager -n gatekeeper-system
```

## üåê Acesso aos Dashboards

### Kiali (Service Mesh)
```bash
kubectl port-forward svc/kiali 20001:20001 -n istio-system
# Acesse: http://localhost:20001
```

### Grafana (M√©tricas)
```bash
kubectl port-forward svc/grafana 3000:3000 -n istio-system
# Acesse: http://localhost:3000
```

### Jaeger (Tracing)
```bash
kubectl port-forward svc/jaeger 16686:16686 -n istio-system
# Acesse: http://localhost:16686
```

### Prometheus (M√©tricas Raw)
```bash
kubectl port-forward svc/prometheus 9090:9090 -n istio-system
# Acesse: http://localhost:9090
```

## üß™ Teste e Desenvolvimento

### Acessar Servi√ßo de Teste
```bash
# Via port-forward
kubectl port-forward svc/neural-test-service 8080:80 -n neural-hive-system
# Acesse: http://localhost:8080

# Teste interno
kubectl exec -n neural-hive-system deployment/neural-test-service -- curl localhost
```

### Deploy de Novos Servi√ßos
```bash
# Template b√°sico
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: meu-servico
  namespace: neural-hive-cognition
  labels:
    app: meu-servico
spec:
  replicas: 1
  selector:
    matchLabels:
      app: meu-servico
  template:
    metadata:
      labels:
        app: meu-servico
      annotations:
        sidecar.istio.io/inject: "true"
    spec:
      containers:
      - name: app
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: meu-servico
  namespace: neural-hive-cognition
spec:
  selector:
    app: meu-servico
  ports:
  - port: 80
    targetPort: 80
EOF
```

## üõ°Ô∏è Pol√≠ticas e Seguran√ßa

### Verificar Pol√≠ticas OPA
```bash
# Listar constraint templates
kubectl get constrainttemplates

# Listar constraints ativos
kubectl get constraints -A

# Verificar viola√ß√µes
kubectl get neuralhivemtlsrequired -A -o yaml
```

### Verificar mTLS
```bash
# Status de peer authentication
kubectl get peerauthentication -A

# Testar mTLS entre servi√ßos
istioctl authn tls-check neural-test-service.neural-hive-system
```

## üîÑ Limpeza e Reset

### Limpar Deployment Local
```bash
# Remover servi√ßos de teste
kubectl delete -f test-service-local.yaml

# Remover pol√≠ticas aplicadas
kubectl delete constrainttemplates --all
kubectl delete peerauthentication -A --all

# Reset completo (CUIDADO!)
kubectl delete namespace neural-hive-cognition neural-hive-orchestration neural-hive-execution neural-hive-observability
```

### Restart de Componentes
```bash
# Restart do Gatekeeper
kubectl rollout restart deployment gatekeeper-controller-manager -n gatekeeper-system
kubectl rollout restart deployment gatekeeper-audit -n gatekeeper-system

# Restart do Istio
kubectl rollout restart deployment istiod -n istio-system
```

## üö® Troubleshooting

### Pods n√£o iniciam
```bash
# Verificar eventos
kubectl get events --sort-by='.lastTimestamp' -A

# Descrever pod problem√°tico
kubectl describe pod <nome-do-pod> -n <namespace>

# Verificar recursos
kubectl top nodes
kubectl top pods -A
```

### Problemas de Conectividade
```bash
# Testar DNS
kubectl run dns-test --image=busybox --rm -it -- nslookup kubernetes.default

# Testar conectividade entre namespaces
kubectl run network-test --image=busybox --rm -it --namespace neural-hive-system -- ping neural-test-service
```

### Problemas com Istio
```bash
# Verificar configura√ß√£o do mesh
istioctl analyze

# Status dos proxies
istioctl proxy-status

# Configura√ß√£o de um pod espec√≠fico
istioctl proxy-config cluster <pod-name>.<namespace>
```

### Debug de Pol√≠ticas OPA
```bash
# Logs detalhados do Gatekeeper
kubectl logs deployment/gatekeeper-controller-manager -n gatekeeper-system --tail=50

# Testar cria√ß√£o de recurso
kubectl apply --dry-run=server -f meu-arquivo.yaml
```

## üìö Refer√™ncias R√°pidas

### Estrutura de Namespaces
- `neural-hive-system`: Componentes principais e testes
- `neural-hive-cognition`: Processamento e interpreta√ß√£o
- `neural-hive-orchestration`: Coordena√ß√£o e workflow
- `neural-hive-execution`: Agentes e workers
- `neural-hive-observability`: M√©tricas, logs e tracing

### Portas Padr√£o
- Kiali: 20001
- Grafana: 3000
- Jaeger: 16686
- Prometheus: 9090
- Servi√ßo de teste: 8080 (via port-forward)

---

ü§ñ **Para mais informa√ß√µes, consulte DEPLOYMENT_LOCAL.md**
## üß™ Testes da Fase 1

### Script R√°pido de Teste
```bash
# Executar teste automatizado da Fase 1
./testar-fase1.sh

# Parar ambiente de teste
docker compose -f docker-compose-test.yml down

# Ver logs em tempo real
docker compose -f docker-compose-test.yml logs -f

# Ver logs espec√≠ficos
docker compose -f docker-compose-test.yml logs -f kafka
docker compose -f docker-compose-test.yml logs -f redis
```

### Testes Manuais

#### Kafka
```bash
# Criar t√≥pico
docker exec kafka kafka-topics --bootstrap-server localhost:9092 \
  --create --topic test-topic --partitions 3 --replication-factor 1

# Listar t√≥picos
docker exec kafka kafka-topics --bootstrap-server localhost:9092 --list

# Publicar mensagem
echo "test message" | docker exec -i kafka kafka-console-producer \
  --bootstrap-server localhost:9092 --topic test-topic

# Consumir mensagens
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 --topic test-topic --from-beginning
```

#### Redis
```bash
# Conectar ao Redis CLI
docker exec -it redis redis-cli

# Testar PING/PONG
docker exec redis redis-cli ping

# SET/GET valores
docker exec redis redis-cli SET mykey "myvalue"
docker exec redis redis-cli GET mykey

# Listar todas as chaves
docker exec redis redis-cli KEYS '*'
```

### Relat√≥rios
```bash
# Ver relat√≥rio de teste
cat TESTE_FASE1_RESULTADO.md

# Gerar novo relat√≥rio
./testar-fase1.sh
```

---

## üéØ Kubernetes - Fase 2 (Kubeadm)

### Comandos do Cluster

```bash
# Status do cluster
kubectl cluster-info
kubectl get nodes
kubectl get pods -A

# Validar Fase 2
./validar-fase2-k8s.sh

# Namespaces
kubectl get namespaces
kubectl get all -n redis-cluster
kubectl get all -n mongodb-cluster
```

### MongoDB

```bash
# Nome do pod (substitua <pod-name> pelo nome real)
MONGODB_POD=$(kubectl get pods -n mongodb-cluster -o name | grep mongodb | head -1 | sed 's/pod\///')

# Conectar ao MongoDB
kubectl exec -it -n mongodb-cluster $MONGODB_POD -- mongosh mongodb://localhost:27017 -u root -p local_dev_password --authenticationDatabase admin

# Testar ping
kubectl exec -n mongodb-cluster $MONGODB_POD -- mongosh mongodb://localhost:27017 -u root -p local_dev_password --authenticationDatabase admin --eval "db.adminCommand('ping')"

# Listar databases
kubectl exec -n mongodb-cluster $MONGODB_POD -- mongosh mongodb://localhost:27017 -u root -p local_dev_password --authenticationDatabase admin --eval "show dbs"

# Inserir documento de teste
kubectl exec -n mongodb-cluster $MONGODB_POD -- mongosh mongodb://localhost:27017/neural_hive -u root -p local_dev_password --authenticationDatabase admin --eval 'db.test.insertOne({msg: "Hello from K8s"})'
```

### Redis

```bash
# Nome do pod
REDIS_POD=$(kubectl get pods -n redis-cluster -o name | head -1)

# Conectar ao Redis
kubectl exec -it -n redis-cluster $REDIS_POD -- redis-cli

# Testar PING
kubectl exec -n redis-cluster $REDIS_POD -- redis-cli ping

# SET/GET
kubectl exec -n redis-cluster $REDIS_POD -- redis-cli SET mykey "value"
kubectl exec -n redis-cluster $REDIS_POD -- redis-cli GET mykey

# Listar todas as chaves
kubectl exec -n redis-cluster $REDIS_POD -- redis-cli KEYS '*'
```

### Persistent Volumes

```bash
# Listar PV e PVC
kubectl get pv
kubectl get pvc -A

# Detalhes de storage MongoDB
kubectl describe pvc -n mongodb-cluster
kubectl get storageclass

# Ver uso de disco
kubectl exec -n mongodb-cluster $MONGODB_POD -- df -h
```

### Helm

```bash
# Listar releases instalados
helm list -A

# Status do MongoDB
helm status mongodb -n mongodb-cluster

# Atualizar valores
helm upgrade mongodb bitnami/mongodb -n mongodb-cluster --set auth.rootPassword=new_password

# Desinstalar (cuidado!)
helm uninstall mongodb -n mongodb-cluster
```

---

## üß™ Testes de Integra√ß√£o

### Teste End-to-End Kubernetes

```bash
# Executar teste completo de integra√ß√£o
./teste-integracao-k8s.py

# O teste valida:
# - Redis CRUD
# - MongoDB CRUD
# - Kafka topics
# - Integra√ß√£o entre componentes
```

### Valida√ß√µes Dispon√≠veis

```bash
# Fase 1 (Docker Compose)
./testar-fase1.sh
./test-intent-flow.py

# Fase 2 (Kubernetes)
./validar-fase2-k8s.sh
./teste-integracao-k8s.py
```

---

## üìä Status do Sistema

### Ver todos os recursos

```bash
# Vis√£o geral
kubectl get all -A | grep -E "(kafka|redis|mongodb)"

# Status dos pods
kubectl get pods -A --field-selector=status.phase=Running

# Usar recursos
kubectl top pods -A
kubectl top nodes

# Persistent Volumes
kubectl get pv,pvc -A
```

### Kafka (Strimzi)

```bash
# Status do cluster Kafka
kubectl get kafka -n kafka

# Listar topics
kubectl get kafkatopic -n kafka

# Logs do broker
kubectl logs neural-hive-kafka-broker-0 -n kafka

# Logs do operator
kubectl logs -l name=strimzi-cluster-operator -n kafka
```

---

## üß™ Teste Fase 1 - Specialists

### Script de Teste Automatizado

```bash
# Executar teste completo dos 5 specialists
./test-specialists-v2.sh

# Resultado esperado:
# ‚úì Cluster Kubernetes acess√≠vel
# ‚úì Todos os deployments com replicas prontas (1/1)
# ‚úì Todos os pods com status Running
# ‚úì Todos os services ativos com portas 50051 (gRPC), 8000 (HTTP), 8080 (Prometheus)
# ‚úì gRPC server iniciado em cada specialist
# ‚úì HTTP server (Uvicorn/FastAPI) iniciado em cada specialist
```

### Verifica√ß√µes Individuais por Specialist

```bash
# Listar todos os specialists
kubectl get deployments -A | grep specialist

# Status dos 5 specialists
for spec in business behavior evolution architecture technical; do
  echo "=== specialist-$spec ==="
  kubectl get pods -n specialist-$spec
  kubectl get svc -n specialist-$spec
done
```

### Verificar Logs dos Specialists

```bash
# Business Specialist
kubectl logs -n specialist-business -l app.kubernetes.io/name=specialist-business --tail=50

# Behavior Specialist
kubectl logs -n specialist-behavior -l app.kubernetes.io/name=specialist-behavior --tail=50

# Evolution Specialist
kubectl logs -n specialist-evolution -l app.kubernetes.io/name=specialist-evolution --tail=50

# Architecture Specialist
kubectl logs -n specialist-architecture -l app.kubernetes.io/name=specialist-architecture --tail=50

# Technical Specialist
kubectl logs -n specialist-technical -l app.kubernetes.io/name=specialist-technical --tail=50
```

### Testar Health Endpoints

```bash
# Testar via port-forward (exemplo com specialist-business)
kubectl port-forward -n specialist-business svc/specialist-business 8000:8000 &
curl http://localhost:8000/health
curl http://localhost:8000/ready
curl http://localhost:8000/metrics

# Ou testar diretamente do pod
kubectl exec -n specialist-business deployment/specialist-business -- wget -qO- http://localhost:8000/health
```

### Verificar Services e IPs

```bash
# Listar IPs e portas de todos os specialists
kubectl get svc -A | grep specialist

# Exemplo de output esperado:
# specialist-business       specialist-business       ClusterIP   10.102.250.6    <none>   50051/TCP,8000/TCP,8080/TCP
# specialist-behavior       specialist-behavior       ClusterIP   10.97.108.160   <none>   50051/TCP,8000/TCP,8080/TCP
# specialist-evolution      specialist-evolution      ClusterIP   10.98.45.222    <none>   50051/TCP,8000/TCP,8080/TCP
# specialist-architecture   specialist-architecture   ClusterIP   10.103.172.21   <none>   50051/TCP,8000/TCP,8080/TCP
# specialist-technical      specialist-technical      ClusterIP   10.103.87.56    <none>   50051/TCP,8000/TCP,8080/TCP
```

### Status de Inicializa√ß√£o

```bash
# Verificar se todos os servidores est√£o rodando
for spec in business behavior evolution architecture technical; do
  echo "=== Verificando $spec ==="
  POD=$(kubectl get pods -n specialist-$spec -l app.kubernetes.io/name=specialist-$spec -o jsonpath='{.items[0].metadata.name}')
  echo "gRPC Server:"
  kubectl logs -n specialist-$spec $POD --tail=100 | grep "gRPC server started" || echo "  N√£o encontrado"
  echo "HTTP Server:"
  kubectl logs -n specialist-$spec $POD --tail=100 | grep -E "Uvicorn running|HTTP server started" || echo "  N√£o encontrado"
  echo ""
done
```

### Troubleshooting Specialists

```bash
# Verificar erros nos logs
for spec in business behavior evolution architecture technical; do
  echo "=== Erros em specialist-$spec ==="
  kubectl logs -n specialist-$spec -l app.kubernetes.io/name=specialist-$spec --tail=100 | grep -i error
  echo ""
done

# Reiniciar um specialist espec√≠fico
kubectl rollout restart deployment/specialist-business -n specialist-business

# Reiniciar todos os specialists
for spec in business behavior evolution architecture technical; do
  kubectl rollout restart deployment/specialist-$spec -n specialist-$spec
done

# Verificar descri√ß√£o do pod para eventos
kubectl describe pod -n specialist-business -l app.kubernetes.io/name=specialist-business
```

### Observa√ß√µes Importantes

- **Modelos MLflow**: Os specialists tentam carregar modelos do MLflow mas usam fallback com heur√≠sticas quando n√£o encontrados
- **Autentica√ß√£o JWT**: Desabilitada por padr√£o em desenvolvimento (modo inseguro)
- **OpenTelemetry**: Instrumenta√ß√£o gRPC pode falhar mas n√£o impede funcionamento
- **Portas Expostas**:
  - 50051: gRPC API
  - 8000: HTTP/REST API (FastAPI)
  - 8080: M√©tricas Prometheus

### Endpoints Dispon√≠veis

Cada specialist exp√µe os seguintes endpoints HTTP:

```bash
GET /health       # Health check b√°sico
GET /ready        # Readiness probe
GET /metrics      # M√©tricas Prometheus
GET /status       # Status detalhado
POST /api/v1/feedback              # Submeter feedback
GET  /api/v1/feedback/{opinion_id} # Obter feedback espec√≠fico
GET  /api/v1/feedback/stats        # Estat√≠sticas de feedback
```

### Testes de Conectividade gRPC

```bash
# Teste completo de conectividade gRPC
./test-grpc-specialists.sh

# Teste de conectividade interna entre specialists
# (executa Python dentro do pod para testar conectividade)
cat test-connectivity-internal.py | kubectl exec -i -n specialist-business deployment/specialist-business -- python3

# Resultados esperados:
# ‚úì Porta gRPC (50051): Todas acess√≠veis
# ‚úì Porta HTTP (8000): Todas acess√≠veis
# ‚ö† Porta Prometheus (8080): Configurada mas pode n√£o estar bind corretamente
```

### Resultados dos Testes Fase 1

**Status Geral**: ‚úÖ APROVADO

- ‚úÖ Cluster Kubernetes: Operacional
- ‚úÖ Deployments: 5/5 com replicas prontas (1/1)
- ‚úÖ Pods: 5/5 em estado Running
- ‚úÖ Services: 5/5 ativos com ClusterIP
- ‚úÖ Endpoints: 5/5 com IPs v√°lidos
- ‚úÖ gRPC Server: 5/5 iniciados e respondendo
- ‚úÖ HTTP Server: 5/5 iniciados e respondendo
- ‚úÖ Conectividade TCP gRPC: 100% (15/15 testes)
- ‚úÖ Conectividade interna: 66.7% (10/15 - porta Prometheus n√£o acess√≠vel)
- ‚úÖ Resolu√ß√£o DNS: 100% entre todos specialists

**Observa√ß√µes**:
- Porta Prometheus (8080) est√° configurada mas n√£o est√° acess√≠vel externamente (poss√≠vel problema de binding)
- Portas gRPC (50051) e HTTP (8000) est√£o totalmente funcionais
- Comunica√ß√£o entre specialists funcionando via DNS do Kubernetes

**Scripts de Teste Dispon√≠veis**:
- `test-specialists-v2.sh` - Teste b√°sico de status e logs
- `test-grpc-specialists.sh` - Teste de conectividade gRPC
- `test-connectivity-internal.py` - Teste interno de conectividade
