# Comandos para Corre√ß√£o de Bloqueadores Cr√≠ticos
## Neural Hive-Mind - Fase 1

---

## üî¥ BLOQUEADOR 1: T√≥picos Kafka Faltantes

### Problema
```
‚ùå plans.ready n√£o existe
‚ùå plans.consensus n√£o existe
```

### Solu√ß√£o (5 minutos)

```bash
# Criar t√≥pico plans.ready
kubectl apply -f - <<EOF
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: plans-ready
  namespace: kafka
  labels:
    strimzi.io/cluster: neural-hive-kafka
spec:
  partitions: 3
  replicas: 1
  config:
    retention.ms: 604800000  # 7 dias
    segment.bytes: 1073741824
    cleanup.policy: delete
EOF

# Criar t√≥pico plans.consensus
kubectl apply -f - <<EOF
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: plans-consensus
  namespace: kafka
  labels:
    strimzi.io/cluster: neural-hive-kafka
spec:
  partitions: 3
  replicas: 1
  config:
    retention.ms: 604800000  # 7 dias
    segment.bytes: 1073741824
    cleanup.policy: delete
EOF
```

### Valida√ß√£o

```bash
# Listar todos os t√≥picos
kubectl get kafkatopic -n kafka

# Verificar status dos novos t√≥picos
kubectl get kafkatopic plans-ready -n kafka -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
kubectl get kafkatopic plans-consensus -n kafka -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'

# Resultado esperado: True
```

---

## üî¥ BLOQUEADOR 2: Publica√ß√£o Kafka Falha

### Problema
```
Script phase1-end-to-end-test.sh n√£o consegue publicar mensagens
Fun√ß√£o kafka_publish_message() falha ap√≥s 3 tentativas
```

### Diagn√≥stico (10 minutos)

```bash
# 1. Verificar conectividade do Kafka
kubectl get svc -n kafka neural-hive-kafka-kafka-bootstrap

# 2. Testar com port-forward
kubectl port-forward -n kafka svc/neural-hive-kafka-kafka-bootstrap 9092:9092 &
PF_PID=$!

# Aguardar 3 segundos
sleep 3

# 3. Testar conectividade local
nc -zv localhost 9092

# 4. Publicar mensagem de teste usando kcat (se dispon√≠vel)
echo '{"test":"message","timestamp":'$(date +%s)'}' | \
  kcat -P -b localhost:9092 -t intentions.business

# OU usando kafka-console-producer localmente
echo '{"test":"message","timestamp":'$(date +%s)'}' | \
  kafka-console-producer.sh --bootstrap-server localhost:9092 --topic intentions.business

# 5. Consumir para validar
kafka-console-consumer.sh --bootstrap-server localhost:9092 \
  --topic intentions.business --from-beginning --max-messages 1

# 6. Limpar port-forward
kill $PF_PID
```

### Solu√ß√£o Alternativa: Script Python

Se a abordagem com pod ef√™mero falhar, usar producer Python:

```bash
# Criar script de publica√ß√£o
cat > /tmp/kafka_producer.py <<'EOF'
#!/usr/bin/env python3
import sys
import json
from kafka import KafkaProducer

def publish_message(bootstrap_servers, topic, message):
    try:
        producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks=1,
            retries=3,
            request_timeout_ms=10000
        )

        future = producer.send(topic, message)
        record_metadata = future.get(timeout=10)

        print(f"‚úÖ Mensagem publicada com sucesso!")
        print(f"   Topic: {record_metadata.topic}")
        print(f"   Partition: {record_metadata.partition}")
        print(f"   Offset: {record_metadata.offset}")

        producer.close()
        return 0
    except Exception as e:
        print(f"‚ùå Erro ao publicar: {e}", file=sys.stderr)
        return 1

if __name__ == "__main__":
    if len(sys.argv) < 4:
        print("Uso: kafka_producer.py <bootstrap_servers> <topic> <json_message>")
        sys.exit(1)

    bootstrap_servers = sys.argv[1]
    topic = sys.argv[2]
    message = json.loads(sys.argv[3])

    sys.exit(publish_message(bootstrap_servers, topic, message))
EOF

chmod +x /tmp/kafka_producer.py

# Instalar depend√™ncia (se necess√°rio)
pip3 install kafka-python

# Criar port-forward
kubectl port-forward -n kafka svc/neural-hive-kafka-kafka-bootstrap 9092:9092 &
PF_PID=$!
sleep 3

# Publicar mensagem de teste
python3 /tmp/kafka_producer.py localhost:9092 intentions.business \
  '{"test":"message","timestamp":'$(date +%s)'}'

# Limpar
kill $PF_PID
```

### Valida√ß√£o

```bash
# Consumir √∫ltimas mensagens do t√≥pico
kubectl port-forward -n kafka svc/neural-hive-kafka-kafka-bootstrap 9092:9092 &
PF_PID=$!
sleep 3

kafka-console-consumer.sh --bootstrap-server localhost:9092 \
  --topic intentions.business --from-beginning --max-messages 5

kill $PF_PID
```

---

## üî¥ BLOQUEADOR 3: Serializa√ß√£o Protobuf (TypeError)

### Problema
```
Consensus Engine:
  [error] Falha ao obter parecer de especialista
  error='RetryError[<Future ... raised TypeError>]'
```

### Diagn√≥stico (15 minutos)

```bash
# 1. Verificar vers√µes de protobuf nos pods
kubectl exec -n consensus-engine \
  $(kubectl get pod -n consensus-engine -l app.kubernetes.io/name=consensus-engine -o jsonpath='{.items[0].metadata.name}') \
  -- pip3 list | grep protobuf

kubectl exec -n specialist-business \
  $(kubectl get pod -n specialist-business -l app.kubernetes.io/name=specialist-business -o jsonpath='{.items[0].metadata.name}') \
  -- pip3 list | grep protobuf

# 2. Verificar schemas no Apicurio Registry
kubectl port-forward -n kafka svc/schema-registry 8081:8081 &
PF_PID=$!
sleep 3

curl http://localhost:8081/apis/ccompat/v6/subjects | jq .

# 3. Baixar schema de cognitive-plan
curl http://localhost:8081/apis/ccompat/v6/subjects/cognitive-plan-value/versions/latest \
  | jq -r '.schema' | jq . > /tmp/cognitive-plan-schema.json

# 4. Baixar schema de specialist-opinion
curl http://localhost:8081/apis/ccompat/v6/subjects/specialist-opinion-value/versions/latest \
  | jq -r '.schema' | jq . > /tmp/specialist-opinion-schema.json

kill $PF_PID

# 5. Verificar arquivos .proto locais
ls -la proto/
cat proto/specialist.proto | grep -A 5 "message Opinion"
```

### Solu√ß√£o: Re-gerar Schemas Protobuf (30 minutos)

```bash
# 1. Re-gerar c√≥digo protobuf para todos os servi√ßos
./scripts/generate_protos.sh

# 2. Re-build imagens dos servi√ßos afetados
# Consensus Engine
docker build -t neural-hive-mind/consensus-engine:1.0.8 services/consensus-engine/
docker tag neural-hive-mind/consensus-engine:1.0.8 neural-hive-mind/consensus-engine:latest

# 5 Specialists (se necess√°rio)
for specialist in business technical behavior evolution architecture; do
  docker build -t neural-hive-mind/specialist-${specialist}:1.0.8 services/specialist-${specialist}/
  docker tag neural-hive-mind/specialist-${specialist}:1.0.8 neural-hive-mind/specialist-${specialist}:latest
done

# 3. Carregar imagens no cluster (se usando kind/minikube)
kind load docker-image neural-hive-mind/consensus-engine:1.0.8
for specialist in business technical behavior evolution architecture; do
  kind load docker-image neural-hive-mind/specialist-${specialist}:1.0.8
done

# OU para containerd direto
ctr -n k8s.io images import consensus-engine-1.0.8.tar
for specialist in business technical behavior evolution architecture; do
  ctr -n k8s.io images import specialist-${specialist}-1.0.8.tar
done

# 4. Atualizar deployments
kubectl set image deployment/consensus-engine -n consensus-engine \
  consensus-engine=neural-hive-mind/consensus-engine:1.0.8

for specialist in business technical behavior evolution architecture; do
  kubectl set image deployment/specialist-${specialist} -n specialist-${specialist} \
    specialist-${specialist}=neural-hive-mind/specialist-${specialist}:1.0.8
done

# 5. Aguardar rollout
kubectl rollout status deployment/consensus-engine -n consensus-engine --timeout=5m

for specialist in business technical behavior evolution architecture; do
  kubectl rollout status deployment/specialist-${specialist} -n specialist-${specialist} --timeout=5m
done
```

### Solu√ß√£o Alternativa: Downgrade Tempor√°rio

Se re-build n√£o for vi√°vel imediatamente:

```bash
# Verificar vers√µes anteriores das imagens
crictl images | grep -E "specialist|consensus"

# Reverter para √∫ltima vers√£o conhecida funcional (se existir)
kubectl set image deployment/consensus-engine -n consensus-engine \
  consensus-engine=neural-hive-mind/consensus-engine:1.0.6

for specialist in business technical behavior evolution architecture; do
  kubectl set image deployment/specialist-${specialist} -n specialist-${specialist} \
    specialist-${specialist}=neural-hive-mind/specialist-${specialist}:1.0.6
done
```

### Valida√ß√£o

```bash
# 1. Verificar logs do Consensus Engine
kubectl logs -n consensus-engine -l app.kubernetes.io/name=consensus-engine --tail=50

# Procurar por:
# ‚úÖ "Specialists gRPC client inicializado"
# ‚úÖ "gRPC channel initialized" (5 vezes)
# ‚ùå "TypeError" ou "RetryError"

# 2. Publicar mensagem de teste e monitorar
kubectl logs -n consensus-engine -l app.kubernetes.io/name=consensus-engine --tail=50 -f &
LOG_PID=$!

# Publicar Intent Envelope (usar m√©todo que funcionar do BLOQUEADOR 2)

# Aguardar 30 segundos e verificar logs
sleep 30
kill $LOG_PID

# 3. Verificar se specialists processaram
for specialist in business technical behavior evolution architecture; do
  echo "=== specialist-$specialist ==="
  kubectl logs -n specialist-$specialist -l app.kubernetes.io/name=specialist-$specialist --tail=20 | grep -i "plan_id"
done
```

---

## ‚ö†Ô∏è PROBLEMA SECUND√ÅRIO: Specialist Business Crashloop

### Diagn√≥stico

```bash
# 1. Identificar pod problem√°tico
kubectl get pods -n specialist-business

# 2. Verificar logs atuais
kubectl logs -n specialist-business specialist-business-5d774d6f95-wzcvp --tail=100

# 3. Verificar logs anteriores (antes do √∫ltimo restart)
kubectl logs -n specialist-business specialist-business-5d774d6f95-wzcvp --previous

# 4. Descrever pod para ver eventos
kubectl describe pod -n specialist-business specialist-business-5d774d6f95-wzcvp
```

### Solu√ß√£o R√°pida

```bash
# Deletar pod problem√°tico (ser√° recriado automaticamente)
kubectl delete pod specialist-business-5d774d6f95-wzcvp -n specialist-business

# Aguardar novo pod ficar ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=specialist-business -n specialist-business --timeout=2m

# Verificar status
kubectl get pods -n specialist-business
```

---

## ‚ö†Ô∏è PROBLEMA SECUND√ÅRIO: Specialist Technical Pod Pending

### Diagn√≥stico

```bash
# Verificar por que est√° Pending
kubectl describe pod -n specialist-technical specialist-technical-685bf56bbd-tzzx2 | grep -A 10 "Events:"

# Verificar recursos do cluster
kubectl describe nodes | grep -A 5 "Allocated resources"
```

### Solu√ß√£o

Se for falta de recursos:

```bash
# Op√ß√£o 1: Escalar down para 1 r√©plica
kubectl scale deployment specialist-technical -n specialist-technical --replicas=1

# Op√ß√£o 2: Reduzir resource requests
kubectl set resources deployment specialist-technical -n specialist-technical \
  --requests=cpu=50m,memory=128Mi --limits=cpu=500m,memory=512Mi
```

---

## üìä Valida√ß√£o Final (Ap√≥s Corre√ß√µes)

### 1. Verificar T√≥picos Kafka

```bash
kubectl get kafkatopic -n kafka

# Resultado esperado:
# intentions.business         Ready
# intentions.infrastructure   Ready
# intentions.security         Ready
# intentions.technical        Ready
# intentions.validation       Ready
# plans.ready                 Ready  ‚Üê NOVO
# plans.consensus             Ready  ‚Üê NOVO
```

### 2. Verificar Todos os Pods

```bash
# Verificar status de todos os componentes da Fase 1
for ns in gateway-intencoes semantic-translation-engine consensus-engine memory-layer-api \
          specialist-business specialist-technical specialist-behavior specialist-evolution specialist-architecture; do
  echo "=== $ns ==="
  kubectl get pods -n $ns
done

# Resultado esperado: Todos Running, nenhum com restarts excessivos
```

### 3. Re-executar Teste E2E

```bash
# Executar teste completo
cd /jimy/Neural-Hive-Mind
./tests/phase1-end-to-end-test.sh --continue-on-error --debug

# Verificar relat√≥rios gerados
ls -lht tests/results/

# Analisar resultado
cat tests/results/phase1-test-summary-*.md
```

### 4. Valida√ß√£o Manual do Fluxo

```bash
# 1. Publicar Intent Envelope
kubectl port-forward -n kafka svc/neural-hive-kafka-kafka-bootstrap 9092:9092 &
PF_PID=$!
sleep 3

INTENT_ID="test-intent-$(date +%s)"
echo "{
  \"id\": \"${INTENT_ID}\",
  \"actor\": {\"type\":\"human\",\"id\":\"test-user\",\"name\":\"Test User\"},
  \"intent\": {
    \"text\":\"Criar workflow de aprova√ß√£o de pedidos\",
    \"domain\":\"business\",
    \"classification\":\"workflow-automation\"
  },
  \"confidence\":0.95,
  \"timestamp\":$(date +%s)000
}" | kafka-console-producer.sh --bootstrap-server localhost:9092 --topic intentions.business

echo "Intent ID: $INTENT_ID"

# 2. Monitorar STE
kubectl logs -n semantic-translation-engine -l app.kubernetes.io/name=semantic-translation-engine -f &
STE_PID=$!

# Aguardar 15 segundos
sleep 15

# 3. Verificar se Plan foi gerado
kill $STE_PID
kubectl logs -n semantic-translation-engine -l app.kubernetes.io/name=semantic-translation-engine --tail=50 | grep "$INTENT_ID"

# 4. Verificar Consensus Engine
kubectl logs -n consensus-engine -l app.kubernetes.io/name=consensus-engine --tail=100 | grep -A 5 "plan_id"

# 5. Verificar MongoDB
MONGO_POD=$(kubectl get pod -n mongodb-cluster -l app=mongodb -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n mongodb-cluster $MONGO_POD -- \
  mongosh --quiet --eval "db.cognitive_ledger.find({intent_id: '${INTENT_ID}'}).pretty()"

# Limpar
kill $PF_PID
```

---

## üìù Checklist de Corre√ß√µes

- [ ] Criar t√≥pico `plans.ready`
- [ ] Criar t√≥pico `plans.consensus`
- [ ] Validar t√≥picos criados (status Ready)
- [ ] Testar publica√ß√£o Kafka via port-forward
- [ ] Resolver TypeError de serializa√ß√£o protobuf
- [ ] Re-gerar schemas protobuf (se necess√°rio)
- [ ] Re-build e re-deploy servi√ßos afetados
- [ ] Validar logs do Consensus Engine (sem TypeError)
- [ ] Deletar pod specialist-business problem√°tico
- [ ] Resolver pod specialist-technical Pending
- [ ] Re-executar teste E2E completo
- [ ] Validar fluxo manual (Intent ‚Üí Plan ‚Üí Decision)
- [ ] Documentar resultados finais

---

## üéØ Crit√©rio de Sucesso

‚úÖ **Teste E2E considerado bem-sucedido quando**:

1. Todos os 7 t√≥picos Kafka est√£o Ready
2. Publica√ß√£o de Intent Envelope funciona sem erros
3. STE gera Cognitive Plan em < 10s
4. 5/5 Specialists avaliam o plano (ou m√≠nimo 3/5)
5. Consensus Engine gera decis√£o consolidada
6. Registros persistidos no MongoDB
7. Nenhum erro de TypeError nos logs
8. Relat√≥rio E2E com status "PASSED" nas 5 fases

---

**Estimativa Total**: 2-4 horas (incluindo troubleshooting)
**Prioridade**: P0 (Cr√≠tico para operacionaliza√ß√£o)
**Pr√≥xima Revis√£o**: Ap√≥s execu√ß√£o de corre√ß√µes
