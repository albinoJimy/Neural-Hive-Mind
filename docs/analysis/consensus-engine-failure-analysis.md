# An√°lise Profunda da Falha: Consensus-Engine em Staging

## Resumo Executivo

**Status:** Deployment do `consensus-engine` falhando no namespace `neural-hive-staging`
- Pods ficam em estado `0/1 Running`
- Readiness probe falha consistentemente com timeout (3s)
- Deployment n√£o conclui rollout ap√≥s ~10 minutos

---

## 1. Diagn√≥stico Detalhado

### 1.1 Situa√ß√£o Atual dos Pods

```
NAMESPACE: neural-hive-staging

consensus-engine-654bf5545c-jjqgv   0/1   Running   0   96s   IP: 10.244.1.126
consensus-engine-654bf5545c-tzxlv   0/1   Running   0   96s   IP: 10.244.2.133
```

**Observa√ß√µes:**
- Ambos os pods foram recriados h√° ~2 minutos (deployment recente)
- Nenhum pod alcan√ßou status "Ready"
- Idade indica m√∫ltiplas tentativas de recria√ß√£o

### 1.2 Configura√ß√£o dos Probes

**Liveness Probe:**
- Endpoint: `http://:8000/health`
- Timeout: 5s
- Period: 10s
- Failure Threshold: 3

**Readiness Probe:**
- Endpoint: `http://:8000/ready` ‚ö†Ô∏è **FALHANDO**
- Timeout: 3s ‚ö†Ô∏è **MUITO CURTO**
- Period: 5s
- Failure Threshold: 3

**Startup Probe:**
- Endpoint: `http://:8000/health`
- Timeout: 5s
- Period: 10s
- Failure Threshold: 15

### 1.3 Mensagens de Erro dos Eventos

```
LAST SEEN   TYPE      REASON      MESSAGE
2m58s       Warning   Unhealthy   Readiness probe failed: Get "http://10.244.2.132:8000/ready": 
                              context deadline exceeded (Client.Timeout exceeded while awaiting headers)
```

**Interpreta√ß√£o:** O readiness probe atinge o timeout de 3s antes de receber resposta do endpoint `/ready`.

---

## 2. An√°lise do Comportamento da Aplica√ß√£o

### 2.1 Inicializa√ß√£o Bem-Sucedida

A aplica√ß√£o inicia corretamente:
```
‚úÖ INFO: Started server process [1]
‚úÖ INFO: Waiting for application startup
‚úÖ INFO: Iniciando Consensus Engine (environment=dev)
‚úÖ INFO: Servidor Prometheus iniciado na porta 8080
‚úÖ INFO: Observabilidade inicializada com sucesso
‚úÖ INFO: Health check 'otel_pipeline' registrado
```

### 2.2 Conectividade MongoDB

```
‚úÖ MongoDB conectado com sucesso
‚úÖ Connection pool created (maxPoolSize: 50)
‚úÖ Server heartbeat succeeding (ismaster: true)
‚úÖ Topology monitoring ativo
‚úÖ Indexes criados/verificados:
   - decision_id_1 (unique)
   - plan_id_1
   - intent_id_1
   - created_at_1
   - hash_1
   - final_decision_1_created_at_-1
   - token_1 (unique) - consensus_explainability
```

### 2.3 Problema Cr√≠tico Identificado: OTEL Pipeline

**Logs recorrentes a cada ~5-7 segundos:**
```
üî¥ WARNING: otel_pipeline_unhealthy - OTEL Collector not reachable
üî¥ DEBUG: neural_hive_observability.health_checks.otel - OTEL collector health check error
```

**Endpoint OTEL configurado:**
```
otel_endpoint=https://opentelemetry-collector.observability.svc.cluster.local:4317
```

### 2.4 Comportamento do Endpoint /ready

**Health Check (/health):**
- ‚úÖ Funciona normalmente
- ‚úÖ Retorna HTTP 200 OK
- ‚úÖ Responde rapidamente

**Readiness Check (/ready):**
- üî¥ Retorna HTTP 503 Service Unavailable
- üî¥ Timeout de 3s √© insuficiente
- üî¥ Bloqueado pela verifica√ß√£o do OTEL Collector

---

## 3. Causa Raiz da Falha

### 3.1 Cadeia de Depend√™ncias

```
Readiness Probe (K8s)
    ‚Üì Timeout: 3s
Endpoint /ready (Aplica√ß√£o)
    ‚Üì Verifica todos os health checks registrados
Health Check 'otel_pipeline'
    ‚Üì Tenta conectar ao OTEL Collector
OTEL Collector (observability namespace)
    ‚Ü¥ INDISPON√çVEL / N√ÉO RESPONDE
    
Resultado: Timeout da requisi√ß√£o > 3s ‚Üí Readiness FAIL
```

### 3.2 Por Que /health Funciona Mas /ready N√£o?

**Endpoint /health:**
- Verifica apenas checks cr√≠ticos de sobreviv√™ncia
- N√£o inclui depend√™ncias externas opcionais
- Retorna r√°pido

**Endpoint /ready:**
- Verifica TODOS os checks registrados
- Inclui 'otel_pipeline' como depend√™ncia obrigat√≥ria
- Aguarda timeout da conex√£o OTEL (muito lento)
- Excede os 3s do readiness probe

### 3.3 Evidence T√©cnica

**C√≥digo fonte impl√≠cito:**
```python
# No m√≥dulo de health checks
health_checks = [
    'memory',           # ‚úÖ R√°pido
    'otel_pipeline',    # ‚ùå Lento (tentando conectar endpoint inalcan√ß√°vel)
]

@app.get("/ready")
def readiness():
    for check in health_checks:
        if not check.is_healthy():
            return 503  # Falha se QUALQUER check falhar
    return 200
```

---

## 4. Impactos do Problema

### 4.1 Efeito no Deployment

```
Deployment Status:
- Replicas: 2 desired | 2 updated | 2 total | 0 available | 2 unavailable
- Conditions:
  - Available: False (MinimumReplicasUnavailable)
  - Progressing: True (ReplicaSetUpdated)

RollingUpdate Strategy:
- MaxUnavailable: 0 (n√£o permite pods indispon√≠veis)
- MaxSurge: 1
- Resultado: Deployment travado, pods antigos mantidos
```

### 4.2 Tentativas de Rollout

```
Timeline de Eventos (√∫ltimos 60 minutos):
- consensus-engine-75474cb659 (60m atr√°s)
- consensus-engine-58695b7cf8 (42m atr√°s)
- consensus-engine-d6857c945 (29m atr√°s)
- consensus-engine-9b69f4dc4 (24m atr√°s)
- consensus-engine-6f84575959 (20m atr√°s)
- consensus-engine-67c9d96744 (8m atr√°s)
- consensus-engine-654bf5545c (atual, 97s)

Total: 7 tentativas de deployment em 60 minutos
```

---

## 5. Solu√ß√µes Propostas

### 5.1 Solu√ß√£o Imediata (Hotfix)

**Op√ß√£o A: Desabilitar health check OTEL no /ready**
```python
# Alterar o check de 'otel_pipeline' para n√£o bloquear readiness
# ou remover do readiness mas manter no health geral
```

**Op√ß√£o B: Aumentar timeout do readiness probe**
```yaml
# No deployment/kubernetes
readinessProbe:
  timeoutSeconds: 10  # Aumentar de 3s para 10s
  periodSeconds: 10   # Aumentar tamb√©m
```

**Op√ß√£o C: Tornar OTEL opcional para readiness**
```python
# L√≥gica de health check modificada
if check == 'otel_pipeline' and check_fails:
    log_warning()  # Logar mas n√£o falhar readiness
    continue  # Permitir que outros checks passem
```

### 5.2 Solu√ß√£o Definitiva

**Investigar por que OTEL Collector est√° indispon√≠vel:**

```bash
# Comandos para diagn√≥stico
kubectl get pods -n observability
kubectl logs -n observability deployment/opentelemetry-collector
kubectl get svc -n observability opentelemetry-collector
```

**Poss√≠veis causas:**
1. Namespace `observability` n√£o existe em staging
2. OTEL Collector n√£o est√° implantado em staging
3. NetworkPolicy bloqueando comunica√ß√£o
4. DNS n√£o resolvendo `opentelemetry-collector.observability.svc.cluster.local`

### 5.3 Solu√ß√£o de Contorno (Workaround)

**Remover readiness probe temporariamente:**
```yaml
# Isso permitir√° o deployment completar
# Mas remove a prote√ß√£o de n√£o enviar tr√°fego para pods n√£o-prontos
readinessProbe: null
```

‚ö†Ô∏è **Risco:** Pods podem receber tr√°fego antes de estarem totalmente inicializados

---

## 6. Recomenda√ß√µes

### Prioridade 1 (Imediata - 5 minutos)
Aumentar timeout do readiness probe de 3s para 10s no Helm chart.

### Prioridade 2 (Curto prazo - 1 hora)
Investigar disponibilidade do OTEL Collector no namespace staging.

### Prioridade 3 (M√©dio prazo - 1 dia)
Implementar health checks graduais:
- `/health/live` - Liveness (m√≠nimo para sobreviver)
- `/health/ready` - Readiness (para receber tr√°fego)
- `/health/startup` - Startup (inicializa√ß√£o completa)

### Prioridade 4 (Longo prazo)
Adicionar circuit breaker para depend√™ncias externas opcionais.

---

## 7. M√©tricas para Monitoramento

```
# Kubectl commands para acompanhamento
kubectl rollout status deployment/consensus-engine -n neural-hive-staging
kubectl get pods -n neural-hive-staging -w
kubectl logs -n neural-hive-staging -l app.kubernetes.io/name=consensus-engine -f
```

---

## Conclus√£o

A falha do deployment √© causada por um **health check excessivamente rigoroso** combinado com **timeout inadequado**. O endpoint `/ready` inclui a verifica√ß√£o do OTEL Collector como uma depend√™ncia obrigat√≥ria, mas o timeout de 3s do readiness probe √© insuficiente para a tentativa de conex√£o falhar graciosamente.

**A√ß√£o recomendada imediata:** Aumentar o timeout do readiness probe ou tornar o check do OTEL Collector n√£o-bloqueante para o readiness.

**Arquivos envolvidos:**
- Helm chart: `helm-charts/consensus-engine/templates/deployment.yaml`
- C√≥digo: `src/observability/health.py` (prov√°vel localiza√ß√£o)
- Configura√ß√£o: `environments/staging/helm-values/consensus-engine-values.yaml`
