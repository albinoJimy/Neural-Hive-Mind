# AnÃ¡lise Profunda: Falha no Deploy do Consensus-Engine

## ðŸ“‹ Resumo Executivo

**Workflow:** Deploy After Build #21551913758  
**Status:** âŒ Falhou apÃ³s 7m21s  
**ServiÃ§o:** consensus-engine  
**Namespace:** neural-hive-staging  
**Erro Principal:** `release consensus-engine failed, and has been uninstalled due to atomic being set: services "consensus-engine" not found`

---

## ðŸ” AnÃ¡lise da Cadeia de Falha

### 1. Fluxo do Deploy

```
[Helm Install]
    â†“
[Pods Criados] â† consensus-engine-654bf5545c-* (2 replicas)
    â†“
[Startup Probe] â† /health porta 8000 âœ… PASSA
    â†“
[Readiness Probe] â† /ready porta 8000 ðŸ”´ FALHA
    â†“
[Timeout 3s] Ã— 3 tentativas = 15s
    â†“
[Pods marcados como NOT READY (0/1)]
    â†“
[Helm aguarda com --wait --timeout 10m]
    â†“
[Timeout atingido ~7 minutos]
    â†“
[Flag --atomic ativada]
    â†“
[Rollback automÃ¡tico executado]
    â†“
[Service removido durante cleanup]
    â†“
[ERRO: services "consensus-engine" not found]
```

### 2. Timeline dos Eventos

| Timestamp | Evento | Status |
|-----------|--------|--------|
| 22:30:25 | Helm inicia instalaÃ§Ã£o | ðŸŸ¡ Iniciando |
| 22:30:25 | Pods criados (2 replicas) | ðŸŸ¡ Criando |
| 22:30:25 | Startup probe iniciado | ðŸŸ¡ Verificando |
| 22:30:30 | Liveness probe iniciado | ðŸŸ¡ Verificando |
| 22:30:30 | Readiness probe iniciado | ðŸ”´ Falhando |
| 22:30:30-22:37:00 | 62 falhas de readiness | ðŸ”´ Falhando |
| 22:37:04 | Helm timeout (7m21s) | ðŸ”´ Timeout |
| 22:37:04 | Atomic rollback executado | ðŸŸ  Rollback |
| 22:37:04 | Service removido | ðŸ”´ Erro |

---

## ðŸŽ¯ Causa Raiz Identificada

### Problema Principal: Readiness Probe Timeout

**ConfiguraÃ§Ã£o Atual:**
```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8000
  periodSeconds: 5
  timeoutSeconds: 3      â† ðŸ”´ MUITO CURTO
  failureThreshold: 3
  initialDelaySeconds: 0 â† ðŸ”´ SEM DELAY INICIAL
```

**Problema:** O endpoint `/ready` estÃ¡ **demorando mais de 3 segundos** para responder, causando timeout consistente.

### Por que o /ready demora?

Baseado nos logs, o endpoint `/ready` verifica mÃºltiplas dependÃªncias:

1. **MongoDB** âœ… (funcionando - logs mostram conexÃ£o OK)
2. **OTEL Collector** ðŸ”´ (indisponÃ­vel - timeout na conexÃ£o)
   - Endpoint: `opentelemetry-collector.observability.svc.cluster.local:4317`
   - Logs mostram: `otel_pipeline_unhealthy - OTEL Collector not reachable`
3. **Possivelmente outras dependÃªncias**:
   - Kafka (9092)
   - Redis (6379)
   - gRPC Specialists (50051)

**Cadeia de Timeout:**
```
Readiness Probe (K8s) â”€â”€â–º /ready (App) â”€â”€â–º Health Checks
                                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                           â”‚                           â”‚
                   MongoDB                   OTEL Collector           Outros
                   âœ… OK                     ðŸ”´ Timeout >3s            ?
                   (<100ms)                  (3-5s tentativa)
```

### Impacto do OTEL Collector

**Logs recorrentes (a cada 5-7 segundos):**
```json
{
  "timestamp": "2026-01-31T22:36:32.780023+00:00",
  "level": "DEBUG",
  "logger": "neural_hive_observability.health_checks.otel",
  "message": "OTEL collector health check error: ",
  "module": "otel",
  "function": "_check_collector_health",
  "line": 164
}

2026-01-31 22:36:32 [warning] otel_pipeline_unhealthy  
message=OTEL Collector not reachable
```

**Tempo de timeout:** A tentativa de conexÃ£o ao OTEL Collector provavelmente demora ~3-5 segundos antes de falhar, excedendo o timeout do readiness probe (3s).

---

## ðŸ“Š AnÃ¡lise TÃ©cnica Detalhada

### 1. Estado dos Pods

```
consensus-engine-654bf5545c-jjqgv   0/1   Running   0   9m9s   10.244.1.126
consensus-engine-654bf5545c-tzxlv   0/1   Running   0   9m9s   10.244.2.133
consensus-engine-77cf87c964-5f8gm   0/1   Running   0   3m18s  10.244.2.134
```

**AnÃ¡lise:**
- **Status:** Running âœ… (container iniciou)
- **Ready:** 0/1 ðŸ”´ (readiness probe falhou)
- **Restarts:** 0 âœ… (aplicaÃ§Ã£o nÃ£o crashou)
- **Idade:** MÃºltiplas revisÃµes criadas (indica tentativas de deployment)

### 2. Network Policy

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
spec:
  egress:
  - DNS (port 53 UDP) âœ…
  - Kafka:9092 (namespace: kafka) âœ…
  - MongoDB:27017 (namespace: mongodb-cluster) âœ…
  - Redis:6379 (namespace: redis-cluster) âœ…
  - gRPC:50051 (namespace: neural-hive-specialists) âœ…
  - OTEL:4317,4318 âŒ (NÃƒO CONFIGURADO!)
```

**Problema identificado:** A NetworkPolicy **nÃ£o permite acesso ao namespace observability** onde o OTEL Collector estÃ¡!

### 3. Probes Comparison

| Probe | Endpoint | Timeout | Estado | PropÃ³sito |
|-------|----------|---------|--------|-----------|
| **startupProbe** | /health | 5s | âœ… Passa | Verifica se app iniciou |
| **livenessProbe** | /health | 5s | âœ… Passa | Verifica se app estÃ¡ viva |
| **readinessProbe** | /ready | 3s | ðŸ”´ Falha | Verifica se app pode receber trÃ¡fego |

**Problema:** O readiness probe tem o **menor timeout** (3s vs 5s) mas verifica **mais dependÃªncias** (incluindo OTEL).

### 4. Helm Release Status

```bash
Name: consensus-engine
Namespace: neural-hive-staging
Status: pending-upgrade  â† ðŸ”´ Travado
Revision: 2
```

**Status `pending-upgrade`:** Indica que o Helm estÃ¡ aguardando o deployment completar, mas os pods nunca ficaram prontos.

### 5. AusÃªncia de ConfiguraÃ§Ã£o Staging

**Arquivo nÃ£o encontrado:**
```
environments/staging/helm-values/consensus-engine-values.yaml âŒ
```

**Impacto:** O deployment usa valores padrÃ£o do chart, que podem nÃ£o ser adequados para staging.

---

## ðŸŽ¨ CenÃ¡rio da Falha

### CenÃ¡rio 1: Timeout do Readiness Probe (Principal)

**Probabilidade:** â­â­â­â­â­ (99%)

**DescriÃ§Ã£o:**
1. Pod inicia e servidor HTTP fica disponÃ­vel
2. Startup probe chama `/health` â†’ retorna 200 OK rapidamente
3. Readiness probe chama `/ready` â†’ tenta verificar OTEL Collector
4. OTEL Collector estÃ¡ indisponÃ­vel ou inalcanÃ§Ã¡vel
5. AplicaÃ§Ã£o tenta conectar por 3-5 segundos
6. Timeout de 3s do readiness probe Ã© atingido
7. Kubernetes marca pod como NOT READY
8. ApÃ³s 3 falhas consecutivas (15s), pod continua NOT READY
9. Helm aguarda atÃ© 10 minutos mas pods nunca ficam ready
10. Helm falha e faz rollback (--atomic)

**EvidÃªncias:**
- Logs mostram warning `otel_pipeline_unhealthy` recorrente
- Eventos Kubernetes: `Readiness probe failed: context deadline exceeded`
- NetworkPolicy nÃ£o inclui regra para namespace observability

### CenÃ¡rio 2: Bloqueio de Rede (SecundÃ¡rio)

**Probabilidade:** â­â­â­ (30%)

**DescriÃ§Ã£o:**
- NetworkPolicy pode estar bloqueando comunicaÃ§Ã£o interna
- Embora as regras de egress pareÃ§am corretas, pode haver problemas de DNS ou resoluÃ§Ã£o de serviÃ§o

**EvidÃªncias:**
- NetworkPolicy permite acesso a mÃºltiplos serviÃ§os externos
- Mas nÃ£o permite acesso ao observability namespace

---

## ðŸ”§ SoluÃ§Ãµes Propostas

### SoluÃ§Ã£o 1: Corrigir NetworkPolicy (Prioridade: ðŸ”´ ALTA)

**Adicionar regra de egress para OTEL Collector:**

```yaml
# helm-charts/consensus-engine/templates/networkpolicy.yaml
spec:
  egress:
    # ... regras existentes ...
    - ports:
      - port: 4317
        protocol: TCP
      - port: 4318
        protocol: TCP
      to:
      - namespaceSelector:
          matchLabels:
            kubernetes.io/metadata.name: observability
```

### SoluÃ§Ã£o 2: Aumentar Timeout do Readiness Probe (Prioridade: ðŸ”´ ALTA)

**Modificar values.yaml:**
```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8000
  periodSeconds: 10       # Aumentar de 5 para 10
  timeoutSeconds: 10      # Aumentar de 3 para 10
  failureThreshold: 5     # Aumentar de 3 para 5
  initialDelaySeconds: 5  # Adicionar delay inicial
```

### SoluÃ§Ã£o 3: Criar ConfiguraÃ§Ã£o Staging (Prioridade: ðŸŸ¡ MÃ‰DIA)

**Criar arquivo:** `environments/staging/helm-values/consensus-engine-values.yaml`

```yaml
# ConfiguraÃ§Ãµes especÃ­ficas para staging
replicaCount: 2

readinessProbe:
  timeoutSeconds: 10
  periodSeconds: 10
  failureThreshold: 5
  initialDelaySeconds: 5

# Desabilitar OTEL em staging se nÃ£o estiver disponÃ­vel
observability:
  enabled: false  # Ou configurar endpoint alternativo
```

### SoluÃ§Ã£o 4: Modificar CÃ³digo da AplicaÃ§Ã£o (Prioridade: ðŸŸ¢ BAIXA)

**Tornar OTEL opcional para readiness:**
```python
# health.py - pseudocÃ³digo
async def readiness_check():
    checks = [
        check_mongodb(),      # ObrigatÃ³rio
        check_kafka(),        # ObrigatÃ³rio
        check_redis(),        # ObrigatÃ³rio
    ]
    
    # OTEL Ã© opcional - nÃ£o bloqueia readiness
    try:
        await asyncio.wait_for(check_otel(), timeout=2.0)
    except TimeoutError:
        logger.warning("OTEL indisponÃ­vel, continuando...")
    
    return all(checks)
```

### SoluÃ§Ã£o 5: Remover Flag --atomic Temporariamente (Workaround)

**No workflow:**
```yaml
# .github/workflows/deploy-after-build.yml
# Comentar ou remover:
# "--atomic"
# "--cleanup-on-fail"
```

âš ï¸ **Risco:** Deployment pode ficar em estado inconsistente se falhar

---

## ðŸ“‹ Plano de AÃ§Ã£o

### Fase 1: Hotfix Imediato (5 minutos)

1. **Escalar timeout do readiness probe via kubectl:**
```bash
kubectl patch deployment consensus-engine -n neural-hive-staging -p '{"spec":{"template":{"spec":{"containers":[{"name":"consensus-engine","readinessProbe":{"timeoutSeconds":10,"periodSeconds":10}}]}}}}'
```

2. **Verificar se pods ficam ready:**
```bash
kubectl get pods -n neural-hive-staging -w
```

### Fase 2: CorreÃ§Ã£o Definitiva (30 minutos)

1. **Atualizar NetworkPolicy no Helm chart**
2. **Atualizar valores padrÃ£o do readiness probe**
3. **Criar staging values file**
4. **Commit e push das alteraÃ§Ãµes**

### Fase 3: VerificaÃ§Ã£o (10 minutos)

1. **Re-executar workflow de deploy**
2. **Monitorar rollout:**
```bash
kubectl rollout status deployment/consensus-engine -n neural-hive-staging
```
3. **Verificar pods:**
```bash
kubectl get pods -n neural-hive-staging
```

---

## ðŸ“Š MÃ©tricas de Impacto

| MÃ©trica | Valor | Impacto |
|---------|-------|---------|
| **Tentativas de deploy** | 7 em 60 minutos | ðŸ”´ Alto |
| **Tempo mÃ©dio de falha** | ~7 minutos | ðŸ”´ Alto |
| **Pods criados** | 14+ | ðŸŸ¡ MÃ©dio |
| **Service indisponÃ­vel** | 100% | ðŸ”´ CrÃ­tico |
| **Rollback automÃ¡tico** | 100% | ðŸŸ¡ MÃ©dio |

---

## ðŸ” Comandos para InvestigaÃ§Ã£o

```bash
# Verificar logs em tempo real
kubectl logs -n neural-hive-staging -l app.kubernetes.io/name=consensus-engine -f

# Testar endpoint /ready manualmente
kubectl exec -n neural-hive-staging consensus-engine-654bf5545c-jjqgv -- curl -v http://localhost:8000/ready

# Verificar se OTEL estÃ¡ acessÃ­vel
kubectl exec -n neural-hive-staging consensus-engine-654bf5545c-jjqgv -- nc -zv opentelemetry-collector.observability.svc.cluster.local 4317

# Verificar eventos do deployment
kubectl get events -n neural-hive-staging --field-selector involvedObject.name=consensus-engine --sort-by='.lastTimestamp'

# Descrever deployment
kubectl describe deployment -n neural-hive-staging consensus-engine

# Verificar status do rollout
kubectl rollout status deployment/consensus-engine -n neural-hive-staging

# HistÃ³rico de revisÃµes do helm
helm history consensus-engine -n neural-hive-staging
```

---

## ðŸ“ ConclusÃ£o

A falha do deployment Ã© causada por uma **combinaÃ§Ã£o de fatores**:

1. **Causa PrimÃ¡ria:** Readiness probe timeout (3s) Ã© insuficiente para verificaÃ§Ã£o do OTEL Collector
2. **Causa SecundÃ¡ria:** NetworkPolicy nÃ£o permite acesso ao namespace observability
3. **Causa TerciÃ¡ria:** AusÃªncia de configuraÃ§Ã£o especÃ­fica para staging

**Impacto:** O deploy falha consistentemente devido ao timeout do readiness probe, causando rollback automÃ¡tico pelo Helm (--atomic).

**RecomendaÃ§Ã£o Imediata:** Aplicar hotfix aumentando o timeout do readiness probe para 10s via kubectl patch, depois implementar correÃ§Ãµes permanentes no Helm chart.

**Arquivos a serem modificados:**
1. `helm-charts/consensus-engine/templates/networkpolicy.yaml`
2. `helm-charts/consensus-engine/values.yaml`
3. `environments/staging/helm-values/consensus-engine-values.yaml` (criar)

---

## ðŸ“š ReferÃªncias

- [Kubernetes Probes Documentation](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
- [Helm Atomic Flag](https://helm.sh/docs/helm/helm_upgrade/#options)
- [NetworkPolicy API](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/)
