"""
Flow C Orchestrator - Complete implementation of Flow C (C1-C6).

Coordinates Intent → Decision → Orchestration → Tickets → Workers → Code Forge → Deploy → Telemetry.
"""

import structlog
import asyncio
from typing import Dict, Any, List
from datetime import datetime, timedelta
from opentelemetry import trace
from prometheus_client import Counter, Histogram

from neural_hive_integration.clients.orchestrator_client import OrchestratorClient
from neural_hive_integration.clients.service_registry_client import ServiceRegistryClient, AgentInfo
from neural_hive_integration.clients.execution_ticket_client import ExecutionTicketClient
from neural_hive_integration.clients.worker_agent_client import WorkerAgentClient
from neural_hive_integration.clients.code_forge_client import CodeForgeClient
from neural_hive_integration.clients.sla_management_client import SLAManagementClient
from neural_hive_integration.models.flow_c_context import FlowCContext, FlowCStep, FlowCResult
from neural_hive_integration.telemetry.flow_c_telemetry import FlowCTelemetryPublisher

logger = structlog.get_logger()
tracer = trace.get_tracer(__name__)

# Metrics
flow_c_duration = Histogram(
    "neural_hive_flow_c_duration_seconds",
    "Flow C end-to-end duration",
    buckets=[60, 300, 900, 1800, 3600, 7200, 14400],  # 1min to 4h
)
flow_c_steps_duration = Histogram(
    "neural_hive_flow_c_steps_duration_seconds",
    "Flow C individual step duration",
    ["step"],
)
flow_c_success = Counter("neural_hive_flow_c_success_total", "Flow C successful executions")
flow_c_failures = Counter("neural_hive_flow_c_failures_total", "Flow C failed executions", ["reason"])
flow_c_sla_violations = Counter("neural_hive_flow_c_sla_violations_total", "Flow C SLA violations")
flow_c_workflow_query_duration = Histogram(
    "neural_hive_flow_c_workflow_query_duration_seconds",
    "Duration of workflow state queries",
    ["query_name"],
)
flow_c_workflow_query_failures = Counter(
    "neural_hive_flow_c_workflow_query_failures_total",
    "Failed workflow state queries",
    ["query_name", "reason"],
)
flow_c_ticket_validation_failures = Counter(
    "neural_hive_flow_c_ticket_validation_failures_total",
    "Ticket schema validation failures",
)
flow_c_ticket_schema_version = Counter(
    "neural_hive_flow_c_ticket_schema_version_total",
    "Ticket schema versions processed",
    ["schema_version"],
)


class FlowCOrchestrator:
    """Orchestrator for complete Flow C integration (C1-C6)."""

    def __init__(self):
        self.orchestrator_client = OrchestratorClient()
        self.service_registry = ServiceRegistryClient()
        self.ticket_client = ExecutionTicketClient()
        self.sla_client = SLAManagementClient()
        self.telemetry = FlowCTelemetryPublisher()
        self.logger = logger.bind(service="flow_c_orchestrator")

    async def initialize(self):
        """Initialize all clients."""
        await self.sla_client.initialize()
        await self.telemetry.initialize()

    async def close(self):
        """Close all clients."""
        await self.orchestrator_client.close()
        await self.service_registry.close()
        await self.ticket_client.close()
        await self.sla_client.close()
        await self.telemetry.close()

    @tracer.start_as_current_span("flow_c.execute")
    async def execute_flow_c(self, consolidated_decision: Dict[str, Any]) -> FlowCResult:
        """
        Execute complete Flow C (C1-C6).

        Flow:
        C1: Validate consolidated decision
        C2: Start Temporal workflow and generate tickets
        C3: Discover available workers
        C4: Assign tickets to workers
        C5: Monitor execution and collect results
        C6: Publish telemetry

        Args:
            consolidated_decision: Consolidated decision from Phase 1

        Returns:
            Flow C execution result
        """
        start_time = datetime.utcnow()
        steps: List[FlowCStep] = []

        # Extract correlation_id with fallback chain:
        # 1. From consolidated_decision
        # 2. From cognitive_plan inside decision
        # 3. Auto-generated by FlowCContext validator
        correlation_id = consolidated_decision.get("correlation_id")
        if not correlation_id:
            cognitive_plan = consolidated_decision.get("cognitive_plan", {})
            correlation_id = cognitive_plan.get("correlation_id")

        if not correlation_id:
            self.logger.warning(
                "correlation_id_missing",
                intent_id=consolidated_decision.get("intent_id"),
                plan_id=consolidated_decision.get("plan_id"),
                decision_id=consolidated_decision.get("decision_id"),
                message="correlation_id ausente na decisão e no plano - será gerado automaticamente"
            )

        # Create context - FlowCContext validará e gerará correlation_id se necessário
        context = FlowCContext(
            intent_id=consolidated_decision["intent_id"],
            plan_id=consolidated_decision["plan_id"],
            decision_id=consolidated_decision["decision_id"],
            correlation_id=correlation_id,  # Pode ser None - validator tratará
            trace_id=format(trace.get_current_span().get_span_context().trace_id, '032x'),
            span_id=format(trace.get_current_span().get_span_context().span_id, '016x'),
            started_at=start_time,
            sla_deadline=start_time + timedelta(hours=4),
            priority=consolidated_decision.get("priority", 5),
            risk_band=consolidated_decision.get("risk_band", "medium"),
        )

        # Log se correlation_id foi gerado automaticamente
        if not correlation_id:
            self.logger.info(
                "correlation_id_generated",
                generated_correlation_id=context.correlation_id,
                intent_id=context.intent_id,
            )

        self.logger.info(
            "starting_flow_c",
            intent_id=context.intent_id,
            plan_id=context.plan_id,
            decision_id=context.decision_id,
        )

        try:
            # C1: Validate Decision
            step_c1 = await self._execute_c1_validate(consolidated_decision, context)
            steps.append(step_c1)

            # C2: Start Workflow and Generate Tickets
            step_c2_start = datetime.utcnow()
            workflow_id, tickets = await self._execute_c2_generate_tickets(
                consolidated_decision, context
            )
            step_c2_end = datetime.utcnow()
            step_c2 = FlowCStep(
                step_name="C2",
                status="completed",
                started_at=step_c2_start,
                completed_at=step_c2_end,
                duration_ms=int((step_c2_end - step_c2_start).total_seconds() * 1000),
                metadata={"workflow_id": workflow_id, "tickets_count": len(tickets)},
            )
            steps.append(step_c2)

            # C3: Discover Workers
            step_c3_start = datetime.utcnow()
            workers = await self._execute_c3_discover_workers(tickets, context)
            step_c3_end = datetime.utcnow()
            step_c3 = FlowCStep(
                step_name="C3",
                status="completed",
                started_at=step_c3_start,
                completed_at=step_c3_end,
                duration_ms=int((step_c3_end - step_c3_start).total_seconds() * 1000),
                metadata={"workers_found": len(workers)},
            )
            steps.append(step_c3)

            # C4: Assign Tickets to Workers
            step_c4_start = datetime.utcnow()
            assignments = await self._execute_c4_assign_tickets(tickets, workers, context)
            step_c4_end = datetime.utcnow()
            step_c4 = FlowCStep(
                step_name="C4",
                status="completed",
                started_at=step_c4_start,
                completed_at=step_c4_end,
                duration_ms=int((step_c4_end - step_c4_start).total_seconds() * 1000),
                metadata={"assignments_count": len(assignments)},
            )
            steps.append(step_c4)

            # C5: Monitor Execution
            step_c5_start = datetime.utcnow()
            results = await self._execute_c5_monitor_execution(tickets, context)
            step_c5_end = datetime.utcnow()
            step_c5 = FlowCStep(
                step_name="C5",
                status="completed",
                started_at=step_c5_start,
                completed_at=step_c5_end,
                duration_ms=int((step_c5_end - step_c5_start).total_seconds() * 1000),
                metadata={"completed": results["completed"], "failed": results["failed"]},
            )
            steps.append(step_c5)

            # C6: Publish Telemetry
            step_c6_start = datetime.utcnow()
            await self._execute_c6_publish_telemetry(context, workflow_id, tickets, results)
            step_c6_end = datetime.utcnow()
            step_c6 = FlowCStep(
                step_name="C6",
                status="completed",
                started_at=step_c6_start,
                completed_at=step_c6_end,
                duration_ms=int((step_c6_end - step_c6_start).total_seconds() * 1000),
            )
            steps.append(step_c6)

            # Calculate metrics
            end_time = datetime.utcnow()
            total_duration = int((end_time - start_time).total_seconds() * 1000)

            # Check SLA
            if end_time > context.sla_deadline:
                flow_c_sla_violations.inc()
                self.logger.warning("flow_c_sla_violated", duration_ms=total_duration)

            result = FlowCResult(
                success=True,
                steps=steps,
                total_duration_ms=total_duration,
                tickets_generated=len(tickets),
                tickets_completed=results["completed"],
                tickets_failed=results["failed"],
                telemetry_published=True,
            )

            flow_c_success.inc()
            flow_c_duration.observe(total_duration / 1000)

            self.logger.info(
                "flow_c_completed",
                duration_ms=total_duration,
                tickets_completed=results["completed"],
            )

            return result

        except Exception as e:
            flow_c_failures.labels(reason=type(e).__name__).inc()
            self.logger.error("flow_c_failed", error=str(e))

            return FlowCResult(
                success=False,
                steps=steps,
                total_duration_ms=0,
                tickets_generated=0,
                tickets_completed=0,
                tickets_failed=0,
                telemetry_published=False,
                error=str(e),
            )

    async def _execute_c1_validate(
        self, decision: Dict[str, Any], context: FlowCContext
    ) -> FlowCStep:
        """C1: Validate consolidated decision."""
        step_start = datetime.utcnow()

        with flow_c_steps_duration.labels(step="C1").time():
            # Validate required fields
            required_fields = ["intent_id", "plan_id", "decision_id", "cognitive_plan"]
            for field in required_fields:
                if field not in decision:
                    raise ValueError(f"Missing required field: {field}")

            await self.telemetry.publish_event(
                event_type="step_completed",
                step="C1",
                intent_id=context.intent_id,
                plan_id=context.plan_id,
                decision_id=context.decision_id,
                workflow_id="",
                ticket_ids=[],
                duration_ms=0,
                status="completed",
                metadata={},
            )

            step_end = datetime.utcnow()
            return FlowCStep(
                step_name="C1",
                status="completed",
                started_at=step_start,
                completed_at=step_end,
                duration_ms=int((step_end - step_start).total_seconds() * 1000),
            )

    async def _execute_c2_generate_tickets(
        self, decision: Dict[str, Any], context: FlowCContext
    ) -> tuple[str, List[Dict[str, Any]]]:
        """C2: Start workflow and generate execution tickets."""
        with flow_c_steps_duration.labels(step="C2").time():
            # Start Temporal workflow
            workflow_id = await self.orchestrator_client.start_workflow(
                cognitive_plan=decision["cognitive_plan"],
                correlation_id=context.correlation_id,
                priority=context.priority,
                sla_deadline_seconds=14400,
            )

            # Aguardar geração de tickets pelo workflow
            await asyncio.sleep(2)

            # Obter tickets do workflow state via query Temporal
            tickets = await self._get_tickets_from_workflow(
                workflow_id=workflow_id,
                cognitive_plan=decision.get("cognitive_plan", {}),
                context=context,
            )

            return workflow_id, tickets

    async def _get_tickets_from_workflow(
        self,
        workflow_id: str,
        cognitive_plan: Dict[str, Any],
        context: FlowCContext,
    ) -> List[Dict[str, Any]]:
        """
        Obtém tickets do workflow state via query Temporal.

        Args:
            workflow_id: ID do workflow Temporal
            cognitive_plan: Plano cognitivo para fallback
            context: Contexto do Flow C

        Returns:
            Lista de tickets em formato dict
        """
        import time

        start_time = time.time()
        tickets = []

        try:
            # Query workflow for tickets via Temporal query
            query_result = await self.orchestrator_client.query_workflow(
                workflow_id=workflow_id,
                query_name="get_tickets",
            )

            # Extrair tickets do resultado
            if isinstance(query_result, dict):
                tickets = query_result.get("tickets", [])
            elif isinstance(query_result, list):
                tickets = query_result
            else:
                tickets = []

            duration = time.time() - start_time
            flow_c_workflow_query_duration.labels(query_name="get_tickets").observe(duration)

            if not tickets:
                self.logger.warning(
                    "no_tickets_from_workflow",
                    workflow_id=workflow_id,
                    plan_id=context.plan_id,
                    reason="workflow returned empty tickets list",
                )
                # Fallback para extração do cognitive_plan
                tickets = await self._extract_tickets_from_plan(cognitive_plan, context)
            else:
                # Validar schema dos tickets
                for ticket in tickets:
                    is_valid, errors = self._validate_ticket_schema(ticket)
                    if not is_valid:
                        self.logger.warning(
                            "ticket_schema_validation_warning",
                            ticket_id=ticket.get("ticket_id", "unknown"),
                            errors=errors,
                        )
                        flow_c_ticket_validation_failures.inc()

                self.logger.info(
                    "tickets_retrieved_from_workflow",
                    workflow_id=workflow_id,
                    tickets_count=len(tickets),
                )

        except Exception as e:
            duration = time.time() - start_time
            flow_c_workflow_query_duration.labels(query_name="get_tickets").observe(duration)
            flow_c_workflow_query_failures.labels(
                query_name="get_tickets",
                reason=type(e).__name__,
            ).inc()

            self.logger.error(
                "failed_to_query_workflow_tickets",
                workflow_id=workflow_id,
                plan_id=context.plan_id,
                error=str(e),
            )
            # Fallback para extração do cognitive_plan
            tickets = await self._extract_tickets_from_plan(cognitive_plan, context)

        return tickets

    async def _extract_tickets_from_plan(
        self,
        cognitive_plan: Dict[str, Any],
        context: FlowCContext,
    ) -> List[Dict[str, Any]]:
        """
        Extrai tickets do cognitive_plan como fallback.

        Args:
            cognitive_plan: Plano cognitivo contendo tasks
            context: Contexto do Flow C

        Returns:
            Lista de tickets criados via ticket_client
        """
        self.logger.info(
            "extracting_tickets_from_plan_fallback",
            plan_id=context.plan_id,
            reason="workflow query failed or returned empty",
        )

        tickets = []
        tasks = cognitive_plan.get("tasks", [])

        if not tasks:
            # Fallback: criar ticket genérico
            tasks = [{"type": "code_generation", "description": "Generate code based on plan"}]

        for task in tasks:
            ticket_data = {
                "plan_id": context.plan_id,
                "task_type": task.get("type", "code_generation"),
                "required_capabilities": task.get("capabilities", ["python", "fastapi"]),
                "payload": {
                    "template_id": task.get("template_id", "default_template"),
                    "parameters": task.get("parameters", {}),
                    "description": task.get("description", ""),
                },
                "sla_deadline": context.sla_deadline.isoformat(),
                "priority": context.priority,
            }
            ticket = await self.ticket_client.create_ticket(ticket_data)

            # Adicionar ticket_id ao payload para workers/Code Forge
            ticket_dict = ticket.model_dump()
            ticket_dict["payload"]["ticket_id"] = ticket.ticket_id
            tickets.append(ticket_dict)

        self.logger.info(
            "tickets_extracted_from_plan",
            plan_id=context.plan_id,
            tickets_count=len(tickets),
        )

        return tickets

    def _validate_ticket_schema(self, ticket: Dict[str, Any]) -> tuple[bool, List[str]]:
        """
        Valida schema do ticket conforme execution-ticket.avsc.

        Campos obrigatórios conforme schema Avro:
        - ticket_id, plan_id, intent_id, decision_id, task_id, task_type,
          description, status, priority, risk_band, sla, qos, security_level, created_at

        Args:
            ticket: Ticket a validar

        Returns:
            Tuple (is_valid, errors)
        """
        errors = []

        # Campos obrigatórios conforme execution-ticket.avsc
        required_fields = [
            "ticket_id",
            "plan_id",
            "intent_id",
            "decision_id",
            "task_id",
            "task_type",
            "status",
            "priority",
            "risk_band",
        ]

        for field in required_fields:
            if field not in ticket:
                errors.append(f"Campo obrigatório ausente: {field}")

        # Validar enums conforme schema Avro
        valid_task_types = ["BUILD", "DEPLOY", "TEST", "VALIDATE", "EXECUTE", "COMPENSATE"]
        task_type = ticket.get("task_type")
        if task_type:
            if isinstance(task_type, str) and task_type.upper() not in valid_task_types:
                # Permitir valores legados (ex: code_generation) com warning
                self.logger.debug(
                    "legacy_task_type_detected",
                    ticket_id=ticket.get("ticket_id"),
                    task_type=task_type,
                )

        valid_statuses = ["PENDING", "RUNNING", "COMPLETED", "FAILED", "COMPENSATING", "COMPENSATED"]
        status = ticket.get("status")
        if status and isinstance(status, str) and status.upper() not in valid_statuses:
            errors.append(f"Status inválido: {status}")

        valid_priorities = ["LOW", "NORMAL", "HIGH", "CRITICAL"]
        priority = ticket.get("priority")
        if priority is not None:
            if isinstance(priority, int):
                # Aceitar prioridade numérica (1-10) para compatibilidade legada
                if priority < 1 or priority > 10:
                    errors.append(f"Prioridade numérica deve estar entre 1-10: {priority}")
            elif isinstance(priority, str) and priority.upper() not in valid_priorities:
                errors.append(f"Prioridade inválida: {priority}")

        # Validar risk_band conforme schema Avro
        valid_risk_bands = ["low", "medium", "high", "critical"]
        risk_band = ticket.get("risk_band")
        if risk_band:
            if isinstance(risk_band, str) and risk_band.lower() not in valid_risk_bands:
                errors.append(f"risk_band inválido: {risk_band}")

        # Validar estrutura SLA (obrigatória conforme schema)
        sla = ticket.get("sla")
        if sla is None:
            errors.append("Campo obrigatório ausente: sla")
        elif not isinstance(sla, dict):
            errors.append("SLA deve ser um objeto")
        else:
            # Validar campos obrigatórios do SLA
            if "deadline" not in sla:
                errors.append("Campo SLA ausente: deadline")
            elif not isinstance(sla["deadline"], (int, float)):
                errors.append("SLA.deadline deve ser timestamp numérico (long)")

            if "timeout_ms" not in sla:
                errors.append("Campo SLA ausente: timeout_ms")
            elif not isinstance(sla["timeout_ms"], (int, float)):
                errors.append("SLA.timeout_ms deve ser numérico (long)")

            if "max_retries" not in sla:
                errors.append("Campo SLA ausente: max_retries")
            elif not isinstance(sla["max_retries"], int):
                errors.append("SLA.max_retries deve ser inteiro (int)")

        # Validar estrutura QoS (obrigatória conforme schema)
        qos = ticket.get("qos")
        if qos is None:
            errors.append("Campo obrigatório ausente: qos")
        elif not isinstance(qos, dict):
            errors.append("QoS deve ser um objeto")
        else:
            # Validar delivery_mode enum
            valid_delivery_modes = ["AT_MOST_ONCE", "AT_LEAST_ONCE", "EXACTLY_ONCE"]
            if "delivery_mode" not in qos:
                errors.append("Campo QoS ausente: delivery_mode")
            elif qos["delivery_mode"] not in valid_delivery_modes:
                errors.append(f"QoS.delivery_mode inválido: {qos['delivery_mode']}")

            # Validar consistency enum
            valid_consistencies = ["EVENTUAL", "STRONG"]
            if "consistency" not in qos:
                errors.append("Campo QoS ausente: consistency")
            elif qos["consistency"] not in valid_consistencies:
                errors.append(f"QoS.consistency inválido: {qos['consistency']}")

            # Validar durability enum
            valid_durabilities = ["TRANSIENT", "PERSISTENT"]
            if "durability" not in qos:
                errors.append("Campo QoS ausente: durability")
            elif qos["durability"] not in valid_durabilities:
                errors.append(f"QoS.durability inválido: {qos['durability']}")

        # Verificar e registrar schema_version
        schema_version = ticket.get("schema_version", 1)
        supported_versions = [1, 2]

        # Registrar métrica de versão
        flow_c_ticket_schema_version.labels(schema_version=str(schema_version)).inc()

        if schema_version not in supported_versions:
            self.logger.warning(
                "unknown_ticket_schema_version",
                ticket_id=ticket.get("ticket_id"),
                schema_version=schema_version,
                supported_versions=supported_versions,
            )
            errors.append(f"schema_version não suportado: {schema_version} (suportados: {supported_versions})")

        return (len(errors) == 0, errors)

    async def _execute_c3_discover_workers(
        self, tickets: List[Dict[str, Any]], context: FlowCContext
    ) -> List[AgentInfo]:
        """C3: Discover available workers via Service Registry."""
        with flow_c_steps_duration.labels(step="C3").time():
            # Collect all required capabilities
            all_capabilities = set()
            for ticket in tickets:
                all_capabilities.update(ticket.get("required_capabilities", []))

            # Discover workers
            workers = await self.service_registry.discover_agents(
                capabilities=list(all_capabilities),
                filters={"status": "healthy"},
            )

            if not workers:
                self.logger.warning("no_workers_available")

            return workers

    async def _execute_c4_assign_tickets(
        self,
        tickets: List[Dict[str, Any]],
        workers: List[AgentInfo],
        context: FlowCContext,
    ) -> List[Dict[str, Any]]:
        """C4: Assign tickets to workers via direct gRPC/HTTP calls."""
        with flow_c_steps_duration.labels(step="C4").time():
            assignments = []

            if not workers:
                self.logger.error("no_workers_available_for_assignment")
                return assignments

            for idx, ticket in enumerate(tickets):
                # Round-robin assignment
                worker = workers[idx % len(workers)]

                # Criar cliente para o worker específico
                worker_client = WorkerAgentClient(base_url=worker.endpoint)

                try:
                    # Criar TaskAssignment
                    from neural_hive_integration.clients.worker_agent_client import TaskAssignment

                    task = TaskAssignment(
                        task_id=f"task_{ticket['ticket_id']}",
                        ticket_id=ticket["ticket_id"],
                        task_type=ticket.get("task_type", "code_generation"),
                        payload=ticket.get("payload", {}),
                        sla_deadline=ticket.get("sla_deadline", context.sla_deadline.isoformat()),
                    )

                    # Despachar tarefa para o worker
                    await worker_client.assign_task(task)

                    # Atualizar status do ticket para 'assigned'
                    await self.ticket_client.update_ticket_status(
                        ticket_id=ticket["ticket_id"],
                        status="assigned",
                        assigned_worker=worker.agent_id,
                    )

                    assignments.append({
                        "ticket_id": ticket["ticket_id"],
                        "worker_id": worker.agent_id,
                        "task_id": task.task_id,
                    })

                    self.logger.info(
                        "ticket_assigned",
                        ticket_id=ticket["ticket_id"],
                        worker_id=worker.agent_id,
                    )
                except Exception as e:
                    self.logger.error(
                        "failed_to_assign_ticket",
                        ticket_id=ticket["ticket_id"],
                        worker_id=worker.agent_id,
                        error=str(e),
                    )
                finally:
                    await worker_client.close()

            return assignments

    async def _execute_c5_monitor_execution(
        self, tickets: List[Dict[str, Any]], context: FlowCContext
    ) -> Dict[str, int]:
        """C5: Monitor ticket execution até deadline SLA ou conclusão."""
        with flow_c_steps_duration.labels(step="C5").time():
            completed = 0
            failed = 0

            # Calcular deadline baseado no SLA (4h) menos tempo já decorrido
            remaining_time = (context.sla_deadline - datetime.utcnow()).total_seconds()
            poll_interval = 60  # Polling a cada 60s para pipelines longos
            max_iterations = int(remaining_time / poll_interval) if remaining_time > 0 else 240  # Max 4h

            self.logger.info(
                "monitoring_execution",
                tickets_count=len(tickets),
                max_iterations=max_iterations,
                poll_interval=poll_interval,
                remaining_sla_seconds=remaining_time,
            )

            # Poll ticket status até conclusão ou deadline
            for iteration in range(max_iterations):
                statuses = []
                for ticket in tickets:
                    try:
                        ticket_obj = await self.ticket_client.get_ticket(ticket["ticket_id"])
                        statuses.append(ticket_obj.status)
                    except Exception as e:
                        self.logger.error(
                            "failed_to_get_ticket_status",
                            ticket_id=ticket["ticket_id"],
                            error=str(e),
                        )
                        statuses.append("unknown")

                # Verificar se todos os tickets foram concluídos
                if all(s in ["completed", "failed"] for s in statuses):
                    completed = statuses.count("completed")
                    failed = statuses.count("failed")
                    self.logger.info(
                        "all_tickets_completed",
                        completed=completed,
                        failed=failed,
                        iteration=iteration,
                    )
                    break

                # Verificar se deadline foi ultrapassado
                if datetime.utcnow() >= context.sla_deadline:
                    self.logger.warning("sla_deadline_reached_during_monitoring")
                    completed = statuses.count("completed")
                    failed = statuses.count("failed")
                    break

                await asyncio.sleep(poll_interval)

            return {"completed": completed, "failed": failed}

    async def _execute_c6_publish_telemetry(
        self,
        context: FlowCContext,
        workflow_id: str,
        tickets: List[Dict[str, Any]],
        results: Dict[str, int],
    ) -> None:
        """C6: Publish telemetry events."""
        with flow_c_steps_duration.labels(step="C6").time():
            await self.telemetry.publish_event(
                event_type="flow_completed",
                step="C6",
                intent_id=context.intent_id,
                plan_id=context.plan_id,
                decision_id=context.decision_id,
                workflow_id=workflow_id,
                ticket_ids=[t["ticket_id"] for t in tickets],
                duration_ms=0,
                status="completed",
                metadata={
                    "tickets_completed": results["completed"],
                    "tickets_failed": results["failed"],
                },
            )
