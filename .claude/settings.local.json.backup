{
  "permissions": {
    "allow": [
      "Bash(do echo '--- specialist-$specialist ---' grep -A2 mongodb: /jimy/Neural-Hive-Mind/helm-charts/specialist-$specialist/values-local.yaml)",
      "Bash(kubectl label:*)",
      "Bash(# Configure Grafana to use Loki as datasource\ncat <<'EOF'\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-loki-datasource\n  namespace: observability\n  labels:\n    grafana_datasource: \"1\"\ndata:\n  loki-datasource.yaml: |\n    apiVersion: 1\n    datasources:\n    - name: Loki\n      type: loki\n      access: proxy\n      url: http://loki:3100\n      isDefault: false\n      jsonData:\n        maxLines: 1000\nEOF | kubectl apply -f -)",
      "Bash(__NEW_LINE__ helm install longhorn longhorn/longhorn )",
      "Bash(# Create professional RBAC roles\ncat <<'EOF'\n---\n# Developer Role - Read-only + limited exec\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: developer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"pods/log\", \"services\", \"endpoints\", \"configmaps\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\", \"pods/portforward\"]\n  verbs: [\"create\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\", \"statefulsets\", \"daemonsets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"batch\"]\n  resources: [\"jobs\", \"cronjobs\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"networking.k8s.io\"]\n  resources: [\"ingresses\", \"networkpolicies\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\n# DevOps Role - Full namespace control\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: devops\nrules:\n- apiGroups: [\"\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- apiGroups: [\"apps\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- apiGroups: [\"batch\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- apiGroups: [\"networking.k8s.io\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- apiGroups: [\"autoscaling\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n- apiGroups: [\"policy\"]\n  resources: [\"poddisruptionbudgets\"]\n  verbs: [\"*\"]\n---\n# SRE Role - Full access + cluster resources\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: sre\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n---\n# Monitoring Role - Read metrics and logs\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"pods/log\", \"nodes\", \"services\", \"endpoints\", \"events\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\", \"statefulsets\", \"daemonsets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"metrics.k8s.io\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"monitoring.coreos.com\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\n# CI/CD Role - Deploy applications\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cicd-deployer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\", \"configmaps\", \"secrets\", \"serviceaccounts\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\", \"statefulsets\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"batch\"]\n  resources: [\"jobs\", \"cronjobs\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"autoscaling\"]\n  resources: [\"horizontalpodautoscalers\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\nEOF | kubectl apply -f -)",
      "Bash(# Create ServiceAccounts for common use cases\ncat <<'EOF'\n---\n# CI/CD ServiceAccount in neural-hive namespace\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: cicd-deployer\n  namespace: neural-hive\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: cicd-deployer-binding\n  namespace: neural-hive\nsubjects:\n- kind: ServiceAccount\n  name: cicd-deployer\n  namespace: neural-hive\nroleRef:\n  kind: ClusterRole\n  name: cicd-deployer\n  apiGroup: rbac.authorization.k8s.io\n---\n# Monitoring ServiceAccount\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: monitoring-reader\n  namespace: observability\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: monitoring-reader-binding\nsubjects:\n- kind: ServiceAccount\n  name: monitoring-reader\n  namespace: observability\nroleRef:\n  kind: ClusterRole\n  name: monitoring\n  apiGroup: rbac.authorization.k8s.io\n---\n# Developer ServiceAccount for neural-hive\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: developer\n  namespace: neural-hive\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developer-binding\n  namespace: neural-hive\nsubjects:\n- kind: ServiceAccount\n  name: developer\n  namespace: neural-hive\nroleRef:\n  kind: ClusterRole\n  name: developer\n  apiGroup: rbac.authorization.k8s.io\nEOF | kubectl apply -f -)",
      "Bash(# Create production-grade Prometheus alerting rules\ncat <<'EOF'\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: production-alerts\n  namespace: observability\n  labels:\n    prometheus: neural-hive\n    role: alert-rules\nspec:\n  groups:\n  - name: kubernetes-system\n    rules:\n    - alert: KubernetesNodeNotReady\n      expr: kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Node {{ $labels.node }} is not ready\"\n        description: \"Node has been unready for more than 5 minutes\"\n    \n    - alert: KubernetesNodeMemoryPressure\n      expr: kube_node_status_condition{condition=\"MemoryPressure\",status=\"true\"} == 1\n      for: 2m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Node {{ $labels.node }} has memory pressure\"\n        description: \"Node is running low on memory\"\n    \n    - alert: KubernetesNodeDiskPressure\n      expr: kube_node_status_condition{condition=\"DiskPressure\",status=\"true\"} == 1\n      for: 2m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Node {{ $labels.node }} has disk pressure\"\n        description: \"Node is running low on disk space\"\n\n  - name: kubernetes-pods\n    rules:\n    - alert: KubernetesPodCrashLooping\n      expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 0\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping\"\n        description: \"Pod has been restarting\"\n    \n    - alert: KubernetesPodNotReady\n      expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~\"Pending|Unknown\"}) > 0\n      for: 15m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready\"\n        description: \"Pod has been in non-ready state for more than 15 minutes\"\n    \n    - alert: KubernetesContainerOOMKilled\n      expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1\n      for: 0m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} was OOMKilled\"\n\n  - name: kubernetes-resources\n    rules:\n    - alert: KubernetesCPUOvercommit\n      expr: sum(kube_pod_container_resource_requests{resource=\"cpu\"}) / sum(kube_node_status_allocatable{resource=\"cpu\"}) > 0.9\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Cluster CPU overcommit\"\n        description: \"Cluster CPU requests are over 90% of allocatable\"\n    \n    - alert: KubernetesMemoryOvercommit\n      expr: sum(kube_pod_container_resource_requests{resource=\"memory\"}) / sum(kube_node_status_allocatable{resource=\"memory\"}) > 0.9\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Cluster memory overcommit\"\n        description: \"Cluster memory requests are over 90% of allocatable\"\n    \n    - alert: KubernetesPVCPending\n      expr: kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1\n      for: 15m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\"\n\n  - name: neural-hive-application\n    rules:\n    - alert: NeuralHiveServiceDown\n      expr: up{namespace=\"neural-hive\"} == 0\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Service {{ $labels.job }} in neural-hive is down\"\n        description: \"Service has been down for more than 2 minutes\"\n    \n    - alert: NeuralHiveHighLatency\n      expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{namespace=\"neural-hive\"}[5m])) by (le, service)) > 2\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High latency in {{ $labels.service }}\"\n        description: \"P99 latency is above 2 seconds\"\n    \n    - alert: NeuralHiveHighErrorRate\n      expr: sum(rate(http_requests_total{namespace=\"neural-hive\",status=~\"5..\"}[5m])) by (service) / sum(rate(http_requests_total{namespace=\"neural-hive\"}[5m])) by (service) > 0.05\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High error rate in {{ $labels.service }}\"\n        description: \"Error rate is above 5%\"\n\n  - name: kafka-alerts\n    rules:\n    - alert: KafkaConsumerLag\n      expr: kafka_consumergroup_lag > 1000\n      for: 10m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Kafka consumer lag is high for {{ $labels.consumergroup }}\"\n        description: \"Consumer group {{ $labels.consumergroup }} has lag > 1000\"\n    \n    - alert: KafkaBrokerDown\n      expr: kafka_brokers < 3\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Kafka broker is down\"\n        description: \"Number of Kafka brokers is below expected\"\n\n  - name: mongodb-alerts\n    rules:\n    - alert: MongoDBDown\n      expr: mongodb_up == 0\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"MongoDB is down\"\n        description: \"MongoDB instance is not responding\"\n    \n    - alert: MongoDBHighConnections\n      expr: mongodb_connections{state=\"current\"} > 500\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"MongoDB high connections\"\n        description: \"MongoDB has more than 500 active connections\"\nEOF | kubectl apply -f -)",
      "Bash(# Create HorizontalPodAutoscalers for neural-hive services\ncat <<'EOF'\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: gateway-intencoes-hpa\n  namespace: neural-hive\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: gateway-intencoes\n  minReplicas: 1\n  maxReplicas: 5\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 50\n        periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 60\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 30\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: consensus-engine-hpa\n  namespace: neural-hive\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: consensus-engine\n  minReplicas: 1\n  maxReplicas: 3\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: semantic-translation-engine-hpa\n  namespace: neural-hive\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: semantic-translation-engine\n  minReplicas: 1\n  maxReplicas: 5\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\nEOF | kubectl apply -f -)",
      "Bash(# Create sample Ingress with TLS for neural-hive gateway\ncat <<'EOF'\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: neural-hive-gateway\n  namespace: neural-hive\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/proxy-body-size: \"50m\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - api.neural-hive.local\n    secretName: neural-hive-gateway-tls\n  rules:\n  - host: api.neural-hive.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: gateway-intencoes\n            port:\n              number: 8000\n---\n# Ingress for Grafana\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - grafana.neural-hive.local\n    secretName: grafana-tls\n  rules:\n  - host: grafana.neural-hive.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-prometheus-grafana\n            port:\n              number: 80\n---\n# Ingress for Longhorn UI\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: longhorn-ingress\n  namespace: longhorn-system\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: longhorn-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: longhorn.neural-hive.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: longhorn-frontend\n            port:\n              number: 80\nEOF | kubectl apply -f -)",
      "Bash(# Create basic auth secret for Longhorn using heredoc\ncat <<'EOF'\napiVersion: v1\nkind: Secret\nmetadata:\n  name: longhorn-basic-auth\n  namespace: longhorn-system\ntype: Opaque\nstringData:\n  auth: \"admin:$apr1$7qJN3pYX$OZsVqCzHCRu/qJxcBLhWp0\"\nEOF | kubectl apply -f -)",
      "Bash(kubectl get:*)",
      "Bash(helm repo add:*)",
      "Bash(helm repo update:*)",
      "Bash(helm install:*)",
      "Bash(kubectl describe:*)",
      "Bash(helm uninstall:*)",
      "Bash(kubectl logs:*)",
      "Bash(# Update ingresses with external-dns target annotation\ncat <<'EOF'\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: neural-hive-gateway\n  namespace: neural-hive\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/proxy-body-size: \"50m\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n    external-dns.alpha.kubernetes.io/hostname: api.elysiumii.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\"\n    external-dns.alpha.kubernetes.io/target: \"37.60.241.150\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - api.elysiumii.com\n    secretName: neural-hive-gateway-tls\n  rules:\n  - host: api.elysiumii.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: gateway-intencoes\n            port:\n              number: 8000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    external-dns.alpha.kubernetes.io/hostname: grafana.elysiumii.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\"\n    external-dns.alpha.kubernetes.io/target: \"37.60.241.150\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - grafana.elysiumii.com\n    secretName: grafana-tls\n  rules:\n  - host: grafana.elysiumii.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-prometheus-grafana\n            port:\n              number: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: longhorn-ingress\n  namespace: longhorn-system\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: longhorn-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    external-dns.alpha.kubernetes.io/hostname: longhorn.elysiumii.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\"\n    external-dns.alpha.kubernetes.io/target: \"37.60.241.150\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - longhorn.elysiumii.com\n    secretName: longhorn-tls\n  rules:\n  - host: longhorn.elysiumii.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: longhorn-frontend\n            port:\n              number: 80\nEOF | kubectl apply -f -)",
      "Bash(# Create new ClusterIssuer with DNS-01 solver\ncat <<'EOF'\n---\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod-dns\nspec:\n  acme:\n    email: admin@elysiumii.com\n    server: https://acme-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-prod-dns-account-key\n    solvers:\n    - dns01:\n        cloudflare:\n          apiTokenSecretRef:\n            name: cloudflare-api-token\n            key: api-token\n      selector:\n        dnsZones:\n        - \"elysiumii.com\"\n---\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging-dns\nspec:\n  acme:\n    email: admin@elysiumii.com\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-staging-dns-account-key\n    solvers:\n    - dns01:\n        cloudflare:\n          apiTokenSecretRef:\n            name: cloudflare-api-token\n            key: api-token\n      selector:\n        dnsZones:\n        - \"elysiumii.com\"\nEOF | kubectl apply -f -)",
      "Bash(# Update Ingresses to use DNS-01 issuer\ncat <<'EOF'\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: neural-hive-gateway\n  namespace: neural-hive\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod-dns\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/proxy-body-size: \"50m\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\n    external-dns.alpha.kubernetes.io/hostname: api.elysiumii.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\"\n    external-dns.alpha.kubernetes.io/target: \"37.60.241.150\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - api.elysiumii.com\n    secretName: neural-hive-gateway-tls\n  rules:\n  - host: api.elysiumii.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: gateway-intencoes\n            port:\n              number: 8000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod-dns\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    external-dns.alpha.kubernetes.io/hostname: grafana.elysiumii.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\"\n    external-dns.alpha.kubernetes.io/target: \"37.60.241.150\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - grafana.elysiumii.com\n    secretName: grafana-tls\n  rules:\n  - host: grafana.elysiumii.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-prometheus-grafana\n            port:\n              number: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: longhorn-ingress\n  namespace: longhorn-system\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod-dns\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: longhorn-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\n    external-dns.alpha.kubernetes.io/hostname: longhorn.elysiumii.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\"\n    external-dns.alpha.kubernetes.io/target: \"37.60.241.150\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - longhorn.elysiumii.com\n    secretName: longhorn-tls\n  rules:\n  - host: longhorn.elysiumii.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: longhorn-frontend\n            port:\n              number: 80\nEOF | kubectl apply -f -)",
      "Bash(dig:*)",
      "Bash(whois:*)",
      "Bash(echo:*)",
      "Bash(# Create new Ingresses with HTTP-01 (not DNS-01) for elysiumii.site\ncat <<'EOF'\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: neural-hive-gateway\n  namespace: neural-hive\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/proxy-body-size: \"50m\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - api.elysiumii.site\n    secretName: neural-hive-gateway-tls\n  rules:\n  - host: api.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: gateway-intencoes\n            port:\n              number: 8000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - grafana.elysiumii.site\n    secretName: grafana-tls\n  rules:\n  - host: grafana.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-prometheus-grafana\n            port:\n              number: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: longhorn-ingress\n  namespace: longhorn-system\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: longhorn-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - longhorn.elysiumii.site\n    secretName: longhorn-tls\n  rules:\n  - host: longhorn.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: longhorn-frontend\n            port:\n              number: 80\nEOF | kubectl apply -f -)",
      "Bash(curl:*)",
      "Bash(ss:*)",
      "Bash(docker ps:*)",
      "Bash(docker stop:*)",
      "Bash(docker rm:*)",
      "Bash(__NEW_LINE__ helm upgrade ingress-nginx ingress-nginx/ingress-nginx )",
      "Bash(jq -r \".[] | \"\"\\(.name): \\(.value // .valueFrom.secretKeyRef.name)\"\"\")",
      "Bash(kubectl exec:*)",
      "Bash(xargs:*)",
      "Bash(kubectl rollout:*)",
      "Bash(pkill:*)",
      "Bash(for specialist in business technical behavior evolution architecture)",
      "Bash(do echo '--- specialist-$specialist ---')",
      "Bash(jq:*)",
      "Bash(done)",
      "Bash(do echo)",
      "Bash(cat:*)",
      "Bash(docker build:*)",
      "Bash(.)",
      "Bash(./scripts/build-base-images.sh:*)",
      "Bash(docker run:*)",
      "Bash(./scripts/build-and-push-to-registry.sh:*)",
      "Bash(openssl s_client:*)",
      "Bash(openssl x509:*)",
      "Bash(for specialist in specialist-business specialist-technical specialist-behavior specialist-evolution specialist-architecture)",
      "Bash(do)",
      "Bash(kubectl run curl-test --rm -it --restart=Never --image=curlimages/curl:latest -n neural-hive -- curl -s http://gateway-intencoes/health)",
      "Bash(kubectl delete:*)",
      "Bash(# Create NGINX Ingress Dashboard\ncat <<'EOF'\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-dashboard-nginx-ingress\n  namespace: observability\n  labels:\n    grafana_dashboard: \"1\"\ndata:\n  nginx-ingress.json: |\n    {\n      \"annotations\": {\n        \"list\": []\n      },\n      \"editable\": true,\n      \"fiscalYearStartMonth\": 0,\n      \"graphTooltip\": 0,\n      \"id\": null,\n      \"links\": [],\n      \"liveNow\": false,\n      \"panels\": [\n        {\n          \"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"},\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\"mode\": \"palette-classic\"},\n              \"mappings\": [],\n              \"thresholds\": {\"mode\": \"absolute\", \"steps\": [{\"color\": \"green\", \"value\": null}]},\n              \"unit\": \"reqps\"\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 0, \"y\": 0},\n          \"id\": 1,\n          \"options\": {\"colorMode\": \"value\", \"graphMode\": \"area\", \"justifyMode\": \"auto\", \"orientation\": \"auto\", \"reduceOptions\": {\"calcs\": [\"lastNotNull\"], \"fields\": \"\", \"values\": false}, \"textMode\": \"auto\"},\n          \"pluginVersion\": \"10.0.0\",\n          \"targets\": [{\"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"}, \"expr\": \"sum(rate(nginx_ingress_controller_requests[5m]))\", \"refId\": \"A\"}],\n          \"title\": \"Total Requests/sec\",\n          \"type\": \"stat\"\n        },\n        {\n          \"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"},\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\"mode\": \"palette-classic\"},\n              \"mappings\": [],\n              \"thresholds\": {\"mode\": \"absolute\", \"steps\": [{\"color\": \"green\", \"value\": null}, {\"color\": \"yellow\", \"value\": 0.01}, {\"color\": \"red\", \"value\": 0.05}]},\n              \"unit\": \"percentunit\"\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\"h\": 8, \"w\": 12, \"x\": 12, \"y\": 0},\n          \"id\": 2,\n          \"options\": {\"colorMode\": \"value\", \"graphMode\": \"area\", \"justifyMode\": \"auto\", \"orientation\": \"auto\", \"reduceOptions\": {\"calcs\": [\"lastNotNull\"], \"fields\": \"\", \"values\": false}, \"textMode\": \"auto\"},\n          \"pluginVersion\": \"10.0.0\",\n          \"targets\": [{\"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"}, \"expr\": \"sum(rate(nginx_ingress_controller_requests{status=~\\\"5..\\\"}[5m])) / sum(rate(nginx_ingress_controller_requests[5m]))\", \"refId\": \"A\"}],\n          \"title\": \"Error Rate (5xx)\",\n          \"type\": \"stat\"\n        },\n        {\n          \"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"},\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\"mode\": \"palette-classic\"},\n              \"custom\": {\"axisCenteredZero\": false, \"axisColorMode\": \"text\", \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 10, \"gradientMode\": \"none\", \"hideFrom\": {\"legend\": false, \"tooltip\": false, \"viz\": false}, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": {\"type\": \"linear\"}, \"showPoints\": \"never\", \"spanNulls\": false, \"stacking\": {\"group\": \"A\", \"mode\": \"none\"}, \"thresholdsStyle\": {\"mode\": \"off\"}},\n              \"mappings\": [],\n              \"thresholds\": {\"mode\": \"absolute\", \"steps\": [{\"color\": \"green\", \"value\": null}]},\n              \"unit\": \"reqps\"\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\"h\": 8, \"w\": 24, \"x\": 0, \"y\": 8},\n          \"id\": 3,\n          \"options\": {\"legend\": {\"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true}, \"tooltip\": {\"mode\": \"multi\", \"sort\": \"none\"}},\n          \"targets\": [{\"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"}, \"expr\": \"sum by (ingress) (rate(nginx_ingress_controller_requests[5m]))\", \"legendFormat\": \"{{ingress}}\", \"refId\": \"A\"}],\n          \"title\": \"Requests by Ingress\",\n          \"type\": \"timeseries\"\n        },\n        {\n          \"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"},\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\"mode\": \"palette-classic\"},\n              \"custom\": {\"axisCenteredZero\": false, \"axisColorMode\": \"text\", \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 10, \"gradientMode\": \"none\", \"hideFrom\": {\"legend\": false, \"tooltip\": false, \"viz\": false}, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": {\"type\": \"linear\"}, \"showPoints\": \"never\", \"spanNulls\": false, \"stacking\": {\"group\": \"A\", \"mode\": \"none\"}, \"thresholdsStyle\": {\"mode\": \"off\"}},\n              \"mappings\": [],\n              \"thresholds\": {\"mode\": \"absolute\", \"steps\": [{\"color\": \"green\", \"value\": null}]},\n              \"unit\": \"s\"\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\"h\": 8, \"w\": 24, \"x\": 0, \"y\": 16},\n          \"id\": 4,\n          \"options\": {\"legend\": {\"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true}, \"tooltip\": {\"mode\": \"multi\", \"sort\": \"none\"}},\n          \"targets\": [{\"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"}, \"expr\": \"histogram_quantile(0.50, sum(rate(nginx_ingress_controller_request_duration_seconds_bucket[5m])) by (le, ingress))\", \"legendFormat\": \"p50 {{ingress}}\", \"refId\": \"A\"}, {\"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"}, \"expr\": \"histogram_quantile(0.95, sum(rate(nginx_ingress_controller_request_duration_seconds_bucket[5m])) by (le, ingress))\", \"legendFormat\": \"p95 {{ingress}}\", \"refId\": \"B\"}, {\"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"}, \"expr\": \"histogram_quantile(0.99, sum(rate(nginx_ingress_controller_request_duration_seconds_bucket[5m])) by (le, ingress))\", \"legendFormat\": \"p99 {{ingress}}\", \"refId\": \"C\"}],\n          \"title\": \"Request Latency by Ingress\",\n          \"type\": \"timeseries\"\n        },\n        {\n          \"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"},\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\"mode\": \"palette-classic\"},\n              \"custom\": {\"axisCenteredZero\": false, \"axisColorMode\": \"text\", \"axisLabel\": \"\", \"axisPlacement\": \"auto\", \"barAlignment\": 0, \"drawStyle\": \"line\", \"fillOpacity\": 10, \"gradientMode\": \"none\", \"hideFrom\": {\"legend\": false, \"tooltip\": false, \"viz\": false}, \"lineInterpolation\": \"linear\", \"lineWidth\": 1, \"pointSize\": 5, \"scaleDistribution\": {\"type\": \"linear\"}, \"showPoints\": \"never\", \"spanNulls\": false, \"stacking\": {\"group\": \"A\", \"mode\": \"normal\"}, \"thresholdsStyle\": {\"mode\": \"off\"}},\n              \"mappings\": [],\n              \"thresholds\": {\"mode\": \"absolute\", \"steps\": [{\"color\": \"green\", \"value\": null}]},\n              \"unit\": \"reqps\"\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\"h\": 8, \"w\": 24, \"x\": 0, \"y\": 24},\n          \"id\": 5,\n          \"options\": {\"legend\": {\"calcs\": [], \"displayMode\": \"list\", \"placement\": \"bottom\", \"showLegend\": true}, \"tooltip\": {\"mode\": \"multi\", \"sort\": \"none\"}},\n          \"targets\": [{\"datasource\": {\"type\": \"prometheus\", \"uid\": \"prometheus\"}, \"expr\": \"sum by (status) (rate(nginx_ingress_controller_requests[5m]))\", \"legendFormat\": \"{{status}}\", \"refId\": \"A\"}],\n          \"title\": \"Requests by Status Code\",\n          \"type\": \"timeseries\"\n        }\n      ],\n      \"refresh\": \"30s\",\n      \"schemaVersion\": 38,\n      \"style\": \"dark\",\n      \"tags\": [\"nginx\", \"ingress\"],\n      \"templating\": {\"list\": []},\n      \"time\": {\"from\": \"now-1h\", \"to\": \"now\"},\n      \"timepicker\": {},\n      \"timezone\": \"\",\n      \"title\": \"NGINX Ingress Controller\",\n      \"uid\": \"nginx-ingress\",\n      \"version\": 1,\n      \"weekStart\": \"\"\n    }\nEOF | kubectl apply -f -)",
      "Bash(for spec in specialist-business specialist-technical specialist-behavior specialist-evolution specialist-architecture)",
      "Bash(do echo '--- $spec ---' kubectl logs -n neural-hive deploy/$spec --tail=500)",
      "Bash(find:*)",
      "Bash(source .env.training)",
      "Bash(export LLM_PROVIDER LLM_MODEL LLM_API_KEY MLFLOW_TRACKING_URI MONGODB_URI)",
      "Bash(./train_all_specialists.sh:*)",
      "Bash(export MLFLOW_TRACKING_URI=http://localhost:5000:*)",
      "Bash(export LLM_PROVIDER LLM_MODEL LLM_API_KEY)",
      "Bash(export ALLOW_SYNTHETIC_FALLBACK=true)",
      "Bash(export DATASET_DIR=/jimy/Neural-Hive-Mind/ml_pipelines/training/data:*)",
      "Bash(python3:*)",
      "Bash(pip install:*)",
      "Bash(pip3 install:*)",
      "Bash(pip3 show:*)",
      "Bash(/usr/bin/python3:*)",
      "Bash(export OUTPUT_DIR=/jimy/Neural-Hive-Mind/ml_pipelines/training/data:*)",
      "Bash(export NUM_SAMPLES=100)",
      "Bash(./generate_all_datasets.sh:*)",
      "Bash(pip3 --version:*)",
      "Bash(/usr/local/bin/python3.11:*)",
      "Bash(for specialist in business behavior evolution architecture)",
      "Bash(for specialist in technical business behavior evolution architecture)",
      "Bash(kubectl set env:*)",
      "Bash(apt-get install:*)",
      "Bash(kubectl run:*)",
      "Bash(do echo \"=== specialist-$specialist ===\" kubectl logs -n neural-hive deploy/specialist-$specialist --since=2m)",
      "Bash(source:*)",
      "Bash(export MLFLOW_TRACKING_URI=$MLFLOW_TRACKING_URI:-http://localhost:5000)",
      "Bash(export TRAINING_DATASET_PATH=$TRAINING_DATASET_PATH:-/jimy/Neural-Hive-Mind/ml_pipelines/training/data/specialist_{specialist_type_training.parquet})",
      "Bash(export TRAINING_DATASET_PATH=\"/jimy/Neural-Hive-Mind/ml_pipelines/training/data/specialist_{specialist_type}_training.parquet\")",
      "Bash(netstat:*)",
      "Bash(do echo echo '--- specialist-$specialist ---' kubectl logs -n neural-hive deploy/specialist-$specialist --tail=20)",
      "Bash(cp:*)",
      "Bash(do echo \"Atualizando specialist-$specialist...\")",
      "Bash(kubectl set image deployment/specialist-$specialist -n neural-hive specialist=37.60.241.150:30500/specialist-$specialist:latest done)",
      "Bash(kubectl set image:*)",
      "Bash(kubectl patch:*)",
      "Bash(grep:*)",
      "Bash(kubectl create configmap:*)",
      "Bash(kubectl apply:*)",
      "Bash(helm get values:*)",
      "Bash(helm upgrade:*)",
      "Bash(for img in specialist-business specialist-technical specialist-behavior specialist-evolution specialist-architecture)",
      "Bash(do echo -n '$img: ' curl -s http://37.60.241.150:30500/v2/$img/tags/list)",
      "Bash(docker images:*)",
      "Bash(sort:*)",
      "Bash(for svc in specialist-business specialist-technical specialist-behavior specialist-evolution)",
      "Bash(do echo '=== $svc ===' kubectl exec -n neural-hive deploy/$svc -- curl -s http://localhost:8000/status)",
      "Bash(do echo -n \"specialist-$spec: \" kubectl exec -n neural-hive deploy/specialist-$spec -- curl -s http://localhost:8000/health)",
      "Bash(docker push:*)",
      "Bash(docker tag:*)",
      "Bash(for spec in business technical behavior evolution architecture)",
      "Bash(do echo -n 'specialist-$spec: ' kubectl exec -n neural-hive deploy/specialist-$spec -- curl -s http://localhost:8000/status)",
      "Bash(for spec in technical behavior evolution architecture)",
      "Bash(do echo '=== SPECIALIST-$spec ===' kubectl logs -n neural-hive deploy/specialist-$spec --since=5m)",
      "Bash(for spec in technical business behavior evolution architecture)",
      "Bash(do echo '=== specialist-$spec ===' kubectl exec -n neural-hive deploy/specialist-$spec -- curl -s http://localhost:8000/status)",
      "Bash(export PYTHONPATH=\"/jimy/Neural-Hive-Mind/libraries/python:/jimy/Neural-Hive-Mind/ml_pipelines:$PYTHONPATH\")",
      "Bash(export LLM_PROVIDER=groq)",
      "Bash(export LLM_MODEL=llama-3.1-70b-versatile)",
      "Bash(export LLM_API_KEY=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)",
      "Bash(export LLM_MODEL=llama-3.3-70b-versatile)",
      "Bash(ls:*)",
      "Bash(for f in /jimy/Neural-Hive-Mind/ml_pipelines/training/data/specialist_*_base.parquet)",
      "Bash(do echo \"=== $f ===\")",
      "Bash(export LLM_PROVIDER=deepseek)",
      "Bash(export LLM_MODEL=deepseek-chat)",
      "Bash(export LLM_API_KEY=sk-6cd0e5de616c4d5bb7232fe87f8c9bfd)",
      "Bash(for f in data/specialist_*_base.parquet)",
      "Bash(export NUM_SAMPLES=30)",
      "Bash(export MIN_QUALITY_SCORE=0.5)",
      "Bash(chmod:*)",
      "Bash(bash -n:*)",
      "Bash(DOCKER_BUILDKIT=1 docker build:*)",
      "Bash(for service in analyst-agents code-forge execution-ticket-service guard-agents mcp-tool-catalog optimizer-agents orchestrator-dynamic queen-agent scout-agents self-healing-engine service-registry sla-management-system worker-agents)",
      "Bash(mv:*)",
      "Bash(# Aplicar MCP Tool Catalog alerts com namespace correto (observability)\ncat <<'EOF'\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: mcp-tool-catalog-alerts\n  namespace: observability\n  labels:\n    app.kubernetes.io/name: mcp-tool-catalog\n    app.kubernetes.io/component: alerting\n    prometheus: neural-hive\nspec:\n  groups:\n  - name: mcp-tool-catalog\n    interval: 30s\n    rules:\n    - alert: MCPToolCatalogDown\n      expr: up{job=\"mcp-tool-catalog\"} == 0\n      for: 1m\n      labels:\n        severity: critical\n        component: mcp-tool-catalog\n      annotations:\n        summary: \"MCP Tool Catalog Service is down\"\n        description: \"MCP Tool Catalog has been down for more than 1 minute.\"\n    - alert: MCPHighSelectionLatency\n      expr: histogram_quantile(0.95, rate(mcp_tool_selection_duration_seconds_bucket[5m])) > 5\n      for: 5m\n      labels:\n        severity: warning\n        component: mcp-tool-catalog\n      annotations:\n        summary: \"High tool selection latency (p95 > 5s)\"\n        description: \"95th percentile of tool selection duration is {{ $value }}s.\"\n    - alert: MCPPodCrashLooping\n      expr: rate(kube_pod_container_status_restarts_total{pod=~\"mcp-tool-catalog.*\"}[15m]) > 0\n      for: 5m\n      labels:\n        severity: critical\n        component: mcp-tool-catalog\n      annotations:\n        summary: \"Pod is crash looping\"\n        description: \"Pod {{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes.\"\nEOF | kubectl apply -f -)",
      "Bash(# Aplicar circuit-breaker-alerts com header correto\ncat <<'EOF'\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: circuit-breaker-alerts\n  namespace: observability\n  labels:\n    prometheus: neural-hive\nspec:\n  groups:\n  - name: circuit_breakers\n    interval: 30s\n    rules:\n    - alert: CircuitBreakerOpen\n      expr: circuit_breaker_state == 1\n      for: 2m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Circuit breaker {{ $labels.circuit }} is open in {{ $labels.service }}\"\n    - alert: CircuitBreakerHighFailureRate\n      expr: rate(circuit_breaker_failures_total[5m]) > 0.1\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"High failure rate in circuit {{ $labels.circuit }} ({{ $labels.service }})\"\nEOF | kubectl apply -f -)",
      "Bash(# Aplicar model-performance-alerts com header correto\ncat <<'EOF'\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: model-performance-alerts\n  namespace: observability\n  labels:\n    prometheus: neural-hive\nspec:\n  groups:\n  - name: model_performance_alerts\n    interval: 30s\n    rules:\n    - alert: ModelPerformanceDegraded\n      expr: neural_hive_model_performance_score < 0.6\n      for: 1h\n      labels:\n        severity: warning\n        component: ml-monitoring\n      annotations:\n        summary: \"Performance do modelo {{ $labels.specialist_type }} degradada\"\n        description: \"Performance do modelo está abaixo de 60%\"\n    - alert: ModelPrecisionBelowThreshold\n      expr: neural_hive_mlflow_model_precision < 0.75\n      for: 30m\n      labels:\n        severity: warning\n        component: ml-monitoring\n      annotations:\n        summary: \"Precision do modelo {{ $labels.specialist_type }} abaixo do threshold\"\n    - alert: AutoRetrainFailed\n      expr: increase(neural_hive_auto_retrain_triggered_total{status=\"failed\"}[15m]) > 0\n      for: 5m\n      labels:\n        severity: critical\n        component: ml-monitoring\n      annotations:\n        summary: \"Auto-retrain falhou para {{ $labels.specialist_type }}\"\n    - alert: ModelNotLoaded\n      expr: neural_hive_specialist_model_loaded == 0\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Modelo {{ $labels.specialist_type }} não carregado\"\n        description: \"Specialist está usando heurísticas ao invés de modelo ML\"\n  - name: model_degradation\n    interval: 30s\n    rules:\n    - alert: FeatureExtractionLatencyHigh\n      expr: histogram_quantile(0.95, sum(rate(neural_hive_specialist_feature_extraction_duration_seconds_bucket[5m])) by (le, specialist_type)) > 30\n      for: 10m\n      labels:\n        severity: critical\n        component: ml-monitoring\n      annotations:\n        summary: \"Feature extraction muito lenta para {{ $labels.specialist_type }}\"\n    - alert: ModelConfidenceCritical\n      expr: neural_hive_specialist_confidence_score < 0.1\n      for: 5m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Modelo {{ $labels.specialist_type }} com confiança crítica\"\nEOF | kubectl apply -f -)",
      "Bash(# Criar Ingress para Prometheus\ncat <<'EOF'\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: prometheus-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required - Prometheus\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - prometheus.elysiumii.site\n    secretName: prometheus-tls\n  rules:\n  - host: prometheus.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-prometheus-kub-prometheus\n            port:\n              number: 9090\nEOF | kubectl apply -f -)",
      "Bash(# Criar Ingress para Alertmanager\ncat <<'EOF'\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: alertmanager-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: alertmanager-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required - Alertmanager\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - alertmanager.elysiumii.site\n    secretName: alertmanager-tls\n  rules:\n  - host: alertmanager.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-prometheus-kub-alertmanager\n            port:\n              number: 9093\nEOF | kubectl apply -f -)",
      "Bash(# Criar Ingress para Jaeger UI\ncat <<'EOF'\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: jaeger-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: jaeger-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required - Jaeger\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - jaeger.elysiumii.site\n    secretName: jaeger-tls\n  rules:\n  - host: jaeger.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-jaeger-query\n            port:\n              number: 16686\nEOF | kubectl apply -f -)",
      "Bash(# Criar Ingress para Loki (opcional, para queries via API)\ncat <<'EOF'\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: loki-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: loki-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required - Loki\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - loki.elysiumii.site\n    secretName: loki-tls\n  rules:\n  - host: loki.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: loki\n            port:\n              number: 3100\nEOF | kubectl apply -f -)",
      "Bash(# Gerar senha segura e criar secrets de autenticação básica\n# Usando htpasswd format: admin:$apr1$...\n\n# Criar secret para Prometheus (admin/NeuralHive2024!)\ncat <<'EOF'\napiVersion: v1\nkind: Secret\nmetadata:\n  name: prometheus-basic-auth\n  namespace: observability\ntype: Opaque\nstringData:\n  auth: \"admin:$apr1$Xz7qK2pL$8vR4Y1mN5wQ3zE6tU9jH0.\"\nEOF | kubectl apply -f -)",
      "Bash(# Criar secret para Alertmanager\ncat <<'EOF'\napiVersion: v1\nkind: Secret\nmetadata:\n  name: alertmanager-basic-auth\n  namespace: observability\ntype: Opaque\nstringData:\n  auth: \"admin:$apr1$Xz7qK2pL$8vR4Y1mN5wQ3zE6tU9jH0.\"\nEOF | kubectl apply -f -)",
      "Bash(# Criar secret para Jaeger\ncat <<'EOF'\napiVersion: v1\nkind: Secret\nmetadata:\n  name: jaeger-basic-auth\n  namespace: observability\ntype: Opaque\nstringData:\n  auth: \"admin:$apr1$Xz7qK2pL$8vR4Y1mN5wQ3zE6tU9jH0.\"\nEOF | kubectl apply -f -)",
      "Bash(# Criar secret para Loki\ncat <<'EOF'\napiVersion: v1\nkind: Secret\nmetadata:\n  name: loki-basic-auth\n  namespace: observability\ntype: Opaque\nstringData:\n  auth: \"admin:$apr1$Xz7qK2pL$8vR4Y1mN5wQ3zE6tU9jH0.\"\nEOF | kubectl apply -f -)",
      "Bash(# Atualizar Ingress do Jaeger para usar o serviço correto (all-in-one)\ncat <<'EOF'\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: jaeger-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: jaeger-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required - Jaeger\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - jaeger.elysiumii.site\n    secretName: jaeger-tls\n  rules:\n  - host: jaeger.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-jaeger-allinone\n            port:\n              number: 16686\nEOF | kubectl apply -f -)",
      "Bash(# Deletar e recriar os ingresses\nkubectl delete ingress prometheus-ingress alertmanager-ingress jaeger-ingress loki-ingress -n observability 2>/dev/null\n\nsleep 3\n\n# Recriar todos\ncat <<'EOF'\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: prometheus-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - prometheus.elysiumii.site\n    secretName: prometheus-tls\n  rules:\n  - host: prometheus.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-prometheus-kub-prometheus\n            port:\n              number: 9090\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: alertmanager-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: alertmanager-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - alertmanager.elysiumii.site\n    secretName: alertmanager-tls\n  rules:\n  - host: alertmanager.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-prometheus-kub-alertmanager\n            port:\n              number: 9093\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: jaeger-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: jaeger-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - jaeger.elysiumii.site\n    secretName: jaeger-tls\n  rules:\n  - host: jaeger.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: neural-hive-jaeger-allinone\n            port:\n              number: 16686\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: loki-ingress\n  namespace: observability\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: loki-basic-auth\n    nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - loki.elysiumii.site\n    secretName: loki-tls\n  rules:\n  - host: loki.elysiumii.site\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: loki\n            port:\n              number: 3100\nEOF | kubectl apply -f -\n\necho \"\"Ingresses recriados\"\")",
      "Bash(/usr/bin/docker build:*)",
      "Bash(docker:*)",
      "Bash(timedatectl status:*)",
      "Bash(ssh:*)",
      "Bash(python -c:*)",
      "Bash(./generate_all_specialists.sh:*)",
      "Bash(pip3 uninstall:*)",
      "Bash(mkdir:*)",
      "Bash(# Verificar se já existe port-forward para MLflow pgrep -f \"\"port-forward.*mlflow.*5000\"\" && echo \"\"Port-forward já existe\"\" || { echo \"\"Iniciando port-forward para MLflow...\"\" kubectl port-forward -n mlflow svc/mlflow 5000:5000 & sleep 3 echo \"\"Port-forward iniciado\"\" } # Testar conexão curl -s http://localhost:5000/health || curl -s http://localhost:5000/api/2.0/mlflow/experiments/list)",
      "Bash(# Matar port-forward antigo e criar novo pkill -f \"\"port-forward.*mlflow.*5001\"\" || true sleep 1 # Criar novo port-forward em background nohup kubectl port-forward -n mlflow svc/mlflow 5001:5000 2>&1 & sleep 3 # Testar conexão curl -s http://localhost:5001/health && echo \"\"MLflow health OK\"\" || \\ curl -s http://localhost:5001/)",
      "Bash(# Usar kubectl exec para acessar MLflow de dentro do cluster kubectl exec -n mlflow deployment/mlflow -- curl -s http://localhost:5000/api/2.0/mlflow/experiments/list)",
      "Bash(# Verificar se curl está disponível no pod MLflow kubectl exec -n mlflow deployment/mlflow -- which curl || echo \"\"curl não disponível\"\" # Tentar com wget kubectl exec -n mlflow deployment/mlflow -- wget -qO- http://localhost:5000/api/2.0/mlflow/experiments/list 2>&1)",
      "Bash(# Copiar arquivos parquet para o pod MLflow kubectl cp /jimy/Neural-Hive-Mind/ml_pipelines/training/data/ mlflow/mlflow-6f55659b89-pdbm9:/tmp/training-data/ echo \"\"Arquivos copiados\"\" kubectl exec -n mlflow deployment/mlflow -- ls -la /tmp/training-data/)",
      "Bash(# Verificar status atual dos specialists kubectl get pods -n neural-hive -l app.kubernetes.io/component=specialist || \\ kubectl get pods -n neural-hive)",
      "Bash(git fetch:*)",
      "Bash(git remote set-url:*)",
      "Bash(ssh-add:*)",
      "Bash(git add:*)",
      "Bash(git commit -m \"$(cat <<''EOF''\nfeat: major update with ML pipelines, observability, helm charts and services improvements\n\n- Add ML training pipelines and specialist models\n- Improve observability with OpenTelemetry, Jaeger, Prometheus\n- Update all helm charts with local configurations\n- Add new services: queen-agent, temporal, docker-registry\n- Improve security with Vault/SPIFFE integration\n- Add comprehensive testing and validation scripts\n- Update all service dependencies and Dockerfiles\n- Add documentation for deployment, ML, and operations\n\n🤖 Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\nEOF\n)\")",
      "Bash(git push:*)",
      "Bash(git reset:*)",
      "Bash(# Verificar se minikube ainda está staged e removê-lo git rm --cached minikube || echo \"\"minikube não encontrado no staging\"\" # Verificar arquivos grandes restantes git diff --cached --stat)",
      "Bash(# Adicionar .gitignore atualizado\ngit add .gitignore\n\n# Fazer o commit\ngit commit -m \"\"$(cat <<''EOF''\nfeat: major update with ML pipelines, observability, helm charts and services improvements\n\n- Add ML training pipelines and specialist models\n- Improve observability with OpenTelemetry, Jaeger, Prometheus\n- Update all helm charts with local configurations\n- Add new services: queen-agent, temporal, docker-registry\n- Improve security with Vault/SPIFFE integration\n- Add comprehensive testing and validation scripts\n- Update all service dependencies and Dockerfiles\n- Add documentation for deployment, ML, and operations\n- Exclude large terraform providers and binary files\n\n🤖 Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\nEOF\n)\"\")",
      "Bash(git config:*)",
      "Bash(git rm:*)",
      "Bash(git commit:*)",
      "Bash(wc:*)",
      "Bash(fi)",
      "Bash(# Análise 2: Verificar versões de protobuf/grpc que funcionam com etcd3 pip index versions etcd3 || pip show etcd3 || echo \"\"Consultando PyPI...\"\" curl -s https://pypi.org/pypi/etcd3/json | jq -r ''.info.requires_dist[]'')",
      "Bash(# Verificar quais serviços usam etcd3 grep -r \"\"etcd3\\|etcd\"\" /jimy/Neural-Hive-Mind/services/*/src/*.py)",
      "Bash(tree:*)",
      "Bash(/tmp/build-all-services.sh:*)",
      "Bash(# Verificar quais serviços já foram publicados curl -s http://37.60.241.150:30500/v2/_catalog | jq -r ''.repositories[]'')",
      "Bash(/jimy/Neural-Hive-Mind/scripts/normalize-dependencies.sh)",
      "Bash(# Corrigir esse e outros padrões quebrados sed -i ''s/opentelemetry-instrumentation-aiohttp==3.11.11-client==0.42b0/opentelemetry-instrumentation-aiohttp-client==0.50b0/g'' /jimy/Neural-Hive-Mind/services/execution-ticket-service/requirements.txt # Limpar comentários quebrados nos protobuf for file in /jimy/Neural-Hive-Mind/services/*/requirements*.txt; do sed -i ''s/protobuf==5.29.2 5.x/protobuf 5.x/g'' \"\"$file\"\" sed -i ''s/grpcio==1.68.1 1.75.1/grpcio 1.75.1/g'' \"\"$file\"\" done echo \"\"Correções aplicadas!\"\" # Verificar grpcio novamente echo \"\"\"\" echo \"\"Versões de grpcio:\"\" grep -h \"\"^grpcio==\"\" /jimy/Neural-Hive-Mind/services/*/requirements*.txt)",
      "Bash(/tmp/build-all.sh:*)",
      "Bash(while read repo)",
      "Bash(do echo \"=== $repo ===\" curl -s \"http://37.60.241.150:30500/v2/$repo/tags/list\")",
      "Bash(pip index:*)",
      "Bash(pip3 index:*)",
      "Bash(python -m grpc_tools.protoc:*)",
      "Bash(schemas/specialist-opinion/specialist.proto)",
      "Bash(deactivate)",
      "Bash(# Ver os COPY statements problemáticos grep -n \"\"COPY services/\"\" /jimy/Neural-Hive-Mind/services/*/Dockerfile)",
      "Bash(while read dir)",
      "Bash(if [[ -f \"/jimy/Neural-Hive-Mind/base-images/$dir/Dockerfile\" ]])",
      "Bash(then)",
      "Bash(__NEW_LINE__ echo \"\")",
      "Bash(if [[ -f \"/jimy/Neural-Hive-Mind/services/$dir/Dockerfile\" ]])",
      "Bash(do if [[ -f \"/jimy/Neural-Hive-Mind/base-images/$dir/Dockerfile\" ]])",
      "Bash(then echo \"neural-hive-mind/$dir\")",
      "Bash(do if [[ -f \"/jimy/Neural-Hive-Mind/services/$dir/Dockerfile\" ]])",
      "Bash(for dir in /jimy/Neural-Hive-Mind/base-images/*/)",
      "Bash(if [ -f \"$dirDockerfile\" ])",
      "Bash(basename:*)",
      "Bash(# Extract common dependency versions from requirements files echo \"\"=== OpenTelemetry ===\"\" grep -rh \"\"opentelemetry\"\" /jimy/Neural-Hive-Mind/services/*/requirements*.txt)",
      "Bash(uniq echo -e \"\\\\n=== gRPC ===\" grep -rh \"grpcio\" /jimy/Neural-Hive-Mind/services/*/requirements*.txt)",
      "Bash(uniq echo -e \"\\\\n=== FastAPI/Uvicorn ===\" grep -rh \"fastapi\\\\|uvicorn\" /jimy/Neural-Hive-Mind/services/*/requirements*.txt)",
      "Bash(uniq echo -e \"\\\\n=== Pydantic ===\" grep -rh \"pydantic\" /jimy/Neural-Hive-Mind/services/*/requirements*.txt)",
      "Bash(uniq echo -e \"\\\\n=== Prometheus ===\" grep -rh \"prometheus\" /jimy/Neural-Hive-Mind/services/*/requirements*.txt)",
      "Bash(uniq echo -e \"\\\\n=== Structlog ===\" grep -rh \"structlog\" /jimy/Neural-Hive-Mind/services/*/requirements*.txt)",
      "Bash(docker image prune:*)",
      "Bash(docker rmi:*)",
      "Bash(c318e336065b 8f91ee4dc67c 7c6212cb1f99 4075a3f8c3f8 8d7a968b2baf )",
      "Bash(34191f25dfc1 28223f2e117a 81a348707bc6 27c0fe46e802 92ecf14cac59 )",
      "Bash(2972707bf933 de588d03dc7e 737a6bfce745 97cd6e02b70c 8374c72d5110 )",
      "Bash(93d2a2ba455c 69dedb3047fa e105cfd78d67 dab4742eee4e 89ec47deeedd )",
      "Bash(8c93ef988991 180d0ddc8fca 8ec9f084ab23)",
      "Bash(kubectl cluster-info:*)",
      "Bash(kubectl top:*)",
      "Bash(kubectl scale:*)",
      "Bash(for chart in analyst-agents gateway-intencoes semantic-translation-engine sla-management-system worker-agents)",
      "Bash(do echo \"=== $chart ===\" grep -n \"CORS_ORIGINS\\\\|ELASTICSEARCH_HOSTS\\\\|allowed_origins\\\\|allowed_hosts\\\\|kafka_topics\\\\|cluster_nodes\\\\|bootstrap_servers\\\\|supported_task_types\\\\|allowed_test_commands\" /jimy/Neural-Hive-Mind/helm-charts/$chart/templates/*.yaml)",
      "Bash(docker history:*)",
      "Bash(docker inspect:*)",
      "Bash(git log:*)",
      "Bash(pip show:*)",
      "Bash(__NEW_LINE__ echo \"=== Reconstruindo Specialists ===\")",
      "Bash(for spec in specialist-architecture specialist-behavior specialist-business specialist-evolution specialist-technical)",
      "Bash(do:*)",
      "Bash(if [ $? -eq 0 ])",
      "Bash(else)",
      "Bash(# Verificar quais helm charts têm values-local.yaml echo \"\"=== Helm charts com values-local.yaml ===\"\" ls helm-charts/*/values-local.yaml)",
      "Bash(# Verificar imagens no registry curl -s http://37.60.241.150:30500/v2/_catalog)",
      "Bash(curl -s http://37.60.241.150:30500/v2/_catalog)",
      "Bash(__NEW_LINE__ echo \"=== Deploy concluído ===\")",
      "Bash(# Verificar recursos disponíveis nos nodes kubectl top nodes || kubectl describe nodes)",
      "Bash(# Reduzir recursos em todos os values-local.yaml for file in helm-charts/*/values-local.yaml; do # Verificar se o arquivo tem section de resources if grep -q \"\"resources:\"\" \"\"$file\"\"; then # Reduzir CPU requests para 50m e limits para 200m sed -i ''s/cpu: [0-9]*m/cpu: 50m/g'' \"\"$file\"\" sed -i ''s/cpu: \"\"[0-9]*m\"\"/cpu: \"\"50m\"\"/g'' \"\"$file\"\" fi done # Verificar um exemplo grep -A 10 \"\"resources:\"\" helm-charts/consensus-engine/values-local.yaml)",
      "Bash(helm list:*)",
      "Bash(# Verificar se a função existe na biblioteca grep -r \"\"instrument_grpc_channel\"\" libraries/python/neural_hive_observability/)",
      "Bash(curl -s http://37.60.241.150:30500/v2/worker-agents/tags/list)",
      "Bash(kill:*)",
      "Bash(while ps aux)",
      "Bash(do sleep 10)",
      "Bash(kubectl rollout restart:*)",
      "Bash(curl -s http://37.60.241.150:30500/v2/python-observability-base/tags/list)",
      "Bash(for service in analyst-agents code-forge consensus-engine execution-ticket-service explainability-api gateway-intencoes guard-agents mcp-tool-catalog memory-layer-api optimizer-agents orchestrator-dynamic queen-agent scout-agents self-healing-engine semantic-translation-engine service-registry sla-management-system specialist-architecture specialist-behavior specialist-business specialist-evolution specialist-technical worker-agents)",
      "Bash(do echo \"=== $service ===\" head -3 /jimy/Neural-Hive-Mind/services/$service/Dockerfile)",
      "Bash(timeout 120 docker build:*)",
      "Bash(kubectl version:*)",
      "Bash(kubectl debug:*)",
      "Bash(timeout 300 docker build:*)",
      "Bash(kubectl taint:*)",
      "Bash(ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@37.60.241.150 \"top -bn1 | head -20\")",
      "Read(//home/jimy/.kube/**)",
      "Bash(ssh-keygen:*)",
      "Read(//home/jimy/.ssh/**)",
      "Read(//tmp/**)",
      "Bash(scp:*)",
      "Bash(du:*)",
      "Bash(helm dependency:*)",
      "Bash(helm template:*)",
      "Bash(helm version:*)",
      "Bash(tar:*)",
      "Bash(/bin/bash:*)",
      "Bash(helm search hub:*)",
      "Bash(rm:*)",
      "Bash(for chart in specialist-architecture specialist-evolution specialist-behavior)",
      "Bash(git ls-files:*)",
      "Bash(helm lint:*)",
      "Bash(for chart in code-forge execution-ticket-service mcp-tool-catalog memory-layer-api service-registry)",
      "Bash(for chart in otel-collector self-healing-engine semantic-translation-engine sla-management-system sigstore-policy-controller)",
      "Bash(rsync:*)",
      "Bash(for:*)",
      "Bash(PYTHONPATH=\"libraries/neural_hive_integration:libraries/python/neural_hive_observability:$PYTHONPATH\" python3:*)",
      "Bash(libraries/python/neural_hive_specialists/config.py )",
      "Bash(libraries/python/neural_hive_specialists/base_specialist.py )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/)",
      "Bash(services/specialist-business/src/http_server_fastapi.py )",
      "Bash(services/specialist-architecture/src/http_server_fastapi.py )",
      "Bash(services/specialist-behavior/src/http_server_fastapi.py )",
      "Bash(services/specialist-evolution/src/http_server_fastapi.py )",
      "Bash(services/specialist-technical/src/http_server_fastapi.py )",
      "Bash(helm-charts/specialist-business/values.yaml )",
      "Bash(helm-charts/specialist-architecture/values.yaml )",
      "Bash(helm-charts/specialist-behavior/values.yaml )",
      "Bash(helm-charts/specialist-evolution/values.yaml )",
      "Bash(helm-charts/specialist-technical/values.yaml )",
      "Bash(helm-charts/specialist-business/templates/deployment.yaml )",
      "Bash(helm-charts/specialist-architecture/templates/deployment.yaml )",
      "Bash(helm-charts/specialist-behavior/templates/deployment.yaml )",
      "Bash(helm-charts/specialist-evolution/templates/deployment.yaml )",
      "Bash(helm-charts/specialist-technical/templates/deployment.yaml )",
      "Bash(helm-charts/specialist-business/values-local.yaml )",
      "Bash(helm-charts/specialist-technical/values-local.yaml )",
      "Bash(helm-charts/specialist-behavior/values-local.yaml )",
      "Bash(helm-charts/specialist-evolution/values-local.yaml )",
      "Bash(helm-charts/specialist-architecture/values-local.yaml )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/helm-charts/)",
      "Bash(/usr/local/bin/kubectl logs:*)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/services/guard-agents/ )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/services/scout-agents/ )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/services/mcp-tool-catalog/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/)",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/guard-agents/)",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/scout-agents/)",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/mcp-tool-catalog/)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/libraries/neural_hive_integration/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/libraries/neural_hive_integration/)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/guard-agents/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/helm-charts/guard-agents/)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/scout-agents/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/helm-charts/scout-agents/)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/mcp-tool-catalog/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/helm-charts/mcp-tool-catalog/)",
      "Bash(PYTHONPATH=. python3 -m pytest:*)",
      "Bash(kubectl config current-context:*)",
      "Bash(/usr/local/bin/kubectl get pods -n neural-hive)",
      "Bash(/usr/local/bin/kubectl get pods -n observability)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/schemas/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/schemas/)",
      "Bash(test:*)",
      "Bash(KUBECONFIG=~/.kube/neural-hive-config kubectl get:*)",
      "Bash(kubectl config get-contexts:*)",
      "Bash(kubectl config use-context:*)",
      "Bash(git status:*)",
      "Bash(services/code-forge/ )",
      "Bash(services/consensus-engine/ )",
      "Bash(services/gateway-intencoes/ )",
      "Bash(services/guard-agents/ )",
      "Bash(services/memory-layer-api/ )",
      "Bash(services/optimizer-agents/ )",
      "Bash(services/orchestrator-dynamic/ )",
      "Bash(services/queen-agent/ )",
      "Bash(services/scout-agents/ )",
      "Bash(services/self-healing-engine/ )",
      "Bash(services/semantic-translation-engine/ )",
      "Bash(services/service-registry/ )",
      "Bash(services/sla-management-system/ )",
      "Bash(services/worker-agents/ )",
      "Bash(libraries/ )",
      "Bash(schemas/ )",
      "Bash(apiVersion: v1)",
      "Bash(kind: ConfigMap)",
      "Bash(metadata:)",
      "Bash(name: coredns)",
      "Bash(namespace: kube-system)",
      "Bash(data:)",
      "Bash(Corefile:)",
      "Bash(.:53 {)",
      "Bash(errors)",
      "Bash(health {)",
      "Bash(lameduck:*)",
      "Bash(})",
      "Bash(ready)",
      "Bash(kubernetes cluster.local in-addr.arpa ip6.arpa {)",
      "Bash(pods insecure:*)",
      "Bash(fallthrough in-addr.arpa ip6.arpa)",
      "Bash(ttl:*)",
      "Bash(prometheus :9153)",
      "Bash(forward . /etc/resolv.conf {)",
      "Bash(max_concurrent 1000)",
      "Bash(cache 30:*)",
      "Bash(loop)",
      "Bash(reload)",
      "Bash(loadbalance)",
      "Bash(EOF)",
      "Bash(kind: Service)",
      "Bash(name: opentelemetry-collector)",
      "Bash(namespace: observability)",
      "Bash(spec:)",
      "Bash(type: ClusterIP)",
      "Bash(ports:)",
      "Bash(port: 4317)",
      "Bash(targetPort: 4317)",
      "Bash(port: 4318)",
      "Bash(targetPort: 4318)",
      "Bash(port: 14250)",
      "Bash(targetPort: 14250)",
      "Bash(selector:)",
      "Bash(app.kubernetes.io/instance: otel-collector)",
      "Bash(app.kubernetes.io/name: opentelemetry-collector)",
      "Bash(kind: ServiceAccount)",
      "Bash(name: otel-collector-neural-hive-otel-collector)",
      "Bash(if [ -f \"helm-charts/$chart/values-local.yaml\" ])",
      "Bash(name: sla-management-system-config)",
      "Bash(namespace: neural-hive)",
      "Bash(labels:)",
      "Bash(app.kubernetes.io/component: sla-management-system)",
      "Bash(app.kubernetes.io/instance: sla-management-system)",
      "Bash(app.kubernetes.io/name: sla-management-system)",
      "Bash(app.kubernetes.io/part-of: neural-hive-mind)",
      "Bash(neural-hive.io/component: sla-management-system)",
      "Bash(neural-hive.io/domain: sla-management)",
      "Bash(neural-hive.io/layer: monitoring)",
      "Bash(alertmanager_url: \"http://alertmanager.monitoring.svc.cluster.local:9093\")",
      "Bash(alertmanager_webhook_path: \"/webhooks/alertmanager\")",
      "Bash(calculator_burn_rate_fast_threshold: \"14.4\")",
      "Bash(calculator_burn_rate_slow_threshold: \"6\")",
      "Bash(calculator_interval_seconds: \"30\")",
      "Bash(calculator_window_days: \"30\")",
      "Bash(environment: \"production\")",
      "Bash(kafka_bootstrap_servers: \"kafka-bootstrap.kafka.svc.cluster.local:9092\")",
      "Bash(kafka_budget_topic: \"sla.budgets\")",
      "Bash(kafka_freeze_topic: \"sla.freeze.events\")",
      "Bash(kafka_violations_topic: \"sla.violations\")",
      "Bash(log_level: \"INFO\")",
      "Bash(policy_auto_unfreeze_enabled: \"true\")",
      "Bash(policy_freeze_threshold_percent: \"20\")",
      "Bash(policy_unfreeze_threshold_percent: \"50\")",
      "Bash(postgresql_database: \"sla_management\")",
      "Bash(postgresql_host: \"postgres-sla.neural-hive-data.svc.cluster.local\")",
      "Bash(postgresql_pool_max_size: \"10\")",
      "Bash(postgresql_pool_min_size: \"2\")",
      "Bash(postgresql_port: \"5432\")",
      "Bash(prometheus_max_retries: \"3\")",
      "Bash(prometheus_timeout_seconds: \"30\")",
      "Bash(prometheus_url: \"http://prometheus-server.monitoring.svc.cluster.local:9090\")",
      "Bash(redis_cache_ttl_seconds: \"60\")",
      "Bash(redis_cluster_nodes: \"redis-cluster.redis-cluster.svc.cluster.local:6379\")",
      "Bash(service_name: \"sla-management-system\")",
      "Bash(version: \"1.0.0\")",
      "Bash(kind: Secret)",
      "Bash(name: execution-ticket-service-secrets)",
      "Bash(type: Opaque)",
      "Bash(stringData:)",
      "Bash(MONGODB_URI: \"mongodb://mongodb.mongodb-cluster.svc.cluster.local:27017/neural_hive\")",
      "Bash(KAFKA_SASL_PASSWORD: \"\")",
      "Bash(LLM_API_KEY: \"\")",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/analyst-agents/templates/hpa.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/analyst-agents/templates/poddisruptionbudget.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/analyst-agents/templates/networkpolicy.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/analyst-agents/templates/serviceaccount.yaml)",
      "Bash(__NEW_LINE__ rm /home/jimy/NHM/Neural-Hive-Mind/helm-charts/optimizer-agents/templates/servicemonitor.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/optimizer-agents/templates/hpa.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/optimizer-agents/templates/poddisruptionbudget.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/optimizer-agents/templates/networkpolicy.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/optimizer-agents/templates/serviceaccount.yaml)",
      "Bash(__NEW_LINE__ rm /home/jimy/NHM/Neural-Hive-Mind/helm-charts/explainability-api/templates/servicemonitor.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/explainability-api/templates/hpa.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/explainability-api/templates/poddisruptionbudget.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/explainability-api/templates/networkpolicy.yaml )",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/helm-charts/explainability-api/templates/serviceaccount.yaml)",
      "Bash(__NEW_LINE__ echo \"Templates removidos\")",
      "Bash(POSTGRES_PASSWORD: \"neural_hive_password\")",
      "Bash(JWT_SECRET_KEY: \"neural-hive-jwt-secret-key-2024\")",
      "Bash(kind: PersistentVolumeClaim)",
      "Bash(name: postgres-sla-data)",
      "Bash(namespace: neural-hive-data)",
      "Bash(accessModes:)",
      "Bash(resources:*)",
      "Bash(requests:)",
      "Bash(storage: 5Gi)",
      "Bash(storageClassName: local-path)",
      "Bash(name: postgres-sla-credentials)",
      "Bash(POSTGRES_USER: \"sla_user\")",
      "Bash(POSTGRES_PASSWORD: \"neural_hive_sla_2024\")",
      "Bash(POSTGRES_DB: \"sla_management\")",
      "Bash(apiVersion: apps/v1)",
      "Bash(kind: Deployment)",
      "Bash(name: postgres-sla)",
      "Bash(app: postgres-sla)",
      "Bash(replicas: 1)",
      "Bash(matchLabels:)",
      "Bash(template:)",
      "Bash(containers:)",
      "Bash(image: postgres:15-alpine)",
      "Bash(envFrom:)",
      "Bash(volumeMounts:)",
      "Bash(mountPath: /var/lib/postgresql/data)",
      "Bash(cpu: 100m)",
      "Bash(memory: 256Mi)",
      "Bash(limits:*)",
      "Bash(cpu: 500m)",
      "Bash(memory: 512Mi)",
      "Bash(livenessProbe:)",
      "Bash(exec:)",
      "Bash(command:)",
      "Bash(initialDelaySeconds: 30)",
      "Bash(periodSeconds: 10)",
      "Bash(readinessProbe:)",
      "Bash(initialDelaySeconds: 5)",
      "Bash(periodSeconds: 5)",
      "Bash(volumes:)",
      "Bash(persistentVolumeClaim:)",
      "Bash(claimName: postgres-sla-data)",
      "Bash(targetPort: 5432)",
      "Bash(name: sla-management-system-secret)",
      "Bash(POSTGRESQL__USER: \"sla_user\")",
      "Bash(POSTGRESQL__PASSWORD: \"neural_hive_sla_2024\")",
      "Bash(REDIS__PASSWORD: \"\")",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/services/analyst-agents/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/analyst-agents/)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/services/optimizer-agents/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/optimizer-agents/)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/services/explainability-api/ )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/explainability-api/)",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/analyst-agents/Dockerfile)",
      "Bash(# Aguardar 10 segundos para os pods reiniciarem sleep 10 kubectl get pods -n neural-hive -o wide)",
      "Bash(services/analyst-agents/ )",
      "Bash(services/explainability-api/ )",
      "Bash(services/mcp-tool-catalog/ )",
      "Bash(services/execution-ticket-service/ )",
      "Bash(./scripts/build.sh:*)",
      "Bash(# Matar processos antigos parados pkill -f \"\"docker push 37.60.241.150:30500/analyst-agents:latest\"\" || true # Verificar imagens já pushadas echo \"\"=== Imagens no Registry ===\"\" for service in code-forge memory-layer-api analyst-agents explainability-api optimizer-agents mcp-tool-catalog execution-ticket-service; do echo -n \"\"$service: \"\" curl -s http://37.60.241.150:30500/v2/$service/tags/list)",
      "Bash(# Matar processo de deploy anterior pkill -f \"\"helm upgrade\"\" || true # Verificar status atual dos pods kubectl get pods -n neural-hive)",
      "Bash(tee:*)",
      "Bash(/home/jimy/NHM/Neural-Hive-Mind/services/execution-ticket-service/Dockerfile )",
      "Bash(root@37.60.241.150:/root/Neural-Hive-Mind/services/execution-ticket-service/)",
      "Bash(# Aguardar 30 segundos para os pods iniciarem sleep 30 kubectl get pods -n neural-hive -o wide)",
      "Bash(# Aguardar 20 segundos para os novos pods serem criados sleep 20 kubectl get pods -n neural-hive)",
      "Bash(# Verificar quais serviços usam pydantic-settings grep -l \"\"pydantic-settings\\|pydantic_settings\"\" /home/jimy/NHM/Neural-Hive-Mind/services/*/requirements.txt)",
      "Bash(# Aguardar 30 segundos para os pods reiniciarem sleep 30 kubectl get pods -n neural-hive)",
      "Bash(# Testar o template após atualizar dependências helm template mcp-tool-catalog /home/jimy/NHM/Neural-Hive-Mind/helm-charts/mcp-tool-catalog --set enableServiceLinks=false)",
      "Bash(kubectl create:*)",
      "Bash(apiVersion: \"clickhouse.altinity.com/v1\")",
      "Bash(kind: \"ClickHouseInstallation\")",
      "Bash(name: \"neural-hive\")",
      "Bash(namespace: \"clickhouse\")",
      "Bash(defaults:)",
      "Bash(templates:)",
      "Bash(dataVolumeClaimTemplate: data-volume)",
      "Bash(serviceTemplate: svc-template)",
      "Bash(configuration:)",
      "Bash(users:)",
      "Bash(default/password: \"\")",
      "Bash(default/networks/ip: \"::/0\")",
      "Bash(clusters:)",
      "Bash(layout:)",
      "Bash(shardsCount: 1)",
      "Bash(replicasCount: 1)",
      "Bash(zookeeper:)",
      "Bash(nodes:)",
      "Bash(port: 2181)",
      "Bash(serviceTemplates:)",
      "Bash(generateName: clickhouse-{chi})",
      "Bash(port: 8123)",
      "Bash(port: 9000)",
      "Bash(volumeClaimTemplates:)",
      "Bash(resources:)",
      "Bash(storage: 10Gi)",
      "Bash(name: \"clickhouse\")",
      "Bash(podTemplates:)",
      "Bash(image: clickhouse/clickhouse-server:23.8)",
      "Bash(limits:)",
      "Bash(memory: \"1Gi\")",
      "Bash(podTemplate: pod-template)",
      "Bash(dataVolumeClaimTemplate: data-volume-template)",
      "Bash(kubectl auth can-i:*)",
      "Bash(name: clickhouse-data)",
      "Bash(namespace: clickhouse)",
      "Bash(kind: StatefulSet)",
      "Bash(name: clickhouse)",
      "Bash(serviceName: clickhouse)",
      "Bash(app: clickhouse)",
      "Bash(name: http)",
      "Bash(name: native)",
      "Bash(mountPath: /var/lib/clickhouse)",
      "Bash(claimName: clickhouse-data)",
      "Bash(targetPort: 8123)",
      "Bash(targetPort: 9000)",
      "Bash(# optimizer-agents: verificar values-local.yaml cat /home/jimy/NHM/Neural-Hive-Mind/helm-charts/optimizer-agents/values-local.yaml || cat /home/jimy/NHM/Neural-Hive-Mind/helm-charts/optimizer-agents/values.yaml)",
      "Bash(# self-healing-engine: verificar deployment e imagem kubectl get deployment self-healing-engine -n neural-hive -o jsonpath=''{.spec.template.spec.containers[0].image}'' echo \"\"\"\" kubectl describe pod -l app.kubernetes.io/name=self-healing-engine -n neural-hive)",
      "Bash(# Verificar helm values do execution-ticket-service para PostgreSQL config cat /home/jimy/NHM/Neural-Hive-Mind/helm-charts/execution-ticket-service/values-local.yaml)",
      "Bash(apiVersion: networking.k8s.io/v1)",
      "Bash(kind: Ingress)",
      "Bash(name: neural-hive-gateway)",
      "Bash(annotations:)",
      "Bash(cert-manager.io/cluster-issuer: \"letsencrypt-prod\")",
      "Bash(nginx.ingress.kubernetes.io/ssl-redirect: \"true\")",
      "Bash(nginx.ingress.kubernetes.io/proxy-body-size: \"50m\")",
      "Bash(nginx.ingress.kubernetes.io/proxy-read-timeout: \"300\")",
      "Bash(nginx.ingress.kubernetes.io/proxy-send-timeout: \"300\")",
      "Bash(ingressClassName: nginx)",
      "Bash(tls:)",
      "Bash(secretName: neural-hive-gateway-tls)",
      "Bash(rules:)",
      "Bash(http:)",
      "Bash(paths:)",
      "Bash(pathType: Prefix)",
      "Bash(backend:)",
      "Bash(service:)",
      "Bash(name: gateway-intencoes)",
      "Bash(port:)",
      "Bash(number: 8080)",
      "Bash(# Delete old CrashLoopBackOff pods and old optimizer-agents pods kubectl delete pod -n neural-hive analyst-agents-789f65f758-xf56l code-forge-6bdc5c57fd-rljwq optimizer-agents-77f67b6cb9-jqfsm || true # Wait a moment and check status sleep 5 kubectl get pods -n neural-hive)",
      "Bash(# Delete CrashLoopBackOff pods kubectl delete pod -n neural-hive analyst-agents-789f65f758-x52ln code-forge-6bdc5c57fd-bq67w code-forge-76667db75-4d2lj || true # Delete pending cronjob pods kubectl delete pod -n neural-hive memory-layer-api-data-quality-check-29457120-5rtrk memory-layer-api-enforce-retention-29456760-gwtz4 memory-layer-api-sync-mongodb-clickhouse-29457000-szj6b || true # Scale down duplicate self-healing-engine pods (keep only 1) kubectl scale deployment self-healing-engine -n neural-hive --replicas=1 || true # Delete one of the optimizer-agents kubectl delete pod -n neural-hive optimizer-agents-77f67b6cb9-mf9gp || true sleep 3 kubectl get pods -n neural-hive)",
      "Bash(# Delete pending cronjob pods kubectl delete pod -n neural-hive memory-layer-api-data-quality-check-29457120-mcr2f memory-layer-api-enforce-retention-29456760-h9mtn memory-layer-api-sync-mongodb-clickhouse-29457000-zvt4z --grace-period=0 --force || true # Scale down duplicate deployments kubectl scale deployment self-healing-engine -n neural-hive --replicas=1 || true # Check running pods'' memory usage kubectl top pods -n neural-hive --sort-by=memory)",
      "Bash(# Delete extra self-healing-engine pods (should only have 1) kubectl delete pod -n neural-hive self-healing-engine-86895596fc-hxh89 self-healing-engine-86895596fc-6c4bd --grace-period=0 --force || true # Scale optimizer-agents to 1 kubectl scale deployment optimizer-agents -n neural-hive --replicas=1 || true # Delete old ReplicaSets for code-forge and analyst-agents kubectl get rs -n neural-hive)",
      "Bash(# Delete old crashing pods kubectl delete pod -n neural-hive analyst-agents-6d6d4d89c9-cj78r code-forge-886b8f586-vsrjl --grace-period=0 --force || true # Restart sla-operator to use new image kubectl rollout restart deployment sla-management-system-operator -n neural-hive sleep 15 && kubectl get pods -n neural-hive)",
      "Bash(# Delete crashing pods kubectl delete pod -n neural-hive analyst-agents-6d6d4d89c9-dmwpn code-forge-886b8f586-nrnjz --grace-period=0 --force || true # Scale to just 1 replica kubectl scale deployment analyst-agents -n neural-hive --replicas=1 || true # Check remaining ReplicaSets kubectl get rs -n neural-hive)",
      "Bash(# Delete old ReplicaSets completely kubectl delete rs -n neural-hive analyst-agents-6d6d4d89c9 code-forge-886b8f586 --ignore-not-found sleep 5 # Check current status kubectl get pods -n neural-hive)",
      "Bash(# Delete pending cronjob pods kubectl delete pod -n neural-hive memory-layer-api-data-quality-check-29457120-2l5vh memory-layer-api-enforce-retention-29456760-94qqs memory-layer-api-sync-mongodb-clickhouse-29457000-v9jcv --grace-period=0 --force || true # Scale down optimizer-agents entirely kubectl scale deployment optimizer-agents -n neural-hive --replicas=0 # Delete extra self-healing-engine pods kubectl get pods -n neural-hive)",
      "Bash(# Check the CMD in Dockerfile and how the app starts head -60 /home/jimy/NHM/Neural-Hive-Mind/services/analyst-agents/src/main.py)",
      "Bash(# Check cluster memory usage kubectl top nodes || echo \"\"metrics-server not available\"\" kubectl get pods -n neural-hive)",
      "Bash(# Delete old analyst-agents pods and check events kubectl delete pod -n neural-hive analyst-agents-6b8647c945-vvl4g --force --grace-period=0 || true kubectl get events -n neural-hive --sort-by=''.lastTimestamp'')",
      "Bash(# Check pod memory usage sorted by memory kubectl top pods -n neural-hive --no-headers)",
      "Bash(# Delete cronjob pods kubectl delete pod -n neural-hive memory-layer-api-data-quality-check-29457120-5z2lk memory-layer-api-enforce-retention-29456760-k2ngl memory-layer-api-sync-mongodb-clickhouse-29457000-pnlpw --force --grace-period=0 # Wait for terminating pods and check again sleep 15 kubectl get pods -n neural-hive)",
      "Bash(# Delete cronjob pods and scale self-healing-engine to 1 kubectl delete pod -n neural-hive memory-layer-api-data-quality-check-29457360-g7zq4 memory-layer-api-sync-mongodb-clickhouse-29457360-b7hd5 --force --grace-period=0 # Scale to exactly 1 replica kubectl scale deployment self-healing-engine -n neural-hive --replicas=1 # Delete all but one self-healing-engine ReplicaSet kubectl delete rs -n neural-hive self-healing-engine-b65c66887 sleep 10 kubectl get pods -n neural-hive)",
      "Bash(# Check memory usage on nodes kubectl top nodes # Check total allocated memory requests kubectl get pods -n neural-hive -o jsonpath=''{range .items[*]}{.spec.containers[0].resources.requests.memory}{\"\"\\n\"\"}{end}'')",
      "Bash(awk:*)",
      "Bash(xargs -I{} sh -c 'echo \"\"=== {} ===\"\" && head -30 {}')",
      "Bash(docker info:*)",
      "Bash(for script in scripts/build/*.sh scripts/lib/*.sh)",
      "Bash(do if bash -n \"$script\")",
      "Bash(for spec in specialist-business specialist-technical specialist-architecture)",
      "Bash(do kubectl exec -n neural-hive deploy/consensus-engine -- python3 -c \"import socket; s=socket.socket(); s.settimeout(3); s.connect((''$spec.neural-hive.svc.cluster.local'', 50051)); print(''✓ $spec''); s.close()\")",
      "Bash(kubectl rollout status:*)",
      "Bash(helm history:*)",
      "Bash(helm rollback:*)",
      "Bash(kubeadm token create:*)",
      "Bash(ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no root@89.117.60.74 \"hostname && cat /etc/os-release | grep PRETTY_NAME\")",
      "Bash(ssh root@37.60.241.150 \"ssh-copy-id -o StrictHostKeyChecking=no root@89.117.60.74\")",
      "Bash(ssh root@37.60.241.150 \"cat /root/.ssh/id_rsa.pub\")",
      "Bash(ssh root@37.60.241.150 \"ssh -o StrictHostKeyChecking=no root@89.117.60.74 ''hostname && cat /etc/os-release | grep PRETTY_NAME''\")",
      "Bash(ssh root@37.60.241.150 \"ssh root@89.117.60.74 ''swapoff -a && sed -i \"\"/ swap / s/^/#/\"\" /etc/fstab && cat <<EOF | tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\nmodprobe overlay && modprobe br_netfilter && cat <<EOF | tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.ipv4.ip_forward                 = 1\nEOF\nsysctl --system''\")",
      "Bash(ssh root@37.60.241.150 \"ssh root@89.117.60.74 ''apt-get update && apt-get install -y containerd''\")",
      "Bash(ssh root@37.60.241.150 \"ssh root@89.117.60.74 ''mkdir -p /etc/containerd && containerd config default > /etc/containerd/config.toml && sed -i \"\"s/SystemdCgroup = false/SystemdCgroup = true/\"\" /etc/containerd/config.toml && systemctl restart containerd && systemctl enable containerd''\")",
      "Bash(ssh root@37.60.241.150 \"ssh root@89.117.60.74 ''apt-get install -y apt-transport-https ca-certificates curl gpg && mkdir -p /etc/apt/keyrings && curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg && echo \"\"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /\"\" | tee /etc/apt/sources.list.d/kubernetes.list''\")",
      "Bash(ssh root@37.60.241.150 \"ssh root@89.117.60.74 ''apt-get update && apt-get install -y kubelet kubeadm kubectl && apt-mark hold kubelet kubeadm kubectl''\")",
      "Bash(ssh root@37.60.241.150 \"ssh root@89.117.60.74 ''kubeadm join 37.60.241.150:6443 --token 7iwcu9.c5y90nfpx4ogz29e --discovery-token-ca-cert-hash sha256:aa809c60d71383b75447b3573683af29c57643feb8862bc5cfe1e1853bfd6562''\")",
      "Bash(kubectl wait:*)",
      "Bash(./scripts/validate-image-tags.sh:*)",
      "Bash(for service in /home/jimy/NHM/Neural-Hive-Mind/services/specialist-*/src)",
      "Bash(REGISTRY_URL=\"37.60.241.150:30500\" DOCKER_BUILDKIT=1 docker build:*)",
      "Bash(ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 root@37.60.241.150 \"docker --version\")",
      "Bash(ssh root@37.60.241.150 \"docker images | grep python-mlops-base\")",
      "Bash(bash scripts/validation/validate-service-registry.sh:*)",
      "Bash(python3 -m grpc_tools.protoc:*)",
      "Bash(python -m pytest:*)",
      "Bash(python3 -m pytest:*)",
      "Bash(services/analyst-agents/src/observability/__init__.py )",
      "Bash(services/analyst-agents/src/observability/tracing.py )",
      "Bash(services/optimizer-agents/src/observability/tracing.py )",
      "Bash(services/execution-ticket-service/src/observability/__init__.py )",
      "Bash(services/execution-ticket-service/src/observability/tracing.py )",
      "Bash(services/queen-agent/src/observability/__init__.py )",
      "Bash(services/queen-agent/src/observability/tracing.py )",
      "Bash(services/scout-agents/src/main.py )",
      "Bash(services/consensus-engine/src/clients/specialists_grpc_client.py )",
      "Bash(CHANGELOG.md)",
      "Bash(git push)",
      "Bash(python:*)",
      "Bash(pip3 list:*)",
      "Bash(PYTHONPATH=\"src:$PYTHONPATH\" python3 -m pytest:*)",
      "Bash(pytest:*)",
      "Bash(PYTHONPATH=\"${PYTHONPATH}:libraries/python/neural_hive_specialists:libraries/python/neural_hive_observability\" pytest:*)",
      "Bash(PYTHONPATH=\".:../../libraries/python/neural_hive_specialists:../../libraries/python/neural_hive_observability\" pytest:*)",
      "Bash(python -m py_compile:*)",
      "Bash(python3 -m py_compile:*)",
      "Bash(claude /doctor:*)"
    ],
    "deny": [],
    "ask": []
  }
}
